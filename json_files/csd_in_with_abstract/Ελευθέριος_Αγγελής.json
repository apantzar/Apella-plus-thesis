[{
    "name": "\u0395\u03bb\u03b5\u03c5\u03b8\u03ad\u03c1\u03b9\u03bf\u03c2 \u0391\u03b3\u03b3\u03b5\u03bb\u03ae\u03c2",
    "romanize name": "Eleftherios Angelis",
    "School-Department": "\u03a0\u03bb\u03b7\u03c1\u03bf\u03c6\u03bf\u03c1\u03b9\u03ba\u03ae\u03c2",
    "University": "\u0391\u03a0\u0398",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 18588,
    "Scholar name": "Lefteris (Eleftherios) Angelis",
    "Scholar id": "IwB4XFYAAAAJ",
    "Affiliation": "Professor of Statistics and Information Systems, Aristotle University of Thessaloniki",
    "Citedby": 5916,
    "Interests": [
        "Empirical Software Engineering",
        "Statistics",
        "Information Systems"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=IwB4XFYAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Benchmarking effort estimation models using archetypal analysis",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2639490.2639502",
            "Abstract": "The research on software cost estimation has resulted not only to a large number of prediction methodologies and improvement techniques, but also to numerous methods for evaluating and comparing them. The identification of the best prediction model for a specific dataset is still an open issue since the evaluation of candidate models is essentially a multi-criteria problem. Model comparison usually involves statistical hypothesis tests with respect to a single criterion, while for multiple criteria, aggregating methods are usually employed. In the current study, we investigate the alternative approach of benchmarking, which is different from model comparison. The general idea is first to choose among the competitors few\" reference models\" with special, preferably divergent performance characteristics with respect to multiple criteria and then to examine the placement of all the other models in relation to the reference \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:JoZmwDi-zQgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An evolutionary algorithm for A-optimal incomplete block designs",
            "Publication year": 2003,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/0094965031000097151",
            "Abstract": "Evolutionary algorithms are heuristic stochastic search and optimization techniques with principles taken from natural genetics. They are procedures mimicking the evolution process of an initial population through genetic transformations. This paper is concerned with the problem of finding A-optimal incomplete block designs for multiple treatment comparisons represented by a matrix of contrasts. An evolutionary algorithm for searching optimal, or nearly optimal, incomplete block designs is described in detail. Various examples regarding the application of the algorithm to some well-known problems illustrate the good performance of the algorithm",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:4DMP91E08xMC",
            "Publisher": "Taylor & Francis Group"
        },
        {
            "Title": "A prototype system for educational data warehousing and mining",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4621562/",
            "Abstract": "Universities are encountering growing demands by legislators and communities who are clamoring for valuable information about student achievement and university system accountability. Not only are universities required to measure annual progress for every single student, but government (through ministries for education) aid is directly linked to these results. The department of Informatics of Aristotle university of Thessaloniki has developed a data warehouse solution that assists the analysis of educational data. In this paper we present the design and development of the proposed data warehouse solution, which facilitates better and more thorough analysis of departmentpsilas data. The proposed system constitutes an integrated platform for a thorough analysis of departmentpsilas past data. Analysis of data could be achieved with OLAP operations. Moreover, we propose a thorough statistical analysis with an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:j3f4tGmQtD8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "StatREC: A graphical user interface tool for visual hypothesis testing of cost prediction models",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2365324.2365331",
            "Abstract": "Background: During the previous decades there has been noted a significantly increased research interest on the construction of prediction models for accurate estimation of software cost. Despite the development of sophisticated methodologies, there is a continuous debate concerning the divergent and controversial conclusions of the related literature. Nowadays, due to this fact, the research community attempts to systematically base the whole comparison and evaluation process on formal frameworks and structured guidelines in concordance with modern statistical practices and methodologies, so as to resolve the problem of inconsistent findings.Aims: Towards this direction, we present StatREC, a Graphical User Interface, which facilitates the visualization and hypothesis testing of error distributions through their graphical representation as REC curves.Conclusions: The advantage of StatREC is that it provides \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:XiSMed-E-HIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Survival analysis for the duration of software projects",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1509283/",
            "Abstract": "In the area of software engineering various methods have been proposed in order to predict the cost of a software project in terms of the effort or of the productivity. An important feature which is closely related to the cost is the duration of a software project. In this paper we deal with the problem of studying and modeling the distribution of the time from specification until delivery of a software product. Specifically, we investigate the use of a statistical methodology known from biostatistics as survival analysis. The purpose of such an analysis is to describe the distribution of the duration and also to identify important factors that affect it. The great advantage of survival analysis is that we can utilize information not only from the completed projects in a dataset but also from ongoing projects. The general principles of the methodology are described with examples from applications to known data sets",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:L8Ckcad2t8MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Code quality analysis in open source software development",
            "Publication year": 2002,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1046/j.1365-2575.2002.00117.x",
            "Abstract": " Proponents of open source style software development claim that better software is produced using this model compared with the traditional closed model. However, there is little empirical evidence in support of these claims. In this paper, we present the results of a pilot case study aiming: (a) to understand the implications of structural quality; and (b) to figure out the benefits of structural quality analysis of the code delivered by open source style development. To this end, we have measured quality characteristics of 100 applications written for Linux, using a software measurement tool, and compared the results with the industrial standard that is proposed by the tool. Another target of this case study was to investigate the issue of modularity in open source as this characteristic is being considered crucial by the proponents of open source for this type of software development. We have empirically assessed the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:u5HHmVD_uO8C",
            "Publisher": "Blackwell Science Ltd"
        },
        {
            "Title": "Integrating user-centered design practices into agile Web development: A case study",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7785424/",
            "Abstract": "In this study we empirically investigate the integration of User-Centered Design (UCD) practices into agile Web development. We propose a hybrid process framework which extends the agile development process, adding lightweight usability engineering practices. We conducted a case study in academia with postgraduate students to empirically evaluate the impact of the hybrid process to the quality of the web products, based on a defect quality analysis through the development phases from user stories gathering up to delivery of the products. The statistical analysis of the quantitative data showed a great improvement in the distribution of the closing-defects across the development phases and full reduction of the defects number, leading to increased quality.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:0KyAp5RtaNEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Prediction Model for the Result of Percutaneous Coronary Intervention in Coronary Chronic Total Occlusions",
            "Publication year": 0,
            "Publication url": "https://avestia.com/ICSTA2021_Proceedings/files/paper/ICSTA_129.pdf",
            "Abstract": "Coronary chronic total occlusions (CTOs) are very common in patients undergoing coronary angiography. There has been an increasing acceptance of the percutaneous coronary interventions (PCI) in CTOs. The success rate of PCI has been boosted over the last few years by, among else, operator experience and advances in technology, even achieving levels of approximately 90%. This study proposes a prediction model for the classification of the cases in successful and unsuccessful operations and addresses the problem of class imbalance in the response variable (operation result). It is based on the EuroCTO Registry, which is the largest database available worldwide consisting of 29,995 cases for the period 2008-2018. Binary logistic regression analysis and down-sampling were applied within a customized step-algorithm and standard statistical accuracy measures were employed for the assessment of the prediction model, such as sensitivity, specificity and the value of the area under the ROC (AUROC) curve. The analysis revealed new predictive factors, validating at the same time the impact of well-known predictors. A brief comparison has been performed with other models from the literature, which showed that the proposed model performs similarly or better than its contemporary competitors.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:HtS1dXgVpQUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Desmoglein-3/\u03b3-catenin and E-cadherin/\u00df-catenin differential expression in oral leukoplakia and squamous cell carcinoma",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/article/10.1007/s00784-013-0937-z",
            "Abstract": "The purpose of this study was to investigate gene/protein expression alterations of intercellular connections\u2019 components in oral leukoplakia (OLs) and squamous-cell carcinoma (OSCCs).Expression of desmogleins-2,3 (Dsg2/Dsg3), E-cadherin, and their cytoplasmic ligand, \u03b2/\u03b3-catenins were quantitatively assessed in HSC-3 cells growing as monolayer cultures (ML)/multicellular aggregates (MCAs), using RT-PCR/Western blot, whereas their localization was detected by immunofluorescence. Furthermore, their expression was semi-quantitatively investigated in tissues from 25 OLs/25 OSCCs, using automated immunohistochemistry.The steady-state levels of Dsg3 RNA transcripts increased as HSC-3 cells enter their exponential phase of growth, before a dramatic decrease to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:vRqMK49ujn8C",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Towards an affordable brain computer interface for the assessment of programmers\u2019 mental workload",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1071581918300934",
            "Abstract": "This paper provides a proof of concept for the use of wearable technology, and specifically wearable Electroencephalography (EEG), in the field of Empirical Software Engineering. Particularly, we investigated the brain activity of Software Engineers (SEngs) while performing two distinct but related mental tasks: understanding and inspecting code for syntax errors. By comparing the emerging EEG patterns of activity and neural synchrony, we identified brain signatures that are specific to code comprehension. Moreover, using the programmer's rating about the difficulty of each code snippet shown, we identified neural correlates of subjective difficulty during code comprehension. Finally, we attempted to build a model of subjective difficulty based on the recorded brainwave patterns. The reported results show promise towards novel alternatives to programmers\u2019 training and education. Findings of this kind may \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:kuK5TVdYjLIC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "The developer's dilemma: factors affecting the decision to repay code debt",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3194164.3194174",
            "Abstract": "The set of concepts collectively known as Technical Debt (TD) assume that software liabilities set up a context that can make a future change more costly or impossible; and therefore repaying the debt should be pursued. However, software developers often disagree with an automatically generated list of improvement suggestions, which they consider not fitting or important for their own code. To shed light into the reasons that drive developers to adopt or reject refactoring opportunities (ie TD repayment), we have performed an empirical study on the potential factors that affect the developers' decision to agree with the removal of a specific TD liability. The study has been addressed to the developers of four well-known open-source applications. To increase the response rate, a personalized assessment has first been sent to each developer, summarizing his/her own contribution to the TD of the corresponding project \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:tuHXwOkdijsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Methods for Statistical and Visual Comparison of Imputation Methods for Missing Data in Software Cost Estimation",
            "Publication year": 2011,
            "Publication url": "https://www.igi-global.com/chapter/modern-software-engineering-concepts-practices/51974",
            "Abstract": "Software Cost Estimation is a critical phase in the development of a software project, and over the years has become an emerging research area. A common problem in building software cost models is that the available datasets contain projects with lots of missing categorical data. The purpose of this chapter is to show how a combination of modern statistical and computational techniques can be used to compare the effect of missing data techniques on the accuracy of cost estimation. Specifically, a recently proposed missing data technique, the multinomial logistic regression, is evaluated and compared with four older methods: listwise deletion, mean imputation, expectation maximization and regression imputation with respect to their effect on the prediction accuracy of a least squares regression cost model. The evaluation is based on various expressions of the prediction error and the comparisons are conducted \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:zA6iFVUQeVQC",
            "Publisher": "IGI Global"
        },
        {
            "Title": "Discovering patterns of correlation and similarities in software project data with the Circos visualization tool",
            "Publication year": 2011,
            "Publication url": "https://arxiv.org/abs/1110.1303",
            "Abstract": "Software cost estimation based on multivariate data from completed projects requires the building of efficient models. These models essentially describe relations in the data, either on the basis of correlations between variables or of similarities between the projects. The continuous growth of the amount of data gathered and the need to perform preliminary analysis in order to discover patterns able to drive the building of reasonable models, leads the researchers towards intelligent and time-saving tools which can effectively describe data and their relationships. The goal of this paper is to suggest an innovative visualization tool, widely used in bioinformatics, which represents relations in data in an aesthetic and intelligent way. In order to illustrate the capabilities of the tool, we use a well known dataset from software engineering projects.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:b0M2c_1WBrUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Data\u2010driven benchmarking in software development effort estimation: The few define the bulk",
            "Publication year": 2020,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2258",
            "Abstract": "The rapid evolvement of software development effort estimation models created the need for empirical evaluation of their quality. The empirical evaluation is based either on hypothesis tests with respect to a single criterion or on aggregating methods for multiple criteria. However, a model can be considered as a multidimensional entity performing differently on alternative datasets and its performance can be divergent when expressed by alternative criteria.In this study, we explore this multidimensional nature of models by considering them as points in two different spaces (domain and criteria spaces).Introducing an alternative approach for data\u2010driven benchmarking, a new framework based on archetypal analysis is proposed for evaluation purposes of multiple models.The benefits of the framework are illustrated through a large\u2010scale experimental setup on a set of 93 effort \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:a9-T7VOCCH8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "reviewers of 2016",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/article/10.1007/s10664-016-9495-8",
            "Abstract": "For helping us deliver timely decisions to our authors, the Editors-in-Chief and Publisher would like to thank the following individuals that contributed reviews between November 1, 2015 and November 1, 2016. We applaud your efforts and dedication to the community.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:BUYA1_V_uYcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sequence variation, common tissue expression patterns and learning models: a genome-wide survey of vertebrate ribosomal proteins",
            "Publication year": 2020,
            "Publication url": "https://academic.oup.com/nargab/article-abstract/2/4/lqaa088/5958143",
            "Abstract": "Ribosomal genes produce the constituents of the ribosome, one of the most conserved subcellular structures of all cells, from bacteria to eukaryotes, including animals. There are notions that some protein-coding ribosomal genes vary in their roles across species, particularly vertebrates, through the involvement of some in a number of genetic diseases. Based on extensive sequence comparisons and systematic curation, we establish a reference set for ribosomal proteins (RPs) in eleven vertebrate species and quantify their sequence conservation levels. Moreover, we correlate their coordinated gene expression patterns within up to 33 tissues and assess the exceptional role of paralogs in tissue specificity. Importantly, our analysis supported by the development and use of machine learning models strongly proposes that the variation in the observed tissue-specific gene expression of RPs is rather species \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:OTTXONDVkokC",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "An experimental investigation of personality types impact on pair effectiveness in pair programming",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/article/10.1007/s10664-008-9093-5",
            "Abstract": "In this paper, pair programming is empirically investigated from the perspective of developer personalities and temperaments and how they affect pair effectiveness. A controlled experiment was conducted to investigate the impact of developer personalities and temperaments on communication, pair performance and pair viability-collaboration. The experiment involved 70 undergraduate students and the objective was to compare pairs of heterogeneous developer personalities and temperaments with pairs of homogeneous personalities and temperaments, in terms of pair effectiveness. Pair effectiveness is expressed in terms of pair performance, measured by communication, velocity, design correctness and passed acceptance tests, and pair collaboration-viability measured by developers\u2019 satisfaction, knowledge acquisition and participation. The results have shown that there is important difference \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:hqOjcs7Dif8C",
            "Publisher": "Springer US"
        },
        {
            "Title": "Associating the severity of vulnerabilities with their description",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-39564-7_22",
            "Abstract": "Software vulnerabilities constitute a major problem for today\u2019s world, which relies more than ever to technological achievements. The characterization of vulnerabilities\u2019 severity is an issue of major importance in order to address them and extensively study their impact on information systems. That is why scoring systems have been developed for the ranking of vulnerabilities\u2019 severity. However, the severity scores are based on technical information and are calculated by combining experts\u2019 assessments. The motivation for the study conducted in this paper was the question of whether the severity of vulnerabilities is directly related to their description. Hence, the associations of severity scores and individual characteristics with vulnerability descriptions\u2019 terms were studied using Text Mining, Principal Components and correlation analysis techniques, applied to all vulnerabilities registered in the National \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:j8SEvjWlNXcC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "CSIT 2016 Program Committee",
            "Publication year": 0,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7549435/",
            "Abstract": "Program Committee Page 1 CSIT 2016 Program Committee Ahmad Saifan, Yarmouk University, \nJordan Hazem Qattous, Applied Science University, Jordan Daniel Neagu, University of \nBradford, UK Mohammad Hamdaqa, University of Waterloo, Canada Ibrahim Aljarah, University \nof Jordan, Jordan Flaminia Luccio, Ca' Foscari University of Venice, Italy Raed Seetan, \nSlippery Rock University, USA Qasem Obeidat, Imam Muhammad ibn Saud Islamic University, \nKSA Mohammed Akour, Yarmouk University, Jordan Salimur Choudhury, Algoma University, \nCanada Danny Ho, NFA Estimation Inc, Canada Jamal Alsakran, University of Jordan, Jordan \nOmar Al-Azzam, St. Cloud State University, USA Mohamed Saad, University of Sharjah, UAE \nOmar Alsheikh Salem, Applied Science University, Jordan Ahmed Manasrah, Yarmouk \nUniversity, Jordan Nouh Alhindawi, Jadara University, Jordan Abdallah Alma'Aitah, Jordan of (\u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:VLnqNzywnoUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Assigning Gene Ontology terms to biotext by classification methods",
            "Publication year": 0,
            "Publication url": "https://www.academia.edu/download/30803918/B_019-028_Theodosiou.pdf",
            "Abstract": "Biomedical literature databases constitute valuable repositories of up to date scientific knowledge. The development of efficient classification methods in order to facilitate the organization of these databases and the extraction of novel biomedical knowledge is becoming increasingly important. Several of these methods use bio-ontologies, like Gene Ontology to concisely describe and classify biological documents. The purpose of this paper is to compare two classical statistical classification methods, namely multinomial logistic regression (MLR) and linear discriminant analysis (LDA), to a machine learning classification method, called support vector machines (SVM). Although all the methods have been used with success for classifying texts, there is not a direct comparison between them for classifying biological text to specific Gene Ontology terms. The results from the study show that LDA performs better (accuracy 80.32%) than SVM (77.18%) and MLR (57.4%). LDA not only performs well in the assignment of Gene Ontology terms to documents, but also reduces the dimensions of the original data, making them easier to manage.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:4vMrXwiscB8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "What do developers talk about open source software licensing?",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9226294/",
            "Abstract": "Free and open source software has gained a lot of momentum in the industry and the research community. Open source licenses determine the rules, under which the open source software can be further used and distributed. Previous works have examined the usage of open source licenses in the framework of specific projects or online social coding platforms, examining developers specific licensing views for specific software. However, the questions practitioners ask about licenses and licensing as captured in Question and Answer websites also constitute an important aspect toward understanding practitioners general licenses and licensing concerns. In this paper, we investigate open source license discussions using data from the Software Engineering, Open Source and Law Stack Exchange sites that contain relevant data. We describe the process used for the data collection and analysis, and discuss the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:WJVC3Jt7v1AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An adaptive model for competences assessment of IT professionals",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-15898-3_6",
            "Abstract": "Emerging technologies such as Big Data and Cloud Computing in the field of information technology imposes further needs (requests) for professional competences in organizations and IT companies. The ultimate goal is to comply with industrial changes characterizedby adaptive solutions for fostering human-machine interactions. Here competence and job knowledge play a great role in organizations. This paper discusses the concept ofan adaptive competence profiling platform in the context of EU funded project ComProFITS. The main goal is (i) reinforcing competence analytics, and (ii) improving the quality of personnel selection and job performance in the IT sector. This project reflects the results of the research and development activities based on needs analysis with a Spanish IT company.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:JQOojiI6XY0C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Personality, emotional intelligence and work preferences in software engineering: An empirical study",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584914000639",
            "Abstract": "There is an increasing awareness among Software Engineering (SE) researchers and practitioners that more focus is needed on understanding the engineers developing software. Previous studies show significant associations between the personalities of software engineers and their work preferences.Various studies on personality in SE have found large, small or no effects and there is no consensus on the importance of psychometric measurements in SE. There is also a lack of studies employing other psychometric instruments or using larger datasets. We aim to evaluate our results in a larger sample, with software engineers in an earlier state of their career, using advanced statistics.An operational replication study where extensive psychometric data from 279 master level students have been collected in a SE program at a Swedish University. Personality data based on the Five-Factor \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:Mojj43d5GZwC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A Study of Knowledge Sharing related to Covid-19 Pandemic in Stack Overflow",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2004.09495",
            "Abstract": "The Covid-19 outbreak, beyond its tragic effects, has changed to an unprecedented extent almost every aspect of human activity throughout the world. At the same time, the pandemic has stimulated enormous amount of research by scientists across various disciplines, seeking to study the phenomenon itself, its epidemiological characteristics and ways to confront its consequences. Information Technology, and particularly Data Science, drive innovation in all related to Covid-19 biomedical fields. Acknowledging that software developers routinely resort to open question and answer communities like Stack Overflow to seek advice on solving technical issues, we have performed an empirical study to investigate the extent, evolution and characteristics of Covid-19 related posts. In particular, through the study of 464 Stack Overflow questions posted mainly in February and March 2020 and leveraging the power of text mining, we attempt to shed light into the interest of developers in Covid-19 related topics and the most popular technological problems for which the users seek information. The findings reveal that indeed this global crisis sparked off an intense and increasing activity in Stack Overflow with most post topics reflecting a strong interest on the analysis of Covid-19 data, primarily using Python technologies.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:XvxMoLDsR5gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Competency profiling for software engineers: literature review and a new model",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2801948.2801960",
            "Abstract": "Today's ICT sector is fast growing and very competitive. Prospective employees and specially software engineers are competing hard in ICT companies where there is a variety of roles and positions requiring different skills and competencies. So, there is a vital need for formal processes and criteria for evaluating the adequacy of the candidate's competencies with respect to the job requirements. The aim of this research is to create a competency profiling model for software engineers and specifically to build a hierarchical structure which will capture different aspects of various competencies so as to facilitate ICT companies with a tool that can aid them to reach the optimal decision during recruitment processes and vocational training. First of all, we provide a literature review regarding related models in software engineering. Then, we present a new customizable, three-level model, designed according to the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:tKAzc9rXhukC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reasons for bottlenecks in very large-scale system of systems development",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584914001074",
            "Abstract": "System of systems (SoS) is a set or arrangement of systems that results when independent and useful systems are to be incorporated into a larger system that delivers unique capabilities. Our investigation showed that the development life cycle (i.e. the activities transforming requirements into design, code, test cases, and releases) in SoS is more prone to bottlenecks in comparison to single systems.The objective of the research is to identify reasons for bottlenecks in SoS, prioritize their significance according to their effect on bottlenecks, and compare them with respect to different roles and different perspectives, i.e. SoS view (concerned with integration of systems), and systems view (concerned with system development and delivery).The research method used is a case study at Ericsson AB.Results show that the most significant reasons for bottlenecks are related to requirements \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:eMMeJKvmdy0C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Investigating the impact of personality and temperament traits on pair programming: a controlled experiment replication",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6511782/",
            "Abstract": "This paper presents the partial results of an extended replicated pair programming experiment, conducted at the Alexander Technological Educational Institute of Thessaloniki, Greece during the two semesters of the year 2009-10. The entire study involved 160 second semester students attending a Software Engineering advanced programming course with pair programming and test-driven development as teaching methodology. A series of formal experiments were conducted to investigate, as in the first experiment, the impact of developer personalities and temperaments on pair performance. In this paper we report partial results and we focus on pair performance in terms of communication, time to complete assignments and overall score. The main variables were statistically analyzed in order to test differences between two personality groups: A control group with homogeneous personalities in pairs and an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:B3FOqHPlNUQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A simulation tool for efficient analogy based cost estimation",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/article/10.1023/A:1009897800559",
            "Abstract": "Estimation of a software project effort, based on project analogies, is a promising method in the area of software cost estimation. Projects in a historical database, that are analogous (similar) to the project under examination, are detected, and their effort data are used to produce estimates. As in all software cost estimation approaches, important decisions must be made regarding certain parameters, in order to calibrate with local data and obtain reliable estimates. In this paper, we present a statistical simulation tool, namely the bootstrap method, which helps the user in tuning the analogy approach before application to real projects. This is an essential step of the method, because if inappropriate values for the parameters are selected in the first place, the estimate will be inevitably wrong. Additionally, we show how measures of accuracy and in particular, confidence intervals, may be computed for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:u-x6o8ySG0sC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "journal homepage: www. elsevier. com/locate/yjbin",
            "Publication year": 0,
            "Publication url": "https://core.ac.uk/download/pdf/82695309.pdf",
            "Abstract": "Table of Contents Page 1 Volume 44, Issue 6, December 2011 CONTENTS Covered in the \nabstract and citation database SciVerse Scopus\u00ae. Full text available on SciVerse Science \nDirect\u00ae. T. Theodosiou, IS Vizirianakis, L. Angelis, A. Tsaftaris, N. Darzentas. MeSHy: Mining \nunanticipated PubMed information using frequencies of occurrences and concurrences of MeSH \nterms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 919 Zhihui Luo, Meliha Yetisgen-Yildiz, Chunhua \nWeng. Dynamic categorization of clinical research eligibility criteria by hierarchical clustering. . \n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 927 Cyril Dalmasso, Philippe \nBro\u00ebt. Detection of chromosomal abnormalities using high resolution arrays in clinical \ncancer research . . . 936 Thomas G. Kannampallil, Guido F. Schauer, Trevor Cohen, Vimla L. \nPatel. Considering complexity in healthcare systems . . . . 943 R. Khajouei, LWP Peute, A. , . .\u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:uJ-U7cs_P_0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automatic extraction of structure, content and usage data statistics of web sites",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1810617.1810685",
            "Abstract": "In this paper we present a web mining tool which automatically extracts the structure, content and usage data statistics of web sites. This work inspired by the fact that web mining consists of three axes: web structure mining, web content mining and web usage mining. Each one of those axes is using the structure, content and usage data respectively. The scope is to use the developed multi-thread web crawler as a tool to automatically extract from web pages data that are associated with each one of those three axes in order afterwards to compute several useful descriptive statistics and apply advanced mathematical and statistical methods. A description of our system is provided as well as some experimentation results.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Evaluating the Extreme Programming System\u2013An Empirical Study",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-24853-8_29",
            "Abstract": "In this paper we discuss an empirical study about the success and difficulties 15 Greek software companies experienced applying Extreme Programming [1] as a holistic system in software development. Based on a generic XP system including feedback influences and using as a research tool a cause-effect model including social-technical affecting factors, the study statistically evaluates XP practices application by the software companies. Data were collected from 30 managers and developers, using the sample survey technique with questionnaires and interviews, in a time period of six months. Practices were analysed separately using Descriptive Statistics and as a whole by building up different models using stepwise Discriminant Analysis. The results have shown that companies, facing various problems with some practices, prefer to develop their own tailored XP method and way of working-practices to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:mVmsd5A6BfQC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Synthetic metrics for evaluating runtime quality of software architectures with complex tradeoffs",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5349844/",
            "Abstract": "Runtime quality of software, such as availability and throughput, depends on architectural factors and execution environment characteristics (e.g. CPU speed, network latency). Although the specific properties of the underlying execution environment are unknown at design time, the software architecture can be used to assess the inherent impact of the adopted design decisions on runtime quality. However, the design decisions that arise in complex software architectures exhibit non trivial interdependences. This work introduces an approach that discovers the most influential factors, by exploiting the correlation structure of the analyzed metrics via factor analysis of simulation data. A synthetic performance metric is constructed for each group of correlated metrics. The variability of these metrics summarizes the combined factor effects hence it is easier to assess the impact of the analyzed architecture decisions on the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:iH-uZ7U-co4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Towards an integrated platform for big data analysis",
            "Publication year": 2013,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-34471-8_4",
            "Abstract": "The amount of data in the world is expanding rapidly. Every day, huge amounts of data are created by scientific experiments, companies, and end users\u2019 activities. These large data sets have been labeled as \u201cBig Data\u201d, and their storage, processing and analysis presents a plethora of new challenges to computer science researchers and IT professionals. In addition to efficient data management, additional complexity arises from dealing with semi-structured or unstructured data, and from time critical processing requirements. In order to understand these massive amounts of data, advanced visualization and data exploration techniques are required.Innovative approaches to these challenges have been developed during recent years, and continue to be a hot topic for research and industry in the future. An investigation of current approaches reveals that usually only one or two aspects are addressed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:tOudhMTPpwUC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "A large-scale empirical study of practitioners' use of object-oriented concepts",
            "Publication year": 0,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.863.2113&rep=rep1&type=pdf",
            "Abstract": "We present the first results from a survey carried out over the second quarter of 2009 examining how theories in objectoriented design are understood and used by software developers. We collected 3785 responses from software developers world-wide, which we believe is the largest survey of its kind. We targeted the use of encapsulation, class size as measured by number of methods, and depth of a class in the inheritance hierarchy. We found that, while overall practitioners followed advice on encapsulation, there was some variation of adherence to it. For class size and depth there was substantially less agreement with expert advice. In addition, large inconsistencies were found within the use and perception of object-oriented concepts within the investigated group of developers. The results of this survey has deep reaching consequences for both practitioners and researchers as they highlight and confirm central issues.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:IUKN3-7HHlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using Repository of Repositories (RoRs) to Study the Growth of F/OSS Projects: A Meta-Analysis Research Approach",
            "Publication year": 2010,
            "Publication url": "https://scholar.google.com/scholar?cluster=1056130931335341638&hl=en&oi=scholarr",
            "Abstract": "Free/Open Source Software (F/OSS) repositories contain valuable data and their usefulness in studying software development and community activities continues to attract a lot of research attention. A trend in F/OSS studies is the use of metadata stored in a repository of repositories or RoRs. This paper utilizes data obtained from such RoRs-FLOSSmole-to study the types of projects being developed by the F/OSS community. We downloaded projects by topics data in five areas (Database, Internet, Software Development, Communications, and Games/Entertainment) from Flossmoles raw and summary data of the sourceforge repository. Time series analysis show the numbers of projects in the five topics are growing linearly. Further analysis supports our hypothesis that F/OSS development is moving up the stack from developer tools and infrastructure support to end-user applications such as Databases. The findings \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:BqipwSGYUEgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A large-scale empirical study of practitioners' use of object-oriented concepts",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1806799.1806820",
            "Abstract": "We present the first results from a survey carried out over the second quarter of 2009 examining how theories in object-oriented design are understood and used by software developers. We collected 3785 responses from software developers world-wide, which we believe is the largest survey of its kind. We targeted the use of encapsulation, class size as measured by number of methods, and depth of a class in the inheritance hierarchy. We found that, while overall practitioners followed advice on encapsulation, there was some variation of adherence to it. For class size and depth there was substantially less agreement with expert advice. In addition, inconsistencies were found within the use and perception of object-oriented concepts within the investigated group of developers. The results of this survey has deep reaching consequences for both practitioners and researchers as they highlight and confirm central \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:fPk4N6BV_jEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Discovering Causal Patterns with Structural Equation Modeling: Application to Toll\u2010Like Receptor Signaling Pathway in Chronic Lymphocytic Leukemia",
            "Publication year": 2015,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119078845.ch28",
            "Abstract": "This chapter provides the biological background of the research regarding chronic lymphocytic leukemia (CLL) and toll\u2010like receptor (TLR) pathways. Stimulation of TLRs induces an immediate signaling cascade resulting in the production of inflammatory cytokines and the expression of costimulatory molecules. The chapter then describes the theory and methodology regarding structural equation modeling (SEM). Structural equation models can generally be interpreted as carriers of substantive causal or probabilistic information. In SEM theory, there are two categorizations of variables: exogenous and endogenous and latent and observed variables. The chapter also presents the application of the method to a data set by building models and estimating the causal relationships of the variables. It examines the relationships among genes included in the TLR signaling pathway, from a statistical point of view. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:_Re3VWB3Y0AC",
            "Publisher": "John Wiley & Sons, Inc"
        },
        {
            "Title": "Comparing software cost prediction models by a visualization tool",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4725751/",
            "Abstract": "A crucial issue in the software cost estimation area that has attracted the interest of software project managers is the selection of the best prediction method for estimating the cost of a project. Most of the prediction techniques estimate the cost from historical data. The selection of the best model is based on accuracy measures that are functions of the predictive error, whereas the significance of the differences can be evaluated through statistical procedures. However, statistical tests cannot be applied easily by non-experts while there are difficulties in the interpretation of their results. The purpose of this paper is to introduce the utilization of a visualization tool, the regression error characteristic curves in order to compare different prediction models easily, by a simple inspection of a graph. Moreover, these curves are adjusted to accuracy measures appeared in software cost estimation literature and the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Is the Market Value of Software Vendors Affected by Software Vulnerability Announcements?",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-56288-9_61",
            "Abstract": "Nowadays, information security constitutes an urgent issue for businesses and researchers. The security vulnerabilities existing in computer systems are sources of different problems. An indirect and emerging issue, regarding the economic consequences of the vulnerabilities, is the impact of software vulnerability announcements to the stock price of the responsible software vendors. The scope of this paper is the study of the stock market reaction when vulnerability announcements occur and the correlation analysis between the impact of these events and vulnerability severity according to scoring systems. To find the impact to the stock market, the well-established in economics event study methodology was used. The dataset in this research was collected from the US-CERT (United States Computer Emergency Readiness Team) website, consisting of year\u2019s 2014 records while the total number of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:URolC5Kub84C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Ensemble Software Development Effort Estimation Using Data Envelopment Analysis",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3437120.3437307",
            "Abstract": "The field of Software Development Software Estimation in Software Engineering is critical due to its practical importance and very challenging due to the lack of a globally best model, able to predict accurately the effort (and therefore the cost) of any new software project. After a long research period on different models and their improvements, the research interest is directed towards ensemble methods, ie methods which combine the results of different single models. Furthermore, it is desirable to characterise the accuracy of models by different criteria. In this study, we develop a methodology based on the Data Envelopment Analysis technique, well-known in operation research, so as to rank different models based on multiple criteria and then to combine the best of them in order to achieve better prediction performance. The experimentation involves 93 models applied to 10 datasets and provides very promising \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:9c2xU6iGI7YC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Establishment of computational biology in Greece and Cyprus: Past, present, and future",
            "Publication year": 2019,
            "Publication url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007532",
            "Abstract": "We review the establishment of computational biology in Greece and Cyprus from its inception to date and issue recommendations for future development. We compare output to other countries of similar geography, economy, and size\u2014based on publication counts recorded in the literature\u2014and predict future growth based on those counts as well as national priority areas. Our analysis may be pertinent to wider national or regional communities with challenges and opportunities emerging from the rapid expansion of the field and related industries. Our recommendations suggest a 2-fold growth margin for the 2 countries, as a realistic expectation for further expansion of the field and the development of a credible roadmap of national priorities, both in terms of research and infrastructure funding.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:NXb4pA-qfm4C",
            "Publisher": "Public Library of Science"
        },
        {
            "Title": "Identifying knowledge brokers that yield software engineering knowledge in OSS projects",
            "Publication year": 2006,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584906000073",
            "Abstract": "Much research on open source software development concentrates on developer lists and other software repositories to investigate what motivates professional software developers to participate in open source software projects. Little attention has been paid to individuals who spend valuable time in lists helping participants on some mundane yet vital project activities. Using three Debian lists as a case study we investigate the impact of knowledge brokers and their associated activities in open source projects. Social network analysis was used to visualize how participants are affiliated with the lists. The network topology reveals substantial community participation. The consequence of collaborating in mundane activities for the success of open source software projects is discussed. The direct beneficiaries of this research are in the identification of knowledge experts in open source software projects.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A permutation test based on regression error characteristic curves for software cost estimation models",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/article/10.1007/s10664-011-9177-5",
            "Abstract": "Regression Error Characteristic (REC) curves provide a visualization tool, able to characterize graphically the prediction power of alternative predictive models. Due to the benefits of using such a visualization description of the whole distribution of error, REC analysis was recently introduced in software cost estimation to aid the decision of choosing the most appropriate cost estimation model during the management of a forthcoming project.Although significant information can be retrieved from a readable graph, REC curves are not able to assess whether the divergences between the alternative error functions can constitute evidence for a statistically significant difference.In this paper, we propose a graphical procedure that utilizes (a) the process of repetitive permutations and (b) and the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:bFI3QPDXJZMC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Combining regression and estimation by analogy in a semi-parametric model for software cost estimation",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1414004.1414017",
            "Abstract": "Software Cost Estimation is the task of predicting the effort or productivity required to complete a software project. Two of the most known techniques appeared in literature so far are Regression Analysis and Estimation by Analogy. The results of the empirical studies show the lack of convergence in choosing the best prediction technique between the parametric Regression Analysis and the non-parametric Estimation by Analogy models. In this paper, we introduce the use of a semi-parametric model that achieves to incorporate some parametric information into a non-parametric model combining in this way regression and analogy. Furthermore, we demonstrate the procedure of building such a model on two well-known datasets and we present the comparative results based on the predictive accuracy of the new technique using several accuracy measures. We also perform statistical tests on the residuals in order to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:dhFuZR0502QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Overestimation and underestimation of software cost models: Evaluation by visualization",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6619528/",
            "Abstract": "Software Cost Estimation (SCE) is a process related to the well-balanced management of a software project. Despite the evolving research activity, the task of estimating accurately the budget and the delivering time has been a research problem for many decades. Nowadays, the cost of a project is still estimated with error. The study of the error produced by estimation models or techniques has been focused on the sources producing it. Usually, the various studies consider underestimations and overestimations of the actual cost to have equal importance. However this is hardly true in practice and such a consideration could be extremely risky for an organization and of course for the customers. In this study, we consider the problem of weighing differently the overestimation and underestimation and we introduce in SCE the utilization of a recently presented graphical methodology, namely the analysis by Regression \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:1qzjygNMrQYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Methods of constructing A-efficient BTIUB designs",
            "Publication year": 1993,
            "Publication url": "https://scholar.google.com/scholar?cluster=7955110908645118362&hl=en&oi=scholarr",
            "Abstract": "In this paper some methods of constructing balanced treatment incomplete block designs with two different block sizes (called BTIUB designs), are given in order to compare test treatments with a control in unequal blocks. These methods are based on the use of the incidence matrices of block designs with known patterns, such as balanced incomplete block designs and group divisible designs. Each method produces at least one A-optimal BTIUB design, while almost all of the designs constructed, within reasonable limits of the parameters, for practical situations, are highly A-efficient.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:eJXPG6dFmWUC",
            "Publisher": "UTIL MATH PUBL INC"
        },
        {
            "Title": "INFSO-ICT-257992 SmartSantander SEN2SOC Experiment D3. 3",
            "Publication year": 0,
            "Publication url": "https://scholar.google.com/scholar?cluster=14378781152320533330&hl=en&oi=scholarr",
            "Abstract": "This document describes the SEN2SOC experiment results running on top of the platform developed for social network observer, Web and mobile application prototypes. The document summarizes the results and all relevant additional information related to the SEN2SOC experiment\u2019s implementation. Moreover, the document proposes potential improvements to be carried out on the experimentation platform.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:kzcrU_BdoSEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ESTIMATING LEARNING BY ANALOGY: A CASE STUDY IN THE UAE UNIVERSITY",
            "Publication year": 2010,
            "Publication url": "https://scholar.google.com/scholar?cluster=3480626317253531992&hl=en&oi=scholarr",
            "Abstract": "This work is concerned with how to measure and, most importantly, how to predict the \u201camount\u201d of learning in \u201cproblem-based\u201d learning programs. Due to difficulties in dealing quantitatively with educational aspects like learning, which are rather abstract when it comes to measuring, this is not a typical problem which can be dealt with easily traditional statistical analysis. The ordinal nature of data collected from surveys requires suitable estimation methods. In this paper we use a method called \u201cestimation by analogy\u201d (EbA), known from its application to software engineering problems, for estimating the amount of learning in a class which is based on historical data containing information from previous learning measurements. The approach is simple since it does not require any mathematical background and assumptions; it is intuitively appealing since it is based on the idea of \u201cfinding the most similar cases\u201d and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:lSLTfruPkqcC",
            "Publisher": "IATED"
        },
        {
            "Title": "Combining probabilistic models for explanatory productivity estimation",
            "Publication year": 2008,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584907000730",
            "Abstract": "In this paper Association Rules (AR) and Classification and Regression Trees (CART) are combined in order to deliver an effective conceptual estimation framework. AR descriptive nature is exploited by identifying logical associations between project attributes and the required effort for the development of the project. CART method on the other hand has the benefit of acquiring general knowledge from specific examples of projects and is able to provide estimates for all possible projects. The particular methods have the ability of learning and modelling associations in data and hence they can be used to describe complex relationships in software cost data sets that are not immediately apparent. Potential benefits of combining these probabilistic methods involve the ability of the final model to reveal the way in which particular attributes can increase or decrease productivity and the fact that such assumptions vary \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:_kc_bZDykSQC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "ASSESSING THE MODIFIABILITY OF TWO OBJECT-ORIENTED DESIGN ALTERNATIVES--A CONTROLLED EXPERIMENT REPLICATION",
            "Publication year": 2004,
            "Publication url": "https://www.academia.edu/download/30842049/assessing_object_oriented_software_changeability_with_design_metrics.pdf",
            "Abstract": "This paper presents a replication study of a controlled experiment, investigating the impact of many design characteristics on one of the most desirable quality factors, modifiability. Two alternative design structures were used; a responsibility-driven (RD) versus a control-oriented \u201cmainframe\u201d(MF) design. Two groups of undergraduate students participated, each performing on one of the two designs. The subjects designed, implemented in Java, and tested a set of three maintenance tasks in order to assess the degree of their understanding, effort, and performance. The results indicate that the RD version due to its delocalised structure, exhibited higher correctness, better extensibility, and design stability, than the MF version. In order to provide an objective assessment of the differences between the two versions, a considerable number of metrics were used on the delivered solutions, quantifying separately each produced design\u2019s characteristics.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:rO6llkc54NcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multinomial logistic regression applied on software productivity prediction",
            "Publication year": 2003,
            "Publication url": "https://www.academia.edu/download/41833400/649sentas.pdf",
            "Abstract": "In software cost estimation various methods have been proposed to yield a prediction of the productivity of a software project. Most of the methods produce point estimates. However, in practice it is more realistic and useful to have a method providing interval predictions. Although some methods accompany a point estimate with a prediction interval, it is also reasonable to use a method predicting the interval in which the cost will fall. In this paper, we consider a method called Multinomial Logistic Regression using as dependent variable the predefined cost intervals and as predictor variables the attributes, similar to the ones characterizing completed projects of the available data set. The method builds a model, which classifies any new software project, according to estimated probabilities, in one of the predefined intervals. The proposed method was applied to a well-known data set and was validated with respect to its fitting and predictive accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:RHpTSmoSYBkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A framework of statistical and visualization techniques for missing data analysis in software cost estimation",
            "Publication year": 2015,
            "Publication url": "https://www.igi-global.com/chapter/a-framework-of-statistical-and-visualization-techniques-for-missing-data-analysis-in-software-cost-estimation/117920",
            "Abstract": "Software Cost Estimation (SCE) is a critical phase in software development projects. However, due to the growing complexity of the software itself, a common problem in building software cost models is that the available datasets contain lots of missing categorical data. The purpose of this chapter is to show how a framework of statistical, computational, and visualization techniques can be used to evaluate and compare the effect of missing data techniques on the accuracy of cost estimation models. Hence, the authors use five missing data techniques: Multinomial Logistic Regression, Listwise Deletion, Mean Imputation, Expectation Maximization, and Regression Imputation. The evaluation and the comparisons are conducted using Regression Error Characteristic curves, which provide visual comparison of different prediction models, and Regression Error Operating Curves, which examine predictive power of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:LjlpjdlvIbIC",
            "Publisher": "IGI Global"
        },
        {
            "Title": "Understanding knowledge sharing activities in free/open source software projects: An empirical study",
            "Publication year": 2008,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121207000842",
            "Abstract": "Free/Open Source Software (F/OSS) projects are people-oriented and knowledge intensive software development environments. Many researchers focused on mailing lists to study coding activities of software developers. How expert software developers interact with each other and with non-developers in the use of community products have received little attention. This paper discusses the altruistic sharing of knowledge between knowledge providers and knowledge seekers in the Developer and User mailing lists of the Debian project. We analyze the posting and replying activities of the participants by counting the number of email messages they posted to the lists and the number of replies they made to questions others posted. We found out that participants interact and share their knowledge a lot, their positing activity is fairly highly correlated with their replying activity, the characteristics of posting and replying \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:UeHWp8X0CEIC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Not all IGHV3-21 chronic lymphocytic leukemias are equal: prognostic considerations",
            "Publication year": 2015,
            "Publication url": "https://ashpublications.org/blood/article-abstract/125/5/856/34069",
            "Abstract": "An unresolved issue in chronic lymphocytic leukemia (CLL) is whether IGHV3-21 gene usage, in general, or the expression of stereotyped B-cell receptor immunoglobulin defining subset #2 (IGHV3-21/IGLV3-21), in particular, determines outcome for IGHV3-21-utilizing cases. We reappraised this issue in 8593 CLL patients of whom 437 (5%) used the IGHV3-21 gene with 254/437 (58%) classified as subset #2. Within subset #2, immunoglobulin heavy variable (IGHV)-mutated cases predominated, whereas non\u2013subset #2/IGHV3-21 was enriched for IGHV-unmutated cases (P = .002). Subset #2 exhibited significantly shorter time-to-first-treatment (TTFT) compared with non\u2013subset #2/IGHV3-21 (22 vs 60 months, P = .001). No such difference was observed between non\u2013subset #2/IGHV3-21 vs the remaining CLL with similar IGHV mutational status. In conclusion, IGHV3-21 CLL should not be axiomatically \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:1yQoGdGgb4wC",
            "Publisher": "American Society of Hematology"
        },
        {
            "Title": "An empirical approach to evaluate students participation in free/open source software projects",
            "Publication year": 2006,
            "Publication url": "https://www.academia.edu/download/30803923/200608C040.pdf",
            "Abstract": "Despite the popularity and global adoption of Free/Open Source Software (F/OSS), the implementation of teaching and learning methodologies in the context of traditional software engineering courses is still a novelty. This paper summarizes the approach we used to evaluate the participation of students in a F/OSS teaching and learning framework. The framework was implemented as a pilot study in which students volunteered to participate in software testing in F/OSS projects. The evaluation approach we discuss here is based on (a) students' participation in software testing and (b) results from two online surveys. In the evaluation process we utilized a statistical approach (both descriptive and inferential) we considered appropriate for assessing the students. We hope that our evaluation approach will help us further understand and strengthen empirical and experimental software engineering research in this emerging technology.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:mB3voiENLucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Duration analysis of software projects",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=2618817295256149647&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:e_rmSamDkqQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Special issue on software quality of advanced software applications",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s11219-020-09497-z",
            "Abstract": "This issue of the Software Quality Journal is devoted to studies on the quality of software in various advanced applications in business, industry, and everyday life. Following an open call and a thorough review process, the articles accepted for publication cover various aspects relevant to the generic topic. Various areas are represented in the published papers, such as embedded systems, gap analysis, component-based software engineering, open source software analysis, software management, bug reports, and domain-specific modeling. Moreover, different qualitative and quantitative methodologies are used such as empirical studies, questionnaires, prioritization techniques, statistical analysis, software repositories mining, recommendation systems, text analysis, and systematic literature mapping. This issue consists of 7 papers that are briefly discussed as follows: The article by D. Sas and P. Avgeriou entitled \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:umqufdRvDiIC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Clinical effect of stereotyped B-cell receptor immunoglobulins in chronic lymphocytic leukaemia: a retrospective multicentre study",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S2352302614000052",
            "Abstract": "About 30% of cases of chronic lymphocytic leukaemia (CLL) carry quasi-identical B-cell receptor immunoglobulins and can be assigned to distinct stereotyped subsets. Although preliminary evidence suggests that B-cell receptor immunoglobulin stereotypy is relevant from a clinical viewpoint, this aspect has never been explored in a systematic manner or in a cohort of adequate size that would enable clinical conclusions to be drawn.For this retrospective, multicentre study, we analysed 8593 patients with CLL for whom immunogenetic data were available. These patients were followed up in 15 academic institutions throughout Europe (in Czech Republic, Denmark, France, Greece, Italy, Netherlands, Sweden, and the UK) and the USA, and data were collected between June 1, 2012, and June 7, 2013. We retrospectively assessed the clinical implications of CLL B-cell receptor immunoglobulin \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:WqliGbK-hY8C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Analyzing measurements of the R statistical open source software",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6479797/",
            "Abstract": "Software quality is one of the main goals of effective programming. Although it has a quite ambiguous meaning, quality can be measured by several metrics, which have been appropriately formulated through the years. Software measurement is a particularly important procedure, as it provides meaningful information about the software artifact. This procedure is even more emerging when we refer to open source software, where the need for shared knowledge is crucial for the maintenance and evolution of the code. A paradigm of open source project where code quality is especially important is the scientific language R. This paper aims to perform measurements on the R statistical open source software, examine the relationships among the observed metrics and special attributes of the R software and search for certain characteristics that define its behavior and structure. For this purpose, a random sample of 508 R \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:tS2w5q8j5-wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Investigation of the relationship between sleep disorders and xerostomia",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s00784-019-03029-1",
            "Abstract": "To investigate the relationship between sleep disorders, morning hyposalivation, and subjective feeling of dry mouth.A cross-sectional, observational, clinical study was carried out in a homogenous population sample which consists of Greek male soldiers without any medical history. After the application of oral modified Schirmer test, the sample was divided into a study group (n\u2009=\u200963) (MST\u2009<\u200925 mm/3 min) and a control group (n\u2009=\u2009110) (MST\u2009\u2265\u200925 mm/3 min). In order to assess daytime sleepiness, risk of obstructive sleep apnea (OSA), sleep quality, sleep bruxism (SB), and subjective feeling of dry mouth, all the participants filled in the following scales in Greek version: Epworth Sleepiness Scale (ESS), Pittsburgh Sleep Quality Index (PSQI), Berlin Questionnaire (BQ), a SB questionnaire, and Xerostomia Inventory (XI) respectively. In every subgroup that came of ESS \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:_axFR9aDTf0C",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "A framework for comparing multiple cost estimation methods using an automated visualization toolkit",
            "Publication year": 2015,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S095058491400127X",
            "Abstract": "The importance of accurate predictions in Software Cost Estimation and the related challenging research problems, led to the introduction of a plethora of methodologies in literature. However, the wide variety of cost estimation methods, the techniques for improving them and the different measures of accuracy have caused new problems such as the inconsistent findings and the conclusion instability. Today, there is a confusion regarding the choice of the most appropriate method for a specific dataset and therefore a need for well-established statistical frameworks as well as for automated tools that will reinforce and lead a comprehensive experimentation and comparison process, based on the thorough study of the cost estimation errors.The purpose of this paper is to present a framework for visualization and statistical comparison of the errors of several cost estimation methods. It is based on an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:Y5dfb0dijaUC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Simulation metamodeling for the design of reliable object based systems",
            "Publication year": 2004,
            "Publication url": "http://delab.csd.auth.gr/~katsaros/eurosim2004.pdf",
            "Abstract": "Replication is a suitable approach for the provision of fault tolerance and load balancing in distributed systems. Object replication takes place on the basis of well-designed interaction protocols that preserve object state consistency in an application transparent manner. The published analytic performance models may only be applied in single-server process replication schemes and are not suitable for schemes composed of miscellaneous policies, such as those arising in object based systems. In this work we make use of a simulation metamodeling approach that allows the comparative evaluation of composite fault tolerance schemes, on the basis of small size uniform experimental designs. Our approach opens the possibility to take into account different design concerns in a combined manner (eg fault tolerance combined with load balancing and multithreading). We provide results in terms of a case system study that reveals a dependence of the optimal adjustments on the system load level. This finding suggests the device of dynamically adjusted fault tolerance schemes.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:JV2RwH3_ST0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Full length articles",
            "Publication year": 2008,
            "Publication url": "https://www.academia.edu/download/36614786/ets_11_3_contents.pdf",
            "Abstract": "Journal of Educational Technology & Society Page 1 iii ISSN 1436-4522. \u00a9 International Forum \nof Educational Technology & Society (IFETS). The authors and the forum jointly retain the \ncopyright of the articles. Permission to make digital or hard copies of part or all of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advantage and that copies bear the full citation on the first page. \nCopyrights for components of this work owned by others than IFETS must be honoured. \nAbstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to \nredistribute to lists, requires prior specific permission and/or a fee. Request permissions from the \neditors at kinshuk@massey.ac.nz. ISSN 1436-4522 (online) and 1176-3647 (print). \u00a9 \nInternational Forum of Educational Technology & Society (IFETS). The authors and the forum the . \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:HtEfBTGE9r8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "AN OvERVIEW OF RANDOM SEARCH ALGORITHMS WITH APPLICATIONS IN OPTIMIZATION PROBLEMS",
            "Publication year": 0,
            "Publication url": "https://scholar.google.com/scholar?cluster=2228510373093814918&hl=en&oi=scholarr",
            "Abstract": "Simulated Annealing (SA) and Genetic Algorithms (GAs) are well\u2013known adaptive search procedures with a wide spectrum of applications in optimization problems. Their common feature is the use of random choice as mechanism for the search towards an optimal solution. The lack of mathematical restrictive assumptions and the simple rules governing the processes, provide flexibility and efficiency in all fields of scientific research. In this overview, we discuss the main principles of SA and GAs along with their differences from other types of search procedures such as the calculusbased methods and the enumerative schemes. The good performance of the algorithms is illustrated by certain examples developed using mathematical software. These examples include optimization of simple functions as well as complicated combinatorial problems involved in a particular statistical research area, the design of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:mlAyqtXpCwEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Incorporating resting state dynamics in the analysis of encephalographic responses by means of the Mahalanobis\u2013Taguchi strategy",
            "Publication year": 2013,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0957417412012249",
            "Abstract": "The analysis of encephalographic responses has mostly been attempted via signal analytic techniques aiming at revealing the useful information from recordings which are considered as contaminated by the ubiquitous ongoing (or background) brain activity. There is continuously accumulating evidence for the existence of well-defined resting-state-networks (RSNs) in the brain, which play a crucial role in the generation of spontaneous activity and the associated neural responses. Hence, the signal plus noise is no longer a valid model and the ongoing fluctuations may influence the response.We introduce here the use of a multivariate statistical methodology, known as Mahalanobis\u2013Taguchi (MT) strategy, which can be tailored to the spontaneous fluctuations so as to optimize the subsequent response detection. A subject-specific version of the MT strategy that combines the original methodology with a clustering \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Software productivity estimation based on association rules",
            "Publication year": 2004,
            "Publication url": "http://users.uowm.gr/sbibi/download/C3.pdf",
            "Abstract": "Software process improvement models have as a target to help software organizations produce successfully, under the expected quality and within time and budget constraints their projects. For this purpose various steps are suggested by these models for the improvement and measurement of the processes followed. One of them involves project planning which is strongly connected with software cost or productivity estimation. Target of this study is to provide a method that could be adopted by an organization in order to estimate the required productivity for the completion of a software development project. Association Rules (AR) is a suitable technique capable of discovering knowledge concerning productivity. The proposed method is applied and evaluated on two different data sets, namely the COCOMO81 dataset and the Maxwell dataset. The evaluation shows that AR is a promising method whose results can be confirmed intuitively.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:hC7cP41nSMkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Study of gene expressions' correlation structures in subgroups of Chronic Lymphocytic Leukemia Patients",
            "Publication year": 2019,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1532046419301297",
            "Abstract": "In chronic lymphocytic leukemia (CLL) the interaction of leukemic cells with the microenvironment ultimately affects patient outcome. CLL cases can be divided in two subgroups with different clinical course based on the mutational status of the immunoglobulin heavy variable (IGHV) genes: mutated CLL (M-CLL) and unmutated CLL (U-CLL). Since in CLL, the differentiated relation of genes between the two subgroups is of greater importance than the individual gene behavior, this paper investigates the differences between the groups\u2019 gene interactions, by comparing their correlation structures. Fisher\u2019s test and Zou\u2019s confidence intervals are employed to detect differences of correlation coefficients. Afterwards, networks created by the genes participating in most differences are estimated with the use of structural equation models (SEM). The analysis is enhanced with graph modeling in order to visualize the between \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:86PQX7AUzd4C",
            "Publisher": "Academic Press"
        },
        {
            "Title": "A statistical framework for analyzing the duration of software projects",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/article/10.1007/s10664-007-9051-7",
            "Abstract": "The duration of a software project is a very important feature, closely related to its cost. Various methods and models have been proposed in order to predict not only the cost of a software project but also its duration. Since duration is essentially the random length of a time interval from a starting to a terminating event, in this paper we present a framework of statistical tools, appropriate for studying and modeling the distribution of the duration. The idea for our approach comes from the parallelism of duration to the life of an entity which is frequently studied in biostatistics by a certain statistical methodology known as survival analysis. This type of analysis offers great flexibility in modeling the duration and in computing various statistics useful for inference and estimation. As in any other statistical methodology, the approach is based on datasets of measurements on projects. However, one of the most important \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:ULOm3_A8WrAC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Prioritized test-driven reverse engineering process: A case study",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7388099/",
            "Abstract": "In this study we empirically investigate the adaptation of Test-Driven Development (TDD) practice into software Reverse Engineering (RE) process. We call this adaptation as Test-Driven Reverse Engineering (TDRE) process. We propose a two-layer prioritization process, which firstly prioritizes the already-implemented functionalities using the Cumulative Voting (CV) method and three prioritization criteria (importance, complexity and dependency), and secondly prioritizes test-cases for each prioritized functionality, using the same criteria. We conducted a case study in academia with students to empirically evaluate the usability and effectiveness of the prioritization process and the TDD adaptation into RE process. The results have shown that students with a good performance in testing had also good performance in designing UML class-diagrams. Moreover, the implementation of hierarchical test-cases for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:evX43VCCuoAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Validation and interpretation of Web users\u2019 sessions clusters",
            "Publication year": 2007,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0306457306001889",
            "Abstract": "Understanding users\u2019 navigation on the Web is important towards improving the quality of information and the speed of accessing large-scale Web data sources. Clustering of users\u2019 navigation into sessions has been proposed in order to identify patterns and similarities which are then managed in the context of Web users oriented applications (searching, e-commerce, etc.). This paper deals with the problem of assessing the quality of user session clusters in order to make inferences regarding the users\u2019 navigation behavior. A common model-based clustering algorithm is used to result in clusters of Web users\u2019 sessions. These clusters are validated by using a statistical test, which measures the distances of the clusters\u2019 distributions to infer their dissimilarity and distinguishing level. Furthermore, a visualization method is proposed in order to interpret the relation between clusters. Using real data sets, we illustrate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:_FxGoFyzp5QC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Statistical Analysis in The Research of Human Factor in Software Engineering: Invited Talk Paper",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3351556.3351593",
            "Abstract": "The role of the human factor in software engineering is critical and complicated. The stochastic nature of the human factor and the processes in all software life cycle require systematic data collection and sound statistical analysis and modeling. For almost two decades a variety of problems and methods related to the human factor on software engineering were the main subject of research for of the STAINS (Statistics and Information Systems) research group at School of Informatics, Aristotle University of Thessaloniki. This article presents a review of these research efforts in the area known as empirical software engineering.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:YohjEiUPhakC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Modeling the relationship between software effort and size using Deming regression",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1868328.1868339",
            "Abstract": "Background: The relation between software effort and size has been modeled in literature as exponential, in the sense that the natural logarithm of effort is expressed as a linear function of the logarithm of size. The common approach to estimate the parameters of the linear model is ordinary least squares regression which has been extensively applied to various datasets. The least squares estimation takes into account only the error arising from the dependent variable (effort), while the measurement of independent variable (size) is considered free of errors.Aims: The basis of the study is that in practice the assumption of measuring the size without error is hardly true, since the size of a software project depends on the precision of the tool of measurement and often by the subjectivity of the rater. Moreover, the sizes of projects comprising a dataset have been measured by different measurement tools and this adds \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:M3NEmzRMIkIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Predicting fault-prone software components in telecommunication assembly patches with imbalanced datasets",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7034592/",
            "Abstract": "Software quality is imperative in the telecommunication industry. This has led to research towards the development of classification models, able to give a pre-release prediction of whether a software module is faulty or not. However, the data collected within this domain are typically imbalanced. This causes bias on the performance of any prediction model. In this study a hybrid framework for evaluating the fault-proneness of telecommunication software components in a highly imbalanced setting is presented.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:dQ2og3OwTAUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Competence assessment as an expert system for human resource management: A mathematical approach",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0957417416305875",
            "Abstract": "Efficient human resource management needs accurate assessment and representation of available competences as well as effective mapping of required competences for specific jobs and positions. In this regard, appropriate definition and identification of competence gaps express differences between acquired and required competences. Using a detailed quantification scheme together with a mathematical approach is a way to support accurate competence analytics, which can be applied in a wide variety of sectors and fields. This article describes the combined use of software technologies and mathematical and statistical methods for assessing and analyzing competences in human resource information systems. Based on a standard competence model, which is called a Professional, Innovative and Social competence tree, the proposed framework offers flexible tools to experts in real enterprise environments \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:t7zJ5fGR-2UC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Ranking and clustering software cost estimation models through a multiple comparisons algorithm",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6235961/",
            "Abstract": "Software Cost Estimation can be described as the process of predicting the most realistic effort required to complete a software project. Due to the strong relationship of accurate effort estimations with many crucial project management activities, the research community has been focused on the development and application of a vast variety of methods and models trying to improve the estimation procedure. From the diversity of methods emerged the need for comparisons to determine the best model. However, the inconsistent results brought to light significant doubts and uncertainty about the appropriateness of the comparison process in experimental studies. Overall, there exist several potential sources of bias that have to be considered in order to reinforce the confidence of experiments. In this paper, we propose a statistical framework based on a multiple comparisons algorithm in order to rank several cost \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:SP6oXDckpogC",
            "Publisher": "IEEE"
        },
        {
            "Title": "ACID Sim Tools: A simulation framework for distributed transaction processing architectures",
            "Publication year": 2008,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.5495&rep=rep1&type=pdf",
            "Abstract": "Modern network centric information systems implement highly distributed architectures that usually include multiple application servers. Application design is mainly based on the fundamental object-oriented principles and the adopted architecture matches the logical decomposition of applications (into several tiers like presentation, logic and data) to their software and hardware structuring. The provided recovery solutions ensure an at-mostonce service request processing by an existing transaction processing infrastructure. However, in published works performance evaluation of transaction processing aspects is focused on the computational model of database servers. Also, there are no available tools which enable exploring the performance and availability trade-offs that arise when applying different combinations of concurrency control, atomic commit and recovery protocols. This paper introduces ACID Sim Tools, a publicly available tool and at the same time an open source framework for interactive and batch-mode simulation of transaction processing architectures that adopt the basic assumptions of an object-based computational model.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:ZeXyd9-uunAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An investigation of software effort phase distribution using compositional data analysis",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6328177/",
            "Abstract": "One of the most significant problems faced by project managers is to effectively distribute the project resources and effort among the various project activities. Most importantly, project success depends on how well, or how balanced, the work effort is distributed among the project phases. This paper aims to obtain useful information regarding the correlation of the composition of effort attributed in phases for around 1,500 software projects of the ISBSG R11 database based on a promising statistical method called Compositional Data Analysis (CoDA). The motivation for applying this analysis is the observation that certain types of project data (effort distributions and attributes) do not relate in a direct way but present a spurious correlation. Effort distribution is compared to the project life-cycle activities, organization type, language type, function points and other prime project attributes. The findings are beneficial for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:WbkHhVStYXYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "2012 IEEE 35th Software Engineering Workshop SEW 2012",
            "Publication year": 0,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6479794/",
            "Abstract": "Table of contents Page 1 2012 IEEE 35th Software Engineering Workshop SEW 2012 Table \nof Contents Foreword....................................................................................................................................................................vii \nConference Committee / Reviewers....................................................................................................................viii \nAnalysis Analyzing Measurements of the R Statistical Open Source Software ...........................................................................1 \nSophia Voulgaropoulou, Georgios Spanos, and Lefteris Angelis A Comment Analysis \nApproach for Program Comprehension ....................................................................................11 \nJos\u00e9 Lu\u00eds Freitas, Daniela da Cruz, and Pedro Rangel Henriques Investigating Automatic \nStatic Analysis Results to Identify Quality Problems: An Inductive Study .............................................................................................................................................................21 \nAntonio Vetro, Nico Zazworka, Forrest Shull, Carolyn Seaman, A. ..\u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:4fKUyHm3Qg0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Applied multiresponse metamodeling for queuing network simulation experiments: problems and perspectives",
            "Publication year": 2001,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.3691&rep=rep1&type=pdf",
            "Abstract": "A complete performance evaluation study of a simulated system should consider possible alternatives and response predictions to potential parameter changes. Simulation sensitivity analysis and metamodeling constitute an efficient approach for this kind of problems. However, this approach is usually despised, mainly because, a sophisticated methodological treatment is required. Such a methodology should take into account, peculiarities, inherent to queuing network models, as for example, multiple responses, large number of model parameters, many qualitative parameters etc. This work aims to illustrate the combined use of the proper statistical techniques to cope with this sort of problems and to show the need for a sound methodological framework that will bring this approach closer to the queuing network simulation practice.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:FAceZFleit8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Estimating the development cost of custom software",
            "Publication year": 2003,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S037872060200099X",
            "Abstract": "In this paper an approach for the estimation of software development costs is presented. The method is based on the characterization of the software to be developed in terms of project and environment attributes and comparison with some similar completed project(s) recovered from a historical database. A case study is also presented, focusing on the calibration and application of the method on 59 information systems implementing supply chain functions in industry. Various strategies are explored, the best of which predicted effort quite effectively, with a mean estimation error of 24% with respect to the actual effort.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Integrating non-parametric models with linear components for producing software cost estimations",
            "Publication year": 2015,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121214002088",
            "Abstract": "A long-lasting endeavor in the area of software project management is minimizing the risks caused by under- or over-estimations of the overall effort required to build new software systems. Deciding which method to use for achieving accurate cost estimations among the many methods proposed in the relevant literature is a significant issue for project managers. This paper investigates whether it is possible to improve the accuracy of estimations produced by popular non-parametric techniques by coupling them with a linear component, thus producing a new set of techniques called semi-parametric models (SPMs). The non-parametric models examined in this work include estimation by analogy (EbA), artificial neural networks (ANN), support vector machines (SVM) and locally weighted regression (LOESS). Our experimentation shows that the estimation ability of SPMs is superior to their non-parametric \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:eq2jaN3J8jMC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A Controlled Experiment Concerning Traditional and Distance Learning of UML Sequence Diagram",
            "Publication year": 2003,
            "Publication url": "https://www.academia.edu/download/48977526/C7-SfetsosUML.pdf",
            "Abstract": "Modern software companies are faced with the problem of training or updating their software engineers in UML topics. Because of limited time availability, often this becomes a difficult and expensive task. The study presented in this paper is a controlled experiment, concerning the ability to learn UML Sequence Diagrams, by means of two different approaches: The Traditional learning approach, learning in a classroom with the presence of a teacher and the Distance learning approach using web-based instructional material. The latter offers more flexibility to a software company, since the trainees need not interrupt their scheduled assignments. The core issue of investigation is whether Distance learning can be as effective as Traditional learning, when a software engineer must learn the UML language. A controlled experiment, comparing the two learning approaches, was performed with the participation of 49 students, separated into two groups. The results of the study show that there is no statistically significant difference between the two groups, indicating that Traditional place-based and Distance learning courses can both be effective methods for delivering course information. Therefore, the Distance learning approach seems to be appropriate for teaching UML.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:nrtMV_XWKgEC",
            "Publisher": "Department of Informatics in the Aristotle University of Thessaloniki, forthcoming"
        },
        {
            "Title": "Assessment of vulnerability severity using text mining",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3139367.3139390",
            "Abstract": "Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:g3aElNc5_aQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Bayesian belief network cost estimation model that incorporates cultural and project leadership factors",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5356306/",
            "Abstract": "In this study, an analysis is performed in order to explore whether and how culture and leadership factors have an impact on the accuracy of software effort and cost estimation. A survey on software development projects within government departments in the United Arab Emirates (UAE) was undertaken. A Bayesian Belief Network (BBN) cost estimation model incorporating organizational and intercultural factors was developed and evaluated. The results indicated that the inclusion of such data into explanatory estimation models such as BBNs could provide useful information and increase the accuracy of final estimates.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:ldfaerwXgEUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Bayesian belief networks as a software productivity estimation tool",
            "Publication year": 2003,
            "Publication url": "https://www.academia.edu/download/30803925/10.1.1.108.7639.pdf",
            "Abstract": "Defining the required productivity in order to complete successfully and within time and budget constraints a software development project is actually a reasoning problem that should be modelled under uncertainty. The contribution of this paper is the analysis of the applicability of probabilistic reasoning approaches, in particular Bayesian Belief Networks (BBN), to this problem. BBNs are capable of discovering the dependencies and independencies among the attributes of a project and defining the direct impact of some of them on productivity. Uncertainty is depicted through the use of estimate intervals and probabilities: the estimation is actually an interval within which the productivity of a project is likely to fall in, with a certain probability, considering both an optimistic and a pessimistic situation. The use of predefined intervals is another important feature of the method, allowing the control of the estimation process and the generation of meaningful intervals, appealing and understood by software managers. The ability of the method to classify correctly the rest of the attributes in one of their discrete values is also tested, paying further attention on the software development mode. The method is applied and evaluated on the widely known COCOMO81 dataset. The evaluation shows that BBN is a promising method whose results can be confirmed intuitively. BBN are easily interpreted, allow flexibility in the estimation, can support expert judgment and create models considering all the information that lay in a dataset by including all productivity factors in the final model.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:kNdYIx-mwKoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "It is my great pleasure to welcome you to the main Euromicro event 2019, hosting the parallel conferences SEAA (45th Euromicro Conference on Software Engineering and Advanced Applications) and DSD (22nd Euromicro Conference on Digital System Design) in Kallithea, Chalkidiki, Greece. These two well-reputed and long-standing conferences are organized jointly so as to",
            "Publication year": 0,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8875051/",
            "Abstract": "The following topics are dealt with: field programmable gate arrays; embedded systems; logic design; Internet of Things; multiprocessing systems; cryptography; learning (artificial intelligence); formal specification; formal verification; fault tolerance.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:GtLg2Ama23sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Construction of generalized binary proper efficiency-balanced block designs with two different replication numbers",
            "Publication year": 1994,
            "Publication url": "https://www.jstor.org/stable/25052843",
            "Abstract": "Necessary and sufficient conditions for a generalized binary proper block design with two different replication numbers to be efficiency-balanced (EB) are given. Binary block designs such as balanced incomplete block designs, group divisible designs, balanced treatment incomplete block designs and balanced bipartite block designs are used here to construct generalized binary proper EB designs with two different replication numbers whose existence has been an open problem.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:KxtntwgDAa4C",
            "Publisher": "Indian Statistical Institute"
        },
        {
            "Title": "Understanding specialized ribosomal protein functions and associated ribosomopathies by navigating across sequence, literature, and phenotype information resources",
            "Publication year": 2019,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/B9780128095560000034",
            "Abstract": "The ubiquitously expressed ribosomal proteins (RPs) constitute essential components of the ribosome, the organelle responsible for protein production in the cell. Historically, these proteins have been solely viewed as building blocks necessary for maintaining the structural integrity of the translational machinery. Recently, however, many studies have emerged challenging this traditional notion. Through various interactions with specific proteins and mRNA cis-regulatory elements, ribosomal proteins exert unforeseen control over translation and cellular homeostasis. Moreover, perturbation of their expression levels results in the development of diseases with highly tissue-specific symptoms, commonly known as ribosomopathies. The present study aims to describe the multifunctional roles of RPs in the biomedical literature and their association with disease phenotypes. We show that the level of literature coverage \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:q3CdL3IzO_QC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Automatic Identification of Skills' Dependency",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3351556.3351580",
            "Abstract": "Paper introduces a new approach of extracting relation interdependences of human skills hidden in large data sets. The dependence is presented by If-Then fuzzy rules. It is enable both to grasp the relation and to account for the appeared expertise levels. The theoretical achievements are proved by detecting dependencies between different types of sport skills. The extracted knowledge is useful in managing and selecting a successful play team.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:rmuvC79q63oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Clinical Impact of Stereotyped Antigen Receptors in Chronic Lymphocytic Leukemia",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0006497119721958",
            "Abstract": "In chronic lymphocytic leukemia (CLL), the molecular features of the clonotypic B-cell receptor immunoglobulins (BcR IG) are set from the birth of the clone and in contrast to genetic aberrations, remain stable overtime, rendering the BcR IG a reference biomarker that is usually not significantly affected by clonal evolution. Approximately 30% of CLL cases carry quasi-identical BcR IGs and can be assigned to distinct stereotyped subsets. While preliminary evidence alludes to BcR IG stereotypy being relevant from a clinical viewpoint, this aspect has never been explored systematically or in a cohort of adequate size to enable meaningful conclusions. In order to assess the clinical implications of BcR IG stereotypy, we evaluated clinicobiological data from 8593 CLL patients, particularly focusing on 14 major stereotyped subsets of cases with unmutated (U-CLL) or mutated IGHV genes (M-CLL). The largest subset was \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:UHK10RUVsp4C",
            "Publisher": "Content Repository Only!"
        },
        {
            "Title": "Software productivity and effort prediction with ordinal regression",
            "Publication year": 2005,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584904000837",
            "Abstract": "In the area of software cost estimation, various methods have been proposed to predict the effort or the productivity of a software project. Although most of the proposed methods produce point estimates, in practice it is more realistic and useful for a method to provide interval predictions. In this paper, we explore the possibility of using such a method, known as ordinal regression to model the probability of correctly classifying a new project to a cost category. The proposed method is applied to three data sets and is validated with respect to its fitting and predictive accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Towards individualized software engineering: empirical studies should collect psychometrics",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1370114.1370127",
            "Abstract": "Even though software is developed by humans, research in software engineering primarily focuses on the technologies, methods and processes they use while disregarding the importance of the humans themselves. In this paper we argue that most studies in software engineering should give much more weight to human factors. In particular empirical software engineering studies involving human developers should always consider collecting psychometric data on the humans involved. We focus on personality as one important psychometric factor and present initial results from an empirical study investigating correlations between personality and attitudes to software engineering processes and tools. We discuss what are currently hindering a more wide-spread use of psychometrics and how overcoming these hurdles could lead to a more individualized software engineering.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:0EnyYjriUFMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Formally assessing an instructional tool: A controlled experiment in software engineering",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1393114/",
            "Abstract": "This work describes a controlled experiment concerning the use of a learning aid during the instructional procedure. The core issue of investigation is whether this instructional aid can augment the cognitive transfer of the learners by personalizing the offered knowledge. For this purpose, a controlled experiment was conducted with the participation of 79 undergraduate students. The taught domain was two lessons concerning human-computer interaction: the first in usability engineering and the second in interface evaluation methodologies. A test session was also conducted to collect data on the assessment of the augmentation of the students' knowledge on the domain. Descriptive and inferential statistics were applied to the collected data to test the research hypotheses. The results showed that with regard to the transfer of simple information, this \"lesson sheet\" does not provide any statistically significant \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:4JMBOYKVnBMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Human papillomavirus E7 protein detection as a method of triage to colposcopy of HPV positive women, in comparison to genotyping and cytology. Final results of the PIPAVIR study",
            "Publication year": 2017,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ijc.30761",
            "Abstract": "The objective of the presented cross\u2010sectional\u2010evaluation\u2010screening study is the clinical evaluation of high\u2010risk(hr)HPVE7\u2010protein detection as a triage method to colposcopy for hrHPV\u2010positive women, using a newly developed sandwich\u2010ELISA\u2010assay. Between 2013\u20102015, 2424 women, 30\u201060 years old, were recruited at the Hippokratio Hospital, Thessaloniki/Greece and the Im Mare Klinikum, Kiel/Germany, and provided a cervical sample used for Liquid Based Cytology, HPV DNA genotyping, and E7 detection using five different E7\u2010assays: \u201crecomWell HPV16/18/45KJhigh\u201d, \u201crecomWell HPV16/18/45KJlow\u201d, \u201crecomWell HPV39/51/56/59\u201d, \u201crecomWell HPV16/31/33/35/52/58\u201d and \u201crecomWell HPVHRscreen\u201d (for 16,18,31,33,35,39,45,51,52,56,58,59 E7), corresponding to different combinations of hrHPVE7\u2010proteins. Among 1473 women with eligible samples, those positive for cytology (ASCUS+ 7.2%), and/or \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:Z5m8FVwuT1cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using repository of repositories (RoRs) to study the growth of F/OSS projects: A meta-analysis research approach",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-0-387-72486-7_12",
            "Abstract": "Free/Open Source Software (F/OSS) repositories contain valuable data and their usefulness in studying software development and community activities continues to attract a lot of research attention. A trend in F/OSS studies is the use of metadata stored in a repository of repositories or RoRs. This paper utilizes data obtained from such RoRs -FLOSSmole- to study the types of projects being developed by the F/OSS community. We downloaded projects by topics data in five areas (Database, Internet, Software Development, Communications, and Games/Entertainment) from Flossmole\u2019s raw and summary data of the sourceforge repository. Time series analysis show the numbers of projects in the five topics are growing linearly. Further analysis supports our hypothesis that F/OSS development is moving \u201cup the stack\u201d from developer tools and infrastructure support to end-user applications such as Databases \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:roLk4NBRz8UC",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "Model-based cluster analysis for web users sessions",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11425274_23",
            "Abstract": "One of the main issues in Web usage mining is the discovery of patterns in the navigational behavior of Web users. Standard approaches, such as clustering of users\u2019 sessions and discovering association rules or frequent navigational paths, do not generally allow to characterize or quantify the unobservable factors that lead to common navigational patterns. Therefore, it is necessary to develop techniques that can discover hidden and useful relationships among users as well as between users and Web objects. Correspondence Analysis (CO-AN) is particularly useful in this context, since it can uncover meaningful associations among users and pages. We present a model-based cluster analysis for Web users sessions including a novel visualization and interpretation approach which is based on CO-AN.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:ufrVoPGSRksC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Alternative m-estimators of location and their linear convex combinations",
            "Publication year": 1999,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/03610929908832280",
            "Abstract": "Some alternative M estimators of location are studied. Their \u03c8-functions are defined by a single algebraic expression for all values of the independent variable and they combine good robustness and performance properties. A re-descending M-estiriiator and an M-estimator with monotone \u03c8 are introduced and their linear convex combinations are considered. As a result a class of estimators decreasing to a positive number is obtained with 50% breakdown point. The breakdown point of this class is calculated via a generalization of a Huber's theorem. A simulation study was performed in order to examine the finite sample behavior of the proposed estimators.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:CHSYGLWDkRkC",
            "Publisher": "Marcel Dekker, Inc."
        },
        {
            "Title": "A-optimal incomplete block designs with unequal block sizes for comparing test treatments with a control",
            "Publication year": 1991,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/037837589190072M",
            "Abstract": "A-optimal designs for comparing v test treatments with a control in b blocks of unequal sizes are considered. A new class of designs is defined, namely the BTIUB designs, which can be considered as an extension of the BTIB designs of Bechhofer and Tamhane in the case of blocks with unequal sizes. Some conditions for the existence and construction of BTIUB designs are given. Finally an algorithm for the construction of A-optimal BTIUB designs is developed and it is applied in the case of two block sizes. Tables of some A-optimal BTIUB designs are given.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:xtRiw3GOFMkC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "SEAA 2018",
            "Publication year": 0,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8498149/",
            "Abstract": "[Title page iii] - IEEE Conference Publication IEEE.org IEEE Xplore IEEE-SA IEEE Spectrum \nMore Sites Create Account Personal Sign In Personal Sign In For IEEE to continue sending \nyou helpful information on our products and services, please consent to our updated Privacy \nPolicy. I have read and accepted the IEEE Privacy Policy. Accept & Sign In Email Address \nPassword Sign In Forgot Password? [Title page iii] Abstract: Presents the title page of the \nproceedings record. Published in: 2018 44th Euromicro Conference on Software Engineering \nand Advanced Applications (SEAA) Article #: Date of Conference: 29-31 Aug. 2018 Date \nAdded to IEEE Xplore: 22 October 2018 ISBN Information: Electronic ISBN: 978-1-5386-7383-6 \nPrint on Demand(PoD) ISBN: 978-1-5386-7384-3 INSPEC Accession Number: Persistent Link: \nhttps://xplorestaging.ieee.org/servlet/opac?punumber=8496898 More \u00bb Publisher: IEEE IEEE /: \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:HbR8gkJAVGIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Formal Evaluation of an Instructional ODL Tool",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1652493/",
            "Abstract": "This paper presents the application and evaluation by means of a controlled experiment of an instructional tool during an open and distance learning (ODL) course. The core issue of investigation is whether this instructional aid can support, guide and scaffold the distant student in his/her study. For this purpose, a controlled experiment was conducted with the participation of 191 undergraduate students. Descriptive statistics as well as a variety of statistical methods have been applied to the collected data, in order to test the research hypotheses. The results have shown a statistical significant difference in performance for the student group that used the tool. Finally, concerns about the application of the tool in a broader context and further research on the area are also presented",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:M05iB0D1s5AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Diagnostic accuracy of high-risk HPV DNA genotyping for primary cervical cancer screening and triage of HPV-positive women, compared to cytology: preliminary results of the PIPAVIR study",
            "Publication year": 2017,
            "Publication url": "https://search.proquest.com/openview/c8b815b96696d5cb37d71498da9d9a18/1?pq-origsite=gscholar&cbl=4408561",
            "Abstract": "PurposeThe purpose of the presented PIPAVIR (persistent infections with human papillomaviruses; http://www. pipavir. com) subanalysis is to assess the performance of high-risk (hr) HPV-DNA genotyping as a method of primary cervical cancer screening and triage of HPV positive women to colposcopy compared to liquid-based cytology (LBC) in an urban female population.MethodsWomen, aged 30\u201360, provided cervicovaginal samples at the Family-Planning Centre, Hippokratio Hospital of Thessaloniki, Greece, and the Department of Gynecology and Obstetrics in Mare Klinikum, Kiel, Germany. Cytology and HPV genotyping was performed using LBC and HPV Multiplex Genotyping (MPG), respectively. Women positive for cytology [atypical squamous cells of undetermined significance (ASC-US) or worse] or hrHPV were referred for colposcopy.ResultsAmong 1723/1762 women included in the final analysis \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:vDijr-p_gm4C",
            "Publisher": "Springer"
        },
        {
            "Title": "Temporal trends in chronic Total occlusion interventions in Europe: 17 626 procedures from the European registry of chronic Total occlusion",
            "Publication year": 2018,
            "Publication url": "https://www.ahajournals.org/doi/abs/10.1161/CIRCINTERVENTIONS.117.006229",
            "Abstract": "The study focuses on the evolution of practice, procedural outcomes, and in-hospital complications of chronic total occlusion percutaneous coronary intervention in Europe.Data from 17\u2009626 procedures enrolled in European Registry of Chronic Total Occlusion between January 2008 and June 2015 were assessed. The mean patient age was 63.9\u00b110.9 years; 85% were men. Procedural success increased from 79.7% to 89.3% through the study period. Patients enrolled during the years had increasing comorbidities and lesion complexity (J-CTO score [Multicenter CTO Registry of Japan] increased from 1.76\u00b11.03 in 2008 to 2.17\u00b10.91 in 2015; P for trend, <0.001). Retrograde approach utilization steadily increased from 10.1% in 2008 to 29.9% in 2015 (P for trend, <0.001). Antegrade dissection reentry adoption was low, not exceeding 5.5%. In-hospital mortality decreased \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:S16KYo8Pm5AC",
            "Publisher": "Lippincott Williams & Wilkins"
        },
        {
            "Title": "Heterogeneous Personalities Perform Better in Pair Programming: The Results of a Replication Study.",
            "Publication year": 2013,
            "Publication url": "http://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=15220540&AN=95033290&h=LoMYPG5UYxDl3RikldwLKkVi%2BpOYveBGONlUifHyZt8cqwLKiSApDqi5KGcOZcxfOD6phazgGjGoDU2zCYK%2F%2Bg%3D%3D&crl=c",
            "Abstract": "This article presents the results of an extended replicated pair programming experiment aiming to test, as in the first experiment, the differences of software developers' performance due to the impact of personality composition of pairs. The experiment focused on comparing pairs of developers with heterogeneous personalities and pairs with homogeneous personalities. The data were obtained from a controlled experiment on 80 students during the winter semester of 2009-2010. Performance was measured in terms of communication, time to complete assignments, and overall score. The main variables were statistically analyzed in order to test the significance of the differences using univariate tests, separately for each variable, and the multivariate discriminant analysis for all variables together. The results of the study confirmed that pairs consisting of heterogeneous personalities performed better than pairs with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:V3AGJWp-ZtQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A framework for capturing, statistically modeling and analyzing the evolution of software models",
            "Publication year": 2016,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121216300437",
            "Abstract": "This paper presents a new methodological framework for capturing and statistically modeling the evolution of models in model-driven software development. The framework captures the changes between revisions of models in terms of both low-level (internal) and high-level (developer-visible) edit operations applied between revisions. In our approach, evolution is modeled statistically by using ARMA, GARCH and mixed ARMA-GARCH models. Forecasting and simulation aspects of these time series models are thoroughly assessed. The suitability of the framework is shown by applying it to a large set of design models of real Java systems. Our analysis shows that mixed ARMA-GARCH models are superior to ARMA models.A main motivation for, and application of, the resulting statistical models is to control the generation of realistic model histories which are intended to be used for testing model versioning tools \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:fEOibwPWpKIC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "An experience-based framework for evaluating alignment of software quality goals",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/article/10.1007/s11219-014-9251-5",
            "Abstract": "Efficient quality management of software projects requires knowledge of how various groups of stakeholders involved in software development prioritize the product and project goals. Agreements or disagreements among members of a team may originate from inherent groupings, depending on various professional or other characteristics. These agreements are not easily detected by conventional practices (discussions, meetings, etc.) since the natural language expressions are often obscuring, subjective, and prone to misunderstandings. It is therefore essential to have objective tools that can measure the alignment among the members of a team; especially critical for the software development is the degree of alignment with respect to the prioritization goals of the software product. The paper proposes an experience-based framework of statistical and graphical techniques for the systematic study of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:dTyEYWd-f8wC",
            "Publisher": "Springer US"
        },
        {
            "Title": "A novel feature selection method based on comparison of correlations for human activity recognition problems",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s12652-020-01836-z",
            "Abstract": "In human activity recognition studies it is important to identify an optimal set with the minimum number of features that will potentially improve the recognition rate. In the current paper we introduce a promising feature selection method that exploits the differences on the correlation structure of the features, between the different classes of the target variable. Using the recordings of triaxial accelerometers and gyroscopes, we extracted several features and created subsets according to the activities performed. For each subset, we calculated the pairwise correlation coefficients of the features and compared the feature correlations of different subsets. By identifying the significantly different correlations we ranked the variables participating in those correlations based on their frequency of appearance and thus created a subset of features that will optimize the performance of a classification algorithm. The method \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:4hFrxpcac9AC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Software technologies skills",
            "Publication year": 2019,
            "Publication url": "https://datalab-old.csd.auth.gr/wp-content/uploads/publications/GPVA19.pdf",
            "Abstract": "Software design and development technologies evolve very fast and in unpredicted rates, posing many challenges for programmers who strive to use them properly and to be up-todate, especially since software development demands teamwork and collaboration. As a result, Question and Answer (Q&A) sites, like Stack Overflow, have seen large growth. The questions are characterized by tags, which support developers to easily trace their topic of interest. Very often, these tags refer to technologies that are connected or serve the same purpose. This work is motivated by the fact that despite the volume of questions and technologies change over time, tags inter-connections carry insightful information since they can be utilized to monitor the technology trends and their dynamics given technologies fast simultaneous evolution over time. This work recognizes the value of such connections, to reveal associations of technologies and to support the scientific community, which can be highly inspired by this study, using advanced data analysis tools. Benefits of the present study include early notification of the labor market needs, competency discovery and directions for educational planning. In the present study, graph theory principles and tools were employed to profile chronologically the evolution and the associations of technologies related to tags, and the experimentation carried out has involved the entire set of Stack Overflow questions posted during a decade, between 2008 and 2018. Very interesting conclusions are summarized based on the tags related to Computer Science Technologies (Hard Skills) analysis, revealing non-evident software \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:DJbcl8HfkQkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Discovering Knowledge in Scientific Publications: Potential for Supporting Personalized Medicine Decisions",
            "Publication year": 2014,
            "Publication url": "https://scholar.google.com/scholar?cluster=12162853784048900577&hl=en&oi=scholarr",
            "Abstract": "Personalized medicine involves decision making, based on patientspecific information from various sources, toward the development and application of treatments tailored to individual patients. This process also requires expert knowledge, primarily and painstakingly stored in the form of scientific publications. The biomedical scientific literature is considered one of the most important sources of such knowledge, if not the most important, in terms of the number and type of information it contains. Consequently, it is an invaluable",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:35r97b3x0nAC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "A probabilistic validation algorithm for web users' clusters",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1401178/",
            "Abstract": "Cluster analysis is one of the most important aspects in the data mining process for discovering groups and identifying interesting distributions or patterns over the considered data sets. In the context of Web data mining, model-based clustering algorithms are often used to cluster similar users' sessions in order to determine Website access behaviors. An important issue in cluster analysis is the evaluation of clustering results to find the partitioning that best fits the underlying data. In this paper, we present a novel validation technique for model based clustering approaches.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:-f6ydRqryjwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Ahmed, BN, see Bindal, A., TE May 05 279-289 Allen, E., see Parent, D., TE Aug 05 497-502 Aller, BM, AA Kline, E. Tsang, R. Aravamuthan, AC Rasmusson, and C. Phillips. WeBAL: A Web-based assessment library to enhance teaching and learning in",
            "Publication year": 0,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1532389/",
            "Abstract": "This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:EUQCXRtRnyEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dynamical simulation models of the open source development process",
            "Publication year": 2005,
            "Publication url": "https://www.igi-global.com/chapter/free-open-source-software-development/18725",
            "Abstract": "This chapter will discuss attempts to produce formal mathematical models for dynamical simulation of the development process of Free/Open Source Software (F/OSS) projects. First, a brief overview for simulation methods of closed source software development is given. Then, based on empirical facts reported in F/OSS case studies, we describe a general framework for F/OSS dynamical simulation models and discuss its similarities and differences to closed source software simulation. A specific F/OSS simulation model is introduced. The model is applied to the Apache project and to the gtk+ module of the GNOME project, and simulation outputs are compared to real data. The potential of formal F/OSS simulation models to turn into practical tools used by F/OSS coordinators to predict key project factors is demonstrated. Finally, issues for further research and efforts for improvement of this first-attempt model are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:VaXvl8Fpj5cC",
            "Publisher": "Igi Global"
        },
        {
            "Title": "A multi-target approach to estimate software vulnerability characteristics and severity scores",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121218302061",
            "Abstract": "Software vulnerabilities constitute a great risk for the IT community. The specification of the vulnerability characteristics is a crucial procedure, since the characteristics are used as input for a plethora of vulnerability scoring systems. Currently, the determination of the specific characteristics -that represent each vulnerability- is a process that is performed manually by the IT security experts. However, the vulnerability description can be very informative and useful to predict vulnerability characteristics. The primary goal of this research is the enhancement, the acceleration and the support of the manual procedure of the vulnerability characteristic assignment. To achieve this goal, a model, which combines texts analysis and multi-target classification techniques was developed. This model estimates the vulnerability characteristics and subsequently, calculates the vulnerability severity scores from the predicted \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:oNZyr7d5Mn4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A controlled experiment investigation of an object-oriented design heuristic for maintainability",
            "Publication year": 2004,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121203002401",
            "Abstract": "The study presented in this paper is a controlled experiment, aiming at investigating the impact of a design heuristic, dealing with the `god class' problem, on the maintainability of object-oriented designs. In other words, we wish to better understand to what extent a specific design heuristic contributes to the quality of designs developed. The experiment has been conducted using undergraduate students as subjects, performing on two system designs using the Coad & Yourdon method. The results of this study provide evidence that the investigated design heuristic: (a) affects the evolution of design structures; and (b) considerably affects the way participants apply the inheritance mechanism.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:LkGwnXOMwfcC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Building a software cost estimation model based on categorical data",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/915511/",
            "Abstract": "The paper explores the possibility of generating a multi-organisational software cost estimation model by analysing the software cost data collected by the International Software Benchmarking Standards Group. This database contains data about recently developed projects characterised mostly by attributes of categorical nature such as the project business area, organisation type, application domain and usage of certain tools or methods. The generation of the model is based on a statistical technique which has been proposed as alternative to the standard regression approach, namely the categorical regression or regression with optimal scaling. This technique starts with the quantification of the qualitative attributes (expressed either on nominal or ordinal scale), that appear frequently within such data, and proceeds by using the obtained scores as independent variables of a regression model. The generated \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:d1gkVwhDpl0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The GEnetic Syntax Score: a genetic risk assessment implementation tool grading the complexity of coronary artery disease\u2014rationale and design of the GESS study",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/article/10.1186/s12872-021-02092-5",
            "Abstract": "Coronary artery disease (CAD) remains one of the leading causes of mortality worldwide and is associated with multiple inherited and environmental risk factors. This study is designed to identify, design, and develop a panel of genetic markers that combined with clinical and angiographic information, will facilitate the creation of a personalized risk prediction algorithm (GEnetic Syntax Score\u2014GESS). GESS score could be a reliable tool for predicting cardiovascular risk for future adverse events and for guiding therapeutic strategies.  GESS (ClinicalTrials.gov Identifier: NCT03150680) is a prospective, non-interventional clinical study designed to enroll 1080 consecutive patients with no prior history of coronary revascularization procedure, who undergo scheduled or emergency coronary angiography in AHEPA, University General Hospital of Thessaloniki. Next generation sequencing (NGS) technology will be used to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:HIFyuExEbWQC",
            "Publisher": "BioMed Central"
        },
        {
            "Title": "Diagnostic accuracy of high-risk HPV DNA genotyping for primary cervical cancer screening and triage of HPV-positive women, compared to cytology: preliminary results of the PIPAVIR study",
            "Publication year": 2017,
            "Publication url": "https://search.proquest.com/openview/c8b815b96696d5cb37d71498da9d9a18/1?pq-origsite=gscholar&cbl=4408561",
            "Abstract": "PurposeThe purpose of the presented PIPAVIR (persistent infections with human papillomaviruses; http://www. pipavir. com) subanalysis is to assess the performance of high-risk (hr) HPV-DNA genotyping as a method of primary cervical cancer screening and triage of HPV positive women to colposcopy compared to liquid-based cytology (LBC) in an urban female population.MethodsWomen, aged 30\u201360, provided cervicovaginal samples at the Family-Planning Centre, Hippokratio Hospital of Thessaloniki, Greece, and the Department of Gynecology and Obstetrics in Mare Klinikum, Kiel, Germany. Cytology and HPV genotyping was performed using LBC and HPV Multiplex Genotyping (MPG), respectively. Women positive for cytology [atypical squamous cells of undetermined significance (ASC-US) or worse] or hrHPV were referred for colposcopy.ResultsAmong 1723/1762 women included in the final analysis \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:epqYDVWIO7EC",
            "Publisher": "Springer"
        },
        {
            "Title": "DD-EbA: An algorithm for determining the number of neighbors in cost estimation by analogy using distance distributions",
            "Publication year": 2010,
            "Publication url": "https://arxiv.org/abs/1012.5755",
            "Abstract": "Case Based Reasoning and particularly Estimation by Analogy, has been used in a number of problem-solving areas, such as cost estimation. Conventional methods, despite the lack of a sound criterion for choosing nearest projects, were based on estimation using a fixed and predetermined number of neighbors from the entire set of historical instances. This approach puts boundaries to the estimation ability of such algorithms, for they do not take into consideration that every project under estimation is unique and requires different handling. The notion of distributions of distances together with a distance metric for distributions help us to adapt the proposed method (we call it DD-EbA) each time to a specific case that is to be estimated without loosing in prediction power or computational cost. The results of this paper show that the proposed technique achieves the above idea in a very efficient way.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:blknAaTinKkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Empirical extension of a classification framework for addressing consistency in model based development",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584910001758",
            "Abstract": "Consistency constitutes an important aspect in practical realization of modeling ideas in the process of software development and in the related research which is diverse. A classification framework has been developed, in order to aid the model based software construction by categorizing research problems related to consistency. However, the framework does not include information on the importance of classification elements.The aim was to extend the classification framework with information about the relative importance of the elements constituting the classification. The research question was how to express and obtain this information.A survey was conducted on a sample of 24 stakeholders from academia and industry, with different roles, who answered a quantitative questionnaire. Specifically, the respondents prioritized perspectives and issues using an extended hierarchical voting \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:r0BpntZqJG4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A novel single-trial methodology for studying brain response variability based on archetypal analysis",
            "Publication year": 2015,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0957417415004613",
            "Abstract": "The objective of this paper is to present a methodology for deriving an intelligible synopsis of single-trial (ST) variability in brain responses. An algorithmic procedure, relying on temporal patterning and built over archetypal analysis, is introduced. Archetypical brain waves are first derived from the ensemble of brain responses and then used to unfold the observed variability. Using these archetypes as anchor points, homogeneous groups of ST-responses are detected and contrasted with each other. The new methodology incorporates steps for organizing the variability and presenting it by means of low-dimensional maps. Magnetoencephalographic responses from a visual stimulation paradigm are used for demonstrating and validating the approach. The results show that a small number of archetypes is sufficient for describing reliably the response variability. The groups of ST-responses, delineated around these \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:NJ774b8OgUMC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "SEAA 2019 Committees",
            "Publication year": 0,
            "Publication url": "https://www.computer.org/csdl/proceedings-article/seaa/2019/342100z016/1f8MH0DPgSk",
            "Abstract": "SEAA 2019 Committees IEEE.org Help Cart Jobs Board Create Account Toggle navigation IEEE \nComputer Society Digital Library Jobs Tech News Resource Center Press Room Browse By \nDate Advertising About Us IEEE IEEE Computer Society IEEE Computer Society Digital Library \nMy Subscriptions Magazines Journals Conference Proceedings Institutional Subscriptions \nIEEE IEEE Computer Society More Jobs Tech News Resource Center Press Room Browse \nBy Date Advertising About Us Cart All Advanced Search Conference Cover Image Download \n1.Home 2.Proceedings 3.seaa 2019 SEAA 2019 Committees 2019, pp. 16-16, DOI Bookmark: \n10.1109/SEAA.Keywords Authors SEAA 2019 Committees ,Steering Committee ,Michel \nChaudron, ,Chalmers and Gothenburg University, Sweden ,Onur Demirors, ,Izmir Institute of \nTechnology, Turkey, ,Stefan Biffl, ,Vienna University of Technology, Austria, ,Rick Rabiser, \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:xtoqd-5pKcoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Experimentally assessing a resource\u2010effective design for ODL environments",
            "Publication year": 2004,
            "Publication url": "https://www.emerald.com/insight/content/doi/10.1108/17415650480000028/full/html",
            "Abstract": "This study is concerned with the formal assessment of a Distance Learning Environment (DLE) created to deliver a course on UML sequence diagrams to university\u2010level students, divided into control and treatment groups. An ad\u2010hoc DLE was constructed to deliver instruction to the treatment group, while the control group was taught in a traditional face\u2010to\u2010face way. The main point of concern is whether a DLE can be as effective for the treatment group, as the faceto\u2010 face lecture is for the control group, in terms of gaining mastery on the domain. So, a controlled experiment was organized and executed, in order to measure the participants\u2019 performance in both groups. The results have shown no statistically significant difference for both groups of students. So, it can be argued that in the context of this experiment and by following a DLE\u2010design close enough to the traditional face\u2010to\u2010face approach, one can obtain \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:TQgYirikUcIC",
            "Publisher": "Emerald Group Publishing Limited"
        },
        {
            "Title": "Assessment of Chlamydia psittaci shedding and environmental contamination as potential sources of worker exposure throughout the mule duck breeding process",
            "Publication year": 2016,
            "Publication url": "https://journals.asm.org/doi/abs/10.1128/aem.03179-15",
            "Abstract": "Chlamydia psittaci is an obligate intracellular bacterium responsible for avian chlamydiosis, otherwise known as psittacosis, a zoonotic disease that may lead to severe atypical pneumonia. This study was conducted on seven mule duck flocks harboring asymptomatic birds to explore the circulation and persistence of C. psittaci during the entire breeding process and assess the potential sources of worker exposure. Cloacal swabs and air samples were taken on each occasion requiring humans to handle the birds. In parallel, environmental samples, including dust, water, and soil, were collected. Specific real-time PCR analyses revealed the presence of C. psittaci in all flocks but with three different shedding patterns involving ducks about the age of 4, 8, and 12 weeks with heavy, moderate, and low excretion levels, respectively. Air samples were only positive in flocks harboring heavy shedders. Dust in flocks with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:9pM33mqn1YgC",
            "Publisher": "American Society for Microbiology"
        },
        {
            "Title": "Machine Learning Methods for Customer's Payment Acceptance Prediction in an Electricity Distribution Company",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3139367.3139406",
            "Abstract": "In this short paper, we use machine learning methods to maximise the efficiency in the Hellenic Electricity Distribution Network Operator (HEDNO) SA's project management by predicting which upcoming projects are going to be paid by the customer, thus enabling the company to fetch all needed materials in time, and to avoid unwanted delays in the operations of the company.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:KUbvn5osdkgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The COMALAT approach to individualized E-learning in job-specific language competences",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-662-49275-8_16",
            "Abstract": "COMALAT (Competence Oriented Multilingual Adaptive Language Assessment and Training) project aims to strengthen    the mobility of young workers across Europe, by improving job\u2010specific language competence tailored individually to    particular needs. In this work we will concentrate on the COMALAT learning management system (LMS), which is a language    learning system for Vocational Education and Training (VET). COMALAT LMS aims at providing learning material as an Open    Educational Resource (OER) and is capable of self\u2010adapting to the needs of different learners. Each learner is treated    individually in acquiring new language skills related to job\u2010specific competences. In addition, it is specifically    tailored towards addressing competence areas, and therefore it is not a generic language learning platform. We discuss    some technical details of the COMALAT platform and present the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:4MWp96NkSFoC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Statistical Analysis of Requirements Prioritization for Transition to Web Technologies: A Case Study in an Electric Power Organization",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-03602-1_5",
            "Abstract": "Transition from an existing IT system to modern Web technologies provides multiple benefits to an organization and its customers. Such a transition in a large organization involves various groups of stakeholders who may prioritize differently the requirements of the software under development. In our case study, the organization is a leading domestic company in the field of electricity power. The existing online system supports the customer service along with the technical activities and has more than 1,500 registered users, while simultaneous access can be reached by 300 users. The paper presents an empirical study where 51 employees in different roles prioritize 18 software requirements using hierarchical cumulative voting. The goal of this study is to test significant differences in prioritization between groups of stakeholders. Statistical methods involving data transformation, ANOVA and Discriminant \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:t6usbXjVLHcC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Links between the personalities, views and attitudes of software engineers",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584910000029",
            "Abstract": "Successful software development and management depends not only on the technologies, methods and processes employed but also on the judgments and decisions of the humans involved. These, in turn, are affected by the basic views and attitudes of the individual engineers.The objective of this paper is to establish if these views and attitudes can be linked to the personalities of software engineers.We summarize the literature on personality and software engineering and then describe an empirical study on 47 professional engineers in ten different Swedish software development companies. The study evaluated the personalities of these engineers via the IPIP 50-item five-factor personality test and prompted them on their attitudes towards and basic views on their professional activities.We present extensive statistical analyses of their responses to show that there are multiple \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:M3ejUd6NZC8C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Optimal exact experimental designs with correlated errors through a simulated annealing algorithm",
            "Publication year": 2001,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167947301000111",
            "Abstract": "Simulated annealing (SA) is a stochastic optimization method with principles taken from the physical process called \u201cannealing\u201d which aims to bring a solid to its ground state or a state of minimum energy. SA is known as a simple heuristic tool suitable for providing direct or approximate solutions to a wide variety of combinatorial problems. This paper is concerned with the problem of determining optimal exact experimental designs with n observations and k two-level factors assuming the existence of correlated errors with a known correlation structure. A simulated annealing algorithm has been developed and applied for the search of D- and A-optimal designs. An extensive discussion regarding the right choices of the initial parameters is presented and a method of self-improvement of the algorithm is suggested via a series of repeated executions. Finally, a version of the SA algorithm is used to find optimal exact \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:1sJd4Hv_s6UC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Assigning Gene Ontology terms to biotext by classification methods",
            "Publication year": 0,
            "Publication url": "http://pci2007.upatras.gr/proceedings/PCI2007_volB/B_019-028_Theodosiou.pdf",
            "Abstract": "Biomedical literature databases constitute valuable repositories of up to date scientific knowledge. The development of efficient classification methods in order to facilitate the organization of these databases and the extraction of novel biomedical knowledge is becoming increasingly important. Several of these methods use bio-ontologies, like Gene Ontology to concisely describe and classify biological documents. The purpose of this paper is to compare two classical statistical classification methods, namely multinomial logistic regression (MLR) and linear discriminant analysis (LDA), to a machine learning classification method, called support vector machines (SVM). Although all the methods have been used with success for classifying texts, there is not a direct comparison between them for classifying biological text to specific Gene Ontology terms. The results from the study show that LDA performs better (accuracy 80.32%) than SVM (77.18%) and MLR (57.4%). LDA not only performs well in the assignment of Gene Ontology terms to documents, but also reduces the dimensions of the original data, making them easier to manage.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:D03iK_w7-QYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An analysis of open source software licensing questions in Stack Exchange sites",
            "Publication year": 2021,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121221002107",
            "Abstract": "Free and open source software is widely used in the creation of software systems, whereas many organisations choose to provide their systems as open source. Open source software carries licenses that determine the conditions under which the original software can be used. Appropriate use of licenses requires relevant expertise by the practitioners, and has an important legal angle. Educators and employers need to ensure that developers have the necessary training to understand licensing risks and how they can be addressed. At the same time, it is important to understand which issues practitioners face when they are using a specific open source license, when they are developing new open source software products or when they are reusing open source software. In this work, we examine questions posed about open source software licensing using data from the following Stack Exchange sites: Stack \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:ODE9OILHJdcC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "An empirical study on views of importance of change impact analysis issues",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4527253/",
            "Abstract": "Change impact analysis is a change management activity that previously has been studied much from a technical perspective. For example, much work focuses on methods for determining the impact of a change. In this paper, we present results from a study on the role of impact analysis in the change management process. In the study, impact analysis issues were prioritised with respect to criticality by software professionals from an organisational perspective and a self-perspective. The software professionals belonged to three organisational levels: operative, tactical and strategic. Qualitative and statistical analyses with respect to differences between perspectives as well as levels are presented. The results show that important issues for a particular level are tightly related to how the level is defined. Similarly, issues important from an organisational perspective are more holistic than those important from a self \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:Zph67rFs4hoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Exploring the relation between technical debt principal and interest: An empirical approach",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584920301567",
            "Abstract": "The cornerstones of technical debt (TD) are two concepts borrowed from economics: principal and interest. Although in economics the two terms are related, in TD there is no study on this direction so as to validate the strength of the metaphor.We study the relation between Principal and Interest, and subsequently dig further into the \u2018ingredients\u2019 of each concept (since they are multi-faceted). In particular, we investigate if artifacts with similar levels of TD Principal exhibit a similar amount of TD Interest, and vice-versa.To achieve this goal, we performed an empirical study, analyzing the dataset using the Mantel test. Through the Mantel test, we examined the relation between TD Principal and Interest, and identified aspects that are able to denote proximity of artifacts, with respect to TD. Next, through Linear Mixed Effects (LME) modelling we studied the generalizability of the results.The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:6ZxmRoH8BuwC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A multivariate statistical framework for the analysis of software effort phase distribution",
            "Publication year": 2015,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584914002420",
            "Abstract": "In software project management, the distribution of resources to various project activities is one of the most challenging problems since it affects team productivity, product quality and project constraints related to budget and scheduling.The study aims to (a) reveal the high complexity of modelling the effort usage proportion in different phases as well as the divergence from various rules-of-thumb in related literature, and (b) present a systematic data analysis framework, able to offer better interpretations and visualisation of the effort distributed in specific phases.The basis for the proposed multivariate statistical framework is Compositional Data Analysis, a methodology appropriate for proportions, along with other methods like the deviation from rules-of-thumb, the cluster analysis and the analysis of variance. The effort allocations to phases, as reported in around 1500 software projects of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:VL0QpB8kHFEC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A novel simulation model for the development process of open source software projects",
            "Publication year": 2002,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/spip.163",
            "Abstract": "The present paper presents a first attempt to produce a dynamical simulation model for the development process of open source software projects. First, a general framework for such models is introduced. Then, a specific simulation model is described and demonstrated. The model equations are based on available literature case studies whenever possible or reasonable assumptions when literature data are not adequate. The model is demonstrated against data obtained from a recent case study by A. Mockus, R. Fielding and J. Herbsleb (\u2018A case study of open source software development: the Apache server\u2019) on the Apache www server software so as to reproduce quantitatively real results as closely as possible. Computer simulation results based on the calibrated model are thus presented and analysed. OSS dynamic simulation models could serve as generic predicting tools of key OSS project factors such as \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:MXK_kJrjxJIC",
            "Publisher": "John Wiley & Sons, Ltd."
        },
        {
            "Title": "A Structural Equation Modeling Approach of the Toll-Like Receptor Signaling Pathway in Chronic Lymphocytic Leukemia",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6621348/",
            "Abstract": "Gene pathway identification is an open and active research area that has attracted the interest not only of biomedical scientists but also of a large number of researchers from disciplines related to knowledge discovery from biological data. In this paper, we used Structural Equation Modeling (SEM) in order to statistically investigate the Toll-Like Receptor (TLR) signaling pathway in Chronic Lymphocytic Leukemia (CLL). Specifically, we used Path Analysis, a special case of SEM which is a statistical technique for testing and confirming causal relations based on data and qualitative assumptions. The dataset consists of Real Time PCR data for 84 genes relevant to the TLR signaling pathway, that were obtained from 192 patients with CLL that have been classified based on the mutational status of their clonotypic antigen receptors as mutated CLL (M-CLL) or unmutated CLL (U-CLL). The causal effects among genes \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:5ugPr518TE4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "ComProFITS: A web-based platform for human resources competence assessment",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7388113/",
            "Abstract": "An efficient assessment of human resource competences, followed by the goal oriented analysis of the results, support the identification of the competence gaps in organizations and the allocation of resources towards identified gaps. This paper presents the basic idea as well as the scientific and implementation results of the ComProFITS project as an innovative web-based platform for the evaluation of existing employees and the recruitment of new employees in organizations. The platform integrates research on the statistical assessment of competences, an innovative competence pyramid and many alternative methods of employee evaluation and recruitment. The initial version of the platform is being applied and used in the IT sector in a large organization in Spain with the goal of extending it to the Spanish and European wide enterprises in different sectors.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:2KloaMYe4IUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Bootstrap prediction intervals for a semi-parametric software cost estimation model",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5349952/",
            "Abstract": "The vital task of accurate Software Cost Estimation predictions remains a challenging problem attracting the interest of researchers and practitioners. Although Least Squares (LS) regression and Estimation by Analogy (EbA) are two of the most widely applied methods,there seems to be a discrepancy in choosing the best prediction technique. In this paper, we further extend our previous work on the utilization of a semi-parametric model,called LSEbA that achieves to combine the above-mentioned methods. More precisely, we present a method of constructing prediction intervals by the bootstrap resampling technique. The prediction intervals obtained for LSEbA are compared with those of LS and EbA separately,with the aid of a new methodology that takes into account not only the ability of comparative intervals to capture the actual cost, but also their similarity and their width.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:R3hNpaxXUhUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Effects of Pair Programming-Investigating the Impact of Personality Types on Communication and Collaboration-Viability in Pair Programming--An Empirical Study",
            "Publication year": 2006,
            "Publication url": "https://scholar.google.com/scholar?cluster=5997798057058945529&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:edDO8Oi4QzsC",
            "Publisher": "Berlin: Springer-Verlag, 1973-"
        },
        {
            "Title": "journal homepage: www. elsevier. com/locate/jss",
            "Publication year": 0,
            "Publication url": "http://cgi.di.uoa.gr/~bbaous/docs/Reviewers_2010_JSS.pdf",
            "Abstract": "Thank you_list of reviewers Page 1 List of Reviewers The Editors and the Publisher thank the \nreviewers listed below for completing one or more reviews for the journal during 2009. We \napologize for any inadvertent omissions Imad Abbadi Ahmed Abdelwahab Luca Abeni Pekka \nAbrahamsson A. Abran Silvia Acu\u00f1a Bram Adams Avishek Adhikari Banit Agrawal Sharifah \nMumtazah Syed Ahmad Mahmood Ahmadi Gaeil Ahn Marco Aiello Nabil Ajam Samuel Ajila \nPaulo Alencar Ali Al-Haj Nour Ali Muhammad Ali Babar Harith Al-Jumaily Fernando Alonso \nMohammad Alshayeb Rajeev Alur Angelos Amditis Pierre America Paul Ammann Michele \nAmoretti Pascal Andre Lefteris Angelis Thara Angskun Spyros Antonatos Giuliano Antoniol \nSven Apel Andrea Arcuri Erik Arisholm Martin Arlitt Javier Aroba Veysel Aslantas Colin Atkinson \nPradeep Atrey Nuttapong Attrapadung Oscar Au Man Ho Au Martin Auer Jean-Philippe Paris \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:tYavs44e6CUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Investigating the extreme programming system\u2013An empirical study",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/article/10.1007/s10664-006-6404-6",
            "Abstract": "In this paper we discuss our empirical study about the advantages and difficulties 15 Greek software companies experienced applying Extreme Programming (XP) as a holistic system in software development. Based on a generic XP system including feedback influences and using a cause-effect model including social-technical affecting factors, as our research tool, the study statistically evaluates the application of XP practices in the software companies being studied. Data were collected from 30 managers and developers, using the sample survey technique with questionnaires and interviews, in a time period of six months. Practices were analysed individually, using Descriptive Statistics (DS), and as a whole by building up different models using stepwise Discriminant Analysis (DA). The results have shown that companies, facing various problems with common code ownership, on-site customer, 40\u2013hour \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:YsMSGLbcyi4C",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Regression-based Statistical Bounds on Software Execution Time",
            "Publication year": 2017,
            "Publication url": "https://pdfs.semanticscholar.org/ef7e/5d81735827936bf611d6c12d5a5f0c00d364.pdf",
            "Abstract": "Regression-based Statistical Bounds on Software Execution Time Page 1 Motivation The \nMaximal Regression Model Model Construction Techniques A JPEG Decoder Study \nConclusions Regression-based Statistical Bounds on Software Execution Time Ayoub \nNouri P. Poplavko, L. Angelis, A. Zerzelidis, S. Bensalem, P. Katsaros University of Grenoble-Alpes \n- France August 24, 2017 Vecos\u201917, Montreal Page 2 Motivation The Maximal Regression \nModel Model Construction Techniques A JPEG Decoder Study Conclusions WCET: \nStatistical vs. Common Approaches WCET is useful for ... Schedulability of real-time \nsystems Performance Evaluation, eg, Network Calculus Model-based design, ie building \nfaithful Hw/Sw models 2 / 29 Page 3 Motivation The Maximal Regression Model Model \nConstruction Techniques A JPEG Decoder Study Conclusions WCET: Statistical vs. \nCommon Approaches WCET is useful for ... of -\u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:bz8QjSJIRt4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Clustering web information services",
            "Publication year": 2007,
            "Publication url": "https://www.igi-global.com/chapter/web-data-management-practices/31095",
            "Abstract": "The explosive growth of the Web scale has drastically increased information circulation and dissemination rates. As the number of both Web users and Web sources grows significantly everyday, crucial data management issues, such as clustering on the Web, should be addressed and analyzed. Clustering has been proposed towards improving both the information availability and the Web users\u2019 personalization. Clusters on the Web are either users\u2019 sessions or Web information sources, which are managed in a variation of applications and implementations testbeds. This chapter focuses on the topic of clustering information over the Web, in an effort to overview and survey on the theoretical background and the adopted practices of most popular emerging and challenging clustering research efforts. An up-to-date survey of the existing clustering schemes is given, to be of use for both researchers and practitioners \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:RYcK_YlVTxYC",
            "Publisher": "IGI Global"
        },
        {
            "Title": "Impact metrics of security vulnerabilities: Analysis and weighing",
            "Publication year": 2015,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/19393555.2015.1051675",
            "Abstract": "The number of vulnerabilities discovered and reported during the recent decades is enormous, making an improved ranking and prioritization of vulnerabilities\u2019 severity a major issue for information technology (IT) management. Although several methodologies for ranking and scoring vulnerabilities have been proposed, the Common Vulnerability Scoring System (CVSS) is the open standard with wide acceptance from the information security community. Recently, the Weighted Impact Vulnerability Scoring System (WIVSS) has been proposed as an alternative scoring methodology, which assigns different weights to impact factors of vulnerability in order to achieve higher diversity of values and thus improvement in flexibility of ranking in comparison to CVSS. The purpose of this paper is to expand the idea of WIVSS by defining the sets of weights which provide higher diversity of values. For this reason, an algorithm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:Fu2w8maKXqMC",
            "Publisher": "Taylor & Francis"
        },
        {
            "Title": "A controlled experiment investigation on the impact of an instructional tool for personalized learning",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1215120/",
            "Abstract": "We describe a controlled experiment concerning the use of a learning aid during the instructional procedure. The core issue of investigation is whether this instructional aid can augment the cognitive transfer of the learners by personalizing the offered knowledge. A controlled experiment was performed with the participation of 79 students. The results have shown that for the transfer of simple information this \"lesson sheet\" does not provide any statistically significant advantage, yet for complex information a statistically significant better performance is observed for the student group that used the tool.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:SpbeaW3--B0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Not All IGHV3-21 CLL Are Equal: Subset# 2 Displays a Distinctive Clinicobiological Profile with Remarkable Similarities to Subset# 169, its Close Immunogenetic Relative",
            "Publication year": 2014,
            "Publication url": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:768143",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:ye4kPcJQO24C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Regression-based statistical bounds on software execution time",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-66176-6_4",
            "Abstract": "Our work aims at facilitating the schedulability analysis of non-critical systems, in particular those that have soft real-time constraints, where WCETs can be replaced by less stringent probabilistic bounds, which we call Maximal Execution Times (METs). In our approach, we can obtain adequate probabilistic execution time models by separating the non-random input data dependency from a modeling error that is purely random. To achieve this, we propose to take advantage of the rich set of available statistical model-fitting techniques, in particular linear regression. Although certainly the proposed technique cannot directly achieve extreme probability levels that are usually expected for WCETs, it is an attractive alternative for MET analysis, since it can arguably guarantee safe probabilistic bounds. We demonstrate our method on a JPEG decoder running on an industrial SPARC V8 processor.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:yB1At4FlUx8C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Managing uncertainty in project portfolio cost estimation",
            "Publication year": 2001,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584901001835",
            "Abstract": "Although typically a software development organisation is involved in more than one project simultaneously, the available tools in the area of software cost estimation deal mostly with single software projects. In order to calculate the possible cost of the entire project portfolio, one must combine the single project estimates taking into account the uncertainty involved. In this paper, statistical simulation techniques are used to calculate confidence intervals for the effort needed for a project portfolio. The overall approach is illustrated through the adaptation of the analogy-based method for software cost estimation to cover multiple projects.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:Y0pCki6q_DkC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "On the use of Bayesian belief networks for the prediction of software productivity",
            "Publication year": 2003,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584902001635",
            "Abstract": "In spite of numerous methods proposed, software cost estimation remains an open issue and in most situations expert judgment is still being used. In this paper, we propose the use of Bayesian belief networks (BBNs), already applied in other software engineering areas, to support expert judgment in software cost estimation. We briefly present BBNs and their advantages for expert opinion support and we propose their use for productivity estimation. We illustrate our approach by giving two examples, one based on the COCOMO81 cost factors and a second one, dealing with productivity in ERP system localization.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:9yKSN-GCB0IC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Investigating the applicability of agility assessment surveys: A case study",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121214001927",
            "Abstract": "Agile software development has become popular in the past decade without being sufficiently defined. The Agile principles can be instantiated differently which creates different perceptions of Agility. This has resulted in several frameworks being presented in the research literature to evaluate the level of Agility. However, the evidence of their actual use in practice is limited.The objective is to identify online surveys that assess/profile Agility in practice, and to evaluate the surveys in an industrial setting.The Agility assessment surveys were identified through searching the web. Then, they were explored and two surveys were identified as most promising for our objective. The selected surveys were evaluated in a case study with three Agile teams in a software consultancy company.Each team and its customer separately judged the team's Agility. This outcome was compared with the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:HE397vMXCloC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Dynamical Simulation Models for the Development Process of Open Source Software Projects",
            "Publication year": 0,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.385.8598&rep=rep1&type=pdf",
            "Abstract": "There have been a few studies attempting to define the Open-Source software (OSS) development process in general terms and there have also been a few case studies of OSS projects. The latter studies presented qualitative and quantitative data for the OSS development process, managerial issues and programmer attitudes. The authors in many cases offer descriptive explanations based on plausible assumptions but, as there is no general model to quantify their claims together with their possibly complicated interactions, the validity of such explanations cannot be directly demonstrated. Therefore, there is a need to move from descriptive models based on special cases to a more general quantitative mathematical model that would hopefully reproduce real case results. In the present paper we present a general framework for dynamical simulation models of the development process of OSS projects. Then we present a specific simulation model, which is demonstrated against results from available literature case study of the Apache OSS project. OSS dynamic simulation models could serve as generic predicting tools of key OSS project factors such as project failure/success as well as time dependent factors such as the evolution of source code, defect density, number of programmers and distribution of work effort to distinct project modules and tasks.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:yD5IFk8b50cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Barriers to refactoring",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/fullHtml/10.1145/3131873",
            "Abstract": "Developers know refactoring improves their software, but many find themselves unable to do so when they want to.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:L7CI7m0gUJcC",
            "Publisher": "ACM"
        },
        {
            "Title": "Exploiting the temporal patterning of transient VEP signals: A statistical single-trial methodology with implications to brain\u2013computer interfaces (BCIs)",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0165027014001484",
            "Abstract": "When visual evoked potentials (VEPs) are deployed in brain\u2013computer interfaces (BCIs), the emphasis is put on stimulus design. In the case of transient VEPs (TVEPs) brain responses are never treated individually, i.e. on a single-trial (ST) basis, due to their poor signal quality. Therefore their main characteristic, which is the emergence during early latencies, remains unexplored.Following a pattern-analytic methodology, we investigated the possibility of using single-trial TVEP responses to differentiate between the different spatial locations where a particular visual stimulus appeared and decide whether it was attended or unattended by the subject.Covert spatial attention modulates the temporal patterning of TVEPs in such a way that a brief ST-segment, from a single synthesized sensor, is sufficient for a Mahalanobis-Taguchi (MT) system to decode subject's intention \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:tkaPQYYpVKoC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A Study of Remote and On-site ICT Labor Market Demand using Job Offers from Stack Overflow",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9582599/",
            "Abstract": "As the industry is moving towards digitalized solutions and practices, a growth in remote working has been observed with companies embracing flexibility for their workforce. Global crises, such as the coronavirus pandemic, have also accelerated this process, transforming the labor market. This trend is reflected in job portals, that contain an increasing number of remote job advertisements. Recognizing this evolving change, we perform a thorough study in Stack Overflow, to examine the main characteristics of remote working that discriminate it from its on-site counterpart. By collecting and analyzing 8514 job posts and leveraging text mining and graph theory methodologies, we attempt to pinpoint the primary elements that define each category, from dominant technologies to job positions and top seeking industries. The findings suggest that remote working presents differences from traditional working, being mainly \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:OR75R8vi5nAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Towards analytical evaluation of professional competences in human resource management",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6700529/",
            "Abstract": "Managers of enterprises concern with a major challenge for optimal management of human resources based on availability of domain experts and highly qualified personnel. The process of allocating right people to the right positions in a right time is a key to success. To achieve this goal, managers need to deploy evaluation tools integrated with the gap analysis method. This paper presents the concept and implementation details of an in-house developed software tool for competence evaluation of domain specific competencies and selection of professionals. A generic mathematical representation of competences in this project makes the software tool applied in a wide variety of organizations. A standard competence model has been first defined in this project with 5 main competence categories and related sub-categories including over 70 competence questionnaires in different managerial and employee levels \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:XiVPGOgt02cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A controlled experiment of a method for early requirements triage utilizing product strategies",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-02050-6_3",
            "Abstract": " [Context and motivation] In market-driven product development of software intensive products large numbers of requirements threaten to overload the development organization. It is critical for product management to select the requirements aligned with the overall business goals, product strategies and discard others as early as possible. Thus, there is a need for an effective and efficient method that deals with this challenge and supports product managers in the continuous effort of early requirements triage [1, 2] based on product strategies. This paper evaluates such a method \u2013 A Method for Early Requirements Triage Utilizing Product Strategies (MERTS), which is built based on the needs identified in literature and industry. [Question/problem] The research question answered in this paper is \u201cIf two groups of subjects have a product strategy, one group in NL format and one in MERTS format, will \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:k_IJM867U9cC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Modeling the effect of the badges gamification mechanism on personality traits of Stack Overflow users",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1569190X20300964",
            "Abstract": "Technical Question & Answering (Q&A) websites have become promising sources of information for a wide range of topics and the most popular is Stack Overflow. Aiming to attract the continuous users\u2019 interest, Stack Overflow enabled the gamification system. By rewarding users\u2019 participation and contribution to the community, the site provides motivation to increase their participation. A well-known gamification process is the badge awards. In this paper, we propose a methodology to model and analyze the effect of the badge award gamification process on user\u2019s personality traits based on data recorded before and after award time, employing the Big Five personality Factors. Using statistical univariate analysis and mainly multivariate cluster analysis, we show that: (1) On the average, users\u2019 personality scores for each one of the individual personality traits are not significantly affected by the badge award; (2) When \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:DUooU5lO8OsC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Functional Annotation of Genes through Statistical Analysis of Biomedical Articles",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1508336/",
            "Abstract": "One of the most elaborate and important tasks in biology is the functional annotation of genes. Biologists have developed standardized and structured vocabularies, called bio-ontologies, to assist them in describing the different functions. A critical issue in the assignment of functions to genes is the utilization of knowledge from published biomedical articles. The purpose of this paper is to present a unified and comprehensive statistical methodology for functionally annotating genes using biomedical literature. Specifically, classification models are built using the discriminant analysis method while validation, analysis and interpretation of the results is based on graphical methods and various performance metrics and techniques. The general conclusions from the study are very promising, in the sense that the proposed methodology not only performs well in the assignment of functions to genes, but also provides \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:pqnbT2bcN3wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Clustering classifiers for knowledge discovery from physically distributed databases",
            "Publication year": 2004,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0169023X03001538",
            "Abstract": "Most distributed classification approaches view data distribution as a technical issue and combine local models aiming at a single global model. This however, is unsuitable for inherently distributed databases, which are often described by more than one classification models that might differ conceptually. In this paper we present an approach for clustering distributed classifiers in order to discover groups of similar classifiers and thus similar databases with respect to a specific classification task. We also show that clustering distributed classifiers as a pre-processing step for classifier combination enhances the achieved predictive performance of the ensemble.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:W7OEmFMy1HYC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Investigating the Applicability of Agile Assessment Tools: A Case Study",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=12622034960139219855&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:0N-VGjzr574C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A simulation process for asynchronous event processing systems: Evaluating performance and availability in transaction models",
            "Publication year": 2012,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1569190X12001049",
            "Abstract": "Simulation is essential for understanding the performance and availability behavior of complex systems, but there are significant difficulties when trying to simulate systems with multiple components, which interact with asynchronous communication. A systematic process is needed, in order to cope with the complexity of asynchronous event processing and the failure semantics of the interacting components. We address this problem by introducing an approach that combines formal techniques for faithful representation of the complex system effects and a statistical analysis for simultaneously studying multiple simulation outcomes, in order to interpret them. Our process has been successfully applied to a synthetic workload for distributed transaction processing. We outline the steps followed towards generating a credible simulation model and subsequently we report and interpret the results of the applied statistical \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:OU6Ihb5iCvQC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "SCREENING AND TRIAGE OF HPV-POSITIVE WOMEN",
            "Publication year": 0,
            "Publication url": "https://www.researchgate.net/profile/Kimon-Chatzistamatiou/publication/304215958_Performance_of_high-risk_HPV_DNA_genotyping_for_primary_cervical_cancer_screening_and_triage_of_HPV-positive_women_compared_to_cytology_Results_of_the_PIPAVIR_study/links/57e93a6f08aef8bfcc960b94/Performance-of-high-risk-HPV-DNA-genotyping-for-primary-cervical-cancer-screening-and-triage-of-HPV-positive-women-compared-to-cytology-Results-of-the-PIPAVIR-study.pdf",
            "Abstract": "Background/ObjectivesAs high-risk (HR) HPV types are detected in 99.7% of cervical cancer cases and in most of pre-invasive cases, HPV detection may be an alternative as a screening test for the detection of precancerous cervical lesions\". In fact, HR-HPV detection is increasingly considered a better method of primary screening than cytology\"\". The objective of the presented study is to assess the performance of (HR) HPV DNA genotyping as a method of primary cervical cancer screening and triage of HPV positive women to colposcopy compared to liquid-based cytology (LBC) in an urban population of Greek women.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:p__nRnzSRKYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Software product quality in global software development: Finding groups with aligned goals",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6068381/",
            "Abstract": "The development of a software product in an organization involves various groups of stakeholders who may prioritize the qualities of the product differently. This paper presents an empirical study of 65 individuals in different roles and in different locations, including on shoring, outsourcing and off shoring, prioritizing 24 software quality aspects. Hierarchical cluster analysis is applied to the prioritization data, separately for the situation today and the ideal situation, and the composition of the clusters, regarding the distribution of the inherent groupings within each of them, is analyzed. The analysis results in observing that the roles are not that important in the clustering. However, compositions of clusters regarding the onshore-offshore relationships are significantly different, showing that the offshore participants have stronger tendency to cluster together. In conclusion, stakeholders seem to form clusters of aligned \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:a0OBvERweLwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Software quality across borders: Three case studies on company internal alignment",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S095058491300133X",
            "Abstract": "Software quality issues are commonly reported when offshoring software development. Value-based software engineering addresses this by ensuring key stakeholders have a common understanding of quality.This work seeks to understand the levels of alignment between key stakeholder groups within a company on the priority given to aspects of software quality developed as part of an offshoring relationship. Furthermore, the study aims to identify factors impacting the levels of alignment identified.Three case studies were conducted, with representatives of key stakeholder groups ranking aspects of software quality in a hierarchical cumulative exercise. The results are analysed using Spearman rank correlation coefficients and inertia. The results were discussed with the groups to gain a deeper understanding of the issues impacting alignment.Various levels of alignment were \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:eflP2zaiRacC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Fuzzy Grading for Adaptability in a Learning Platform",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3003733.3003766",
            "Abstract": "Due to the increasing global competition and technology's constant progress, the usage of online training platforms has grown rapidly. One of the problems that these platforms face is the lack of a physical evaluator. Given that flexible evaluating methods are considered necessary in training platforms this is an obstacle we must overcome. In this paper we describe the implementation of a fuzzy grading system that is going to be used in the Competence Oriented Multilingual Adaptive Language Assessment and Training (COMALAT) system as an intelligent evaluating system of the platform. First, we present a literature review on fuzzy evaluation methods and afterwards we describe the technique that we used in order to apply it in the COMALAT platform. Aside from the COMALAT platform, the fuzzy grading system is an innovative evaluation method that can be deployed in other online learning platforms as well, due \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:uc_IGeMz5qoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reliability and validity of the adapted Greek version of scoliosis research society\u201322 (SRS-22) questionnaire",
            "Publication year": 2009,
            "Publication url": "https://scoliosisjournal.biomedcentral.com/articles/10.1186/1748-7161-4-14",
            "Abstract": "The SRS-22 is a valid instrument for the assessment of the health related quality of life of patients with Idiopathic scoliosis. The SRS-22 questionnaire was developed in USA and has been widely used in the English speaking countries. Recently it has been translated and validated in many other languages. The purpose of this study is to evaluate the reliability and validity of the adapted Greek version of the refined Scoliosis Research Society-22 Questionnaire. Following the steps of cross \u2013 cultural adaptation the adapted Greek version of the SRS-22 questionnaire and a validated Greek version of the SF-36 questionnaire were mailed to 68 patients treated surgically for Idiopathic Scoliosis. 51 out of the 68 patients returned the 1st set of questionnaires, while a second set was emailed to 30 randomly selected patients of the first time responders. 20 out of the 30 patients returned the 2nd set. The mean age at the time of operation was16,2 years and the mean age at the time of evaluation was 21,2 years. Descriptive statistics for content analysis were calculated. Reliability assessment was determined by estimating Cronbach's \u03b1 and intraclass correlation coefficient (ICC) respectively. Concurrent validity was evaluated by comparing SRS-22 domains with relevant domains in the SF-36 questionnaire using Pearson's Correlation Coefficient (r). The calculated Cronbach's \u03b1 of internal consistency for three of the corresponding domains (pain 0.85; mental health 0.87; self image 0.83) were very satisfactory and for two domains (function/activity 0.72 and satisfaction 0.67) were good. The ICC of all domains of SRS-22 questionnaire was high (ICC>0.70 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:hFOr9nPyWt4C",
            "Publisher": "BioMed Central"
        },
        {
            "Title": "Exploring the correlation of biomedical article keywords to MeSH terms",
            "Publication year": 0,
            "Publication url": "https://www.academia.edu/download/42091962/download.pdf",
            "Abstract": "The exponential growth in the availability of biomedical information has posed the need to solve retrieval issues raised in huge sequence/biomedical article repositories. Biomedical article databases, like PubMed, are huge repositories of useful biological information given in natural language form and thus not easily processed by computers. Medical Subject Headings (MeSH) terms have been proposed to facilitate the process of electronically retrieving biomedical articles, which are semantically related. However, most of the classification algorithms, used for information retrieval, require numeric representations of either the keywords or the MeSH terms of the articles. These representations are essentially vectors of variables forming large multivariate numerical datasets. In order to combine the information from keyword datasets and MeSH datasets, this paper proposes a multivariate statistical approach which can quantify their relationships and reveal the underlying correlation. The basis of this approach is a mathematical technique, called non-linear canonical correlation analysis (NLCCA). NLCCA can assemble information from several datasets by building a model describing the whole of the data. The method was applied to a large number of articles from PubMed. Certain statistics obtained from the analysis showed that the degree of correlation between MeSH terms and keywords is high. The method results in the reduction of data dimensionality, containing in one dataset with new variables significant information of the original data. These results are very important for the efficient description and visualization of the data in order to explore \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the use of software design models in software development practice: An empirical investigation",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121214001022",
            "Abstract": "Research into software design models in general, and into the UML in particular, focuses on answering the question how design models are used, completely ignoring the question if they are used. There is an assumption in the literature that the UML is the de facto standard, and that use of design models has had a profound and substantial effect on how software is designed by virtue of models giving the ability to do model-checking, code generation, or automated test generation. However for this assumption to be true, there has to be significant use of design models in practice by developers.This paper presents the results of a survey summarizing the answers of 3785 developers answering the simple question on the extent to which design models are used before coding. We relate their use of models with (i) total years of programming experience, (ii) open or closed development, (iii) educational level, (iv \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:AXPGKjj_ei8C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "MeSHy: Mining unanticipated PubMed information using frequencies of occurrences and concurrences of MeSH terms",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1532046411001006",
            "Abstract": "PubMed is the most widely used database of biomedical literature. To the detriment of the user though, the ranking of the documents retrieved for a query is not content-based, and important semantic information in the form of assigned Medical Subject Headings (MeSH) terms is not readily presented or productively utilized. The motivation behind this work was the discovery of unanticipated information through the appropriate ranking of MeSH term pairs and, indirectly, documents. Such information can be useful in guiding novel research and following promising trends.A web-based tool, called MeSHy, was developed implementing a mainly statistical algorithm. The algorithm takes into account the frequencies of occurrences, concurrences, and the semantic similarities of MeSH terms in retrieved PubMed documents to create MeSH term pairs. These are then scored and ranked, focusing on their \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:2P1L_qKh6hAC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Are FLOSS developers committing to CVS/SVN as much as they are talking in mailing lists? Challenges for Integrating data from Multiple Repositories",
            "Publication year": 2008,
            "Publication url": "https://www.academia.edu/download/36505690/49-542008.pdf",
            "Abstract": "This paper puts forward a framework for investigating Free and Open Source Software (F/OSS) developers activities in both source code and mailing lists repositories. We used data dumps of fourteen projects from the FLOSSMetrics (FM) retrieval system. Our intentions are (i) to present a possible methodology, its advantages and disadvantages which can benefit future researchers using some aspects of the FM retrieval system\u2019s data dumps, and (ii) discuss our initial research results on the contributions developers make to both coding and lists activities.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:IWHjjKOFINEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Open source software development should strive for even greater code maintainability",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1022594.1022598",
            "Abstract": "A study of almost six million lines of code tracks how freely accessible source code holds up against time and multiple iterations.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:qjMakFHDy7sC",
            "Publisher": "ACM"
        },
        {
            "Title": "Dynamical Simulation Models of the Open Source",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=9243984993616878261&hl=en&oi=scholarr",
            "Abstract": "This chapter will discuss attempts to produce formal mathematical models for dynamical simulation of the development process of Free/Open Source Software",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:mNrWkgRL2YcC",
            "Publisher": "IGI Global"
        },
        {
            "Title": "A RAkEL-based methodology to estimate software vulnerability characteristics & score-an application to EU project ECHO",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/article/10.1007/s11042-021-11073-x",
            "Abstract": "Software vulnerabilities constitute a critical threat for cybersecurity analysts in the contemporary society, since the successfully exploited vulnerabilities could harm any system in terms of Confidentiality, Integrity and Availability. Similarly, the characterization of vulnerabilities and the assessment of vulnerability risk is a crucial task for cybersecurity managers regarding the resource management. However, the proliferation of software vulnerabilities causes problems related to the response time of the security experts. For this reason, a methodology based on RAndom k-labELsets (RAkEL) is proposed in this paper in order to estimate software vulnerability characteristics and score from the vulnerability technical description. The proposed methodology aims to a) improve an existing multi-target methodology and b) be integrated in a Cyber Threat Intelligence (CTI) information sharing system. The results, in a dataset \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:cWzG1nlazyYC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Prioritization of issues and requirements by cumulative voting: A compositional data analysis framework",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5598119/",
            "Abstract": "Cumulative Voting (CV), also known as Hundred-Point Method, is a simple and straightforward technique, used in various prioritization studies in software engineering. Multiple stakeholders (users, developers, consultants, marketing representatives or customers) are asked to prioritize issues concerning requirements, process improvements or change management in a ratio scale. The data obtained from such studies contain useful information regarding correlations of issues and trends of the respondents towards them. However, the multivariate and constrained nature of data requires particular statistical analysis. In this paper we propose a statistical framework; the multivariate Compositional Data Analysis (CoDA) for analyzing data obtained from CV prioritization studies. Certain methodologies for studying the correlation structure of variables are applied to a dataset concerning impact analysis issues prioritized \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:TFP_iSt0sucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the necessity of multiple university rankings",
            "Publication year": 2019,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/09737766.2018.1550043",
            "Abstract": "Nowadays university rankings are ubiquitous commodities; a plethora of them is published every year by private enterprises, state authorities and universities. University rankings are very popular to governments, journalists, university administrations and families as well. At the same time, they are heavily criticized as being very subjective and contradictory to each other. University rankings have been studied with respect to political, educational and data management aspects. In this paper, we focus on a specific research question regarding the alignment of some well-known such rankings, ultimately targeting to investigate the usefulness of the variety of all these rankings. First, we describe in detail the methodology to collect and homogenize the data and, second, we statistically analyze these data to examine the correlation among the different rankings. The results show that despite their statistically significant \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:PVjk1bu6vJQC",
            "Publisher": "Routledge"
        },
        {
            "Title": "Software cost prediction with predefined interval estimates",
            "Publication year": 2004,
            "Publication url": "https://www.academia.edu/download/41833442/Software_cost_prediction_with_predefined20160131-7658-1joi4vu.pdf",
            "Abstract": "Defining the required productivity in order to complete successfully and within time and budget constraints a software development project is actually a reasoning problem that should be modelled under uncertainty. One way of achieving this is to estimate an interval accompanied by a probability instead of a particular value. In this paper we compare traditional methods that focus on point estimates, methods that focus both on point and interval estimates and methods that produce only predefined interval estimates. In the case of predefined intervals, software cost estimation becomes a classification problem. All the above methods are applied on two different data sets, namely the COCOMO81 dataset and the Maxwell dataset. Also the ability of classification techniques to resolve one classification problem in cost estimation, namely to determine the software development mode based on project attributes, is assessed and compared to reported results.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:HDshCWvjkbEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Investigating the relationship between source code modularity and user satisfaction in open-source",
            "Publication year": 2001,
            "Publication url": "https://scholar.google.com/scholar?cluster=8892655991882507482&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:i2xiXl-TujoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A preliminary Study of Knowledge Sharing related to Covid-19 Pandemic in Stack Overflow",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9226310/",
            "Abstract": "The Covid-19 outbreak has changed to an unprecedented extent almost every aspect of human activity. At the same time, the pandemic has stimulated enormous amount of research by scientists across various disciplines, seeking to study the phenomenon itself, its epidemiological characteristics and ways to confront its consequences. Information Technology, and particularly Data Science, drive innovation in all related to Covid-19 biomedical fields. Acknowledging that software developers routinely resort to open \u2018question & answer\u2019 communities like Stack Overflow to seek advice on solving technical issues, we have performed an empirical study to investigate the extent, evolution and characteristics of Covid-19 related posts. Through the study of 464 Stack Overflow questions posted in February and March 2020 and leveraging the power of text mining, we attempt to shed light into the interest of developers in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:IRz6iEL74y4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "PREVENT: An algorithm for mining intertransactional patterns for the prediction of rare events",
            "Publication year": 2004,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.580.7502&rep=rep1&type=pdf",
            "Abstract": "In this paper we propose a data mining technique for the efficient prediction of rare events, such as heat waves, network intrusions and engine failures, using inter transactional patterns. Data mining is a research area that attempts to assist the decision makers with a set of tools to treat a wide range of real world problems that the traditional statistical and mathematical approaches are not enough in terms of efficiency and computational performance. Transaction databases, such as the ones in this paper that contain sets of events, require special approaches in order to extract valuable temporal knowledge. We utilize the framework of inter-transaction association rules, which associate events across a window of transactions. We propose an approach that extends sequential analysis to predict rare events in transaction databases. We formulate the problem of rare events prediction and we propose PREVENT, an algorithm that produces inter-transactional patterns for the fast and accurate prediction of a user-specified rare event. Finally, we provide experimental results and suggest some ideas for future research.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:KlAtU1dfN6UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "B cell receptor stereotypy makes a clinical difference in CLL: revelations from a multi-institutional series of 4615 cases",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=14499649420003611961&hl=en&oi=scholarr",
            "Abstract": "BALIAKAS P. MINGA E. HADZIDIMITRIOU A. TSANOUSA A. SUTTON L. AGATHANGELIDIS A. SCARFO L. DAVIS Z. YAN J. PLEVOV\u00c1 Karla SANBERG Y. VOJDEMAN F. BOUDJOGRA M. TZENOU T. CHATZOULI M. CHU C. GARDINER A. MANSOURI L. SMEDBY K. PEDERSEN L. MORENO D. VAN LOM K. GIUDICELLI V. TICH\u00dd Boris NGUYEN-KHAC F. PANAGIOTIDIS P. ANAGNOSTOPOULOS A. ANGELIS L. JULIUSSON G. LEFRANC M. GEISLER C. LANGERAK A. POSP\u00cd\u0160ILOV\u00c1 \u0160\u00e1rka CHIORAZZI N. BELESSI C. DAVI F. OSCIER D. DARZENTAS Nikos ROSENQUIST R. GHIA P. STAMATOPOULOS K.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:-FonjvnnhkoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bootstrap Confidence Intervals for Regression Error Characteristic Curves Evaluating the Prediction Error of Software Cost Estimation Models.",
            "Publication year": 2009,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.182.4886&rep=rep1&type=pdf",
            "Abstract": "The importance of Software Cost Estimation at the early stages of the development life cycle is clearly portrayed by the utilization of several algorithmic and artificial intelligence models and methods, appeared so far in the literature. Despite the several comparison studies, there seems to be a discrepancy in choosing the best prediction technique between them. Additionally, the large variation of accuracy measures used in the comparison procedure constitutes an inhibitory factor which complicates the decision-making. In this paper, we further extend the utilization of Regression Error Characteristic analysis, a powerful visualization tool with interesting geometrical properties in order to obtain Confidence Intervals for the entire distribution of error functions. As there are certain limitations due to the small-sized and heavily skewed datasets and error functions, we utilize a simulation technique, namely the bootstrap method in order to evaluate the standard error and bias of the accuracy measures, whereas bootstrap confidence intervals are constructed for the Regression Error Characteristic curves. The tool can be applied to any cost estimation situation in order to study the behavior of comparative statistical or artificial intelligence methods and test the significance of difference between models.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:pyW8ca7W8N0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Experimental evaluation of an instructional supporting tool in distance learning",
            "Publication year": 2008,
            "Publication url": "https://www.jstor.org/stable/pdf/jeductechsoci.11.3.67.pdf",
            "Abstract": "This paper describes a controlled experiment concerning the use of a learning aid during an open and distance learning (ODL) course. The core issue of investigation is whether this instructional aid can support, guide, and scaffold the distant student in his/her study. For this purpose, a controlled experiment was conducted with the participation of 191 undergraduate students at the department of informatics at a university in Greece. The considered domain was two lessons concerning human\u2013computer interaction (HCI), the first in usability engineering and the second in interface evaluation methodologies. A test session was also conducted to collect data on the assessment of the effectiveness of the proposed tool. Descriptive statistics as well as a variety of statistical methods have been applied to the collected data in order to test the research hypotheses. The results have shown a statistically significant difference \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:hMod-77fHWUC",
            "Publisher": "International Forum of Educational Technology & Society"
        },
        {
            "Title": "FOR HPV 16/18 POSITIVE WOMEN, COMPARED TO NO TRIAGE, OR FOR HIGH-RISK HPV (NON 16/18) POSITIVE WOMEN, COMPARED TO CYTOLOGY",
            "Publication year": 0,
            "Publication url": "https://scholar.google.com/scholar?cluster=3047011020999062247&hl=en&oi=scholarr",
            "Abstract": "Background/ObjectivesHigh-risk (hr) HPV detection with HPV16/18 km iS COnSidered a better method of primary cervical screening than cytology\", which tends to become a triage test for hr (non 16/18) HPV positive women to colposcopy\". The objective of the presented study is to assess the performance of the detection of E7HPV protein as a triage method for women positive for either hrHPV (non 16/18) or HPV16/18.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:BwyfMAYsbu0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Performance and effectiveness trade\u2010off for checkpointing in fault\u2010tolerant distributed systems",
            "Publication year": 2007,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1059",
            "Abstract": "Checkpointing has a crucial impact on systems' performance and fault\u2010tolerance effectiveness: excessive checkpointing results in performance degradation, while deficient checkpointing incurs expensive recovery. In distributed systems with independent checkpoint activities there is no easy way to determine checkpoint frequencies optimizing response\u2010time and fault\u2010tolerance costs at the same time. The purpose of this paper is to investigate the potentialities of a statistical decision\u2010making procedure. We adopt a simulation\u2010based approach for obtaining performance metrics that are afterwards used for determining a trade\u2010off between checkpoint interval reductions and efficiency in performance. Statistical methodology including experimental design, regression analysis and optimization provides us with the framework for comparing configurations, which use possibly different fault\u2010tolerance mechanisms \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:5nxA0vEk-isC",
            "Publisher": "John Wiley & Sons, Ltd."
        },
        {
            "Title": "ComProFITS",
            "Publication year": 2016,
            "Publication url": "http://ikee.lib.auth.gr/record/300836",
            "Abstract": "An efficient assessment of human resource competences, followed by the goal oriented analysis of the results, support the identification of the competence gaps in organizations and the allocation of resources towards identified gaps. This paper presents the basic idea as well as the scientific and implementation results of the ComProFITS project as an innovative web-based platform for the evaluation of existing employees and the recruitment of new employees in organizations. The platform integrates research on the statistical assessment of competences, an innovative competence pyramid and many alternative methods of employee evaluation and recruitment. The initial version of the platform is being applied and used in the IT sector in a large organization in Spain with the goal of extending it to the Spanish and European wide enterprises in different sectors.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:LO7wyVUgiFcC",
            "Publisher": "Aristotle University of Thessaloniki"
        },
        {
            "Title": "Visual comparison of software cost estimation models by regression error characteristic analysis",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S016412120900288X",
            "Abstract": "The well-balanced management of a software project is a critical task accomplished at the early stages of the development process. Due to this requirement, a wide variety of prediction methods has been introduced in order to identify the best strategy for software cost estimation. The selection of the best technique is usually based on measures of error whereas in more recent studies researchers use formal statistical procedures. The former approach can lead to unstable and erroneous results due to the existence of outlying points whereas the latter cannot be easily presented to non-experts and has to be carried out by an expert with statistical background. In this paper, we introduce the regression error characteristic (REC) analysis, a powerful visualization tool with interesting geometrical properties, in order to validate and compare different prediction models easily, by a simple inspection of a graph. Moreover, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:bEWYMUwI8FkC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Survival analysis on the duration of open source projects",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584910000790",
            "Abstract": "Open source (FLOSS) project survivability is an important piece of information for many open source stakeholders. Coordinators of open source projects would like to know the chances for the survival of the projects they coordinate. Companies are also interested in knowing how viable a project is in order to either participate or invest in it, and volunteers want to contribute to vivid projects.The purpose of this article is the application of survival analysis techniques for estimating the future development of a FLOSS project.In order to apply such approach, duration data regarding FLOSS projects from the FLOSSMETRICS (This work was partially supported by the European Community\u2019s Sixth Framework Program under the Contract FP6-033982) database were collected. Such database contains metadata for thousands of FLOSS projects, derived from various forges. Subsequently, survival \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:qUcmZB5y_30C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A simulated annealing approach for multimedia data placement",
            "Publication year": 2004,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121203002632",
            "Abstract": "Multimedia applications are characterized by their strong timing requirements and constraints and thus multimedia data storage is a critical issue in the overall system's performance and functionality. This paper describes multimedia data representation models that effectively guide data placement towards the improvement of the Quality of Presentation for the considered multimedia applications. The performance of both constructive placement and iterative improvement placement algorithms is evaluated and discussed. Emphasis is given on placement schemes which are based on the simulated annealing optimization algorithm. A placement policy, based on a self-improving version of the simulated annealing (SISA) algorithm is applied and evaluated. Performance of the placement policies is experimentally evaluated on a simulated tertiary storage subsystem. As proven by the experimentation, the proposed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:YOwf2qJgpHMC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Comparing Cross-vs. Within-Company Effort Estimation Models Using Interval Estimates",
            "Publication year": 0,
            "Publication url": "http://users.uowm.gr/sbibi/download/C10.pdf",
            "Abstract": "This paper investigates whether effort predictions for projects from a single company that were obtained using a cross-company (CC) training set can be as accurate as effort predictions obtained using a within-company (WC) training set. We employed five different cost estimation techniques, two providing point estimates (estimation by analogy and stepwise regression) and three providing predefined interval estimates (ordinal regression, classification and regression trees and Bayesian networks). For the development and evaluation of both cross and within company models ISBSG release 9 was utilized. Our results showed no significant differences between CC and WC-based predictions, for all the cost estimation techniques, after comparing the medians of the absolute errors. Other accuracy metrics were also considered, providing in general similar results.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:SeFeTyx0c_EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The impact of information security events to the stock market: A systematic literature review",
            "Publication year": 2016,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167404816300013",
            "Abstract": "Information security is a highly critical aspect of information systems. Although the literature regarding security assurance is vast, the research on economic consequences of security incidents is quite limited. The purpose of this systematic review is to search, collect and classify event studies related to information security impact on stock prices. In total, 37 related papers conducting 45 studies were found by the systematic search of bibliographic sources. The majority (75.6%) of these studies report statistical significance of the impact of security events to the stock prices of firms.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:tzM49s52ZIMC",
            "Publisher": "Elsevier Advanced Technology"
        },
        {
            "Title": "Software technologies skills: A graph-based study to capture their associations and dynamics",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3351556.3351565",
            "Abstract": "Software design and development technologies evolve very fast and in unpredicted rates, posing many challenges for programmers who strive to use them properly and to be up-to-date, especially since software development demands teamwork and collaboration. As a result, Question and Answer (Q&A) sites, like Stack Overflow, have seen large growth. The questions are characterized by tags, which support developers to easily trace their topic of interest. Very often, these tags refer to technologies that are connected or serve the same purpose. This work is motivated by the fact that despite the volume of questions and technologies change over time, tags inter-connections carry insightful information since they can be utilized to monitor the technology trends and their dynamics given technologies fast simultaneous evolution over time. This work recognizes the value of such connections, to reveal associations of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:Ri6SYOTghG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Archetypal personalities of software engineers and their work preferences: a new perspective for empirical studies",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/article/10.1007/s10664-015-9395-3",
            "Abstract": "As the area of Software Engineering (SE) matures the role of human factors in software development is commonly recognized as important. Increasingly we see empirical studies that investigate the connection between, for example, personalities and preferences, attitudes or performances of software engineers. Statistical analysis holds a key role by providing the means for uncovering associations between various facets of human factors and behavioral effects on projects and outcomes. Traditional statistical techniques tend to explore and interpret the multidimensional personality and behavioral data from an \u201caverage-point\u201d perspective, targeting central trends. This paper introduces a methodology with statistical tools that can provide a new and different perspective for this type of SE data. It seeks the boundaries of a psychometric dataset and discovers reference or \u201cbenchmark\u201d personalities, the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:7T2F9Uy0os0C",
            "Publisher": "Springer US"
        },
        {
            "Title": "Share open source sources-Reply",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=4467179667949932731&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:sSrBHYA8nusC",
            "Publisher": "ASSOC COMPUTING MACHINERY"
        },
        {
            "Title": "Alternative methods using similarities in software effort estimation",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2365324.2365333",
            "Abstract": "A large variety of methods has been proposed in the literature about Software Cost Estimation, in order to increase accuracy when predicting the effort of developing new projects. Estimation by Analogy is one of the most studied techniques in this area the last 20 years. The popularity of the methodology can be explained by its accordance to human problem thinking and solving, the straightforward interpretation and the usually comparable accuracy to other methodologies. Furthermore, the methodology is essentially a special case of non-parametric regression, easily implementable and free of theoretical assumptions, based on the notion of\" similarity\" which is used to define\" neighbors\". All of these reasons led us to study the technique in more depth, considering alternative ways to exploit similarities, in order to assign weights to neighbors. In this paper, our aim is to review the existing weighting practices and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:Tiz5es2fbqcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "BRACE: BootstRap based Analogy Cost Estimation: Automated support for an enhanced effort prediction method",
            "Publication year": 2001,
            "Publication url": "https://www.academia.edu/download/41833403/download.pdf",
            "Abstract": "Estimation by analogy is a technique that has been proposed since a long time as a valid alternative to algorithmic cost estimation and expert judgement. On the other hand, it is widely accepted that public domain software cost estimation techniques need to be calibrated, ie properly adjusted, to historical project data, in order to produce accurate estimates for new projects. Another important requirement for any modern cost estimation method is the ability to produce interval estimates, not mere point estimates. Statistical simulation techniques may be used for both purposes. The paper presents BRACE, a software tool that supports the practical application of the analogy based method using a simulation approach, namely bootstrap.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:maZDTaKrznsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An empirical study of COVID-19 related posts on Stack Overflow: Topics and technologies",
            "Publication year": 2021,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121221001862",
            "Abstract": "The COVID-19 outbreak, also known as the coronavirus pandemic, has left its mark on every aspect of our lives and at the time of this writing is still an ongoing battle. Beyond the immediate global-wide health response, the pandemic has triggered a significant number of IT initiatives to track, visualize, analyze and potentially mitigate the phenomenon. For individuals or organizations interested in developing COVID-19 related software, knowledge-sharing communities such as Stack Overflow proved to be an effective source of information for tackling commonly encountered problems. As an additional contribution to the investigation of this unprecedented health crisis and to assess how fast and how well the community of developers has responded, we performed a study on COVID-19 related posts in Stack Overflow. In particular, we profiled relevant questions based on key post features and their evolution, identified \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:yFnVuubrUp4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Investigating the impact of personality types on communication and collaboration-viability in pair programming\u2013an empirical study",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/11774129_5",
            "Abstract": "This paper presents two controlled experiments (a pilot and the main one) investigating the impact of developer personalities and temperaments on communication, collaboration-pair viability and ultimately effectiveness in pair programming. The objective of the experiments was to compare pairs of mixed/ heterogeneous developer personalities and temperaments with pairs of the same personalities and temperaments, in terms of pair effectiveness. Pair effectiveness is expressed in terms of pair performance, measured by communication, velocity, productivity and customer satisfaction, and pair collaboration-viability measured by developers\u2019 satisfaction, knowledge acquisition and participation (collaboration satisfaction ratio, nuisance ratio, voluntary or mandatory preference, and driver or navigator preference). The results have shown that there is significant difference between the two groups, indicating \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:WF5omc3nYNoC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Causal Models for the Result of Percutaneous Coronary Intervention in Coronary Chronic Total Occlusions",
            "Publication year": 2021,
            "Publication url": "https://www.mdpi.com/2076-3417/11/19/9258",
            "Abstract": "Background: Patients undergoing coronary angiography very frequently exhibit coronary chronic total occlusions (CTOs). Over the last decade, there has been an increasing acceptance of the percutaneous coronary interventions (PCI) in CTOs due to, among else, rising operator experience and advances in technology. This study is an effort to address the problem of identifying important factors related to the success or failure of the PCI. Methods: The analysis is based on the EuroCTO Registry, which is the largest database available worldwide, consisting of 164 variables and 29,995 cases for the period 2008\u20132018. The aim is to assess the dynamics of causal models and causal discovery, using observational data, in predicting the result of the PCI. Causal models use graph structure to assess the cause\u2013effect relationships between variables. In this study, the constrained-based algorithm PC was employed. The focus was to find the local causal structure around the PCI result and use it as a feature selection tool for building a predictive model. Results: The model developed was compared with other modeling approaches from the literature, and it was found to perform equally well or better. Conclusions: The analysis showcased the potential of employing local causal structure in predictive model development. ",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:OcBU2YAGkTUC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Multiple objective optimization of sampling designs for forest inventories using random search algorithms",
            "Publication year": 2004,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0168169903001212",
            "Abstract": "In forest inventories, various sampling techniques, also known as sampling designs, are used for the collection of the necessary information, which will provide precise sample estimates of the tree population characteristics at a low cost. The traditional single objective optimization in sampling is usually achieved either by minimizing the variance of the sample estimates assuming constraints on the cost of all the measurements in the sample or by minimizing the cost assuming constraints on the variance. Another quite realistic approach is to consider the optimization as a multiple objective optimization problem where the cost and the variance are simultaneously minimized functions. This paper proposes variations of a random search algorithm known as simulated annealing algorithm (SAA) for finding optimal sampling designs. The algorithm is first described for the single objective optimization problem and then is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Prioritized test-driven reverse engineering process",
            "Publication year": 2016,
            "Publication url": "http://ikee.lib.auth.gr/record/300849",
            "Abstract": "In this study we empirically investigate the adaptation of Test-Driven Development (TDD) practice into software Reverse Engineering (RE) process. We call this adaptation as Test-Driven Reverse Engineering (TDRE) process. We propose a two-layer prioritization process, which firstly prioritizes the already-implemented functionalities using the Cumulative Voting (CV) method and three prioritization criteria (importance, complexity and dependency), and secondly prioritizes test-cases for each prioritized functionality, using the same criteria. We conducted a case study in academia with students to empirically evaluate the usability and effectiveness of the prioritization process and the TDD adaptation into RE process. The results have shown that students with a good performance in testing had also good performance in designing UML class-diagrams. Moreover, the implementation of hierarchical test-cases for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:8d8msizDQcsC",
            "Publisher": "Aristotle University of Thessaloniki"
        },
        {
            "Title": "A Study on Workload Characterization for a Web Proxy Server.",
            "Publication year": 2003,
            "Publication url": "https://www.academia.edu/download/30843131/10.1.1.81.9975.pdf",
            "Abstract": "The popularity of the World-Wide-Web has increased dramatically in the past few years. Web proxy servers have an important role in reducing server loads, network traffic, and client request latencies. This paper presents a detailed workload characterization study of a busy Web proxy server. The study aims in identifying the major characteristics which will improve modelling of Web proxy accessing. A set of log files is processed for workload characterization. Throughout the study, emphasis is given on identifying the criteria for a Web caching model. A statistical analysis, based on the previous criteria, is presented in order to characterize the major workload parameters. Results of this analysis are presented and the paper concludes with a discussion about workload characterization and content delivery issues.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:u_35RYKgDlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Functional annotation of genes through statistical analysis of biomedical",
            "Publication year": 0,
            "Publication url": "https://www.researchgate.net/profile/Lefteris-Angelis/publication/4174840_Functional_annotation_of_genes_through_statistical_analysis_of_biomedical_articles/links/0912f50f56aeca7b04000000/Functional-annotation-of-genes-through-statistical-analysis-of-biomedical-articles.pdf",
            "Abstract": "One of the most elaborate and important tasks in biology is the functional annotation of genes. Biologists have developed standardized and structured vocabularies, called bio-ontologies, to assist them in describing the different functions. A critical issue in the assignment of functions to genes is the utilization of knowledge from published biomedical articles. The purpose of this paper is to present a unified and comprehensive statistical methodology for functionally annotating genes using biomedical literature. Specifically, classification models are built using the discriminant analysis method while validation, analysis and interpretation of the results is based on graphical methods and various performance metrics and techniques. The general conclusions from the study are very promising, in the sense that the proposed methodology not only performs well in the assignment of functions to genes, but also provides useful and interpretable results regarding the discriminating power of certain keywords in the texts.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:AvfA0Oy_GE0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "The coming of age for big data in systems radiobiology, an engineering perspective",
            "Publication year": 2021,
            "Publication url": "https://www.liebertpub.com/doi/abs/10.1089/big.2019.0144",
            "Abstract": "As high-throughput approaches in biological and biomedical research are transforming the life sciences into information-driven disciplines, modern analytics platforms for big data have started to address the needs for efficient and systematic data analysis and interpretation. We observe that radiobiology is following this general trend, with -omics information providing unparalleled depth into the biomolecular mechanisms of radiation response\u2014defined as systems radiobiology. We outline the design of computational frameworks and discuss the analysis of big data in low-dose ionizing radiation (LDIR) responses of the mammalian brain. Following successful examples and best practices of approaches for the analysis of big data in life sciences and health care, we present the needs and requirements for radiation research. Our goal is to raise awareness for the radiobiology community about the new technological \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:2VqYfGB8ITEC",
            "Publisher": "Mary Ann Liebert, Inc., publishers"
        },
        {
            "Title": "All-or-nothing transforms using quasigroups",
            "Publication year": 2003,
            "Publication url": "http://delab.csd.auth.gr/bci1/Balkan/183Marnas.pdf",
            "Abstract": "In this paper we suggest a new transformation scheme for All-Or-Nothing encryption, originally suggested by Rivest. The new transform concerns the use of quasigroups for the preprocessing of the data before any ordinary encryption method. We describe a method of constructing random quasigroups and we propose a way of using the advantages of quasigroup in Rivest's method. This combination makes the method faster and maintains the advantages against brute-force attacks.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:8k81kl-MbHgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "WIVSS: a new methodology for scoring information systems vulnerabilities",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2491845.2491871",
            "Abstract": "Vulnerabilities of information systems constitute an ever-increasing problem that IT security management must solve. As the number of vulnerabilities is growing exponentially, their ranking and prioritization is a crucial task for organizations and researchers that are involved with the security of computer systems. The open standard to score and rank the vulnerabilities is the Common Vulnerability Scoring System (CVSS) while the focus of this research is to investigate ways to improve it by achieving higher diversity of values and better accuracy. In this paper it is introduced a new vulnerability scoring system, called WIVSS (Weighted Impact Vulnerability Scoring System). The methodology uses a different approach to score vulnerabilities, depending on the different impact of vulnerabilities characteristics. The methodology WIVSS is applied to the most recent 9455 vulnerabilities and the results show improvement in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:mvPsJ3kp5DgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An Interdisciplinary Perspective to the Design and Decision Support of Integral Safety Systems",
            "Publication year": 2013,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1474667015340027",
            "Abstract": "Next generation integral safety systems are expected to provide better protection against traffic accidents by interlinking sensors and actuators of active and passive safety. A series of advanced functions will be used to mitigate collisions and if they cannot be avoided they will at least reduce their severity. We explore the interplay between key technology areas towards a holistic approach in the design and decision support of integral safety systems. First, we refer to the main problems in the design of effective systems and the associated software engineering challenges. Recent advances in sensor data analytics are then explored and their integration with decision support for vehicle control is examined. Finally, we envision that rigorous design techniques based on models for human-machine interaction are essential for achieving adequate performance and robustness of integral safety systems.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:D_sINldO8mEC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Categorical missing data imputation for software cost estimation by multinomial logistic regression",
            "Publication year": 2006,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121205000129",
            "Abstract": "A common problem in software cost estimation is the manipulation of incomplete or missing data in databases used for the development of prediction models. In such cases, the most popular and simple method of handling missing data is to ignore either the projects or the attributes with missing observations. This technique causes the loss of valuable information and therefore may lead to inaccurate cost estimation models. On the other hand, there are various imputation methods used to estimate the missing values in a data set. These methods are applied mainly on numerical data and produce continuous estimates. However, it is well known that the majority of the cost data sets contain software projects with mostly categorical attributes with many missing values. It is therefore reasonable to use some estimating method producing categorical rather than continuous values. The purpose of this paper is to investigate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:3fE2CSJIrl8C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Multiple logistic regression as imputation method applied on software effort prediction",
            "Publication year": 2004,
            "Publication url": "https://www.academia.edu/download/41833457/Multiple_Logistic_Regression_as_Imputati20160131-27253-15km1rz.pdf",
            "Abstract": "A common problem in software cost estimation is the manipulation of incomplete or missing data in databases used for the development of prediction models. In such cases, the most popular and simple method of handling missing data is to ignore either the projects or the attributes with missing observations. This technique causes the loss of valuable information and therefore may lead to inaccurate cost estimation models. On the other hand, there are various imputation methods used to estimate the missing values in a data set. These methods are applied mainly on numerical data and produce continuous estimates. However, it is well known that the majority of the cost data sets contain software projects with mostly categorical attributes with many missing values. It is therefore reasonable to use some estimating method producing categorical rather than continuous values. The purpose of this paper is to investigate the possibility of using such a method for estimating categorical missing values in software cost databases. Specifically, the method known as Multinomial Logistic Regression (MLR) is suggested for imputation and is applied on projects of the ISBSG multiorganizational software database. Comparisons of MLR with other missing data techniques, such as listwise deletion (LD), mean imputation (MI), expectation maximization (EM) and regression imputation (RI) show that the proposed method is efficient, especially when the percentage of missing values is high.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:PoWvk5oyLR8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "ALTERNATIVE M\u2014ESTIMATORS OF LOCATION AND THEIR LINEAR CONVEX COMBINATIONS",
            "Publication year": 1999,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:Dip1O2bNi0gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Gene functional annotation by statistical analysis of biomedical articles",
            "Publication year": 2007,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1386505606001195",
            "Abstract": "Functional annotation of genes is an important task in biology since it facilitates the characterization of genes relationships and the understanding of biochemical pathways. The various gene functions can be described by standardized and structured vocabularies, called bio-ontologies. The assignment of bio-ontolgy terms to genes is carried out by means of applying certain methods to datasets extracted from biomedical articles. These methods originate from data mining and machine learning and include maximum entropy or support vector machines (SVM).The aim of this paper is to propose an alternative to the existing methods for functionally annotating genes. The methodology involves building of classification models, validation and graphical representations of the results and reduction of the dimensions of the dataset.Classification models are constructed by Linear discriminant \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:UebtZRa9Y70C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Cross-study reliability of the open card sorting method",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3290607.3312999",
            "Abstract": "Information architecture forms the foundation of users' navigation experience. Open card sorting is a widely-used method to create information architectures based on users' groupings of the content. However, little is known about the method's cross-study reliability: Does it produce consistent content groupings for similar profile participants involved in different card sort studies? This paper presents an empirical evaluation of the method's cross-study reliability. Six card sorts involving 140 participants were conducted: three open sorts for a travel website, and three for an eshop. Results showed that participants provided highly similar card sorting data for the same content. A rather high agreement of the produced navigation schemes was also found. These findings provide support for the cross-study reliability of the open card sorting method.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:FPJr55Dyh1AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Secure Internet Storage of Files: Design and Implementation",
            "Publication year": 0,
            "Publication url": "https://www.academia.edu/download/30803924/C1.1.pdf",
            "Abstract": "It is evident that Internet becomes a necessary tool for more and more users every day. However, there is also a growing demand for mobility of users in the sense that they need to store and manage their data at any time and place independently and securely. Since free storage space on the net and fast connection is available in public places, many people may decide to use these sources from remote workstations without having to obtain and install special software for this purpose. In this paper we present design and implementation of an on-line application which provides real time encryption and decryption facilities for files stored in a public server. The application offers autonomy to users that wish to administer securely their data from any workstation by simply connecting to the public server.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:g5m5HwL7SMYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Appreciation to Empirical Software Engineering reviewers of 2020",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s10664-021-09952-w.pdf",
            "Abstract": "Appreciation to Empirical Software Engineering reviewers of 2020 Page 1 Appreciation to \nEmpirical Software Engineering reviewers of 2020 \u00a9 Springer Science+Business Media, LLC, part \nof Springer Nature 2021 For helping us deliver timely decisions to our authors, the Editors-in-Chief \nand Publisher would like to thank the following individuals who contributed reviews between \nJanuary 1, 2020 and December 31, 2020. We applaud your efforts and dedication to the \ncommunity. https://doi.org/10.1007/s10664-021-09952-w Yousra Aafer Rabe Abdalkareem \nTamer Abdou Pekka Abrahamsson Rui Abreu Mathieu Acher Bram Adams Amritanshu \nAgrawal Bestoun Ahmed Iftekhar Ahmed Emil Alegroth Daniel Alencar da Costa Aldeida Aleti \nNauman bin Ali Shaukat Ali Saba Alimadadi Kevin Allix Eduardo Almeida Afnan Al-Subaihin \nDomenico Amalfitano Vasco Amaral Sousuke Amasaki Apostolos Ampatzoglou Daniel Le \u2019\u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:bKqednn6t2AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The success factors powering industry-academia collaboration",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5963631/",
            "Abstract": "Collaboration between industry and academia supports improvement and innovation in industry and helps to ensure industrial relevance in academic research. This article presents an exploratory study of the factors for successful collaboration between industry and academia in software research.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:70eg2SAEIzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Quantification of interacting runtime qualities in software architectures: Insights from transaction processing in client\u2013server architectures",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584910001321",
            "Abstract": "Architecture is fundamental for fulfilling requirements related to the non-functional behavior of a software system such as the quality requirement that response time does not degrade to a point where it is noticeable. Approaches like the Architecture Tradeoff Analysis Method (ATAM) combine qualitative analysis heuristics (e.g. scenarios) for one or more quality metrics with quantitative analyses. A quantitative analysis evaluates a single metric such as response time. However, since quality metrics interact with each other, a change in the architecture can affect unpredictably multiple quality metrics.This paper introduces a quantitative method that determines the impact of a design change on multiple metrics, thus reducing the risks in architecture design. As a proof of concept, the method is applied on a simulation model of transaction processing in client server architecture.Factor analysis is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:NMxIlDl6LWMC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Internet based auctions: a survey on models and applications",
            "Publication year": 2001,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/844316.844319",
            "Abstract": "Web based auctions and negotiations have become quite popular due to their implementation and integration in electronic commerce applications. Online auctions have become an effective approach in the buying and selling process, employed in the rapidly emerging Internet-based electronic commerce platforms. The goal of this paper is to outline research efforts in relation to online Internet-based auctions modeling and applications. The key research topics in the area are identified and the paper focus on the issues of auction modeling, use of agents in Web-based auction implementations, auctions computational and combinatorial analysis as well as on security issues involved in online auctioning. The most popular implementations, applications and servers are also referenced and discussed.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:aqlVkmm33-oC",
            "Publisher": "ACM"
        },
        {
            "Title": "Differences in correlation structure of gene expression data, used for variable selection in Support Vector Machines algorithm",
            "Publication year": 0,
            "Publication url": "http://www2.stat-athens.aueb.gr/~emribs/page/EMR2017/bookofabstracts.pdf#page=165",
            "Abstract": "This paper concerns Support Vector Machines (SVM), a supervised learning technique which is applied to data that are already labeled as members of categories. SVM are useful for classification and prediction and the typical application involves a learning phase on a training dataset in order to predict the classes of new cases of a test dataset. As with most machine learning techniques, variable selection methods can increase the SVM accuracy of prediction by reducing the dimensions of the dataset, especially in cases of big data. The basic idea of the present work is that in certain datasets the correlation structure of variables is complicated and di erent in specific classes; therefore the recognition of underlying patterns is of greater importance than studying the behavior of individual variables. Here, by comparing correlation coe cients of two distinct groups, we choose the variables that participate in most pairs \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:uWiczbcajpAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Maximal software execution time: a regression-based approach",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/article/10.1007/s11334-018-0314-9",
            "Abstract": "This work aims at facilitating the schedulability analysis of non-critical systems, in particular those that have soft real-time constraints, where worst-case execution times (WCETs) can be replaced by less stringent probabilistic bounds, which we call maximal execution times (METs). To this end, it is possible to obtain adequate probabilistic execution time models by separating the non-random dependency on input data from a modeling error that is purely random. The proposed approach first utilizes execution time multivariate measurements for building a multiple regression model and then uses the theory related to confidence bounds of coefficients, in order to estimate the upper bound of execution time. Although certainly our method cannot directly achieve extreme probability levels that are usually expected for WCETs, it is an attractive alternative for MET analysis, since it can arguably guarantee safe \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:LI9QrySNdTsC",
            "Publisher": "Springer London"
        },
        {
            "Title": "PuReD-MCL: a graph-based PubMed document clustering methodology",
            "Publication year": 2008,
            "Publication url": "https://academic.oup.com/bioinformatics/article-abstract/24/17/1935/260825",
            "Abstract": " Motivation: Biomedical literature is the principal repository of biomedical knowledge, with PubMed being the most complete database collecting, organizing and analyzing such textual knowledge. There are numerous efforts that attempt to exploit this information by using text mining and machine learning techniques. We developed a novel approach, called PuReD-MCL (Pubmed Related Documents-MCL), which is based on the graph clustering algorithm MCL and relevant resources from PubMed. Methods: PuReD-MCL avoids using natural language processing (NLP) techniques directly; instead, it takes advantage of existing resources, available from PubMed. PuReD-MCL then clusters documents efficiently using the MCL graph clustering algorithm, which is based on graph flow simulation. This process allows users to analyse the results by highlighting important clues, and finally to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:4TOpqqG69KYC",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "Requirements and architecture design principles for a smart city experiment with sensor and social networks integration",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2491845.2491887",
            "Abstract": "Smart city infrastructures offer unique testbeds ground for innovative experimentation and services offering. Sensors networks in cities with integrated social networks activities can improve people-centric services, while improving infrastructures setting. This work summarizes the principles and priorities chosen in a smart city experiment, entitled SEN2SOC which bridges sensor measurements and social networks interactions for supporting smart city services. SEN2SOC prioritizes requirements along particular categories which cover data collection, users sensing along with applications implementation and architectural concerns. These requirements are correlated with the suggested components in an architecture which is flexible enough in order to permit various activities control flow in terms of data preprocessing, conditions detection, statistical analysis as well as applications development and social data mining.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:kRWSkSYxWN8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Postgraduate Studies in Informatics Field of Information Systems",
            "Publication year": 2012,
            "Publication url": "http://ikee.lib.auth.gr/record/128955/files/GRI-2012-8430.pdf",
            "Abstract": "Software quality is one of the main contributing factors to effective programming. Although it has a quite ambiguous meaning and there is a plethora of methods to measure it, quality can be measured by several quality metrics, which have been appropriately formulated through the years. Moreover, software measurement is a particularly important procedure for software development process, as it provides the developer with meaningful information about the process and the product. This master thesis aims to measure an open source software project, and more specifically, the R project, and examine the relationships among the observed metrics and special attributes of the R software. For this purpose, a random sample of 800 R packages were downloaded from the CRAN repository of R and measured, using the SourceMonitor metrics tool. The resulted values of the metrics, along with a significant number of specific attributes of the packages, were examined and analyzed, leading to interesting conclusions. In this thesis, we firstly describe the meanings of software measurement and software quality and then describe some well-known software metrics. The second chapter includes an overview of the metrics tools, which were examined in order to choose the most appropriate for this study. The third chapter discusses about the R project and presents interesting specificities of R packages, which must become known in order for the reader to conceive the following measurement procedure and data analysis. In the fourth chapter, we thoroughly discuss about the measurement of our sample, whereas in the fifth chapter we introduce the results of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:kh2fBNsKQNwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Applying the Mahalanobis-Taguchi strategy for software defect diagnosis",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/article/10.1007/s10515-011-0091-2",
            "Abstract": "The Mahalanobis-Taguchi (MT) strategy combines mathematical and statistical concepts like Mahalanobis distance, Gram-Schmidt orthogonalization and experimental designs to support diagnosis and decision-making based on multivariate data. The primary purpose is to develop a scale to measure the degree of abnormality of cases, compared to \u201cnormal\u201d or \u201chealthy\u201d cases, i.e. a continuous scale from a set of binary classified cases. An optimal subset of variables for measuring abnormality is then selected and rules for future diagnosis are defined based on them and the measurement scale. This maps well to problems in software defect prediction based on a multivariate set of software metrics and attributes. In this paper, the MT strategy combined with a cluster analysis technique for determining the most appropriate training set, is described and applied to well-known datasets in order to evaluate the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:NaGl4SEjCO4C",
            "Publisher": "Springer US"
        },
        {
            "Title": "Athina Tsanousa1, Stavroula Ntoufa2, 3, Nikos Papakonstantinou2, 3",
            "Publication year": 2016,
            "Publication url": "http://ndl.ethernet.edu.et/bitstream/123456789/32441/1/63.pdf#page=572",
            "Abstract": "Statistical methods are applied in many scientific fields where complex systems have to be studied and there is the need to discover relations and patterns among data or",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:5qfkUJPXOUwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Similarity based distributed classification",
            "Publication year": 2002,
            "Publication url": "https://www.academia.edu/download/46076415/Similarity_Based_Distributed_Classificat20160530-23643-inngf5.pdf",
            "Abstract": "Most distributed knowledge discovery approaches view data distribution as a technical issue and combine local models aiming at a single global model. This however, is unsuitable for inherently distributed databases, which often produce models that differ semantically. In this paper we present an approach for distributed classification that uses the pairwise similarity of local models in order to produce a better model for each of the distributed databases. This is achieved by averaging the decisions of all local models weighted by their similarity with the model induced from the origin of the unlabelled data.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:isC4tDSrTZIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving analogy-based software cost estimation by a resampling method",
            "Publication year": 2008,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584907000328",
            "Abstract": "Estimation by analogy (EbA) is a well-known technique for software cost estimation. The popularity of the method is due to its straightforwardness and its intuitively appealing interpretation. However, in spite of the simplicity in application, the theoretical study of EbA is quite complicated. In this paper, we exploit the relation of EbA method to the nearest neighbor non-parametric regression in order to suggest a resampling procedure, known as iterated bagging, for reducing the prediction error. The improving effect of iterated bagging on EbA is validated using both artificial and real datasets from the literature, obtaining very promising results.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:eQOLeE2rZwMC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "ACID Sim Tools: Evaluating tradeoffs between recovery costs and performance in Distributed Transaction Processing Architectures",
            "Publication year": 0,
            "Publication url": "https://scholar.google.com/scholar?cluster=2322462003647065155&hl=en&oi=scholarr",
            "Abstract": "In modern network centric information systems, multi-tier applications are now becoming mainstream. Their design is mainly based on the fundamental principles of objectorientation and the most common recovery solutions ensure at-most-once service request processing, through some form of \u201call-or-nothing\u201d guarantee by an underlying transaction processing architecture. Transactional objects cooperate with the assigned transaction managers to provide system-wide Atomicity, Consistency, Isolation and Durability (ACID) guarantees for the performed operations. Architectural solutions are standardized in reference specifications, with the most notable one the OMG Object Transaction Service (OTS). However, to the best of our knowledge there are no evaluation means with metrics, which can provide insight into the tradeoffs between recovery costs and performance when applying different combinations of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:J_g5lzvAfSwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tables of A-efficient BTIUB Designs",
            "Publication year": 1993,
            "Publication url": "https://scholar.google.com/scholar?cluster=1384438909775140242&hl=en&oi=scholarr",
            "Abstract": "Angelis, Kageyama and Moyssiadis (1992) have developed some methods of constructing balanced treatment incomplete block designs with two different block sizes (BTIUB designs), in order to compare some test treatments with a control in unequal blocks. Each method produces A-optimal or highly A-efficient designs. In this paper, extensive tables of such BTIUB designs constructed by those methods are provided for practical usage.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:_Ybze24A_UAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Selective fusion of heterogeneous classifiers",
            "Publication year": 2005,
            "Publication url": "https://content.iospress.com/articles/intelligent-data-analysis/ida00225",
            "Abstract": "There are two main paradigms in combining different classification algorithms: Classifier Selection and Classifier Fusion. The first one selects a single model for classifying a new instance, while the latter combines the decisions of all models. The work presented in this paper stands in between these two paradigms aiming to tackle the disadvantages and benefit from the advantages of both. In particular, this paper proposes the use of statistical procedures for the selection of the best subgroup among different classification algorithms and the subsequent fusion of the decision of the models in this subgroup with simple methods like Weighted Voting. Extensive experimental results show that the proposed approach, Selective Fusion, improves over simple selection and fusion methods, leading to performance comparable with the state-of-the-art heterogeneous classifier combination method of Stacking, without the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "IOS Press"
        },
        {
            "Title": "An application of quasigroups in all-or-nothing transform",
            "Publication year": 2007,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/01611190601186255",
            "Abstract": "All-Or-Nothing (AON) is an encryption mode for block ciphers with the property that an adversary must decrypt the entire ciphertext in order to determine any plaintext block. In this article, we present a new encryption scheme with the AON property, based on operations defined by quasigroups. The proposed procedure is a reliable and secure preprocessing step to any other common encryption mode, aiming to slow down the brute force searches against block ciphers.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:abG-DnoFyZgC",
            "Publisher": "Taylor & Francis Group"
        },
        {
            "Title": "Inter-transaction association rules mining for rare events prediction",
            "Publication year": 2004,
            "Publication url": "https://intelligence.csd.auth.gr/wp-content/uploads/2019/03/076-Berberidis-Angelis-Vlahavas-SETN04.pdf",
            "Abstract": "Rare events prediction is a very interesting and critical issue that has been approached within various contexts by research areas, such as statistics and machine learning. Data mining has provided a set of tools to treat this problem when the size as well as the inherent features of the data, such as noise, randomness and special data types, become an issue for the traditional methods. Transaction databases that contain sets of events require special approaches in order to extract valuable temporal knowledge. Sequential analysis aims to discover patterns or rules describing the temporal structure of data. In this paper we propose an approach that extends sequential analysis to predict rare events in transaction databases. We utilize the framework of inter-transaction association rules, which associate events across a window of transactions. The proposed algorithm produces rules for the accurate and timely prediction of a userspecified rare event, such as a network intrusion or an engine failure.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Extracting knowledge from on-line sources for software engineering labor market: A mapping study",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8884193/",
            "Abstract": "Software engineering is a continuously evolving sector and the demands of the related labor market result in a wide variety of job openings, ranging from developers to customer service positions. Thus, there is a need to continuously monitor labor market trends using data and analytics. Both employers and employees can benefit by capturing emerging trends which can facilitate continuous learning and training in new technologies, support of better matching between a job offer and the ideal candidate and expertise detection. To fulfill these needs, the results of labor market analytics need to reach the stakeholders timely and accurately. However, often delays occur, which stem from time-consuming approaches based on collecting data from traditional sources, such as questionnaires or interviews. Recently, researchers started leveraging content from digital sources, which are easily accessed and contain a wealth \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:_FM0Bhl9EiAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "MeSHy",
            "Publication year": 2011,
            "Publication url": "http://ikee.lib.auth.gr/record/226532",
            "Abstract": "Motivation PubMed is the most widely used database of biomedical literature. To the detriment of the user though, the ranking of the documents retrieved for a query is not content-based, and important semantic information in the form of assigned Medical Subject Headings (MeSH) terms is not readily presented or productively utilized. The motivation behind this work was the discovery of unanticipated information through the appropriate ranking of MeSH term pairs and, indirectly, documents. Such information can be useful in guiding novel research and following promising trends. Methods A web-based tool, called MeSHy, was developed implementing a mainly statistical algorithm. The algorithm takes into account the frequencies of occurrences, concurrences, and the semantic similarities of MeSH terms in retrieved PubMed documents to create MeSH term pairs. These are then scored and ranked, focusing on their \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:dBIO0h50nwkC",
            "Publisher": "Aristotle University of Thessaloniki"
        },
        {
            "Title": "Managing the Uncertainty of Bias-Variance Tradeoff in Software Predictive Analytics",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7592817/",
            "Abstract": "The importance of providing accurate estimations of software cost in management life cycle has led to an overabundant pool of prediction candidates exhibiting certain advantages and limitations. Thus, there is an imperative need for well-established principles that will aid the right decision-making regarding the selection of the best candidate. Unfortunately, the choice of the most appropriate estimation technique is not a trivial task, due to the multi-faceted nature of error. Accuracy, bias and variance are notions describing different aspects of predictive power that someone has to take into consideration during the validation process. The main objective of this paper is the utilization of visual analytics for the evaluation of two fundamental ingredients of prediction accuracy: the bias and the variance. Through a bootstrap-based resampling algorithm, we provide an easy-to-interpret way in order to acquire significant \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:z_wVstp3MssC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Comparing cost prediction models by resampling techniques",
            "Publication year": 2008,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121207002348",
            "Abstract": "The accurate software cost prediction is a research topic that has attracted much of the interest of the software engineering community during the latest decades. A large part of the research efforts involves the development of statistical models based on historical data. Since there are a lot of models that can be fitted to certain data, a crucial issue is the selection of the most efficient prediction model. Most often this selection is based on comparisons of various accuracy measures that are functions of the model\u2019s relative errors. However, the usual practice is to consider as the most accurate prediction model the one providing the best accuracy measure without testing if this superiority is in fact statistically significant. This policy can lead to unstable and erroneous conclusions since a small change in the data is able to turn over the best model selection. On the other hand, the accuracy measures used in practice are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:Se3iqnhoufwC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Optimal designs with a single two-level factor and n autocorrelated observations",
            "Publication year": 1997,
            "Publication url": "https://scholar.google.com/scholar?cluster=13458586525393136882&hl=en&oi=scholarr",
            "Abstract": "The problem of finding an optimal design with a single two-level factor and n observations, under the presence of correlation between errors, is considered. Assuming errors following a first order autoregressive model, the exact D- and A-optimal designs for the distinct cases of positive and negative correlation were found and their optimality was proved explicitly.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:nb7KW1ujOQ8C",
            "Publisher": "UTIL MATH PUBL INC"
        },
        {
            "Title": "Sensors talk and humans sense towards a reciprocal collective awareness smart city framework",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6649226/",
            "Abstract": "Smart city infrastructures provide unique opportunities for innovative applications developing and testing. Sensor city installations offer the ground for experimenting with user-oriented services, which at the same time can test and improve the infrastructure itself. The proposed work summarizes principles and methodology for and experiment, entitled SEN2SOC which will bridge sensor measurements and social networks interactions via natural language generation for supporting smart city services. SEN2SOC aims at exploiting the SmartSantander infrastructure in a sensor to social reciprocal fashion such that the sensor measurements will be and communicated to the public (citizens, authorities, etc), while social networks users activities in relevance to sensors social postings will be analyzed and summarized both to verify sensors reporting and to develop collective aware applications.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:wbdj-CoPYUoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Non-linear correlation of content and metadata information extracted from biomedical article datasets",
            "Publication year": 2008,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1532046407000561",
            "Abstract": "Biomedical literature databases constitute valuable repositories of up to date scientific knowledge. The development of efficient machine learning methods in order to facilitate the organization of these databases and the extraction of novel biomedical knowledge is becoming increasingly important. Several of these methods require the representation of the documents as vectors of variables forming large multivariate datasets. Since the amount of information contained in different datasets is voluminous, an open issue is to combine information gained from various sources to a concise new dataset, which will efficiently represent the corpus of documents. This paper investigates the use of the multivariate statistical approach, called Non-Linear Canonical Correlation Analysis (NLCCA), for exploiting the correlation among the variables of different document representations and describing the documents with only one \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)| 978-1-6654-2705-0/21/$31.00\u00a9 2021 IEEE| DOI: 10.1109/SEAA53835. 2021.00058",
            "Publication year": 0,
            "Publication url": "https://www.computer.org/csdl/proceedings-article/seaa/2021/270500a393/1y2JArLUEBq",
            "Abstract": "Author Index IEEE.org Help Cart Jobs Board Create Account Toggle navigation IEEE Computer \nSociety Digital Library Jobs Tech News Resource Center Press Room Advertising About Us \nIEEE IEEE Computer Society IEEE Computer Society Digital Library My Subscriptions \nMagazines Journals Conference Proceedings Institutional Subscriptions IEEE IEEE Computer \nSociety More Jobs Tech News Resource Center Press Room Advertising About Us Cart All \nAdvanced Search Conference Cover Image Download 1.Home 2.Proceedings 3.seaa 2021 \nAuthor Index 2021, pp. 393-395, DOI Bookmark: 10.1109/SEAA53835.Keywords Authors Author \nIndex,Almeida, Cl\u00e1uvin , 17,Alonso, Silvio , 62,Alves, Carina , 296,Aman, Hirohisa , \n279,Amasaki, Sousuke , 279,Andrade, Gabriella , 291,Angelis, Lefteris , 252,Antesberger, Tobias \n, 351,Apatsidis, Ioannis , 252,Arai, Benjamin , 109,Ashraf, Adnan , 37,Auer, Florian , 342,, , 234,\u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:3htObqc8RwsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "James Miller, University of Alberta (Canada), chair Richard Selby, Northrop Grumman and University of Southern California (USA), chair Pekka Abrahamsson, VTT Technical Research Centre (Finland) Muhammed Ali Babar, Lero, University of Limerick (Ireland)",
            "Publication year": 0,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1109/ESEM.2009.5314222",
            "Abstract": "Full paper program committee Page 1 Full Paper Program Committee James Miller, University of \nAlberta (Canada), chair Richard Selby, Northrop Grumman and University of Southern California \n(USA), chair Pekka Abrahamsson, VTT Technical Research Centre (Finland) Muhammed Ali \nBabar, Lero, University of Limerick (Ireland) Anneliese Andrews, University of Denver (USA) \nLefteris Angelis, Aristotle University of Thessaloniki (Greece) Maria Teresa Baldassarre, \nUniversity of Bari (Italy) Andrew Begel, Microsoft (USA) Lionel Briand, Simula Research \nLaboratory (Norway) Giovanni Cantone, Universita' degli Studi di Roma \"Tor Vergata\" (Italy) \nJeffrey Carver, University of Alabama (USA) Michel Chaudron, Leiden University (Netherlands) \nMarcus Ciolkowski, Fraunhofer IESE (Germany) Reidar Conradi, Norwegian University of \nTechnology & Science (Norway) Tore Dyba, Sintef (Norway) Sebastian Elbaum, University - \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:RGFaLdJalmkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "CLL: Different ages, dfferent antigen receptor profiles",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=2528296666392605152&hl=en&oi=scholarr",
            "Abstract": "BALIAKAS P. MINGA E. HADZIDIMITRIOU A. TSANOUSA A. SUTTON L. SCARFO L. DAVIS Z. YAN J. PLEVOV\u00c1 Karla SANDBERG Y. JUHL F. BOUDJOGRA M. TZENOU T. CHATZOULI M. AGATHANGELIDIS A. CHU Ch. GARDINER A. SMEDBY K. MANSOURI L. PEDERSEN L. POIRON C. GIUDICELLI V. TICH\u00dd Boris PANAGIOTIDIS P. JULIUSSON G. ANAGNOSTOPOULOS A. ANGELIS L. LEFRANC M. GEISLER Ch. LANGERAK A. POSP\u00cd\u0160ILOV\u00c1 \u0160\u00e1rka CHIORAZZI N. BELESSI Ch. DAVI F. OSCIER D. DARZENTAS Nikos ROSENQUIST R. GHIA P. STAMATOPOULOS K.",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:WZBGuue-350C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Technical debt principal assessment through structural metrics",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8051367/",
            "Abstract": "One of the first steps towards the effective Technical Debt (TD) management is the quantification and continuous monitoring of the TD principal. In the current state-ofresearch and practice the most common ways to assess TD principal are the use of: (a) structural proxies-i.e., most commonly through quality metrics; and (b) monetized proxies-i.e., most commonly through the use of the SQALE (Software Quality Assessment based on Lifecycle Expectations) method. Although both approaches have merit, they seem to rely on different viewpoints of TD and their levels of agreement have not been evaluated so far. Therefore, in this paper, we empirically explore this relation by analyzing data obtained from 20 open source software projects and build a regression model that establishes a relationship between them. The results of the study suggest that a model of seven structural metrics, quantifying different aspects of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:ML0RJ9NH7IQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Mining people analytics from stackoverflow job advertisements",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8051336/",
            "Abstract": "Skills and competences of people participating in online professional networks constitute an ever-increasing new source for data collection and analysis. An important sub-domain of human resources management (HRM) is the recruitment process. Job advertisements and people profiles are main parts of recruitment and since are now available online, they constitute a key factor of a new e-recruitment era. Data mining for erecruitment analysis is important in order to extract a knowledge base for people analytics. Skills and competences are the key variables for people analytics and can be drawn from job advertisements. Leveraging the raw information of online job offers, provides a rich source for people analytics. Detecting the appropriate skills and competences for a job from raw text data and associate them with a job seeker is an increasing challenge. The main objective of this paper is the proposal of a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:vbGhcppDl1QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "LSEbA: least squares regression and estimation by analogy in a semi-parametric model for software cost estimation",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/article/10.1007/s10664-010-9128-6",
            "Abstract": "The importance of Software Cost Estimation at the early stages of the development life cycle is clearly portrayed by the utilization of several models and methods, appeared so far in the literature. The researchers\u2019 interest has been focused on two well known techniques, namely the parametric Regression Analysis and the non-parametric Estimation by Analogy. Despite the several comparison studies, there seems to be a discrepancy in choosing the best prediction technique between them. In this paper, we introduce a semi-parametric technique, called LSEbA that achieves to combine the aforementioned methods retaining the advantages of both approaches. Furthermore, the proposed method is consistent with the mixed nature of Software Cost Estimation data and takes advantage of the whole pure information of the dataset even if there is a large amount of missing values. The paper analytically \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "Springer US"
        },
        {
            "Title": "A Strong Seasonality Pattern for Covid-19 Incidence Rates Modulated by UV Radiation Levels",
            "Publication year": 2021,
            "Publication url": "https://www.mdpi.com/1052250",
            "Abstract": "The Covid-19 pandemic has required nonpharmaceutical interventions, primarily physical distancing, personal hygiene and face mask use, to limit community transmission, irrespective of seasons. In fact, the seasonality attributes of this pandemic remain one of its biggest unknowns. Early studies based on past experience from respiratory diseases focused on temperature or humidity, with disappointing results. Our hypothesis that ultraviolet (UV) radiation levels might be a factor and a more appropriate parameter has emerged as an alternative to assess seasonality and exploit it for public health policies. Using geographical, socioeconomic and epidemiological criteria, we selected twelve North-equatorial-South countries with similar characteristics. We then obtained UV levels, mobility and Covid-19 daily incidence rates for nearly the entire 2020. Using machine learning, we demonstrated that UV radiation strongly associated with incidence rates, more so than mobility did, indicating that UV is a key seasonality indicator for Covid-19, irrespective of the initial conditions of the epidemic. Our findings can inform the implementation of public health emergency measures, partly based on seasons in the Northern and Southern Hemispheres, as the pandemic unfolds into 2021. View Full-Text",
            "Abstract entirety": 1,
            "Author pub id": "IwB4XFYAAAAJ:QYdC8u9Cj1oC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "An Empirical Study on Views of Importance of Change Impact Analysis Issues",
            "Publication year": 0,
            "Publication url": "https://scholar.google.com/scholar?cluster=1077497246460609723&hl=en&oi=scholarr",
            "Abstract": "Change impact analysis is a change management activity that previously has been studied much from a technical perspective. For example, much work focuses on methods for determining the impact of a change. In this paper, we present results from a study on the role of impact analysis in the change management process. In the study, impact analysis issues were prioritised with respect to criticality by software professionals from an organisational perspective and a self-perspective. The software professionals belonged to three organisational levels: operative, tactical and strategic. Qualitative and statistical analyses with respect to differences between perspectives as well as levels are presented. The results show that important issues for a particular level are tightly related to how the level is defined. Similarly, issues important from an organisational perspective are more holistic than those important from a self \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:GnPB-g6toBAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Evaluating the agreement among technical debt measurement tools: building an empirical benchmark of technical debt liabilities",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s10664-020-09869-w",
            "Abstract": "Software teams are often asked to deliver new features within strict deadlines leading developers to deliberately or inadvertently serve \u201cnot quite right code\u201d compromising software quality and maintainability. This non-ideal state of software is efficiently captured by the Technical Debt (TD) metaphor, which reflects the additional effort that has to be spent to maintain software. Although several tools are available for assessing TD, each tool essentially checks software against a particular ruleset. The use of different rulesets can often be beneficial as it leads to the identification of a wider set of problems; however, for the common usage scenario where developers or researchers rely on a single tool, diverse estimates of TD and the identification of different mitigation actions limits the credibility and applicability of the findings. The objective of this study is two-fold: First, we evaluate the degree of agreement among \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IwB4XFYAAAAJ:_5tno0g5mFcC",
            "Publisher": "Springer US"
        }
    ]
}]