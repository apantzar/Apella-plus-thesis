[{
    "name": "\u03a0\u03b1\u03bd\u03b1\u03b3\u03b9\u03ce\u03c4\u03b7\u03c2 \u039a\u03b1\u03c4\u03c3\u03b1\u03c1\u03cc\u03c2",
    "romanize name": "Panagiotis Katsaros",
    "School-Department": "\u03a0\u03bb\u03b7\u03c1\u03bf\u03c6\u03bf\u03c1\u03b9\u03ba\u03ae\u03c2",
    "University": "\u0391\u03a0\u0398",
    "Rank": "\u0391\u03bd\u03b1\u03c0\u03bb\u03b7\u03c1\u03c9\u03c4\u03ae\u03c2 \u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 19323,
    "Scholar name": "Panagiotis Katsaros",
    "Scholar id": "IF0oXd8AAAAJ",
    "Affiliation": "Associate Professor of Computer Science, Aristotle University of Thessaloniki, Greece",
    "Citedby": 1469,
    "Interests": [
        "Software Security",
        "Verification",
        "Model Checking",
        "Formal Methods",
        "Software Architecture"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=IF0oXd8AAAAJ&hl=en",
    "Publications": [
        {
            "Title": "The Sphinx enigma in critical VoIP infrastructures: Human or botnet?",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6623704/",
            "Abstract": "Sphinx was a monster in Greek mythology devouring those who could not solve her riddle. In VoIP, a new service in the role of Sphinx provides protection against SPIT (Spam over Internet Telephony) by discriminating human callers from botnets. The VoIP Sphinx tool uses audio CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) that are controlled by an anti-SPIT policy mechanism. The design of the Sphinx service has been formally verified for the absence of side-effects in the VoIP services (robustness), as well as for its DoS-resistance. We describe the principles and innovations of Sphinx, together with experimental results from pilot use cases.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:ns9cj8rnVeAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Shared memory parallel regenerative queuing network simulation",
            "Publication year": 2001,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.3460&rep=rep1&type=pdf",
            "Abstract": "Discrete-event stochastic simulation is one of the most commonly used tools for performance modeling and evaluation. Parallel/distributed simulation enables a simulation program to execute on a computing system containing multiple processors and aims in reducing the model\u2019s execution time. Three basic types of execution mechanisms have appeared. The first two (the conservative and the optimistic approach) aim in partitioning the simulation model into a number of sub-models, also called logical processes (LPs). Their emphasis, lies on the specification of the appropriate synchronization, deadlock handling and/or memory management algorithms. The third approach (known as the time parallel approach or simply as Multiple Replications in Parallel Time Streams), aims in overcoming the need for sufficiently long runs in steady-state stochastic simulations, by executing multiple replications of the entire model in a parallel fashion. This work, presents a fast parallel OpenMP based implementation, for multivariate queuing network simulations. The simulation results are statistically processed, by applying the classical regenerative method under the Lavenberg & Sauer sequential analysis procedure. The first experimental results indicate significant speedups accompanied by acceptable confidence interval coverage.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:HDshCWvjkbEC",
            "Publisher": "Society for Computer Simulation"
        },
        {
            "Title": "The MedWeT InvenTory daTa SharIng ProTocol",
            "Publication year": 2007,
            "Publication url": "https://www.researchgate.net/profile/Eleni-Fitoka/publication/265080626_THE_MedWet_INVENTORY_DATA_SHARING_PROTOCOL/links/563208dc08ae3de9381de12c/THE-MedWet-INVENTORY-DATA-SHARING-PROTOCOL.pdf",
            "Abstract": "BackgroundAccess to comprehensive national inventories is recognized as an essential prerequisite for many activities necessary for achieving the wise use of wetlands, including policy development, identification and designation of Ramsar sites, documentation of wetland losses and identification of wetlands with potential for restoration (Resolution VII. 20).In response to the urgency to address wetland loss and degradation, the MedWet Scientific and Technical Team has, since 1992, worked on the development and update of appropriate methods and tools for wetland inventories. An inventory method specific for wetlands was developed during the MedWet 1 (ACNAT) project and presented at the Conference on Mediterranean Wetlands in Venice (1996). It consists of a manual explaining the inventory process (Costa et al, 1996), a set of inventory datasheets (Hecker et al, 1996), a habitat description system (Farinha et al, 1996), mapping conventions (Zalidis et al, 1996) and a MedWet database software. In 2005, almost a decade later, MedWet launched the \u201cMedWet information and knowledge network for the sustainable development of wetland ecosystems (MedWet/CODDE)\u201d project under the EU INTEREG IIIC programme (2005-2007) during which the MedWet inventory method has been upgraded, incorporating Water Framework Directive requirements, the Pan-Mediterranean Wetland Inventory, remote sensing techniques and a new web database-namely, the MedWet Web Information System, which has been developed to act as a centralized databank for the Mediterranean wetlands.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:qUcmZB5y_30C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Model\u2010based design of IoT systems with the BIP component framework",
            "Publication year": 2018,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2568",
            "Abstract": "The design of software for networked systems with nodes running an Internet of things operating system faces important challenges due to the heterogeneity of interacting things and the constraints stemming from the often limited amount of available resources. In this context, it is hard to build confidence that a design solution fulfills the application's requirements. This paper introduces a design flow for web service applications of the representational state transfer style that is based on a formal modeling language, the behaviour, interaction, priority (BIP) component framework. The proposed flow applies the principles of separation of concerns in a component\u2010based design process that supports the modular design and reuse of model artifacts. The BIP tools for state\u2010space exploration allow verifying qualitative properties for service responsiveness, ie, the timely handling of events. Moreover, essential quantitative \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:uQVPmWFBlwUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Maximal software execution time: a regression-based approach",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/article/10.1007/s11334-018-0314-9",
            "Abstract": "This work aims at facilitating the schedulability analysis of non-critical systems, in particular those that have soft real-time constraints, where worst-case execution times (WCETs) can be replaced by less stringent probabilistic bounds, which we call maximal execution times (METs). To this end, it is possible to obtain adequate probabilistic execution time models by separating the non-random dependency on input data from a modeling error that is purely random. The proposed approach first utilizes execution time multivariate measurements for building a multiple regression model and then uses the theory related to confidence bounds of coefficients, in order to estimate the upper bound of execution time. Although certainly our method cannot directly achieve extreme probability levels that are usually expected for WCETs, it is an attractive alternative for MET analysis, since it can arguably guarantee safe \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:sfsSB7lKuh0C",
            "Publisher": "Springer London"
        },
        {
            "Title": "Colored Petri net based model checking and failure analysis for E-commerce protocols",
            "Publication year": 2005,
            "Publication url": "https://tidsskrift.dk/daimipb/article/download/7198/6143/#page=271",
            "Abstract": "We present a Colored Petri Net approach to model check three atomicity properties for the NetBill electronic cash system. We verify that the protocol satisfies money atomicity, goods atomicity and certified delivery in the presence of potential site or communication failures and all possible unilateral transaction abort cases. Model checking is performed in CPN Tools, a graphical ML-based tool for editing and analyzing Colored Petri Nets (CP-nets). In case of property violation, protocol failure analysis aims in exploring all property violation scenarios, in order to correct the protocol\u2019s design. Model checking exploits the provided state space exploration functions and the supported Computation Tree like temporal logic (CTL). On the other hand, protocol failure analysis is performed by inspection of appropriately selected markings and if necessary, by interactively simulating certain property violation scenarios. In ecommerce, Colored Petri Net model checking has been used in verifying absence of deadlocks, absence of livelocks and absence of unexpected dead transitions, as well as in verifying a protocol against its service. To the best of our knowledge, our work is the first attempt to employ CP-nets for model checking atomicity properties. We believe that the described approach can also be applied in model checking other functional properties that are not directly related to the structural properties of the generated state space graph.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:9yKSN-GCB0IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Assessing the modifiability of two object-oriented design alternative\u2013a controlled experiment replication",
            "Publication year": 2004,
            "Publication url": "https://www.academia.edu/download/30842049/assessing_object_oriented_software_changeability_with_design_metrics.pdf",
            "Abstract": "This paper presents a replication study of a controlled experiment, investigating the impact of many design characteristics on one of the most desirable quality factors, modifiability. Two alternative design structures were used; a responsibility-driven (RD) versus a control-oriented \u201cmainframe\u201d(MF) design. Two groups of undergraduate students participated, each performing on one of the two designs. The subjects designed, implemented in Java, and tested a set of three maintenance tasks in order to assess the degree of their understanding, effort, and performance. The results indicate that the RD version due to its delocalised structure, exhibited higher correctness, better extensibility, and design stability, than the MF version. In order to provide an objective assessment of the differences between the two versions, a considerable number of metrics were used on the delivered solutions, quantifying separately each produced design\u2019s characteristics.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:9ZlFYXVOiuMC",
            "Publisher": "EUROSIM"
        },
        {
            "Title": "Ensuring business and service requirements in enterprise mashups",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/article/10.1007/s10257-017-0363-x",
            "Abstract": "During the past few years, mashups have gained wide attention as they utilize Web 2.0 technologies in order to combine data, as well as the functionalities of numerous services, in a simple web application. While developing mashups for simple user-specific needs is not a demanding procedure, this is not the case for value-added services that need to satisfy specific properties and business needs, known as enterprise mashups. As a number of business requirements have to be satisfied, and execution faults are less tolerated compared to user-centric scenarios, a rigorous approach for their development is required. In this work we present such an approach utilizing model checking techniques, provided by the behavior, interaction, priorities (BIP) component framework. In addition, a methodology for the transformation of business process model and notation models, describing the business logic of a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:cx97FdCJQX8C",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "A roadmap to electronic payment transaction guarantees and a Colored Petri Net model checking approach",
            "Publication year": 2009,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584908000219",
            "Abstract": "Electronic payment systems play a vital role in modern business-to-consumer and business-to-business e-commerce. Atomicity, fault tolerance and security concerns form a problem domain of interdependent issues that are taken into account to assure the transaction guarantees of interest. We focus on the most notable payment transaction guarantees: money conservation, no double spending, goods atomicity, distributed payment atomicity, certified delivery or validated receipt and the high-level guarantees of fairness and protection of payment participants\u2019 interests. Apart from a roadmap to the forenamed transaction guarantees, this work\u2019s contribution is basically a full-fledged methodology for building and validating high-level protocol models and for proving payment transaction guarantees by model checking them from different participants perspectives (payer perspective, as well as payee perspective). Our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:2osOgNQ5qMEC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Early validation of system requirements and design through correctness-by-construction",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S016412121830150X",
            "Abstract": "Early validation of requirements aims to reduce the need for the high-cost validation testing and corrective measures at late development stages. This work introduces a systematic process for the unambiguous specification of system requirements and the guided derivation of formal properties, which should be implied by the system \u2019s structure and behavior in conjunction with its external stimuli. This rigorous design takes place through the incremental construction of a model using the BIP (Behavior-Interaction-Priorities) component framework. It allows building complex designs by composing simpler reusable designs enforcing given properties. If some properties are neither enforced nor verified, the model is refined or certain requirements are revised. A validated model provides evidence of requirements\u2019 consistency and design correctness. The process is semi-automated through a new tool and existing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:Zbx7W2Xs4QsC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A Process Network Model for Reactive Streaming Software with Deterministic Task Parallelism.",
            "Publication year": 2018,
            "Publication url": "https://library.oapen.org/bitstream/handle/20.500.12657/27803/1002202.pdf?sequence=1#page=103",
            "Abstract": "A formal semantics is introduced for a Process Network model, which combines streaming and reactive control processing with task parallelism properties suitable to exploit multi-cores. Applications that react to environment stimuli are implemented by communicating sporadic and periodic tasks, programmed independently from an execution platform. Two functionally equivalent semantics are defined, one for sequential execution and one real-time. The former ensures functional determinism by implying precedence constraints between jobs (task executions), hence, the program outputs are independent from the task scheduling. The latter specifies concurrent execution on a real-time platform, guaranteeing all model\u2019s constraints; it has been implemented in an executable formal specification language. The model\u2019s implementation runs on multi-core embedded systems, and supports integration of runtime managers for shared HW/SW resources (eg for controlling QoS, resource interference or power consumption). Finally, a model transformation approach has been developed, which allowed to port and statically schedule a real spacecraft on-board application on an industrial multi-core platform.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:aU4yMueWZ3QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Model-based design of energy-efficient applications for IoT systems",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1807.01242",
            "Abstract": "A major challenge that is currently faced in the design of applications for the Internet of Things (IoT) concerns with the optimal use of available energy resources given the battery lifetime of the IoT devices. The challenge is derived from the heterogeneity of the devices, in terms of their hardware and the provided functionalities (e.g data processing/communication). In this paper, we propose a novel method for (i) characterizing the parameters that influence energy consumption and (ii) validating the energy consumption of IoT devices against the system's energy-efficiency requirements (e.g. lifetime). Our approach is based on energy-aware models of the IoT application's design in the BIP (Behavior, Interaction, Priority) component framework. This allows for a detailed formal representation of the system's behavior and its subsequent validation, thus providing feedback for enhancements in the pre-deployment or pre-production stages. We illustrate our approach through a Building Management System, using well-known IoT devices running the Contiki OS that communicate by diverse IoT protocols (e.g. CoAP, MQTT). The results allow to derive tight bounds for the energy consumption in various device functionalities, as well as to validate lifetime requirements through Statistical Model Checking.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:JIEWM9yDoCIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Securing Legacy Code with the TRACER Platform",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2645791.2645796",
            "Abstract": "Software vulnerabilities can severely affect an organization's infrastructure and cause significant financial damage to it. A number of tools and techniques are available for performing vulnerability detection in software written in various programming platforms, in a pursuit to mitigate such defects. However, since the requirements for running such tools and the formats in which they store and present their results vary wildly, it is difficult to utilize many of them in the scope of a project. By simplifying the process of running a variety of vulnerability detectors and collecting their results in an efficient, automated manner during development, the task of tracking security defects throughout the evolution history of software projects is bolstered. In this paper we present tracer, a software framework and platform to support the development of more secure applications by constantly monitoring software projects for vulnerabilities \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:w7CBUyPWg-0C",
            "Publisher": "ACM"
        },
        {
            "Title": "Elastic components: addressing variance of quality properties in components",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4301062/",
            "Abstract": "The quality properties of a software component, although verified by the component developer and even certified by a trusted third-party, might very well be inappropriate for the requirements of a new system. This is what we call the quality mismatch problem: the mismatch between the quality requirements of a new system with the quality properties exhibited by the components that we want to use for its development. This work contributes to the understanding of the quality mismatch problem between component properties and component-based systems requirements. To solve this problem we introduce the concept of elastic components. An elastic component is an open-ended hierarchy of the same pure component with variants that differ between them to the quality properties that they exhibit. We present a quality-driven design approach that can be effectively applied for the design and implementation of elastic \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:WF5omc3nYNoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An intruder model with message inspection for model checking security protocols",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167404809000856",
            "Abstract": "Model checking security protocols is based on an intruder model that represents the eavesdropping or interception of the exchanged messages, while at the same time performs attack actions against the ongoing protocol session(s). Any attempt to enumerate all messages that can be deduced by the intruder and the possible actions in all protocol steps results in an enormous branching of the model's state-space. In current work, we introduce a new intruder model that can be exploited for state-space reduction, optionally in combination with known techniques, such as partial order and symmetry reduction. The proposed intruder modeling approach called Message Inspection (MI) is based on enhancing the intruder's knowledge with metadata for the exchanged messages. In a preliminary simulation run, the intruder tags the analyzed messages with protocol-specific values for a set of predefined parameters. This \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:3fE2CSJIrl8C",
            "Publisher": "Elsevier Advanced Technology"
        },
        {
            "Title": "Program analysis with risk-based classification of dynamic invariants for logical error detection",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167404817300330",
            "Abstract": "The logical errors in programs causing deviations from the intended functionality cannot be detected by automated source code analysis, which mainly focuses on known defects and code vulnerabilities. To this end, we introduce a combination of analysis techniques implemented in a proof-of-concept prototype called PLATO. First, a set of dynamic invariants is inferred from the source code that represents the program's logic. The code is instrumented with assertions from the invariants, which are subsequently valuated through the program's symbolic execution. The findings are ranked using a fuzzy logic system with two scales characterizing their impact: (i) a Severity scale for the execution paths' characteristics and their Information Gain, (ii) a Reliability scale based on the measured Computational Density. Real, as well as synthetic applications with at least four different types of logical errors were analyzed. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:oYwriLWYh5YC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Interlocking control by Distributed Signal Boxes: design and verification with the SPIN model checker",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/11946441_32",
            "Abstract": "Control systems are required to comply with certain safety and liveness correctness properties. In most cases, such systems have an intrinsic degree of complexity and it is not easy to formally analyze them, due to the resulting large state space. Also, exhaustive simulation and testing can easily miss system errors, whether they are life-critical or not. In this work, we introduce an interlocking control approach that is based on the use of the so-called Distributed Signal Boxes (DSBs). The proposed control design is applied to a railway-interlocking problem and more precisely, to the Athens underground metro system. Signal boxes correspond to the network\u2019s interlocking points and communicate only with their neighbor signal boxes. Communication takes place by the use of rendezvous communication channels. This design results in a simple interlocking control approach that compared to other centralized \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:ULOm3_A8WrAC",
            "Publisher": "Springer Berlin/Heidelberg"
        },
        {
            "Title": "Computer Systems Performance Modelling",
            "Publication year": 1993,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:M3NEmzRMIkIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The ACID model checker and code generator for transaction processing",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5191824/",
            "Abstract": "Traditional transaction processing aims in delivering the ACID properties (Atomicity, Consistency, Isolation, Durability), that in our days are often relaxed, due to the need for transaction models that suit modern computing environments and workflow management applications. Typical examples are the requirements of long-running transactions in mobile computing or in the web, as well as the requirements of business-to-business collaborative applications. However, there is lack of tools for automatically verifying correctness of transaction model implementations. This work presents the ACID model checker and code generator, which plays a vital role in developing correct simulation models for the ACID Sim Tools environment. In essence, our contribution introduces an approach for automatically generating provably correct implementations of transaction management, for the transaction model of interest.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:8k81kl-MbHgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Quantitative Model Checking of an RSA-based Email Protocol on Mobile Devices",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5983911/",
            "Abstract": "The current proliferation of mobile devices has resulted in a large diversity of hardware specifications, each designed for different services and applications (e.g. cell phones, smart phones, PDAs). At the same time, e-mail message delivery has become a vital part of everyday communications. This article provides a cost-aware study of an RSA-based e-mail protocol executed upon the widely used Apple iPhone 1,2 with ARM1176JZF-S, operating in an High Speed Downlink Packet Access (HSDPA) mobile environment. The proposed study employs formal analysis techniques, such as probabilistic model checking, and proceeds to a quantitative analysis of the email protocol, taking into account computational parameters derived by the devices' specifications. The value of this study is to form a computer-aided framework which balances the tradeoff between gaining in security, using high-length RSA keys, and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:hqOjcs7Dif8C",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "Cost-aware horizontal scaling of NoSQL databases using probabilistic model checking",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/article/10.1007/s10586-017-0816-5",
            "Abstract": "In this work we target horizontal scaling of NoSQL databases, which exhibit highly varying, unpredictable and difficult to model behavior coupled with transient phenomena during VM removals and/or additions. We propose a solution that is cost-aware, systematic, dependable while it accounts for performance unpredictability and volatility. To this end, we model the elasticity as a dynamically instantiated Markov decision process, which can be both solved and verified using probabilistic model checking. Further, we propose a range of complementary decision making policies, which are thoroughly evaluated in workloads from real traces. The evaluation provides strong insights into the trade-offs between performance and cost that our policies can achieve and prove that we can avoid both over- and under-provisioning.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:fF_gHTpLxhAC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Regenerative queuing network distributed simulation",
            "Publication year": 2000,
            "Publication url": "https://dl.acm.org/doi/abs/10.5555/647917.739663",
            "Abstract": "Regenerative queuing network distributed simulation | Proceedings of the 14th European \nSimulation Multiconference on Simulation and Modelling: Enablers for a Better Quality of Life \nACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About \nSign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences \nPeople More Search ACM Digital Library SearchSearch Advanced Search Browse Browse \nDigital Library Collections More HomeBrowse by TitleProceedingsProceedings of the 14th \nEuropean Simulation Multiconference on Simulation and Modelling: Enablers for a Better Quality \nof LifeRegenerative queuing network distributed simulation ARTICLE Regenerative queuing \nnetwork distributed simulation Share on Authors: Panajotis Katsaros profile image Panajotis \nKatsaros View Profile , Constantine Lazos profile image Constantine Lazos View Profile : \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:mB3voiENLucC",
            "Publisher": "SCS Europe"
        },
        {
            "Title": "Simulation and verification of information flow paths for access control policies specified in the CORBA Security setting",
            "Publication year": 2005,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:hC7cP41nSMkC",
            "Publisher": "EPY"
        },
        {
            "Title": "Model repair for probabilistic systems",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-19835-9_30",
            "Abstract": "We introduce the problem of Model Repair for Probabilistic Systems as follows. Given a probabilistic system M and a probabilistic temporal logic formula \u03c6 such that M fails to satisfy \u03c6, the Model Repair problem is to find an M\u2032 that satisfies \u03c6 and differs from M only in the transition flows of those states in M that are deemed controllable. Moreover, the cost associated with modifying M\u2019s transition flows to obtain M\u2032 should be minimized. Using a new version of parametric probabilistic model checking, we show how the Model Repair problem can be reduced to a nonlinear optimization problem with a minimal-cost objective function, thereby yielding a solution technique. We demonstrate the practical utility of our approach by applying it to a number of significant case studies, including a DTMC reward model of the Zeroconf protocol for assigning IP addresses, and a CTMC model of the highly publicized \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:ufrVoPGSRksC",
            "Publisher": "Springer Berlin/Heidelberg"
        },
        {
            "Title": "Simulation metamodeling for the design of reliable object based systems",
            "Publication year": 2004,
            "Publication url": "http://delab.csd.auth.gr/~katsaros/eurosim2004.pdf",
            "Abstract": "Replication is a suitable approach for the provision of fault tolerance and load balancing in distributed systems. Object replication takes place on the basis of well-designed interaction protocols that preserve object state consistency in an application transparent manner. The published analytic performance models may only be applied in single-server process replication schemes and are not suitable for schemes composed of miscellaneous policies, such as those arising in object based systems. In this work we make use of a simulation metamodeling approach that allows the comparative evaluation of composite fault tolerance schemes, on the basis of small size uniform experimental designs. Our approach opens the possibility to take into account different design concerns in a combined manner (eg fault tolerance combined with load balancing and multithreading). We provide results in terms of a case system study that reveals a dependence of the optimal adjustments on the system load level. This finding suggests the device of dynamically adjusted fault tolerance schemes.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:_kc_bZDykSQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Design of embedded systems with complex task dependencies and shared resource interference (Short Paper)",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-66197-1_28",
            "Abstract": "Languages for embedded systems ensure predictable timing behavior by specifying constraints based on either data streaming or reactive control models of computation. Moreover, various toolsets facilitate the incremental integration of application functionalities and the system design by evolutionary refinement and model-based code generation. Modern embedded systems involve various sources of interference in shared resources (e.g. multicores) and advanced real-time constraints, such as mixed-criticality levels. A sufficiently expressive modeling approach for complex dependency patterns between real-time tasks is needed along with a formal analysis of models for runtime resource managers with timing constraints. Our approach utilizes a model of computation, called Fixed-Priority Process Networks, which ensures functional determinism by unifying streaming and reactive control within a timed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:DPO9WFcz7UcC",
            "Publisher": "Springer"
        },
        {
            "Title": "Test-driving static analysis tools in search of C code vulnerabilities",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6032220/",
            "Abstract": "Recently, a number of tools for automated code scanning came in the limelight. Due to the significant costs associated with incorporating such a tool in the software lifecycle, it is important to know what defects are detected and how accurate and efficient the analysis is. We focus specifically on popular static analysis tools for C code defects. Existing benchmarks include the actual defects in open source programs, but they lack systematic coverage of possible code defects and the coding complexities in which they arise. We introduce a test suite implementing the discussed requirements for frequent defects selected from public catalogues. Four open source and two commercial tools are compared in terms of their effectiveness and efficiency of their detection capability. A wide range of C constructs is taken into account and appropriate metrics are computed, which show how the tools balance inherent analysis \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:L8Ckcad2t8MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Synthesis of attack actions using model checking for the verification of security protocols",
            "Publication year": 2011,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/sec.119",
            "Abstract": "Model checking cryptographic protocols have evolved to a valuable method for discovering counterintuitive security flaws, which makes it possible for a hostile agent to subvert the goals of the protocol. Published works and existing security analysis tools are usually based on general intruder models that embody at least some aspects of the seminal work of Dolev\u2013Yao, in an attempt to detect failures of secrecy. In this work, we propose an alternative intruder model, which is based on a thorough analysis of how potential attacks might proceed. We introduce an intruder model that provides an open\u2010ended base for the integration of multiple basic attack tactics. Those attack tactics have the possibility to be combined, in a way to compose complex attack actions that require a number of procedural steps from the intruder's side, such as a Denial of Service attack. In our model checking approach, protocol correctness is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:roLk4NBRz8UC",
            "Publisher": "John Wiley & Sons, Ltd."
        },
        {
            "Title": "Regression-based Statistical Bounds on Software Execution Time",
            "Publication year": 2017,
            "Publication url": "https://pdfs.semanticscholar.org/ef7e/5d81735827936bf611d6c12d5a5f0c00d364.pdf",
            "Abstract": "Regression-based Statistical Bounds on Software Execution Time Page 1 Motivation The \nMaximal Regression Model Model Construction Techniques A JPEG Decoder Study \nConclusions Regression-based Statistical Bounds on Software Execution Time Ayoub \nNouri P. Poplavko, L. Angelis, A. Zerzelidis, S. Bensalem, P. Katsaros University of Grenoble-Alpes \n- France August 24, 2017 Vecos\u201917, Montreal Page 2 Motivation The Maximal Regression \nModel Model Construction Techniques A JPEG Decoder Study Conclusions WCET: \nStatistical vs. Common Approaches WCET is useful for ... Schedulability of real-time \nsystems Performance Evaluation, eg, Network Calculus Model-based design, ie building \nfaithful Hw/Sw models 2 / 29 Page 3 Motivation The Maximal Regression Model Model \nConstruction Techniques A JPEG Decoder Study Conclusions WCET: Statistical vs. \nCommon Approaches WCET is useful for ... of -\u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:lSLsV1MU4ZUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Quantitative analysis of a certified e-mail protocol in mobile environments: A probabilistic model checking approach",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167404811000435",
            "Abstract": "Formal analysis techniques, such as probabilistic model checking, offer an effective mechanism for model-based performance and verification studies of communication systems\u2019 behavior that can be abstractly described by a set of rules i.e., a protocol. This article presents an integrated approach for the quantitative analysis of the Certified E-mail Message Delivery (CEMD) protocol that provides security properties to electronic mail services. The proposed scheme employs a probabilistic model checking analysis and provides for the first time insights on the impact of CEMD\u2019s error tolerance on computational and transmission cost. It exploits an efficient combination of quantitative analysis and specific computational and communication parameters, i.e., the widely used Texas Instruments TMS320C55x Family operating in an High Speed Downlink Packet Access (HSDPA) mobile environment, where multiple CEMD \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:5nxA0vEk-isC",
            "Publisher": "Elsevier Advanced Technology"
        },
        {
            "Title": "Intrusion Attack Tactics for the model checking of e-commerce security guarantees",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-75101-4_22",
            "Abstract": "In existing security model-checkers the intruder\u2019s behavior is defined as a message deducibility rule base governing use of eavesdropped information, with the aim to find out a message that is meant to be secret or to generate messages that impersonate some protocol participant(s). The advent of complex protocols like those used in e-commerce brings to the foreground intrusion attacks that are not always attributed to failures of secrecy or authentication. We introduce an intruder model that provides an open-ended base for the integration of multiple attack tactics. In our model checking approach, protocol correctness is checked by appropriate user-supplied assertions or reachability of invalid end states. Thus, the analyst can express e-commerce security guarantees that are not restricted to the absence of secrecy and the absence of authentication failures. The described intruder model was implemented \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:u-x6o8ySG0sC",
            "Publisher": "Springer"
        },
        {
            "Title": "Solving Influence Problems on the DeGroot Model with a Probabilistic Model Checking Tool",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3003733.3003780",
            "Abstract": "DeGroot learning is a model of opinion diffusion and formation in a social network of individuals. We examine the behavior of the DeGroot learning model when external strategic players that aim to bias the final consensus of the social network, are introduced to the model. More precisely, we consider the case of a single decision maker and the case of two competing external players, and a fixed number of possible influence actions on each individual. When studying the influence problems, we focus on the stochastic processes underlying the solution of DeGroot problems. In case of one decision maker, the analysis of the DeGroot model leads to the formation of a Markov Decision Process (MDP) and in the case of two external competing players the model is reduced to a Stochastic Game (SG). Since such models are heavily used in probabilistic model checking we apply tools of the field to solve them. Preliminary \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:8JTMrWI6FdcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An Interdisciplinary Perspective to the Design and Decision Support of Integral Safety Systems",
            "Publication year": 2013,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1474667015340027",
            "Abstract": "Next generation integral safety systems are expected to provide better protection against traffic accidents by interlinking sensors and actuators of active and passive safety. A series of advanced functions will be used to mitigate collisions and if they cannot be avoided they will at least reduce their severity. We explore the interplay between key technology areas towards a holistic approach in the design and decision support of integral safety systems. First, we refer to the main problems in the design of effective systems and the associated software engineering challenges. Recent advances in sensor data analytics are then explored and their integration with decision support for vehicle control is examined. Finally, we envision that rigorous design techniques based on models for human-machine interaction are essential for achieving adequate performance and robustness of integral safety systems.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:lSLTfruPkqcC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Towards compositional safety analysis via semantic representation of component failure behaviour",
            "Publication year": 2008,
            "Publication url": "https://ebooks.iospress.nl/volumearticle/4657",
            "Abstract": "In dependable systems engineering safety assessment of complex designs that involve software and hardware components is one of the most difficult tasks required. Due to the different modelling languages and models that are used for complementary tasks, the model and specification artefacts are not easily shared by the experts involved in the design process. Moreover, the structural and semantic differences of the used language representations open a possibility for inconsistencies between the corresponding models. This work explores the role of an ontology representation of component failure behaviour as a basis for automated model transformations, as well as a library of reusable knowledge artefacts to be used in different modelling languages and models. The presented approach was motivated by recent findings and requirements derived from European industrial-driven research and development \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:_FxGoFyzp5QC",
            "Publisher": "IOS Press"
        },
        {
            "Title": "A technique for determining queuing network simulation length based on desired accuracy",
            "Publication year": 2000,
            "Publication url": "https://scholar.google.com/scholar?cluster=11260082337029566672&hl=en&oi=scholarr",
            "Abstract": "Although simulation is a commonly used approach by the computer systems performance engineers and the communication systems engineers, not enough consideration is usually given to the quality of simulation results. However, the queuing network simulation is not only a representation of the dynamic behavior of a system, but it is basically a stochastic process whose output has to be statistically analyzed. Moreover, the random selection of the simulation nm length often leads to multiple replications of the same experiment in order to achieve the desired level of estimation accuracy. This paper shows an algorithm for dynamically determining the simulation run length in regard to the required accuracy. The last, is defined as an acceptable confidence interval length of the estimated Values (confidence intervals are produced by the statistical analysis of the estimation results with the use of the regenerative \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:TFP_iSt0sucC",
            "Publisher": "CRL Publishing"
        },
        {
            "Title": "An adaptable framework for educational software evaluation",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-1-4757-4919-9_23",
            "Abstract": "This paper proposes a framework for educational software evaluation based on the Multiple Criteria Decision Aid methodology. Evaluating educational software products is a twofold process: both the educational and the technical aspect of the evaluated products have to be considered. As far as the product educational effectiveness is concerned, we propose a set of attributes covering both the general educational features and the content of the product. From the technical point of view, a software attribute set based on the ISO/IEC 9126 standard has been chosen together with the accompanying measurement guidelines. Finally, an evaluation example involving three commercial educational software packages for mechanics is presented.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:0EnyYjriUFMC",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "Execution path classification for vulnerability analysis and detection",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-30222-5_14",
            "Abstract": "Various commercial and open-source tools exist, developed both by the industry and academic groups, which are able to detect various types of security bugs in applications\u2019 source code. However, most of these tools are prone to non-negligible rates of false positives and false negatives, since they are designed to detect a priori specified types of bugs. Also, their analysis scalability to large programs is often an issue. To address these problems, we present a new source code analysis technique based on execution path classification. We develop a prototype tool to test our method\u2019s ability to detect different types of information-flow dependent bugs. Our approach is based on classifying the Risk of likely exploits inside source code execution paths using two measuring functions: Severity and Vulnerability. For an Application Under Test (AUT), we analyze every single pair of input vector and program sink in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:oXKBmVzQOggC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Evaluation of composite object replication schemes for dependable server applications",
            "Publication year": 2006,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584905001497",
            "Abstract": "Object oriented dependable server applications often rely on fault-tolerance schemes, which are comprised of different replication policies for the constituent objects (composite replication schemes). This paper introduces a simulation-based evaluation approach for quantifying the tradeoffs between fault-tolerance overhead and fault-tolerance effectiveness in composite replication schemes. Compared to other evaluation approaches: (a) we do not use the well-known reliability blocks based simulation, but a hybrid reliability and system's traffic simulation and (b) we make a clear distinction between the measures used for the fault-affected service response times from those used for the fault-unaffected ones. The first mentioned feature allows taking into account additional concerns other than fault-tolerance, like for example load balancing and multithreading. The second feature renders the proposed approach \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:eQOLeE2rZwMC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "SEAA 2021",
            "Publication year": 0,
            "Publication url": "https://www.computer.org/csdl/proceedings-article/seaa/2021/270500z014/1y2JBR3QKzu",
            "Abstract": "On behalf of the Organizing Committee, we welcome you to the 47th edition of Euromicro Conference on Software Engineering and Advanced Applications (SEAA). As tradition, the EUROMICRO SEAA conference brings together researchers, practitioners from business and industry, and students to present and discuss emerging trends and challenges in the field of Software Engineering and Advanced Applications in Information Technology for software-intensive systems. SEAA continues its commitment towards innovative and advanced fields of software engineering and its applications. Despite the enormous challenges that everybody has faced during these hard times due to the coronavirus pandemic, this year\u2019s edition of SEAA has seen an interesting set of tracks and special sessions that reflect up-to date trends in research and in practice.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:3WNXLiBY60kC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Hands on Dependability Economics",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5211074/",
            "Abstract": "Contemporary societies (from individuals to organizations) depend on services delivered by systems to achieve individual goals, meaning that a system must have engineered and guaranteed dependability, regardless of continuous, rapid and unpredictable technological and context changes. The simplest questions that brought out in the surface are to understand on how to evaluate and how much (money) should we spend on dependability. Dependability risk management is an effective process that with simplicity can determine the likelihood of an accident and the severities of the consequences. On the other hand, we also need quantitative methods, in order to assess the cost of the various measures which can be taken to reduce the dependability risk. In this paper, we survey quantitative methods that can play an important role in Dependability Economics. These methods aim in providing estimations for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:Zph67rFs4hoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Using BIP to reinforce correctness of resource-constrained IoT applications",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7185066/",
            "Abstract": "Internet of Things (IoT) systems process and respond to multiple (external) events, while performing computations for a Sense-Compute-Control (SCC) or a Sense-Only (SO) goal. Given the limitations of the interconnected resource-constrained devices, the execution environment can be based on an appropriate operating system for the IoT. The development effort can be reduced, when applications are built on top of RESTful web services, which can be shared and reused. However, the asynchronous communication between remote nodes is prone to event scheduling delays, which cannot be predicted and taken into account while programming the application. Long delays in message processing and communication, due to packet collisions, are avoided by carefully choosing the data transmission frequencies between the system's nodes. But even when specialized simulators are available, it is still a hard \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:gFrPXmx1TSsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automating the evaluation of educational software",
            "Publication year": 1999,
            "Publication url": "https://www.academia.edu/download/46076208/Automating_the_Evaluation_of_Educational20160530-11884-cqli8h.pdf",
            "Abstract": "This paper proposes a framework for educational software evaluation based on the Multiple Criteria Decision Aid methodology, supported by ESSE, an Expert System for Software Evaluation. An evaluation example is presented that illustrates the overall evaluation process. Evaluating educational software products is a twofold process: both the technical and the educational aspect of the evaluated products have to be considered. As far as the product\u2019s educational effectiveness is concerned, the flexibility of ESSE in problem modeling allows the development and the use of a set of criteria, which clearly describe the context, and the educational setting in which the software products are to be used. From the technical point of view, a software attribute set based on the ISO/IEC 9126 standard has been chosen together with the accompanying measurement guidelines.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:Se3iqnhoufwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Simulation and Verification of Atomicity Properties for an Electronic Cash System",
            "Publication year": 2005,
            "Publication url": "http://delab.csd.auth.gr/papers/ESM2005kogk.pdf",
            "Abstract": "An electronic cash system as any distributed system is subject to site and communication failures, as well as potential security attacks. This work focuses on simulation and verification of three important correctness properties, for ensuring failure resilience or detecting potential property violation scenarios. We refer to the NetBill electronic cash system and the properties to be checked are money atomicity, goods atomicity and certified delivery. We introduce a Colored Petri Net that models NetBill, in the presence of site or communication failures and all possible transaction abort cases. The presented model has been implemented in CPN Tools, a graphical ML-based tool for analyzing Colored Petri Nets. This allows us to combine the provided state space exploration functions and the supported Computation Tree like temporal logic (CTL), for model checking the forenamed atomicity properties. At the same time, it is possible to exploit the provided interactive simulation facilities to explore potential property violation scenarios and correct the protocol\u2019s design.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:Wp0gIr-vW9MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spacecraft early design validation using formal methods",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0951832014001586",
            "Abstract": "The size and complexity of software in spacecraft is increasing exponentially, and this trend complicates its validation within the context of the overall spacecraft system. Current validation methods are labor-intensive as they rely on manual analysis, review and inspection. For future space missions, we developed \u2013 with challenging requirements from the European space industry \u2013 a novel modeling language and toolset for a (semi-)automated validation approach. Our modeling language is a dialect of AADL and enables engineers to express the system, the software, and their reliability aspects. The COMPASS toolset utilizes state-of-the-art model checking techniques, both qualitative and probabilistic, for the analysis of requirements related to functional correctness, safety, dependability and performance. Several pilot projects have been performed by industry, with two of them having focused on the system-level of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:RJNGbXJAtMsC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Steady-state Simulation of Queuing Processes in Parallel Time Streams: Problems and Potentialities",
            "Publication year": 2001,
            "Publication url": "https://www.researchgate.net/profile/Panagiotis-Katsaros/publication/221157048_Steady_state_simulations_on_queuing_processes_in_parallel_time_streams_Problems_and_potentialities/links/54e18a020cf296663792beb8/Steady-state-simulations-on-queuing-processes-in-parallel-time-streams-Problems-and-potentialities.pdf",
            "Abstract": "Thispaper addresses statistical issues that arise in stochastic simulations of the steady-state behavior of queuing processes, when being executed in parallel time streams. This is often a desirable choice since otherwise, computer running times for such simulations tend to be large. A proper statistical methodology, which will enable the simulator to achieve the required statistical precision as quickly as possible, has to be used. A random number generator with a large cycle length has to be chosen, since any correlation across the parallel streams can affect the randomness of the results. Another critical issue is that, no procedure in which the run length is fixed, before the simulation begins, can guarantee accurate results. Instead of this, sequential procedures, which determine the length of the simulation during the course of the run, are preferred. The parallel processing speedup to be achieved, is always dependent on the simulated model\u2019s structure. However, the number of processors to be used, has to be decided on the basis of the chosen accuracy requirements since experimental evidence shows that the quality of the results deteriorates as the number of processors used, increases.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:TQgYirikUcIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "TRACER: A Platform for Securing Legacy Code",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/978-3-319-08593-7_20.pdf",
            "Abstract": "Static Analysis, Software Security, Trusted Applications, Legacy software.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:TiIbgCYny7sC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Architecture-based design: A satellite on-board software case study",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-57666-4_16",
            "Abstract": "In this case study, we apply the architecture-based design approach to the control software of the CubETH satellite. Architectures are a means for ensuring global coordination properties and thus, achieving correctness of complex systems by construction. We illustrate the following three steps of the design approach: (1) definition of a domain-specific taxonomy of architecture styles; (2) design of the software model by applying architectures to enforce the required properties; (3) deadlock-freedom analysis of the resulting model. We provide a taxonomy of architecture styles for satellite on-board software, formally defined by architecture diagrams in the BIP component-based framework. We show how architectures are instantiated from the diagrams and applied to a set of atomic components. Deadlock-freedom of the resulting model is verified using DFinder from the BIP tool-set. We provide additional \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:JdL-Xu2nR38C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Online analysis of security risks in elastic cloud applications",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7742266/",
            "Abstract": "Security-related concerns in elastic cloud applications call for a risk-based approach due to the inherent trade-offs among security and other nonfunctional requirements, such as performance. To this end, the authors advocate a solution that can be efficiently realized through modeling the application behavior as a Markov decision process, on top of which probabilistic model checking is applied. The article explains the main steps in this approach and illustrates its use in online analysis and decision making regarding elasticity decisions. The runtime analysis is capable of providing evidence for key security-related aspects of the running applications, such as the probability of data leakage in the next hour.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:e9bUPLv0EjcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automated detection of logical errors in programs",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-17127-2_3",
            "Abstract": "Static and dynamic program analysis tools mostly focus on the detection of a priori defined defect patterns and security vulnerabilities. Automated detection of logical errors, due to a faulty implementation of applications\u2019 functionality is a relatively uncharted territory. Automation can be based on profiling the intended behavior behind the source code. In this paper, we present a new code profiling method that combines the crosschecking of dynamic program invariants with symbolic execution, an information flow analysis, and the use of fuzzy logic. Our goal is to detect logical errors and exploitable vulnerabilities. The theoretical underpinnings and the practical implementation of our approach are discussed. We test the APP_LogGIC tool that implements the proposed analysis on two real-world applications. The results show that profiling the intended program behavior is feasible in diverse applications. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:1paMEeroeoQC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Product line variability with elastic components and test-driven development",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5172615/",
            "Abstract": "In this work we present a systematic approach for the creation of new variant software components via customization of existing core assets of a software product line. We consider both functional and quality variants and address the issue of a controlled creation of variants which considers the reference architecture and its co-evolution with a number of other artifacts including components and functional and quality test suites. Furthermore we discuss the relationship between the popular agile practice of test-driven development (TDD) and how it can be used to assist the evolution of software components of a software product line.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:UeHWp8X0CEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Inlined monitors for security policy enforcement in web applications",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2491845.2491861",
            "Abstract": "Improper input validation in Web Applications undermines their security and this may have disastrous consequences for the users. Input data can or cannot be harmful depending on how they are used with regard to the interactions with the clients and the accessed sensitive resources (eg databases and files). Existing application frameworks cannot guarantee safe input sanitization with respect to all vulnerabilities. Also, when legacy code is incorporated that was not originally written for the Web, its security hardening is costly and error-prone. We propose a reference monitor inlining approach that treats input injection vulnerabilities as a cross-cutting concern. Our monitors enforce high-level security policies for taint propagation control, by weaving checks and repair actions into the untrusted code. Taint policies are specified into JavaMOP, a programming framework for generating runtime monitors, which are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:RYcK_YlVTxYC",
            "Publisher": "ACM"
        },
        {
            "Title": "Cache activity profiling tool for the LEON4 processor",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7937660/",
            "Abstract": "The performance of modern systems depends significantly on the cache activity. Hence, tools that monitor the cache performance are very useful for optimization of the software or even, whenever this is possible, the hardware, of a system. In this paper, a tool that provides statistics about the cache activity of the LEON4-N2X embedded system is discussed. The tool analyzes the memory access trace of a program executed bare metal and calculates, with the use of the reuse distance, the latency added by the cache activity. Furthermore, the tool measures all the misses occurred and classifies them per their cause.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:uWy0R8PweswC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Applied multiresponse metamodeling for queuing network simulation experiments: problems and perspectives",
            "Publication year": 2001,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.3691&rep=rep1&type=pdf",
            "Abstract": "A complete performance evaluation study of a simulated system should consider possible alternatives and response predictions to potential parameter changes. Simulation sensitivity analysis and metamodeling constitute an efficient approach for this kind of problems. However, this approach is usually despised, mainly because, a sophisticated methodological treatment is required. Such a methodology should take into account, peculiarities, inherent to queuing network models, as for example, multiple responses, large number of model parameters, many qualitative parameters etc. This work aims to illustrate the combined use of the proper statistical techniques to cope with this sort of problems and to show the need for a sound methodological framework that will bring this approach closer to the queuing network simulation practice.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:hFOr9nPyWt4C",
            "Publisher": "EUROSIM"
        },
        {
            "Title": "Approaches and Processes for Managing the Economics of Information Systems",
            "Publication year": 2014,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=WVsrAgAAQBAJ&oi=fnd&pg=PR1&dq=info:bgtMkmBvyqIJ:scholar.google.com&ots=OqS0twy55O&sig=KwhF2pnLQ19JICvz-3tTmdHjoSg",
            "Abstract": "Advancements of information systems are rapidly altering both the economy and society. This progression of information creates operational and strategic advantages which is important to the success of organizations and firms. Approaches and Processes for Managing the Economics of Information Systems explores the value of information and its management by highlighting theoretical and empirical approaches in the economics of information systems. By providing insight into how information systems can generate economic value for businesses and consumers, this book is essential for professors, students, researchers, and developers of information systems.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:dhpJJ7xvgBgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "State Space Reduction with Message Inspection in Security Protocol Model Checking",
            "Publication year": 2009,
            "Publication url": "https://arxiv.org/abs/0909.0174",
            "Abstract": "Model checking is a widespread automatic formal analysis that has been successful in discovering flaws in security protocols. However existing possibilities for state space explosion still hinder analyses of complex protocols and protocol configurations. Message Inspection, is a technique that delimits the branching of the state space due to the intruder model without excluding possible attacks. In a preliminary simulation, the intruder model tags the eavesdropped messages with specific metadata that enable validation of feasibility of possible attack actions. The Message Inspection algorithm then decides based on these metadata, which attacks will certainly fail according to known security principles. Thus, it is a priori known that i.e. an encryption scheme attack cannot succeed if the intruder does not posses the right key in his knowledge. The simulation terminates with a report of the attack actions that can be safely removed, resulting in a model with a reduced state space.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:mVmsd5A6BfQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Correct-by-Construction Web Service Architecture",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6825962/",
            "Abstract": "Service-Oriented Computing aims to facilitate development of large-scale applications out of loosely coupled services. The service architecture sets the framework for achieving coherence and interoperability despite service autonomy and the heterogeneity in data representation and protocols. Service-Oriented Architectures are based on standardized service contracts, in order to infuse characteristic properties (stateless interactions, atomicity etc). However, contracts cannot ensure correctness of services if essential operational details are overlooked, as is usually the case. We introduce a modeling framework for the specification of Web Service architectures, in terms of formal operational semantics. Our approach aims to enable rigorous design of Web Services, based on the Behaviour Interaction Priorities (BIP) component framework and the principles of correctness-by construction. We provide executable BIP \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:yCjxvIMm6_oC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Runtime Verification of Autonomous Driving Systems in CARLA",
            "Publication year": 0,
            "Publication url": "https://depend.csd.auth.gr/media/publications/paper_A6vVMPs.pdf",
            "Abstract": "Urban driving simulators, such as CARLA, provide 3-D environments and useful tools to easily simulate sensorimotor control systems in scenarios with complex multi-agent dynamics. This enables the design exploration at the early system development stages, reducing high infrastructure costs and high risks. However, due to the high-dimensional input and state spaces of closed-loop autonomous driving systems, their testing and verification is very challenging and it has not yet taken advantage of the recent developments in theory and tools for runtime verification. We show here how to integrate the recently introduced rtamt library, for runtime verification of STL (Signal Temporal Logic) specifications, with the CARLA simulator. Finally, we also present the obtained results from monitoring quantitatively interesting requirements for an experimental Adaptive Cruise Control system tested in CARLA.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:8O4vDxvErlEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Energy characterization of IoT systems through design aspect monitoring",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/article/10.1007/s10009-020-00598-5",
            "Abstract": "The technological revolution brought by the Internet of Things (IoT) is characterized by a high level of automation based, to a large extent, on battery autonomy. Important risks hindering its wide adoption, though, are associated with device battery lifetime, which is affected by system design aspects such as connectivity, data processing and storage, as well as security protection against cyber-threats. Even though simulation can help for the energy cost estimation of IoT applications before their actual deployment, it is still challenging, and extensive effort is required to converge to a feasible architectural deployment scenario. This article introduces a method to address this challenge by estimating the energy cost of the IoT design aspects and identifying the feasible deployment scenarios, for an IoT system architecture. The method is illustrated on a smart city application that consists of subsystems for building \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:wgKq3sYidysC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Source code profiling and classification for automated detection of logical errors",
            "Publication year": 2014,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.477.4104&rep=rep1&type=pdf",
            "Abstract": "Research and industrial experience reveal that code reviews as a part of software inspection might be the most cost-effective technique a team can use to reduce defects. Tools that automate code inspection mostly focus on the detection of a priori known defect patterns and security vulnerabilities. Automated detection of logical errors, due to a faulty implementation of applications\u2019 functionality is a relatively uncharted territory. Automation can be based on profiling the intended behavior behind the source code. In this paper, we present a code profiling method based on token classification. Our method combines an information flow analysis, the crosschecking of dynamic invariants with symbolic execution, and code classification heuristics with the use of a fuzzy logic system. Our goal is to detect logical errors and exploitable vulnerabilities. The theoretical underpinnings and the practical implementation of our approach are discussed. We test the APP_LogGIC tool that implements the proposed analysis on two real-world applications. The results show that profiling the intended program behavior is feasible in diverse applications. We discuss in detail the heuristics used to overcome the problem of state space explosion and that of the large data sets. Code metrics and test results are provided to demonstrate the effectiveness of the proposed approach. This paper extends the work that appears in an article currently submitted to an international conference with proceedings. In this adequately extended version of our method we present classification mechanisms that can take into account multiple user input and provide a detailed description of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:pxXbYLTb8EgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Compositional execution semantics for business process verification",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121217302595",
            "Abstract": "Service compositions are programmed as executable business processes in languages like WS-BPEL (or BPEL in short). In such programs, activities are nested within concurrency, isolation, compensation and event handling constructs that cause an overwhelming number of execution paths. Program correctness has to be verified based on a formal definition of the language semantics. For BPEL , previous works have proposed execution semantics in formal languages amenable to model checking. Most of the times the service composition structure is not preserved in the formal model, which impedes tracing the verification findings in the original program. Here, we propose a compositional semantics and a structure-preserving translator of BPEL programs onto the BIP component framework. In addition, we verify essential correctness properties that affect process responsiveness, and the compliance with partner \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:JqN3CTdJtl0C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Single-pass Static Semantic Check for Efficient Translation in YAPL",
            "Publication year": 2003,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:e5wmG9Sq2KIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dependable Horizontal Scaling Based On Probabilistic Model Checking",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7152469/",
            "Abstract": "The focus of this work is the on-demand resource provisioning in cloud computing, which is commonly referredto as cloud elasticity. Although a lot of effort has been invested in developing systems and mechanisms that enable elasticity, the elasticity decision policies tend to be designed without quantifying or guaranteeing the quality of their operation. We present an approach towards the development of more formalized and dependable elasticity policies. We make two distinct contributions. First, we propose an extensible approach to enforcing elasticity through the dynamic instantiation and online quantitative verification of Markov Decision Processes(MDP) using probabilistic model checking. Second, various concrete elasticity models and elasticity policies are studied. We evaluate the decision policies using traces from a realNoSQL database cluster under constantly evolving externalload. We reason about the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:IvSMUa3B7yYC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "On methods and tools for rigorous system design",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/article/10.1007/s10009-021-00632-0",
            "Abstract": "Full a posteriori verification of the correctness of modern software systems is practically infeasible due to the sheer complexity resulting from their intrinsic concurrent nature. An alternative approach consists of ensuring correctness by construction. We discuss the Rigorous System Design (RSD) approach, which relies on a sequence of semantics-preserving transformations to obtain an implementation of the system from a high-level model while preserving all the properties established along the way. In particular, we highlight some of the key requirements for the feasibility of such an approach, namely availability of (1) methods and tools for the design of correct-by-construction high-level models and (2) definition and proof of the validity of suitable domain-specific abstractions. We summarise the results of the extended versions of seven papers selected among those presented at the and the International \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:WGv8Og3F3KgC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Security-aware elasticity for nosql databases",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-23781-7_15",
            "Abstract": "We focus on horizontally scaling NoSQL databases in a cloud environment, in order to meet performance requirements while respecting security constraints. The performance requirements refer to strict latency limits on the query response time. The security requirements are derived from the need to address two specific kinds of threats that exist in cloud databases, namely data leakage, mainly due to malicious activities of actors hosted on the same physical machine, and data loss after one or more node failures. We explain that usually there is a trade-off between performance and security requirements and we derive a model checking approach to drive runtime decisions that strike a user-defined balance between them. We evaluate our proposal using real traces to prove the effectiveness in configuring the trade-offs.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:s2G-WRnXBicC",
            "Publisher": "Springer"
        },
        {
            "Title": "Scalable IoT architecture for balancing performance and security in mobile crowdsensing systems*",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9340165/",
            "Abstract": "Crowdsourcing aims to deliver services and content by aggregating contributions from a large user population. For mobile networks and IoT systems, crowdsourcing is used to gather and process sensor data from mobile devices (crowdsensing), in order to deliver real-time, context-aware services and possibly support user collaboration in extended geographic areas. In applications like geonsensitive navigation, location-based activity sharing and recommendations, the challenge of adequate service quality and user experience may be at stake, as the services are provided securely to an ever-growing user population. This happens due to the inherent trade-off between security and real-time performance that ultimately sets in doubt any scalability prospect beyond a certain user-interaction load. This work introduces a publish-subscribe architecture for mobile crowdsensing systems, which can be transparently \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:E2bRg1zSkIsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Information Technology and Open Source: Applications for Education, Innovation, and Sustainability",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/978-3-642-54338-8.pdf",
            "Abstract": "This volume contains the proceedings of the three satellite events of the 10th International Conference on Software Engineering and Formal Methods (SEFM 2012), which was held during October 1\u20135, 2012, in Thessaloniki, Greece:",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:o9ULDYDKYbIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Probabilistic model checking at runtime for the provisioning of cloud resources",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-23820-3_18",
            "Abstract": "We elaborate on the ingredients of a model-driven approach for the dynamic provisioning of cloud resources in an autonomic manner. Our solution has been experimentally evaluated using a NoSQL database cluster running on a cloud infrastructure. In contrast to other techniques, which work on a best-effort basis, we can provide probabilistic guarantees for the provision of sufficient resources. Our approach is based on the probabilistic model checking of Markov Decision Processes (MDPs) at runtime. We present: (i) the specification of an appropriate MDP model for the provisioning of cloud resources, (ii) the generation of a parametric model with system-specific parameters, (iii) the dynamic instantiation of MDPs at runtime based on logged and current measurements and (iv) their verification using the PRISM model checker for the provisioning/deprovisioning of cloud resources to meet the set goals (This \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:GUYAmugLYisC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Combining Invariant Violation with Execution Path Classification for Detecting Multiple Types of Logical Errors and Race Conditions.",
            "Publication year": 2016,
            "Publication url": "https://www.academia.edu/download/64588486/eb0624ad0d94e10df2cb0601a0a4df22f72a.pdf",
            "Abstract": "Context: Modern automated source code analysis techniques can be very successful in detecting a priori defined defect patterns and security vulnerabilities. Yet, they cannot detect flaws that manifest due to erroneous translation of the software\u2019s functional requirements into the source code. The automated detection of logical errors that are attributed to a faulty implementation of applications\u2019 functionality, is a relatively uncharted territory. In previous research, we proposed a combination of automated analyses for logical error detection. In this paper, we develop a novel business-logic oriented method able to filter mathematical depictions of software logic in order to augment logical error detection, eliminate previous limitations in analysis and provide a formal tested logical error detection classification without subjective discrepancies. As a proof of concept, our method has been implemented in a prototype tool called PLATO that can detect various types of logical errors. Potential logical errors are thus detected that are ranked using a fuzzy logic system with two scales characterizing their impact:(i) a Severity scale, based on the execution paths\u2019 characteristics and Information Gain,(ii) a Reliability scale, based on the measured program\u2019s Computational Density. The method\u2019s effectiveness is shown using diverse experiments. Albeit not without restrictions, the proposed automated analysis seems able to detect a wide variety of logical errors, while at the same time limiting the false positives.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:PklR0melJeUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Stochastic Game-Based Analysis of the DNS Bandwidth Amplification Attack Using Probabilistic Model Checking",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6821109/",
            "Abstract": "The Domain Name System (DNS) is an Internet-wide, hierarchical naming system used to translate domain names into numeric IP addresses. Any disruption of DNS service can have serious consequences. We present a formal game-theoretic analysis of a notable threat to DNS, namely the bandwidth amplification attack (BAA), and the countermeasures designed to defend against it. We model the DNS BAA as a two-player, turn-based, zero-sum stochastic game between an attacker and a defender. The attacker attempts to flood a victim DNS server with malicious traffic by choosing an appropriate number of zombie machines with which to attack. In response, the defender chooses among five BAA countermeasures, each of which seeks to increase the amount of legitimate traffic the victim server processes. To simplify the model and optimize the analysis, our model does not explicitly track the handling of each \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:j7XjBeKFbTsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Economic assessment of externalities for interactive audio media anti-SPIT protection of internet services",
            "Publication year": 2012,
            "Publication url": "https://www.inderscienceonline.com/doi/abs/10.1504/IJESDF.2012.048416",
            "Abstract": "Spam over internet telephony (SPIT) refers to all unsolicited and massive scale attempts to establish voice communication with oblivious users of voice over internet protocol (VoIP) services. SPIT exhibits a significant increase over the last years, thus developing into a serious threat with adverse impact and costs for the business economy. An audio completely automated public Turing test to tell computers and human apart (CAPTCHA) has been introduced as a means to distinguish automated software agents (bots) from human. CAPTCHA has been proposed as a security measure against SPIT. In this paper, we lay the principles for an adequate understanding of the SPAM-related economic models, as well as their analogies to the SPIT phenomenon, so as to weigh the benefits of audio CAPTCHA protection against the incurred costs. Our approach is based on the economic assessment of externalities, i.e., the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:bEWYMUwI8FkC",
            "Publisher": "Inderscience Publishers Ltd"
        },
        {
            "Title": "Performance and effectiveness trade\u2010off for checkpointing in fault\u2010tolerant distributed systems",
            "Publication year": 2007,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1059",
            "Abstract": "Checkpointing has a crucial impact on systems' performance and fault\u2010tolerance effectiveness: excessive checkpointing results in performance degradation, while deficient checkpointing incurs expensive recovery. In distributed systems with independent checkpoint activities there is no easy way to determine checkpoint frequencies optimizing response\u2010time and fault\u2010tolerance costs at the same time. The purpose of this paper is to investigate the potentialities of a statistical decision\u2010making procedure. We adopt a simulation\u2010based approach for obtaining performance metrics that are afterwards used for determining a trade\u2010off between checkpoint interval reductions and efficiency in performance. Statistical methodology including experimental design, regression analysis and optimization provides us with the framework for comparing configurations, which use possibly different fault\u2010tolerance mechanisms \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:u5HHmVD_uO8C",
            "Publisher": "John Wiley & Sons, Ltd."
        },
        {
            "Title": "ACID Sim Tools: A simulation framework for distributed transaction processing architectures",
            "Publication year": 2008,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.5495&rep=rep1&type=pdf",
            "Abstract": "Modern network centric information systems implement highly distributed architectures that usually include multiple application servers. Application design is mainly based on the fundamental object-oriented principles and the adopted architecture matches the logical decomposition of applications (into several tiers like presentation, logic and data) to their software and hardware structuring. The provided recovery solutions ensure an at-mostonce service request processing by an existing transaction processing infrastructure. However, in published works performance evaluation of transaction processing aspects is focused on the computational model of database servers. Also, there are no available tools which enable exploring the performance and availability trade-offs that arise when applying different combinations of concurrency control, atomic commit and recovery protocols. This paper introduces ACID Sim Tools, a publicly available tool and at the same time an open source framework for interactive and batch-mode simulation of transaction processing architectures that adopt the basic assumptions of an object-based computational model.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:Y0pCki6q_DkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Design of embedded systems with complex task dependencies and shared resource interference",
            "Publication year": 2017,
            "Publication url": "https://pedropalomo.github.io/papers/multicore_SEFM_2017_paper_108.pdf",
            "Abstract": "Languages for embedded systems ensure predictable timing behavior by specifying constraints based on either data streaming or reactive control models of computation. Moreover, various toolsets facilitate the incremental integration of application functionalities and the system design by evolutionary refinement and model-based code generation. Modern embedded systems involve various sources of interference in shared resources (eg multicores) and advanced real-time constraints, such as mixed-criticality levels. A sufficiently expressive modeling approach for complex dependency patterns between realtime tasks is needed along with a formal analysis of models for runtime resource managers with timing constraints. Our approach utilizes a model of computation, called Fixed-Priority Process Networks, which ensures functional determinism by unifying streaming and reactive control within a timed automata framework. The tool flow extends the open source TASTE tool-suite with model transformations to the BIP language and code generation tools. We outline the use of our flow on the design of a spacecraft on-board application running on a quad-core LEON4FT processor.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:anDooRL1HQEC",
            "Publisher": "Aristotle University of Thessaloniki"
        },
        {
            "Title": "Probabilistic model checking for the quantification of DoS security threats",
            "Publication year": 2009,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167404809000042",
            "Abstract": "Secure authentication features of communication and electronic commerce protocols involve computationally expensive and memory intensive cryptographic operations that have the potential to be turned into denial-of-service (DoS) exploits. Recent proposals attempt to improve DoS resistance by implementing a trade-off between the resources required for the potential victim(s) with the resources used by a prospective attacker. Such improvements have been proposed for the Internet Key Exchange (IKE), the Just Fast Keying (JFK) key agreement protocol and the Secure Sockets Layer (SSL/TLS) protocol. In present article, we introduce probabilistic model checking as an efficient tool-assisted approach for systematically quantifying DoS security threats. We model a security protocol with a fixed network topology using probabilistic specifications for the protocol participants. We attach into the protocol model, a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:qjMakFHDy7sC",
            "Publisher": "Elsevier Advanced Technology"
        },
        {
            "Title": "A technique for determining queuing network simulation length based on",
            "Publication year": 0,
            "Publication url": "https://depend.csd.auth.gr/media/publications/CRL_Pub.pdf",
            "Abstract": "Although simulation is a commonly used approach by the computer systems performance engineers and the communication systems engineers, not enough consideration is usually given to the quality of simulation results. However, the queuing network simulation is not only a representation of the dynamic behavior of a system, but it is basically a stochastic process whose output has to be statistically analyzed. Moreover, the random selection of the simulation nun length often leads to multiple replications of the same experimentin order to achieve the desired level of estimation accuracy. This paper shows an algorithm for dynamically determining the simulation run length in regard to the required accuracy. The last, is defined as an acceptable confidence interval length of the estimated values (confidence intervals are produced by the statistical analysis of the estimation results with the use of the regenerative \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:xlVdBZVQT58C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Attacking an OT-Based Blind Signature Scheme",
            "Publication year": 2009,
            "Publication url": "https://arxiv.org/abs/0906.2947",
            "Abstract": "In this paper, we describe an attack against one of the Oblivious-Transfer-based blind signatures scheme, proposed in [1]. An attacker with a primitive capability of producing specific-range random numbers, while exhibiting a partial MITM behavior, is able to corrupt the communication between the protocol participants. The attack is quite efficient as it leads to a protocol communication corruption and has a sound-minimal computational cost. We propose a solution to fix the security flaw.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:dhFuZR0502QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A framework for access control with inference constraints",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6032355/",
            "Abstract": "In this paper we present an approach for investigating the feasibility of reducing inference control to access control, as the latter is a more desirable means of preventing unauthorized access to sensitive data. Access control is preferable over inference control in terms of efficiency, but it fails to offer confidentiality in the presence of inference channels. We argue that during the design phase of a data schema and the definition of user roles, inference channels should be considered. An approach is introduced that can be integrated into a risk assessment exercise to assist in determining the roles and/or attributes that lower the risks associated with information disclosure from inference. The residual risk from the remaining inference channels could be treated by well known inference control mechanisms.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:-f6ydRqryjwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Component certification as a prerequisite for widespread OSS reuse",
            "Publication year": 2010,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:BqipwSGYUEgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Approximate and Simulation Based Analysis for Distributed Object Software Performance Models",
            "Publication year": 2003,
            "Publication url": "https://www.researchgate.net/profile/Panagiotis-Katsaros/publication/2922890_Approximate_and_Simulation_Based_Analysis_for_Distributed_Object_Software_Performance_Models/links/54e18a020cf296663792beb9/Approximate-and-Simulation-Based-Analysis-for-Distributed-Object-Software-Performance-Models.pdf",
            "Abstract": "In complex software systems, the effectiveness of model based performance predictions is limited by the availability of appropriate solution techniques. These techniques should allow to take into account the software components interaction effects. In distributed object systems, the main problem is the simultaneous resource possession caused by the synchronous, often nested object invocations, which block the callers, until they get the replies. This paper provides a review of the analysis techniques, which address that fact, while preserving the abstract system view, offered by a queuing network representation. Two of these techniques, were proposed for solving a general class of models, with one or more layers of software servers and a third technique was designed specifically for distributed object software performance models. The advent of an extended flow-equivalent approximation, which is also described, opens new prospects for the development of efficient solution algorithms. Finally, simulation based estimation is discussed, in respect with the applicability of the well-founded and accurate, single-run regenerative method.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:R3hNpaxXUhUC",
            "Publisher": "EUROSIS"
        },
        {
            "Title": "Abstract model repair",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-28891-3_32",
            "Abstract": "Given a Kripke structure M and CTL formula \u03d5, where , the problem of Model Repair is to obtain a new model M\u2032 such that M\u2032\u2009\u22a7\u2009\u03d5. Moreover, the changes made to M to derive M\u2032 should be minimal with respect to all such M\u2032. As in model checking, state explosion can make it virtually impossible to carry out model repair on models with infinite or even large state spaces. In this paper, we present a framework for model repair that uses abstraction refinement to tackle state explosion. Our model-repair framework is based on Kripke Structures, a 3-valued semantics for CTL, and Kripke Modal Transition Systems (KMTSs), and features an abstract-model-repair algorithm for KMTSs. Application to an Automatic Door Opener system is used to illustrate the practical utility of abstract model repair.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:tFzHCjejgA0C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Automated exploit detection using path profiling: The disposition should matter, not the position",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7518025/",
            "Abstract": "Recent advances in static and dynamic program analysis resulted in tools capable to detect various types of security bugs in the Applications under Test (AUT). However, any such analysis is designed for a priori specified types of bugs and it is characterized by some rate of false positives or even false negatives and certain scalability limitations. We present a new analysis and source code classification technique, and a prototype tool aiming to aid code reviews in the detection of general information flow dependent bugs. Our approach is based on classifying the criticality of likely exploits in the source code using two measuring functions, namely Severity and Vulnerability. For an AUT, we analyse every single pair of input vector and program sink in an execution path, which we call an Information Block (IB). A classification technique is introduced for quantifying the Severity (danger level) of an IB by static analysis \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:-38epGy1wY0C",
            "Publisher": "SCITEPRESS"
        },
        {
            "Title": "Synthetic metrics for evaluating runtime quality of software architectures with complex tradeoffs",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5349844/",
            "Abstract": "Runtime quality of software, such as availability and throughput, depends on architectural factors and execution environment characteristics (e.g. CPU speed, network latency). Although the specific properties of the underlying execution environment are unknown at design time, the software architecture can be used to assess the inherent impact of the adopted design decisions on runtime quality. However, the design decisions that arise in complex software architectures exhibit non trivial interdependences. This work introduces an approach that discovers the most influential factors, by exploiting the correlation structure of the analyzed metrics via factor analysis of simulation data. A synthetic performance metric is constructed for each group of correlated metrics. The variability of these metrics summarizes the combined factor effects hence it is easier to assess the impact of the analyzed architecture decisions on the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A survey on the formalisation of system requirements and their validation",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S2590005620300151",
            "Abstract": "System requirements define conditions and capabilities to be met by a system under design. They are a partial definition in natural language, with inevitable ambiguities. Formalisation concerns with the transformation of requirements into a specification with unique interpretation, for resolving ambiguities, underspecified references and for assessing whether requirements are consistent, correct (i.e. valid for an acceptable solution) and attainable. Formalisation and validation of system requirements provides early evidence of adequate specification, for reducing the validation tests and high-cost corrective measures in the later system development phases. This article has the following contributions. First, we characterise the specification problem based on an ontology for some domain. Thus, requirements represent a particular system among many possible ones, and their specification takes the form of mapping their \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:DwWRdx-KAo4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Elastic Component Characterization with respect to Quality Properties: An intuitionistic fuzzy-based approach",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6065100/",
            "Abstract": "Component selection based on quality properties is a fuzzy process because measurable component attributes cannot be attributed with certainty to high-level quality properties such as the ones proposed by the ISO/IEC 9126 quality model and other similar models. In addition, measurable component quality attributes can be characterized differently for different application domains (e.g., a total execution time value can be considered very satisfactory for one application domain and extremely unsatisfactory for another). In this paper we demonstrate the usage of an intuitionistic fuzzy approach in selecting components originating from an elastic component repository. Elastic components are the output of a quality-driven process for component development that results in component variants based on quality discrimination. During reuse, utilization of intuitionistic fuzzy sets can be proven an efficient solution to derive \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:QIV2ME_5wuYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Model checking and code generation for transaction processing software",
            "Publication year": 2012,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1876",
            "Abstract": "In modern transaction processing software, the ACID properties (atomicity, consistency, isolation, durability) are often relaxed, in order to address requirements that arise in computing environments of today. Typical examples are the long\u2010running transactions in mobile computing, in service\u2010oriented architectures and B2B collaborative applications. These new transaction models are collectively known as advanced or extended transactions. Formal specification and reasoning for transaction properties have been limited to proof\u2010theoretic approaches, despite the recent progress in model checking. In this work, we present a model\u2010driven approach for generating a provably correct implementation of the transaction model of interest. The model is specified by state machines for the transaction participants, which are synchronized on a set of events. All possible execution paths of the synchronized state machines are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:kNdYIx-mwKoC",
            "Publisher": "Wiley"
        },
        {
            "Title": "Correct-by-construction model-based design of reactive streaming software for multi-core embedded systems",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s10009-019-00521-7",
            "Abstract": "We present a model-based design approach toward correct-by-construction implementations of reactive streaming software for multi-core systems. A system\u2019s implementation is derived from a high-level process network model by applying semantics-preserving model transformations. The so-called fixed priority process networks (FPPNs) are programmed independently from the execution platform and combine streaming and reactive control behavior with task parallelism for utilizing multi-core processing. We first define the FPPN sequential execution semantics that specifies precedence constraints between job executions of different tasks. Applications are thus rendered such that for any given test stimuli, a deterministic output response is expected. Furthermore, we define the FPPN real-time semantics based on a timed-automata modeling framework. This is provably a functionally equivalent semantics \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:JTtNqH-x4gYC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Correction to: Correct-by-construction model-based design of reactive streaming software for multi-core embedded systems",
            "Publication year": 2020,
            "Publication url": "https://search.proquest.com/openview/3668d35c3c8dc6bae9c01d3c628705a5/1?pq-origsite=gscholar&cbl=46652",
            "Abstract": "Updated Fig. 7 (p i is the job\u2019s process, k i is the job\u2019s invocation count, A i is the invocation time, D i is the absolute deadline and C i is the WCET).",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:svGagg1hbZMC",
            "Publisher": "Springer Nature BV"
        },
        {
            "Title": "Probabilistic model checking of CAPTCHA admission control for DoS resistant anti-SPIT protection",
            "Publication year": 2013,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-41485-5_13",
            "Abstract": "Voice over IP (VoIP) service is expected to play a key role to new ways of communication. It takes advantage of Internet Protocols by using packet networks to transmit voice and multimedia data, thus providing extreme cost savings. On the other hand, this technology has inherited drawbacks, like SPAM over Internet Telephony (SPIT). A well-established method to tackle SPIT is the use of CAPTCHAs. CAPTCHAs are vulnerable to Denial of Service (DoS) attacks, due to their excessive demands for bandwidth. We suggest that anti-SPIT protection should be combined with appropriate admission control policies, for mitigating the effects of DoS attacks. In order to identify how effective is this technique, we quantify the costs and the benefits in bandwidth usage through probabilistic model checking four different admission control policies. We conclude with comments on how appropriate is each policy in tackling \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:YFjsv_pBGBYC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Optimal object state transfer-recovery policies for fault tolerant distributed systems",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1311947/",
            "Abstract": "Recent developments in the field of object-based fault tolerance and the advent of the first OMG FT-CORBA compliant middleware raise new requirements for the design process of distributed fault-tolerant systems. In this work, we introduce a simulation-based design approach based on the optimum effectiveness of the compared fault tolerance schemes. Each scheme is defined as a set of fault tolerance properties for the objects that compose the system. Its optimum effectiveness is determined by the tightest effective checkpoint intervals, for the passively replicated objects. Our approach allows mixing miscellaneous fault tolerance policies, as opposed to the published analytic models, which are best suited in the evaluation of single-server process replication schemes. Special emphasis has been given to the accuracy of the generated estimates using an appropriate simulation output analysis procedure. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:d1gkVwhDpl0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Formal analysis for robust anti-SPIT protection using model checking",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s10207-012-0159-4.pdf",
            "Abstract": "Anti-SPIT policies counter the SPam over Internet Telephony (SPIT) by distinguishing bots launching unsolicited bulks of VoIP calls from human beings. We propose an Anti-SPIT Policy Management mechanism (aSPM) that detects spam calls and prevents VoIP session establishment by the Session Initiation Protocol (SIP). The SPIN model checker is used to formally model and analyze the robustness of the aSPM mechanism in execution scenarios with parallel SIP sessions. In case of a possible design flaw, the model checker provides a trace of the caught unexpected behavior (counterexample), that can be used for the revision of the mechanism\u2019s design. Our SPIN model is parameterized, based on measurements from experiments with VoIP users. Non-determinism plays a key role in representing all possible anti-SPIT policy decisions, in terms of the SIP messages that may be exchanged. The model \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:4JMBOYKVnBMC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Regenerative estimation variants of response times in closed networks of queues",
            "Publication year": 2002,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.412.9153",
            "Abstract": "In this paper, we present a comparison of the possible regenerative estimation variants of response times, in multivariate simulations of closed queuing networks. The underlying stochastic framework of the techniques under study is first described and the applicability of each one of them is discussed. An appropriate sequential control procedure has been selected, in order to produce confidence intervals of the same nominal level and similar width, for the response times of interest. The first experimental results exhibit improved coverage of the corresponding analytic solutions, when a marked job based method is used, instead of an indirect estimation, by simulating a single regeneration sequence of the common number-in-queue process (usually used when estimating other characteristics like throughputs, utilizations, queue lengths etc). This finding clearly implies the simultaneous use of more than one regeneration sequence for multivariate studies that include response time characteristics.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:isC4tDSrTZIC",
            "Publisher": "WSEAS Press"
        },
        {
            "Title": "Security-aware elasticity for NoSQL databases in multi-cloud environments",
            "Publication year": 2017,
            "Publication url": "https://www.inderscienceonline.com/doi/abs/10.1504/IJIIDS.2017.087237",
            "Abstract": "We focus on horizontally scaling NoSQL databases in a cloud environment, in order to meet performance requirements while respecting security constraints. The performance requirements refer to strict latency limits on the query response time. The security requirements are derived from the need to address two specific kinds of threats that exist in cloud databases, namely data leakage, mainly due to malicious activities of actors hosted on the same physical machine, and data loss after one or more node failures. A key feature of our approach is that we account for multiple cloud providers offering resources of different characteristics. We explain that usually there is a trade-off between performance and security requirements and we derive a model checking approach to drive runtime decisions that strike a user-defined balance between them taking into account the infrastructure heterogeneity. Finally, we evaluate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:Mx5hWS9ctUkC",
            "Publisher": "Inderscience Publishers (IEL)"
        },
        {
            "Title": "Cloud elasticity using probabilistic model checking",
            "Publication year": 2014,
            "Publication url": "https://arxiv.org/abs/1405.4699",
            "Abstract": "Cloud computing has become the leading paradigm for deploying large-scale infrastructures and running big data applications, due to its capacity of achieving economies of scale. In this work, we focus on one of the most prominent advantages of cloud computing, namely the on-demand resource provisioning, which is commonly referred to as elasticity. Although a lot of effort has been invested in developing systems and mechanisms that enable elasticity, the elasticity decision policies tend to be designed without guaranteeing or quantifying the quality of their operation. This work aims to make the development of elasticity policies more formalized and dependable. We make two distinct contributions. First, we propose an extensible approach to enforcing elasticity through the dynamic instantiation and online quantitative verification of Markov Decision Processes (MDP) using probabilistic model checking. Second, we propose concrete elasticity models and related elasticity policies. We evaluate our decision policies using both real and synthetic datasets in clusters of NoSQL databases. According to the experimental results, our approach improves upon the state-of-the-art in significantly increasing user-defined utility values and decreasing user-defined threshold violations.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:bCjgOgSFrM0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Inventory, assessment and monitoring of Mediterranean Wetlands: The MedWet Web Information System User Manual",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=8031520823361225042&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:zYLM7Y9cAGgC",
            "Publisher": "AUTH & EKBY. MedWet publication.(Scientific reviewer Nick J Riddiford)"
        },
        {
            "Title": "Quantification of interacting runtime qualities in software architectures: Insights from transaction processing in client-server architectures",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584910001321",
            "Abstract": "Architecture is fundamental for fulfilling requirements related to the non-functional behavior of a software system such as the quality requirement that response time does not degrade to a point where it is noticeable. Approaches like the Architecture Tradeoff Analysis Method (ATAM) combine qualitative analysis heuristics (e.g. scenarios) for one or more quality metrics with quantitative analyses. A quantitative analysis evaluates a single metric such as response time. However, since quality metrics interact with each other, a change in the architecture can affect unpredictably multiple quality metrics.This paper introduces a quantitative method that determines the impact of a design change on multiple metrics, thus reducing the risks in architecture design. As a proof of concept, the method is applied on a simulation model of transaction processing in client server architecture.Factor analysis is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:4TOpqqG69KYC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Report on the Predictable Assembly from Certifiable Code (PACC) Workshop for Educators",
            "Publication year": 2009,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:k_IJM867U9cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ACID Sim Tools: Evaluating tradeoffs between recovery costs and performance in Distributed Transaction Processing Architectures",
            "Publication year": 0,
            "Publication url": "https://scholar.google.com/scholar?cluster=2322462003647065155&hl=en&oi=scholarr",
            "Abstract": "In modern network centric information systems, multi-tier applications are now becoming mainstream. Their design is mainly based on the fundamental principles of objectorientation and the most common recovery solutions ensure at-most-once service request processing, through some form of \u201call-or-nothing\u201d guarantee by an underlying transaction processing architecture. Transactional objects cooperate with the assigned transaction managers to provide system-wide Atomicity, Consistency, Isolation and Durability (ACID) guarantees for the performed operations. Architectural solutions are standardized in reference specifications, with the most notable one the OMG Object Transaction Service (OTS). However, to the best of our knowledge there are no evaluation means with metrics, which can provide insight into the tradeoffs between recovery costs and performance when applying different combinations of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:rJyh6hJnyfgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Performance Analysis of Distributed Software Architectures",
            "Publication year": 2002,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:JV2RwH3_ST0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the design of access control to prevent sensitive information leakage in distributed object systems: a Colored Petri Net model",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=7351039315621659978&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:GnPB-g6toBAC",
            "Publisher": "Springer"
        },
        {
            "Title": "Runtime Verification of Autonomous Driving Systems in CARLA",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-60508-7_9",
            "Abstract": "Urban driving simulators, such as CARLA, provide 3-D environments and useful tools to easily simulate sensorimotor control systems in scenarios with complex multi-agent dynamics. This enables the design exploration at the early system development stages, reducing high infrastructure costs and high risks. However, due to the high-dimensional input and state spaces of closed-loop autonomous driving systems, their testing and verification is very challenging and it has not yet taken advantage of the recent developments in theory and tools for runtime verification. We show here how to integrate the recently introduced  library, for runtime verification of STL (Signal Temporal Logic) specifications, with the CARLA simulator. Finally, we also present the obtained results from monitoring quantitatively interesting requirements for an experimental Adaptive Cruise Control system tested in CARLA.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:T_0gP6tLVL0C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "A simulation process for asynchronous event processing systems: Evaluating performance and availability in transaction models",
            "Publication year": 2012,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1569190X12001049",
            "Abstract": "Simulation is essential for understanding the performance and availability behavior of complex systems, but there are significant difficulties when trying to simulate systems with multiple components, which interact with asynchronous communication. A systematic process is needed, in order to cope with the complexity of asynchronous event processing and the failure semantics of the interacting components. We address this problem by introducing an approach that combines formal techniques for faithful representation of the complex system effects and a statistical analysis for simultaneously studying multiple simulation outcomes, in order to interpret them. Our process has been successfully applied to a synthetic workload for distributed transaction processing. We outline the steps followed towards generating a credible simulation model and subsequently we report and interpret the results of the applied statistical \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:iH-uZ7U-co4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Formal Verification of Network Interlocking Control by Distributed Signal Boxes",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-32872-6_14",
            "Abstract": "Interlocking control prevents certain operations from occurring, unless preceded by specific events. It is used in traffic network control systems (e.g. railway interlocking control), piping and tunneling control systems and in other applications like for example communication network control. Interlocking systems have to comply with certain safety properties and this fact elevates formal modeling as the most important concern in their design. This paper introduces an interlocking control algorithm based on the use of what we call Distributed Signal Boxes (DSBs). Distributed control eliminates the intrinsic complexity of centralized interlocking control solutions, which are mainly developed in the field of railway traffic control. Our algorithm uses types of network control units, which do not store state information. Control units are combined according to a limited number of patterns that in all cases yield safe network \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:lPDSu1ZU3VAC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Model-Based Safety and Assessment: 6th International Symposium, IMBSA 2019, Thessaloniki, Greece, October 16\u201318, 2019, Proceedings",
            "Publication year": 2019,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=fm21DwAAQBAJ&oi=fnd&pg=PP6&dq=info:A8YjcGSoh1kJ:scholar.google.com&ots=0mojKPkWKU&sig=RTyRoxvLhcMaU_g4W8Q5CrQRsiw",
            "Abstract": "This book constitutes the proceedings of the 6th International Symposium on Model-Based Safety and Assessment, IMBSA 2019, held inThessaloniki, Greece, in October 2019. The 24 revised full papers presented were carefully reviewed and selected from 46 initial submissions. The papers are organized in topical sections on safety models and languages; dependability analysis process; safety assessment; safety assessment in automotive industry; AI in safety assessment.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:vkuYBMKU6wEC",
            "Publisher": "Springer Nature"
        },
        {
            "Title": "Economic evaluation of interactive audio media for securing internet services",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-33448-1_7",
            "Abstract": "Internet Telephony (Voice over Internet Protocol or VoIP) has recently become increasingly popular mainly due to its cost advantages and range of advance services. On the same time, SPam over Internet Telephony (SPIT) referred as unsolicited bulk calls sent via VoIP networks by botnets, is expected to become a serious threat in the near future. Audio CAPTCHA (Completely Automated Public Turing test to tell Computers and Human Apart) mechanism were introduced and employed as a security measure to distinguish automated software agents from human beings. The scope of this paper is to present the security economics frame and to have an in-depth review of the related economic models of SPAM and its analogies with SPIT.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:IWHjjKOFINEC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Model Checking for Generation of Test Suites in Software Unit Testing",
            "Publication year": 2010,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:4DMP91E08xMC",
            "Publisher": "Work In Progress Session"
        },
        {
            "Title": "Formal analysis of the kaminsky DNS cache-poisoning attack using probabilistic model checking",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5634315/",
            "Abstract": "We use the probabilistic model checker PRISM to formally model and analyze the highly publicized Kaminsky DNS cache-poisoning attack. DNS (Domain Name System) is an internet-wide, hierarchical naming system used to translate domain names such as google.com into physical IP addresses such as 208.77.188.166. The Kaminsky DNS attack is a recently discovered vulnerability in DNS that allows an intruder to hijack a domain, i.e. corrupt a DNS server so that it replies with the IP address of a malicious web server when asked to resolve URLs within a non-malicious domain such as google.com. A proposed fix for the attack is based on the idea of randomizing the source port a DNS server uses when issuing a query to another server in the DNS hierarchy. We use PRISM to introduce a Continuous Time Markov Chain representation of the Kaminsky attack and the proposed fix, and to perform the required \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:YsMSGLbcyi4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Model-based energy characterization of IoT system design aspects",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-31514-6_10",
            "Abstract": " The advances towards IoT systems with increased autonomy support improvements to existing applications and open new perspectives for other application domains. However, the design of IoT systems is challenging, due to the multiple design aspects that need to be considered. Connectivity and storage aspects are amongst the most significant ones, as IoT devices are resource-constrained and in many cases battery-powered. On top of them, it is also essential to consider privacy and security aspects that are linked to the protection of the IoT system, as well as of the data exchanged through its connectivity interfaces. Ensuring security in an IoT system, though, is an evident need and a complex challenge, due to its impact in the battery lifetime. In this paper, we propose a methodology to manage energy consumption through a model-based approach for the energy characterization of IoT design aspects \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:aNch6Af-aFkC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "A probabilistic attacker model for quantitative verification of DoS security threats",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4591526/",
            "Abstract": "This work introduces probabilistic model checking as a viable tool-assisted approach for systematically quantifying DoS security threats. The proposed analysis is based on a probabilistic attacker model implementing simultaneous N zombie participants, which subvert secure authentication features in communication protocols and electronic commerce systems. DoS threats are expressed as probabilistic reachability properties that are automatically verified through an appropriate Discrete Time Markov Chain representing the protocol participants and attacker models. The overall analysis takes place in a mature probabilistic model checking toolset called PRISM. We believe that the applied quantitative verification approach is a valuable means for comparing protocol implementations with alternative parameter choices, for optimal resistance to the analyzed threats.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:W7OEmFMy1HYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Ontology-based model driven engineering for safety verification",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5598078/",
            "Abstract": "Safety assessment of dependable systems is a complex verification task that is desirable to be explicitly incorporated into the development cycle during the very early stages of a project. The main reason is that the cost to correct a safety error at the late stages of system development is excessively high. Towards this aim, we introduce an ontology-based model-driven engineering process for automating transformations of models that are utilized as reusable artifacts. The logical and syntactical structures of the design and safety models have to conform to a number of metamodel constraints. These constraints are semantically represented by mapping them onto an OWL domain ontology, allowing the incorporation of a Description Logic OWL reasoner and inference rules, in order to detect lacks of model elements and semantically inconsistent parts. Model validation throughout the ontology-based transformation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:ZeXyd9-uunAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Structured performance modeling and analysis for object based distributed software systems",
            "Publication year": 2002,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.1074&rep=rep1&type=pdf",
            "Abstract": "In this paper, we address the problems related to the performance modeling of object based software systems, distributed across multiple platforms. Classical flat queuing models seem to be inappropriate, because of the inevitable complexity caused by the platform heterogeneity and the dual client/server role that objects often play in their interactions. Models should be structured vertically, in hierarchical levels, as well as horizontally, in interconnected model components. Each component may be using several analytically intractable queuing network extensions, for the precise representation of the synchronization phenomena that arise in the various object interaction cases. The overall model structure is utilized not only for incrementally specifying a complex workload and resource contention description, but also for approximately analyzing it, by successive flow equivalence aggregations. In this context, we discuss the properties that the model should possess, for achieving satisfactory levels of accuracy. The modeling of common design cases, like the synchronous object invocation, the distributed callback and the multithreading process structure, is illustrated, in the context of CORBA based object interactions. The suggested approach promotes the accomplishment of the appropriate type of analysis at the most suitable level of abstraction, in respect to the specific credibility and cost requirements of each study.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:MXK_kJrjxJIC",
            "Publisher": "International Society for Computers and their Applications"
        },
        {
            "Title": "Program analysis with risk-based classification of dynamic invariants for",
            "Publication year": 2008,
            "Publication url": "https://www.academia.edu/download/52099502/COSE_SI_SECRYPT_Proof.pdf",
            "Abstract": "The logical errors in programs causing deviations from the intended functionality cannot be detected by automated source code analysis, which mainly focuses on known defects and code vulnerabilities. To this end, we introduce a combination of analysis techniques implemented in a proof-of-concept prototype called PLATO. First, a set of dynamic invariants is inferred from the source code that represent the program\u2019s logic. The code is instrumented with assertions from the invariants, which are subsequently valuated through the program\u2019s symbolic execution. The findings are ranked using a fuzzy logic system with two scales characterizing their impact:(i) a Severity scale for the execution paths\u2019 characteristics and their Information Gain,(ii) a Reliability scale based on the measured Computational Density. Real, as well as synthetic applications with at least four different types of logical errors were analyzed. The method\u2019s effectiveness was assessed based on a dataset from 25 experiments. Albeit not without restrictions, the proposed automated analysis seems able to detect a wide variety of logical errors, while it filters out the false positives.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:bkKuixW_xMkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A formally verified mechanism for countering SPIT",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-21694-7_11",
            "Abstract": "Voice over IP (VoIP) is a key technology, which provides new ways of communication. It enables the transmission of telephone calls over the Internet, which delivers economical telephony that can clearly benefit both consumers and businesses, but it also provides a cheap method of mass advertising. Those bulks unsolicited calls are known as SPam over Internet Telephony (SPIT). In this paper we illustrate an anti-SPIT policy-based management (aSPM) mechanism which can handle the SPIT phenomenon. Moreover, we introduce a formal verification as a mean for validating the effectiveness of the aSPM against its intended goals. We provide model checking results that report upper bounds in the duration of call session establishment for the analyzed anti-SPIT policy over the Session Initiation Protocol (SIP) and prove the absence of deadlocks.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:RHpTSmoSYBkC",
            "Publisher": "Springer"
        },
        {
            "Title": "Abstract Model Repair",
            "Publication year": 2015,
            "Publication url": "https://lmcs.episciences.org/1587",
            "Abstract": "Given a Kripke structure M and CTL formula , where M does not satisfy , the problem of Model Repair is to obtain a new model M' such that M' satisfies . Moreover, the changes made to M to derive M' should be minimum with respect to all such M'. As in model checking, state explosion can make it virtually impossible to carry out model repair on models with infinite or even large state spaces. In this paper, we present a framework for model repair that uses abstraction refinement to tackle state explosion. Our framework aims to repair Kripke Structure models based on a Kripke Modal Transition System abstraction and a 3-valued semantics for CTL. We introduce an abstract-model-repair algorithm for which we prove soundness and semi-completeness, and we study its complexity class. Moreover, a prototype implementation is presented to illustrate the practical utility of abstract-model-repair on an Automatic Door Opener system model and a model of the Andrew File System 1 protocol.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:RVqaWcrwK10C",
            "Publisher": "Episciences. org"
        },
        {
            "Title": "Formal Analysis of the DNS Bandwidth Amplification Attack and its Countermeasures Using Probabilistic Model Checking",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6113920/",
            "Abstract": "The DNS Bandwidth Amplification Attack (BAA) is a distributed denial-of-service attack in which a network of computers floods a DNS server with responses to requests that have never been made. Amplification enters into the attack by virtue of the fact that a small 60-byte request can be answered by a substantially larger response of 4,000 bytes or more in size. We use the PRISM probabilistic model checker to introduce a Continuous Time Markov Chain model of the DNS BAA and three recently proposed countermeasures, and to perform an extensive cost-benefit analysis of the countermeasures. Our analysis, which is applicable to both DNS and DNSSec (a security extension of DNS), is based on objective metrics that weigh the benefits for a server in terms of the percentage increase in the processing of legitimate packets against the cost incurred by incorrectly dropping legitimate traffic. The results we obtain \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:7PzlFSSx8tAC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "The Organon and the logic perspective of computation",
            "Publication year": 0,
            "Publication url": "https://www.researchgate.net/profile/Panagiotis-Katsaros/publication/303407444_The_Organon_and_the_logic_perspective_of_computation/links/5742236a08ae9f741b3756ec/The-Organon-and-the-logic-perspective-of-computation.pdf",
            "Abstract": "At a moment of culmination of the philosophical thought, ancient Greek philosophers were focused on a rational explanation of the world based exclusively on evidence and reasoning. In this era, Aristotle saw the need for a separate discipline to study and develop the act of pure reason, irrespective of what it is about. He wrote five treatises, known collectively as the Organon, that are still the object of study and of inspiration for scholars around the world, towards the quest of new logical insights [1-12].Having seen the Organon as a tool used by all the sciences, Aristotle introduced a theory of deductive inferences, which are known as syllogisms. The syllogism is defined as \u201ca logos (speech) in which, certain things having been supposed, something different from the things supposed results of necessity because of their being so\u201d. However, logical inference is not a matter of the contents of the supposed things. Schemes with dummy letters (variables) are used to stand for the terms (subjects and predicates) in the premises, like eg that \u201cAll A are B\u201d or that \u201cNo B is C\u201d. A scheme cannot be a syllogism, if there are terms to substitute for the letters that make the premises true and the conclusion false. This characterization of an argument\u2019s validity based on its logical form was first achieved by Aristotle, who is therefore considered as the father of formal logic. The syllogisms were the dominant form of logical reasoning with no major breakthroughs until the 19th century advances in mathematical logic, where the focus of interest is the logical reasoning in artificial languages.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:dMpQl7XwOw4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Abstract model repair for probabilistic systems",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0890540118300221",
            "Abstract": "Given a Discrete Time Markov Chain M and a probabilistic temporal logic formula \u03c6, where M violates \u03c6, the problem of Model Repair is to obtain a new model M\u2032, such that M\u2032 satisfies \u03c6. Additionally, the changes made to M in order to obtain M\u2032 should be minimum with respect to all such M\u2032. The state explosion problem makes the repair of large probabilistic systems almost infeasible. In this paper, we use the abstraction of Discrete Time Markov Chains in order to speed-up the process of model repair for temporal logic reachability properties. We present a framework based on abstraction and refinement, which reduces the state space of the probabilistic system to repair at the price of obtaining an approximate solution. A metric space is defined over the set of DTMCs, in order to measure the differences between the initial and the repaired models. For the repair, we introduce an algorithm and we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:MqTxh1vmwXEC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Inventory, assessment and monitoring of Mediterranean Wetlands: The MedWet Inventory Data Sharing Protocol",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=2076792693912849939&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:IjCSPb-OGe4C",
            "Publisher": "EKBY, TdV & AUTH. MedWet publication.(Scientific reviewer Nick J Riddiford)"
        },
        {
            "Title": "A simulation test-bed for the design of dependable e-services",
            "Publication year": 2003,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.217.2429",
            "Abstract": "In this paper, we present the design and development of a simulation test-bed, for the performance analysis of dependable and mission critical e-services. Key characteristics of such systems, like for example, the object state transfer and recovery policies to be applied together with the chosen load distribution strategy, play the determinant role in the service\u2019s performance and customer response perception. Analytic models usually fail to realistically capture the effects of the available design alternatives. On the other hand, the published simulation studies are usually bound to algorithms, which are not covered by the recently published standards, for the development of object based services. Our work aims to provide a comparison framework that adheres to the published standards, allows the compositional development of the service configurations of interest and the estimation of meaningful performance measures, in an efficient way. Key-Words:-simulation, quality of service, fault tolerance, load distribution, state transfer and recovery schemes, performance evaluation, distributed object systems, e-services 1",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Static program analysis of multi-applet JavaCard applications",
            "Publication year": 2010,
            "Publication url": "https://www.igi-global.com/chapter/software-engineering-secure-systems/48414",
            "Abstract": "Java Card provides a framework of classes and interfaces that hide the details of the underlying smart card interface and make it possible to load and run on the same card several applets, from different application providers with complex trust relationships. This fact paves the way for new business applications, but the card issuer has to secure absence of malicious or faulty card applets. He has to be able to check that (i) applets do not cause illicit method invocations that violate temporal restrictions of inter-applet communication,(ii) applets protect themselves from unwanted information flow to third parties and (iii) it is not possible for an unhandled Java Card API exception to leave an applet in an unpredictable state that is potentially dangerous for the application\u2019s security. The authors explore recent advances in theory and tool support of static program analysis and they present an approach for automatic \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:aqlVkmm33-oC",
            "Publisher": "Information Science Reference"
        },
        {
            "Title": "Static program analysis for Java Card applets",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-85893-5_2",
            "Abstract": "The Java Card API provides a framework of classes and interfaces that hides the details of the underlying smart card interface, thus relieving developers from going through the swamps of microcontroller programming. This allows application developers to concentrate most of their effort on the details of application, assuming proper use of the Java Card API calls regarding (i) the correctness of the methods\u2019 invocation targets and their arguments and (ii) temporal safety, i.e. the requirement that certain method calls have to be used in certain orders. Several characteristics of the Java Card applets and their multiple-entry-point program structure make it possible for a potentially unhandled exception to reach the invoked entry point. This contingency opens a possibility to leave the applet in an unpredictable state that is potentially dangerous for the application\u2019s security. Our work introduces automatic static \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:KlAtU1dfN6UC",
            "Publisher": "Springer"
        },
        {
            "Title": "Formal analysis of DeGroot Influence Problems using probabilistic model checking",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1569190X18301369",
            "Abstract": "DeGroot learning is a model of opinion diffusion and formation in a social network. We examine the behaviour of the DeGroot learning model when external strategic players that aim to influence the opinion formation process are introduced. More specifically, we consider the case of a single decision maker and that of two competing players, with a fixed number of possible influence actions for each of them. In the former case, the DeGroot model takes the form of a Markov Decision Process (MDP), while in the latter case it takes the form of a Stochastic Game (SG). These models are solved using probabilistic model checking techniques, as well as other solution techniques beyond model checking. The viability of our analysis is attested on a well-known social network, the Zachary\u2019s karate club. Finally, the evaluation of influence in a social network simultaneously with the decision maker\u2019s cost is supported, which is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:eIKNFFVQvJAC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Process network models for embedded system design based on the real-time BIP execution engine",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1806.09850",
            "Abstract": "Existing model-based processes for embedded real-time systems support the analysis of various non-functional properties, most notably schedulability, through model checking, simulation or other means. The analysis results are then used for modifying the system's design, so that the expected properties are satisfied. A rigorous model-based design flow differs in that it aims at a system implementation derived from high-level models by applying a sequence of semantics-preserving transformations. Properties established at any design step are preserved throughout the subsequent steps including the executable implementation. We introduce such a design flow using a process network model of computation for application design at a high level, which combines streaming and reactive control processing with task parallelism. The schedulability of the so-called FPPNs (Fixed Priority Process Networks) is well-studied and various solutions have been presented. This article focuses on the design flow's steps for deriving executable implementations on the BIP (Behavior - Interaction - Priority) runtime environment. FPPNs are designed using the TASTE toolset, a convenient architecture description interface. In this way, the developers do not program explicitly low-level real-time OS services and the schedulability properties are guaranteed throughout the design steps by construction. The approach has been validated on the design of a real spacecraft on-board application that has been scheduled for execution on an industrial multicore platform.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:Pqt4MY__2vwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Rigorous analysis of service composability by embedding WS-BPEL into the BIP component framework",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6257823/",
            "Abstract": "Behavioral correctness of service compositions refers to the absence of service interaction flaws, so that essential service properties like deadlock freedom are preserved and correctness properties related to safety and liveness are assured. Model checking is a widespread technique and it is based on extracting an abstract model representation of the program defining a service orchestration or choreography. During model extraction, the original structure of the service composition cannot be preserved and backwards traceability of the verification findings is not possible. We propose a rigorous analysis within the BIP component framework. Being rigorous means that the analyst is able to reason on which properties hold and why. The BIP language offers a sound execution semantics for a minimal set of primitives and constructs for modeling and composing layered components. We formally define the WS-BPEL 2.0 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:r0BpntZqJG4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "\u0398\u03b5\u03c9\u03c1\u03af\u03b1 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u03cd \u03ba\u03b1\u03b9 \u03b5\u03c6\u03b1\u03c1\u03bc\u03bf\u03b3\u03ad\u03c2",
            "Publication year": 2015,
            "Publication url": "https://repository-web.kallipos.gr/handle/11419/5744",
            "Abstract": "\u0397 \u0398\u03b5\u03c9\u03c1\u03af\u03b1 \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u03cd \u03b1\u03bd\u03b1\u03c0\u03c4\u03cd\u03c7\u03b8\u03b7\u03ba\u03b5 \u03b3\u03b9\u03b1 \u03bd\u03b1 \u03bc\u03b5\u03bb\u03b5\u03c4\u03b7\u03b8\u03bf\u03cd\u03bd \u03b8\u03b5\u03bc\u03b5\u03bb\u03b9\u03ce\u03b4\u03b7 \u03b6\u03b7\u03c4\u03ae\u03bc\u03b1\u03c4\u03b1 \u03c3\u03c7\u03b5\u03c4\u03b9\u03ba\u03ac \u03bc\u03b5 \u03c4\u03b7\u03bd \u03ad\u03bd\u03bd\u03bf\u03b9\u03b1 \u03c4\u03bf\u03c5 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u03cd \u03ba\u03b1\u03b9 \u03c4\u03b9\u03c2 \u03b4\u03c5\u03bd\u03b1\u03c4\u03cc\u03c4\u03b7\u03c4\u03b5\u03c2 \u03c4\u03c9\u03bd \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03b9\u03ba\u03ce\u03bd \u03bc\u03b1\u03c2 \u03bc\u03b7\u03c7\u03b1\u03bd\u03ce\u03bd. \u039f \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03cc\u03c2 \u03b5\u03af\u03bd\u03b1\u03b9 \u03ad\u03bd\u03b1 \u03c6\u03c5\u03c3\u03b9\u03ba\u03cc \u03c6\u03b1\u03b9\u03bd\u03cc\u03bc\u03b5\u03bd\u03bf, \u03c0\u03bf\u03c5 \u03b5\u03ba\u03c4\u03b5\u03bb\u03b5\u03af\u03c4\u03b1\u03b9 \u03c3\u03b5 \u03ad\u03bd\u03b1 \u03ba\u03bb\u03b5\u03b9\u03c3\u03c4\u03cc \u03c3\u03cd\u03c3\u03c4\u03b7\u03bc\u03b1 \u03c0\u03bf\u03c5 \u03c4\u03bf \u03bf\u03bd\u03bf\u03bc\u03ac\u03b6\u03bf\u03c5\u03bc\u03b5 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ae. \u03a0\u03bf\u03b9\u03bf\u03b9 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u03af \u03b5\u03af\u03bd\u03b1\u03b9 \u03c0\u03c1\u03b1\u03b3\u03bc\u03b1\u03c4\u03bf\u03c0\u03bf\u03b9\u03ae\u03c3\u03b9\u03bc\u03bf\u03b9 \u03ba\u03b1\u03b9 \u03c0\u03bf\u03b9\u03bf\u03b9 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u03af \u03b5\u03af\u03bd\u03b1\u03b9 \u03b1\u03b4\u03cd\u03bd\u03b1\u03c4\u03bf\u03b9; \u03a0\u03ce\u03c2 \u03bc\u03c0\u03bf\u03c1\u03bf\u03cd\u03bc\u03b5 \u03bd\u03b1 \u03be\u03ad\u03c1\u03bf\u03c5\u03bc\u03b5 \u03b1\u03bd \u03ad\u03bd\u03b1 \u03b4\u03bf\u03b8\u03ad\u03bd \u03c0\u03c1\u03cc\u03b2\u03bb\u03b7\u03bc\u03b1 \u03bc\u03c0\u03bf\u03c1\u03b5\u03af \u03bd\u03b1 \u03b5\u03c0\u03b9\u03bb\u03c5\u03b8\u03b5\u03af \u03ba\u03b1\u03b9 \u03b1\u03bd \u03b1\u03c5\u03c4\u03cc \u03bc\u03c0\u03bf\u03c1\u03b5\u03af \u03bd\u03b1 \u03b3\u03af\u03bd\u03b5\u03b9 \u03b5\u03c0\u03b1\u03c1\u03ba\u03ce\u03c2 \u03b3\u03c1\u03ae\u03b3\u03bf\u03c1\u03b1; \u0393\u03b9\u03b1 \u03bd\u03b1 \u03b1\u03c0\u03b1\u03bd\u03c4\u03b7\u03b8\u03bf\u03cd\u03bd \u03c4\u03b1 \u03c0\u03b1\u03c1\u03b1\u03c0\u03ac\u03bd\u03c9 \u03b5\u03c1\u03c9\u03c4\u03ae\u03bc\u03b1\u03c4\u03b1, \u03bf\u03b9 \u03b5\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03bf\u03bd\u03b5\u03c2 \u03b5\u03bc\u03c0\u03bd\u03ad\u03c5\u03c3\u03c4\u03b7\u03ba\u03b1\u03bd \u03b1\u03c6\u03b1\u03b9\u03c1\u03b5\u03c4\u03b9\u03ba\u03ac \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u03cd, \u03cc\u03c0\u03c9\u03c2 \u03bf\u03b9 \u03b1\u03bd\u03b1\u03b4\u03c1\u03bf\u03bc\u03b9\u03ba\u03ad\u03c2 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2, \u03bf \u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03cc\u03c2-\u03bb, \u03bf\u03b9 \u03bc\u03b7\u03c7\u03b1\u03bd\u03ad\u03c2 Turing \u03ba\u03b1\u03b9 \u03bf\u03b9 \u03bc\u03b7\u03c7\u03b1\u03bd\u03ad\u03c2 \u03c0\u03b5\u03c0\u03b5\u03c1\u03b1\u03c3\u03bc\u03ad\u03bd\u03c9\u03bd \u03ba\u03b1\u03c4\u03b1\u03c3\u03c4\u03ac\u03c3\u03b5\u03c9\u03bd. \u039f \u03bf\u03c1\u03b9\u03c3\u03bc\u03cc\u03c2 \u03c4\u03c9\u03bd \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03c9\u03bd \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u03cd \u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03af\u03b6\u03b5\u03c4\u03b1\u03b9 \u03b1\u03c0\u03cc \u03bc\u03af\u03b1 \u03bc\u03b1\u03b8\u03b7\u03bc\u03b1\u03c4\u03b9\u03ba\u03ae \u03b1\u03c5\u03c3\u03c4\u03b7\u03c1\u03cc\u03c4\u03b7\u03c4\u03b1, \u03c0\u03bf\u03c5 \u03b5\u03af\u03bd\u03b1\u03b9 \u03b1\u03bd\u03b1\u03b3\u03ba\u03b1\u03af\u03b1 \u03b3\u03b9\u03b1 \u03c4\u03b7\u03bd \u03b1\u03c0\u03cc\u03b4\u03b5\u03b9\u03be\u03b7 \u03b1\u03c0\u03bf\u03c4\u03b5\u03bb\u03b5\u03c3\u03bc\u03ac\u03c4\u03c9\u03bd \u03c0\u03bf\u03c5 \u03b1\u03c0\u03b1\u03bd\u03c4\u03bf\u03cd\u03bd \u03c3\u03c4\u03b1 \u03b5\u03c1\u03c9\u03c4\u03ae\u03bc\u03b1\u03c4\u03b1 \u03c0\u03bf\u03c5 \u03c4\u03ad\u03b8\u03b7\u03ba\u03b1\u03bd. \u0397 \u039c\u03b7\u03c7\u03b1\u03bd\u03ae Turing, \u03ad\u03bd\u03b1 \u03b8\u03b5\u03c9\u03c1\u03b7\u03c4\u03b9\u03ba\u03cc \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf \u03bc\u03b7\u03c7\u03b1\u03bd\u03ae\u03c2, \u03c0\u03bf\u03c5 \u03c0\u03c1\u03cc\u03c4\u03b5\u03b9\u03bd\u03b5 \u03bf Alan Turing \u03c4\u03bf 1936 \u03ad\u03c7\u03b5\u03b9 \u03b1\u03c0\u03bf\u03b4\u03b5\u03b9\u03c7\u03b8\u03b5\u03af \u03c9\u03c2 \u03c4\u03bf \u03bc\u03ad\u03c7\u03c1\u03b9 \u03c3\u03ae\u03bc\u03b5\u03c1\u03b1 \u03c0\u03b9\u03bf \u03b5\u03ba\u03c6\u03c1\u03b1\u03c3\u03c4\u03b9\u03ba\u03cc \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf, \u03ba\u03b1\u03b8\u03ce\u03c2 \u03b1\u03c5\u03c4\u03cc \u03bc\u03c0\u03bf\u03c1\u03b5\u03af \u03bd\u03b1 \u03b1\u03bd\u03b1\u03c0\u03b1\u03c1\u03b1\u03c3\u03c4\u03ae\u03c3\u03b5\u03b9 \u03c4\u03bf\u03bd \u03bf\u03c0\u03bf\u03b9\u03bf\u03b4\u03ae\u03c0\u03bf\u03c4\u03b5 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03cc \u03bc\u03b9\u03b1\u03c2 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03b9\u03ba\u03ae\u03c2 \u03bc\u03b7\u03c7\u03b1\u03bd\u03ae\u03c2. \u039f\u03b9 \u03bc\u03b7\u03c7\u03b1\u03bd\u03ad\u03c2 \u03c0\u03b5\u03c0\u03b5\u03c1\u03b1\u03c3\u03bc\u03ad\u03bd\u03c9\u03bd \u03ba\u03b1\u03c4\u03b1\u03c3\u03c4\u03ac\u03c3\u03b5\u03c9\u03bd \u03c3\u03c5\u03bd\u03b4\u03c5\u03ac\u03b6\u03bf\u03c5\u03bd \u03c4\u03b7\u03bd \u03b1\u03c0\u03bb\u03cc\u03c4\u03b7\u03c4\u03b1 \u03c4\u03b7\u03c2 \u03b1\u03bd\u03b1\u03c0\u03b1\u03c1\u03ac\u03c3\u03c4\u03b1\u03c3\u03b7\u03c2 \u03c4\u03bf\u03c5 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u03cd \u03bc\u03b5 \u03c3\u03b7\u03bc\u03b1\u03bd\u03c4\u03b9\u03ba\u03ad\u03c2 \u03b4\u03c5\u03bd\u03b1\u03c4\u03cc\u03c4\u03b7\u03c4\u03b5\u03c2 \u03ad\u03ba\u03c6\u03c1\u03b1\u03c3\u03b7\u03c2 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03ce\u03bd \u03ba\u03b1\u03b9 \u03b3\u03b9 \u03b1\u03c5\u03c4\u03cc \u03ad\u03c7\u03bf\u03c5\u03bd \u03c0\u03bf\u03bb\u03bb\u03ad\u03c2 \u03b5\u03c6\u03b1\u03c1\u03bc\u03bf\u03b3\u03ad\u03c2 \u03c3\u03c4\u03b7 \u03c3\u03c7\u03b5\u03b4\u03af\u03b1\u03c3\u03b7 \u03ba\u03b1\u03b9 \u03b1\u03bd\u03ac\u03bb\u03c5\u03c3\u03b7 \u03c4\u03b7\u03c2 \u03c3\u03c5\u03bc\u03c0\u03b5\u03c1\u03b9\u03c6\u03bf\u03c1\u03ac\u03c2 \u03c3\u03c5\u03c3\u03c4\u03b7\u03bc\u03ac\u03c4\u03c9\u03bd \u03cc\u03c0\u03c9\u03c2 \u03bf\u03b9 \u03b1\u03c5\u03c4\u03cc\u03bc\u03b1\u03c4\u03b5\u03c2 \u03bc\u03b7\u03c7\u03b1\u03bd\u03ad\u03c2 \u03c0\u03ce\u03bb\u03b7\u03c3\u03b7\u03c2, \u03c4\u03b1 \u03b7\u03bb\u03b5\u03ba\u03c4\u03c1\u03bf\u03bd\u03b9\u03ba\u03ac \u03c0\u03b1\u03b9\u03c7\u03bd\u03af\u03b4\u03b9\u03b1, \u03bf\u03b9 \u03bc\u03bf\u03bd\u03ac\u03b4\u03b5\u03c2 \u03b5\u03bb\u03ad\u03b3\u03c7\u03bf\u03c5 \u03c4\u03c9\u03bd CPUs \u03ba\u03b1\u03b9 \u03ac\u03bb\u03bb\u03b5\u03c2 \u03b5\u03c6\u03b1\u03c1\u03bc\u03bf\u03b3\u03ad\u03c2 \u03cc\u03c0\u03c9\u03c2 \u03b7 \u03b1\u03bd\u03ac\u03bb\u03c5\u03c3\u03b7 \u03ba\u03b5\u03b9\u03bc\u03ad\u03bd\u03bf\u03c5, \u03b7 \u03b1\u03bd\u03ac\u03bb\u03c5\u03c3\u03b7 \u03c0\u03c1\u03c9\u03c4\u03bf\u03ba\u03cc\u03bb\u03bb\u03c9\u03bd \u03b4\u03b9\u03ba\u03c4\u03cd\u03c9\u03bd \u03ba\u03b1\u03b9 \u03b7 \u03b5\u03c0\u03b5\u03be\u03b5\u03c1\u03b3\u03b1\u03c3\u03af\u03b1 \u03c6\u03c5\u03c3\u03b9\u03ba\u03ae\u03c2 \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1\u03c2. \u0394\u03b9\u03ac\u03c6\u03bf\u03c1\u03b5\u03c2 \u03b5\u03c0\u03b5\u03ba\u03c4\u03ac\u03c3\u03b5\u03b9\u03c2 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03c9\u03bd \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u03cd \u03cc\u03c0\u03c9\u03c2 \u03b1\u03c5\u03c4\u03ac \u03c0\u03bf\u03c5 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "IF0oXd8AAAAJ:-fu4zM_6qcIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The MedWeT Web InforMaTIon SySTeM USer ManUal",
            "Publication year": 0,
            "Publication url": "https://medwet.org/codde/6_WebInfo/WebInfo-Manual.pdf",
            "Abstract": ". BackgroundThe MedWet Web Information System (MedWet/WIS) is the latest version of the MedWet Database. It is the result of a number of successive and gradually improving versions that have been developed since 1996, when the first MedWet Database was created. The MedWet/WIS was developed in order to adopt new database and web technologies, integrate GIS tools and web service facilities and address new demands in data access and reporting requirements.From its inception, the MedWet Database was created to enter, store and analyse the data recorded by applying the MedWet inventory method. The MedWet Database is a mirror of the proposed MedWet Data Forms, which means that all data categories included in the Data Forms have corresponding fields in the Database.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:qxL8FJ1GzNcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Regression-based statistical bounds on software execution time",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-66176-6_4",
            "Abstract": "Our work aims at facilitating the schedulability analysis of non-critical systems, in particular those that have soft real-time constraints, where WCETs can be replaced by less stringent probabilistic bounds, which we call Maximal Execution Times (METs). In our approach, we can obtain adequate probabilistic execution time models by separating the non-random input data dependency from a modeling error that is purely random. To achieve this, we propose to take advantage of the rich set of available statistical model-fitting techniques, in particular linear regression. Although certainly the proposed technique cannot directly achieve extreme probability levels that are usually expected for WCETs, it is an attractive alternative for MET analysis, since it can arguably guarantee safe probabilistic bounds. We demonstrate our method on a JPEG decoder running on an industrial SPARC V8 processor.",
            "Abstract entirety": 1,
            "Author pub id": "IF0oXd8AAAAJ:1Aeql8wG3wEC",
            "Publisher": "Springer, Cham"
        }
    ]
}]