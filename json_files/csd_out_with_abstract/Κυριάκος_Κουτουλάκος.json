[{
    "name": "\u039a\u03c5\u03c1\u03b9\u03ac\u03ba\u03bf\u03c2 \u039a\u03bf\u03c5\u03c4\u03bf\u03c5\u03bb\u03ac\u03ba\u03bf\u03c2",
    "romanize name": "Kyriakos Koutoulakos",
    "School-Department": "Computer Science",
    "University": "University of Toronto",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 7124,
    "Scholar name": "Kyros Kutulakos",
    "Scholar id": "bQJWJPYAAAAJ",
    "Affiliation": "Professor of Computer Science, University of Toronto",
    "Citedby": 7857,
    "Interests": [
        "Computational Imaging",
        "Computational Photography",
        "Sensing",
        "Computer Vision",
        "Computer Graphics"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=bQJWJPYAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Depth from defocus in the wild",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2017/html/Tang_Depth_From_Defocus_CVPR_2017_paper.html",
            "Abstract": "We consider the problem of two-frame depth from defocus in conditions unsuitable for existing methods yet typical of everyday photography: a handheld cellphone camera, a small aperture, a non-stationary scene and sparse surface texture. Our approach combines a global analysis of image content---3D surfaces, deformations, figure-ground relations, textures---with local estimation of joint depth-flow likelihoods in tiny patches. To enable local estimation we (1) derive novel defocus-equalization filters that induce brightness constancy across frames and (2) impose a tight upper bound on defocus blur---just three pixels in radius---through an appropriate choice of the second frame. For global analysis we use a novel piecewise-spline scene representation that can propagate depth and flow across large irregularly-shaped regions. Our experiments show that this combination preserves sharp boundaries and yields good depth and flow maps in the face of significant noise, uncertainty, non-rigidity, and data sparsity.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:fc7zyzPI2QAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dual-tap computational photography image sensor with per-pixel pipelined digital memory for intra-frame coded multi-exposure",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8844261/",
            "Abstract": "A coded-exposure-pixel image sensor for computational imaging applications is presented. Each frame exposure time is divided into N subframes. Within each subframe, each pixel sorts photo-generated charge into two charge taps depending on that pixel's 1-bit binary code. N global updates of arbitrary pixel-wise codes are implemented in each frame to enable N short global pixel-specific subexposures within one frame. To make these subexposures global, two latches per pixel are utilized in a pipelined fashion. The code for the next subframe is loaded into latch 1 in a row parallel fashion, while the code for the current subframe is being applied by latch 2 globally for photo-generated charge sorting during the current subexposure. A 280 H  \u00d7 176 V  image sensor prototype with 11.2-\u03bcm pixel pitch has been fabricated in a 0.11-\u03bcm CMOS image sensor (CIS) technology. The image sensor has been demonstrated \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:rrpmhsargb8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Method and system for optimizing depth imaging",
            "Publication year": 2021,
            "Publication url": "https://patents.google.com/patent/US20210241475A1/en",
            "Abstract": "There is provided a system and method for optimizing depth imaging. The method including: illuminating one or more scenes with illumination patterns; capturing one or more images of each of the scenes; reconstructing the scenes; estimating the reconstruction error and a gradient of the reconstruction error; iteratively performing until the reconstruction error reaches a predetermined error condition: determining a current set of control vectors and current set of reconstruction parameters; illuminating the one or more scenes with the illumination patterns governed by the current set of control vectors; capturing one or more images of each of the scenes while the scene is being illuminated with at least one of the illumination patterns; reconstructing the scenes from the one or more captured images using the current reconstruction parameters; and estimating an updated reconstruction error and gradient; and outputting at \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:f-E_jMG6T4AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Frequency analysis of transient light transport with applications in bare sensor imaging",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-33718-5_39",
            "Abstract": "Light transport has been analyzed extensively, in both the primal domain and the frequency domain; the latter provides intuition of effects introduced by free space propagation and by optical elements, and allows for optimal designs of computational cameras for tailored, efficient information capture. Here, we relax the common assumption that the speed of light is infinite and analyze free space propagation in the frequency domain considering spatial, temporal, and angular light variation. Using this analysis, we derive analytic expressions for cross-dimensional information transfer and show how this can be exploited for designing a new, time-resolved bare sensor imaging system.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:LPZeul_q3PIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Semidefinite programming heuristics for surface reconstruction ambiguities",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-88682-2_11",
            "Abstract": "We consider the problem of reconstructing a smooth surface under constraints that have discrete ambiguities. These problems arise in areas such as shape from texture, shape from shading, photometric stereo and shape from defocus. While the problem is computationally hard, heuristics based on semidefinite programming may reveal the shape of the surface.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:u_35RYKgDlwC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Augmenting Reality Using Affine Object",
            "Publication year": 2001,
            "Publication url": "https://scholar.google.com/scholar?cluster=6003404754329151471&hl=en&oi=scholarr",
            "Abstract": "An augmented reality system is a system that creates a view of a real scene that visually incorporates into the scene computer-generated images of three-dimensional (3D) virtual objects. As the user of such a system moves about the real scene the virtual objects appear as if they actually exist in the scene. One motivation for augmenting reality in this way is to enhance the performance of real-world tasks. The performance requirements for an augmented reality system are:(1) merge images of 3D virtual objects with images of the real environment,(2) generate a consistent view of those objects from all views of the real scene, and (3) perform these operations in real time to be interactive with the user. Augmented reality can be compared to the more commonly known virtual reality. Virtual reality systems immerse a user in an environment that is completely computer generated. Augmented reality systems, on the other hand, strive to maintain the user's immersion in the real environment. The rationale behind this is twofold. First, real environments contain a wealth of information, much of which is impossible to model and simulate by computer. Secondly, if the end goal is",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:X1xEhyGaivYC",
            "Publisher": "Lawrence Erlbaum Associates"
        },
        {
            "Title": "Turbopixels: Fast superpixels using geometric flows",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4912213/",
            "Abstract": "We describe a geometric-flow-based algorithm for computing a dense oversegmentation of an image, often referred to as superpixels. It produces segments that, on one hand, respect local image boundaries, while, on the other hand, limiting undersegmentation through a compactness constraint. It is very fast, with complexity that is approximately linear in image size, and can be applied to megapixel sized images with high superpixel densities in a matter of minutes. We show qualitative demonstrations of high-quality results on several complex images. The Berkeley database is used to quantitatively compare its performance to a number of oversegmentation algorithms, showing that it yields less undersegmentation than algorithms that lack a compactness constraint while offering a significant speedup over N-cuts, which does enforce compactness.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:vV6vV6tmYwMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Light-Efficient Photography",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5740919/",
            "Abstract": "In this paper, we consider the problem of imaging a scene with a given depth of field at a given exposure level in the shortest amount of time possible. We show that by 1) collecting a sequence of photos and 2) controlling the aperture, focus, and exposure time of each photo individually, we can span the given depth of field in less total time than it takes to expose a single narrower-aperture photo. Using this as a starting point, we obtain two key results. First, for lenses with continuously variable apertures, we derive a closed-form solution for the globally optimal capture sequence, i.e., that collects light from the specified depth of field in the most efficient way possible. Second, for lenses with discrete apertures, we derive an integer programming problem whose solution is the optimal sequence. Our results are applicable to off-the-shelf cameras and typical photography conditions, and advocate the use of dense, wide \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:P5F9QuxV20EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Focal Stack Photography: High-Performance Photography with a Conventional Camera.",
            "Publication year": 2009,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.362.5992&rep=rep1&type=pdf",
            "Abstract": "We look at how a seemingly small change in the photographic process\u2014capturing a focal stack at the press of a button, instead of a single photo\u2014can boost significantly the optical performance of a conventional camera. By generalizing the familiar photographic concepts of \u201cdepth of field\u201d and \u201cexposure time\u201d to the case of focal stacks, we show that focal stack photography has two performance advantages:(1) it allows us to capture a given depth of field much faster than one-shot photography, and (2) it leads to higher signal-to-noise ratios when capturing wide depths of field with a restricted exposure time. We consider these advantages in detail and discuss their implications for photography.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:tkaPQYYpVKoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Energy optimized imaging system with synchronized dynamic control of directable beam light source and reconfigurably masked photo-sensor",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US20200018592A1/en",
            "Abstract": "An energy optimized imaging system that includes a light source that has the ability to illuminate specific pixels in a scene, and a sensor that has the ability to capture light with specific pixels of its sensor matrix, temporally synchronized such that the sensor captures light only when the light source is illuminating pixels in the scene.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:ddB7do2jUx8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "3d shape and indirect appearance by structured light transport",
            "Publication year": 2014,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2014/html/OToole_3D_Shape_and_2014_CVPR_paper.html",
            "Abstract": "We consider the problem of deliberately manipulating the direct and indirect light flowing through a time-varying, fully-general scene in order to simplify its visual analysis. Our approach rests on a crucial link between stereo geometry and light transport: while direct light always obeys the epipolar geometry of a projector-camera pair, indirect light overwhelmingly does not. We show that it is possible to turn this observation into an imaging method that analyzes light transport in real time in the optical domain, prior to acquisition. This yields three key abilities that we demonstrate in an experimental camera prototype:(1) producing a live indirect-only video stream for any scene, regardless of geometric or photometric complexity;(2) capturing images that make existing structured-light shape recovery algorithms robust to indirect transport; and (3) turning them into one-shot methods for dynamic 3D shape capture.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:h-xndbdg2koC",
            "Publisher": "Unknown"
        },
        {
            "Title": "CMOS image sensor architecture for primal-dual coding",
            "Publication year": 2017,
            "Publication url": "https://www.imagesensors.org/Past%20Workshops/2017%20Workshop/2017%20Papers/R48.pdf",
            "Abstract": "A CMOS image sensor architecture for primal-dual coding (PDC), the developed image sensor and the sensing side of the system, as well as preliminary sensor test results are presented in this paper. The architecture proposed in this work uses pixels with the embedded 2-bit latches which are responsible for the pre-loading and storing of the exposure codes. The subsequent exposure code (mask) can therefore be loaded while the current mask is being used for exposure, resulting in a pipelined coding operation which does not interfere with the pixel exposure time. The mask loading is done serially via a vertical metal line (one line per-column), making both the imager architecture and the pixel array scalable towards high pixel resolutions. The sensor is designed using a 0.35\u00b5m image sensor optimized CMOS process resulting in the total pixel pitch of 25\u00b5m. The pixel includes a photo-gate based photodetector, two 1-bit latches, required logic gates, two charge collection buckets (floating diffusions) and corresponding symmetric readout with two source-followers (one for each bucket), resulting in a pixel fill-factor of 20.5%. Every pixel column features a programmable gain amplifier whose outputs are time-multiplexed over 3 analog output pads. Analog-to-digital conversion is performed off-chip by 3\u00d7 16-bit ADCs. The 60\u00d7 80 pixel imager consumes 7mW of power while operating at 25 fps. The sensor measurement results show that the loading of the complete PDC mask for the whole array can be performed in 30\u00b5s, resulting in a large number of masks that can be applied during a single exposure time, therefore creating a very promising \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:6LV2YwJzdtgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Shape from planar curves: A linear escape from flatland",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4270045/",
            "Abstract": "We revisit the problem of recovering 3D shape from the projection of planar curves on a surface. This problem is strongly motivated by perception studies. Applications include single-view modeling and fully uncalibrated structured light. When the curves intersect, the problem leads to a linear system for which a direct least-squares method is sensitive to noise. We derive a more stable solution and show examples where the same method produces plausible surfaces from the projection of parallel (non-intersecting) planar cross sections.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:yD5IFk8b50cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Computational imaging of the electric grid",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US20200177789A1/en",
            "Abstract": "Systems and methods for imaging scenes illuminated by light sources that are powered by alternating current. Data concerning these light sources are extracted from the imagery. Systems comprising a rolling shutter imaging sensor and configured to de-flicker images with spatial flicker are also provided.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:TuM7UPshZo8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Self-Supervised Robot Learning",
            "Publication year": 2019,
            "Publication url": "https://www.ri.cmu.edu/wp-content/uploads/2019/06/dgandhi_thesis.pdf",
            "Abstract": "Supervised learning has been used in robotics to solve various tasks like navigation, fine manipulation, etc. While it has shown a promising result, in most cases the supervision comes from the human agent. However, relying on human is a huge bottleneck to scale up these approaches. In this thesis, we try to take the human out of the loop and try to solve the task in a self-supervised manner. More specifically, we have applied self-supervised learning to 2 tasks, for drone navigation and exploration in reinforcement learning. In drone navigation, drone first collides with lots of objects and based on it, it learns a policy to avoid them. In reinforcement learning, we try to learn exploration policy in a self-supervised manner for both stochastic and deterministic environment.How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this work, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in conjunction with positive data sampled from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:EmjvLWWcsQIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sequence-to-sequence alignment",
            "Publication year": 2004,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.307.8351",
            "Abstract": "In this paper, we consider the problem of estimating the spatiotemporal alignment between N unsynchronized video sequences of the same dynamic 3D scene, captured from distinct viewpoints. Unlike most existing methods, which work for N 2 and rely on a computationally intensive search in the space of temporal alignments, we present a novel approach that reduces the problem for general N to the robust estimation of a single line in IR N. This line captures all temporal relations between the sequences and can be computed without any prior knowledge of these relations. Considering that the spatial alignment is captured by the parameters of fundamental matrices, an iterative algorithm is used to refine simultaneously the parameters representing the temporal and spatial relations between the sequences. Experimental results with real-world and synthetic sequences show that our method can accurately align the videos even when they have large misalignments (eg, hundreds of frames), when the problem is seemingly ambiguous (eg, scenes with roughly periodic motion), and when accurate manual alignment is difficult (eg, due to slow-moving objects). Index Terms\u2014Video synchronization, object tracking, epipolar geometry, spatiotemporal alignment, image and video registration. \u00c7 1",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:Mojj43d5GZwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A probabilistic theory of occupancy and emptiness",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-47977-5_8",
            "Abstract": "This paper studies the inference of 3D shape from a set of n noisy photos. We derive a probabilistic framework to specify what one can infer about 3D shape for arbitrarily-shaped, Lambertian scenes and arbitrary viewpoint configurations. Based on formal definitions of visibility, occupancy, emptiness, and photo-consistency, the theoretical development yields a formulation of the Photo Hull Distribution, the tightest probabilistic bound on the scene\u2019s true shape that can be inferred from the photos. We show how to (1) express this distribution in terms of image measurements, (2) represent it compactly by assigning an occupancy probability to each point in space, and (3) design a stochastic reconstruction algorithm that draws fair samples (i.e., 3D photo hulls) from it. We also present experimental results for complex 3D scenes.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:lSLTfruPkqcC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Visualizing light transport phenomena with a primal-dual coding video camera",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2614066.2614090",
            "Abstract": "We present a primal-dual coding video camera, an optical device that captures video in which the flow of light through a scene has been manipulated. This camera has the ability to visualize indirect transport effects (caustics, inter-reflections, volumetric scattering, etc.), measure direct-only light paths (surface reflections), capture transport that occurs within specific regions of 3D space, and perform structured light imaging in the presence of complex indirect effects. The operating principle behind this technology is to simultaneously code the light that goes into a scene with the light that comes out. Specifically, over the course of a single camera exposure period, we project a sequence of light patterns onto the scene in lockstep with a second sequence of mask patterns that modulates the light incident on the camera sensor. This optical procedure creates RAW, unprocessed photos where a scene's light transport \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:MSzX15-gZgkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Rolling shutter imaging on the electric grid",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8368472/",
            "Abstract": "Flicker of AC-powered lights is useful for probing the electric grid and unmixing reflected contributions of different sources. Flicker has been sensed in great detail with a specially-designed camera tethered to an AC outlet. We argue that even an untethered smartphone can achieve the same task. We exploit the inter-row exposure delay of the ubiquitous rolling-shutter sensor. When pixel exposure time is kept short, this delay creates a spatiotemporal wave pattern that encodes (1) the precise capture time relative to the AC, (2) the response function of individual bulbs, and (3) the AC phase that powers them. To sense point sources, we induce the spatiotemporal wave pattern by placing a star filter or a paper diffuser in front of the camera's lens. We demonstrate several new capabilities, including: high-rate acquisition of bulb response functions from one smartphone photo; recognition of bulb type and phase from one \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:w_ORaKkuc5QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "End-to-end video compressive sensing using anderson-accelerated unrolled networks",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9105237/",
            "Abstract": "Compressive imaging systems with spatial-temporal encoding can be used to capture and reconstruct fast-moving objects. The imaging quality highly depends on the choice of encoding masks and reconstruction methods. In this paper, we present a new network architecture to jointly design the encoding masks and the reconstruction method for compressive high-frame-rate imaging. Unlike previous works, the proposed method takes full advantage of denoising prior to provide a promising frame reconstruction. The network is also flexible enough to optimize full-resolution masks and efficient at reconstructing frames. To this end, we develop a new dense network architecture that embeds Anderson acceleration, known from numerical optimization, directly into the neural network architecture. Our experiments show the optimized masks and the dense accelerated network respectively achieve 1.5 dB and 1 dB \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:kxd3qP2_5uAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Closed loop predictive control of adaptive optics systems with convolutional neural networks",
            "Publication year": 2021,
            "Publication url": "https://academic.oup.com/mnras/article-abstract/503/2/2944/6159481",
            "Abstract": "Predictive wavefront control is an important and rapidly developing field of adaptive optics (AO). Through the prediction of future wavefront effects, the inherent AO system servo-lag caused by the measurement, computation, and application of the wavefront correction can be significantly mitigated. This lag can impact the final delivered science image, including reduced strehl and contrast, and inhibits our ability to reliably use faint guide stars. We summarize here a novel method for training deep neural networks for predictive control based on an adversarial prior. Unlike previous methods in the literature, which have shown results based on previously generated data or for open-loop systems, we demonstrate our network\u2019s performance simulated in closed loop. Our models are able to both reduce effects induced by servo-lag and push the faint end of reliable control with natural guide stars, improving K-band \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:Agc8PWtS8JkC",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "Computational imaging on the electric grid",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2017/html/Sheinin_Computational_Imaging_on_CVPR_2017_paper.html",
            "Abstract": "Night beats with alternating current (AC) illumination. By passively sensing this beat, we reveal new scene information which includes: the type of bulbs in the scene, the phases of the electric grid up to city scale, and the light transport matrix. This information yields unmixing of reflections and semi-reflections, nocturnal high dynamic range, and scene rendering with bulbs not observed during acquisition. The latter is facilitated by a database of bulb response functions for a range of sources, which we collected and provide. To do all this, we built a novel coded-exposure high-dynamic-range imaging technique, specifically designed to operate on the grid's AC lighting.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:xEMdJR0kL_sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Non-rigid structure from locally-rigid motion",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5540002/",
            "Abstract": "We introduce locally-rigid motion, a general framework for solving the M-point, N-view structure-from-motion problem for unknown bodies deforming under orthography. The key idea is to first solve many local 3-point, N-view rigid problems independently, providing a \u201csoup\u201d of specific, plausibly rigid, 3D triangles. The main advantage here is that the extraction of 3D triangles requires only very weak assumptions: (1) deformations can be locally approximated by near-rigid motion of three points (i.e., stretching not dominant) and (2) local motions involve some generic rotation in depth. Triangles from this soup are then grouped into bodies, and their depth flips and instantaneous relative depths are determined. Results on several sequences, both our own and from related work, suggest these conditions apply in diverse settings - including very challenging ones (e.g., multiple deforming bodies). Our starting point is a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:zA6iFVUQeVQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Defocus deblurring and superresolution for time-of-flight depth cameras",
            "Publication year": 2015,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Xiao_Defocus_Deblurring_and_2015_CVPR_paper.html",
            "Abstract": "Continuous-wave time-of-flight (ToF) cameras show great promise as low-cost depth image sensors in mobile applications. However, they also suffer from several challenges, including limited illumination intensity, which mandates the use of large numerical aperture lenses, and thus results in a shallow depth of field, making it difficult to capture scenes with large variations in depth. Another shortcoming is the limited spatial resolution of currently available ToF sensors. In this paper we analyze the image formation model for blurred ToF images. By directly working with raw sensor measurements but regularizing the recovered depth and amplitude images, we are able to simultaneously deblur and super-resolve the output of ToF cameras. Our method outperforms existing methods on both synthetic and real datasets. In the future our algorithm should extend easily to cameras that do not follow the cosine model of continuous-wave sensors, as well as to multi-frequency or multi-phase imaging employed in more recent ToF cameras.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:rUnQDpM0TEQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Photo-consistent reconstruction of semitransparent scenes by density-sheet decomposition",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4135680/",
            "Abstract": "This paper considers the problem of reconstructing visually realistic 3D models of dynamic semitransparent scenes, such as fire, from a very small set of simultaneous views (even two). We show that this problem is equivalent to a severely underconstrained computerized tomography problem, for which traditional methods break down. Our approach is based on the observation that every pair of photographs of a semitransparent scene defines a unique density field, called a density sheet, that 1) concentrates all its density on one connected, semitransparent surface, 2) reproduces the two photos exactly, and 3) is the most spatially compact density field that does so. From this observation, we reduce reconstruction to the convex combination of sheet-like density fields, each of which is derived from the density sheet of two input views. We have applied this method specifically to the problem of reconstructing 3D models \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:SeFeTyx0c_EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Wavefront reconstruction and prediction with convolutional neural networks",
            "Publication year": 2018,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10703/107031F/Wavefront-reconstruction-and-prediction-with-convolutional-neural-networks/10.1117/12.2312590.short",
            "Abstract": "While deep learning has led to breakthroughs in many areas of computer science, its power has yet to be fully exploited in the area of adaptive optics (AO) and astronomy as a whole. In this paper we describe the first steps taken to apply deep, convolutional neural networks to the problem of wavefront reconstruction and prediction and demonstrate their feasibility of use in simulation. Our preliminary results show we are able to reconstruct wavefronts comparably well to current state of the art methods. We further demonstrate the ability to predict future wavefronts up to five simulation steps with under 1nm RMS wavefront error.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:fixghrsIJ_wC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "Extending Image Sensor Dynamic Range by Scene-aware Pixelwise-adaptive Coded Exposure",
            "Publication year": 2019,
            "Publication url": "https://www.imagesensors.org/Past%20Workshops/2019%20Workshop/2019%20Papers/P17.pdf",
            "Abstract": "We present a method for extending the dynamic range of an image sensor by frame-to-frame adaptive adjustment of exposure of every pixel, based on changes in the brightness of the scene for that pixel. The method employs an image sensor with a coded-exposure-pixel (CEP)[1], where the exposure time of each frame is divided into N subframes, each of unary or binary-weighted duration. In each subframe the pixel is re-programmed with a 1-bit binary code to be either on (code 1) or off (code 0), depending on the light flux captured by that pixel in the previous frame. The photo-generated charge is integrated across all subframes. The N 1-bit codes per pixel are computed by an off-sensor processor, and are loaded to a digital latch within that pixel, one per subframe. At the end of a frame, a high-dynamic-range (HDR) image is reconstructed by using that frame\u2019s pixel codes to normalize the pixel digital outputs to a uniform exposure. The dynamic range is experimentally demonstrated to increase by up to 20log10 (2N\u2212 1) dB at the full frame rate and at the native resolution of the image sensor.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:Q17yWvk9gpwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A neural rendering framework for free-viewpoint relighting",
            "Publication year": 2020,
            "Publication url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_A_Neural_Rendering_Framework_for_Free-Viewpoint_Relighting_CVPR_2020_paper.html",
            "Abstract": "We present a novel Relightable Neural Renderer (RNR) for simultaneous view synthesis and relighting using multi-view image inputs. Existing neural rendering (NR) does not explicitly model the physical rendering process and hence has limited capabilities on relighting. RNR instead models image formation in terms of environment lighting, object intrinsic attributes, and light transport function (LTF), each corresponding to a learnable component. In particular, the incorporation of a physically based rendering process not only enables relighting but also improves the quality of view synthesis. Comprehensive experiments on synthetic and real data show that RNR provides a practical and effective solution for conducting free-viewpoint relighting.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:lBZ7XAAYe3oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Temporal frequency probing for 5D transient analysis of global light transport",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2601097.2601103",
            "Abstract": "We analyze light propagation in an unknown scene using projectors and cameras that operate at transient timescales. In this new photography regime, the projector emits a spatio-temporal 3D signal and the camera receives a transformed version of it, determined by the set of all light transport paths through the scene and the time delays they induce. The underlying 3D-to-3D transformation encodes scene geometry and global transport in great detail, but individual transport components (e.g., direct reflections, inter-reflections, caustics, etc.) are coupled nontrivially in both space and time.To overcome this complexity, we observe that transient light transport is always separable in the temporal frequency domain. This makes it possible to analyze transient transport one temporal frequency at a time by trivially adapting techniques from conventional projector-to-camera transport. We use this idea in a prototype that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:K-tzbvM8PMoC",
            "Publisher": "ACM"
        },
        {
            "Title": "A theory of shape by space carving",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/article/10.1023/A:1008191222954",
            "Abstract": "In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the photo hull, that (1) can be computed directly from photographs of the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm, called Space Carving, for computing this shape and present experimental results on complex real-world scenes. The approach is designed to (1) capture photorealistic shapes that accurately model scene appearance from a wide range of viewpoints, and (2) account for the complex interactions between occlusion, parallax, shading, and their view-dependent effects on scene-appearance.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:isC4tDSrTZIC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Light-efficient photography",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-88693-8_4",
            "Abstract": "We consider the problem of imaging a scene with a given depth of field at a given exposure level in the shortest amount of time possible. We show that by (1) collecting a sequence of photos and (2) controlling the aperture, focus and exposure time of each photo individually, we can span the given depth of field in less total time than it takes to expose a single narrower-aperture photo. Using this as a starting point, we obtain two key results. First, for lenses with continuously-variable apertures, we derive a closed-form solution for the globally optimal capture sequence, i.e., that collects light from the specified depth of field in the most efficient way possible. Second, for lenses with discrete apertures, we derive an integer programming problem whose solution is the optimal sequence. Our results are applicable to off-the-shelf cameras and typical photography conditions, and advocate the use of dense, wide \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:MtS25d97-7AC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Method for epipolar time of flight imaging",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US20200092533A1/en",
            "Abstract": "Energy-efficient epipolar imaging is applied to the ToF domain to significantly expand the versatility of ToF sensors. The described system exhibits 15+ m range outdoors in bright sunlight; robustness to global transport effects such as specular and diffuse inter-reflections; interference-free 3D imaging in the presence of many ToF sensors, even when they are all operating at the same optical wavelength and modulation frequency; and blur-and distortion-free 3D video in the presence of severe camera shake. The described embodiments are broadly applicable in consumer and robotics domains.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:ocbgtyEEUOwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multi-view scene capture by surfel sampling: From video streams to non-rigid 3D motion, shape and reflectance",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/article/10.1023/A:1020145606604",
            "Abstract": "In this paper we study the problem of recovering the 3D shape, reflectance, and non-rigid motion properties of a dynamic 3D scene. Because these properties are completely unknown and because the scene's shape and motion may be non-smooth, our approach uses multiple views to build a piecewise-continuous geometric and radiometric representation of the scene's trace in space-time. A basic primitive of this representation is the dynamic surfel, which (1) encodes the instantaneous local shape, reflectance, and motion of a small and bounded region in the scene, and (2) enables accurate prediction of the region's dynamic appearance under known illumination conditions. We show that complete surfel-based reconstructions can be created by repeatedly applying an algorithm called Surfel Sampling that combines sampling and parameter estimation to fit a single surfel to a small, bounded region of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:BqipwSGYUEgC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Utilizing optical aberrations for extended-depth-of-field panoramas",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-37447-0_28",
            "Abstract": "Optical aberrations in off-the-shelf photographic lenses are commonly treated as unwanted artifacts that degrade image quality. In this paper we argue that such aberrations can be useful, as they often produce point-spread functions (PSFs) that have greater frequency-preserving abilities in the presence of defocus compared to an ideal thin lens. Specifically, aberrated and defocused PSFs often contain sharp, edge-like structures that vary with depth and image position, and become increasingly anisotropic away from the image center. In such cases, defocus blur varies spatially and preserves high spatial frequencies in some directions but not others. Here we take advantage of this fact to create extended-depth-of-field panoramas from overlapping photos taken with off-the-shelf lenses and a wide aperture. We achieve this by first measuring the lens PSF through a one-time calibration and then using multi \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:eflP2zaiRacC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Technical Perspective: The dawn of computational light transport",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/fullHtml/10.1145/2975163",
            "Abstract": "It is easy to forget, when casually observing our surroundings, that the speed of light is finite. Light travels so quickly that even though it may scatter, refract, or bounce many times on nearby surfaces before reaching our eyes, these events are spaced just trillionths of a second apart\u2014too fast for any conventional camera to resolve, and certainly too fast for our own visual system to perceive as anything but instantaneous.But are these individual light transport events really beyond the realm of direct visual observation? What would the world look like if we had a chance to observe it with a trillion-frame-per-second video camera? And what insights might one gain from such observations?",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:uGrg30pLAbkC",
            "Publisher": "ACM"
        },
        {
            "Title": "Transparent and specular object reconstruction",
            "Publication year": 2010,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2010.01753.x",
            "Abstract": " This state of the art report covers reconstruction methods for transparent and specular objects or phenomena. While the 3D acquisition of opaque surfaces with Lambertian reflectance is a well\u2010studied problem, transparent, refractive, specular and potentially dynamic scenes pose challenging problems for acquisition systems. This report reviews and categorizes the literature in this field.  Despite tremendous interest in object digitization, the acquisition of digital models of transparent or specular objects is far from being a solved problem. On the other hand, real\u2010world data is in high demand for applications such as object modelling, preservation of historic artefacts and as input to data\u2010driven modelling techniques. With this report we aim at providing a reference for and an introduction to the field of transparent and specular object reconstruction.  We describe acquisition approaches for different classes of objects \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:pyW8ca7W8N0C",
            "Publisher": "Blackwell Publishing Ltd"
        },
        {
            "Title": "Confocal stereo",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s11263-008-0164-2.pdf",
            "Abstract": "We present confocal stereo, a new method for computing 3D shape by controlling the focus and aperture of a lens. The method is specifically designed for reconstructing scenes with high geometric complexity or fine-scale texture. To achieve this, we introduce the confocal constancy property, which states that as the lens aperture varies, the pixel intensity of a visible in-focus scene point will vary in a scene-independent way, that can be predicted by prior radiometric lens calibration. The only requirement is that incoming radiance within the cone subtended by the largest aperture is nearly constant. First, we develop a detailed lens model that factors out the distortions in high resolution SLR cameras (12MP or more) with large-aperture lenses (e.g., f1.2). This allows us to assemble an A\u00d7F aperture-focus image (AFI) for each pixel, that collects the undistorted measurements over all A apertures and F focus \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:HoB7MX3m0LUC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Ambiguous 3D Photography.",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=9612816690591372636&hl=en&oi=scholarr",
            "Abstract": "Photographs are a rich source of information about the 3D layout of the world around us. Unfortunately, this information is largely ambiguous even for the simplest of scenes (ie, opaque Lambertian objects) and this ambiguity becomes much more severe when a scene is transparent or refractive. In this talk I will discuss two reconstruction problems where ambiguity analysis can be used to our advantage. The first problem involves inferring the 3D shape of an unknown, opaque scene from a set of noisy photos. In this case, a formal probabilistic analysis of visibility, occupancy, emptiness and photo-consistency gives us the tightest probabilistic bound on the true scene that can be inferred from the photos. I will show how this bound can be represented compactly by assigning an occupancy probability to each point in space and how it leads to a simple stochastic 3D reconstruction algorithm. The second problem \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:KxtntwgDAa4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A theory of Fermat paths for non-line-of-sight shape reconstruction",
            "Publication year": 2019,
            "Publication url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xin_A_Theory_of_Fermat_Paths_for_Non-Line-Of-Sight_Shape_Reconstruction_CVPR_2019_paper.html",
            "Abstract": "We present a novel theory of Fermat paths of light between a known visible scene and an unknown object not in the line of sight of a transient camera. These light paths either obey specular reflection or are reflected by the object's boundary, and hence encode the shape of the hidden object. We prove that Fermat paths correspond to discontinuities in the transient measurements. We then derive a novel constraint that relates the spatial derivatives of the path lengths at these discontinuities to the surface normal. Based on this theory, we present an algorithm, called Fermat Flow, to estimate the shape of the non-line-of-sight object. Our method allows, for the first time, accurate shape recovery of complex objects, ranging from diffuse to specular, that are hidden around the corner as well as hidden behind a diffuser. Finally, our approach is agnostic to the particular technology used for transient imaging. As such, we demonstrate mm-scale shape recovery from pico-second scale transients using a SPAD and ultrafast laser, as well as micron-scale reconstruction from femto-second scale transients using interferometry. We believe our work is a significant advance over the state-of-the-art in non-line-of-sight imaging.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:QZWLLlSfqgYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "What does an aberrated photo tell us about the lens and the scene?",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6528316/",
            "Abstract": "We investigate the feasibility of recovering lens properties, scene appearance and depth from a single photo containing optical aberrations and defocus blur. Starting from the ray intersection function of a rotationally-symmetric compound lens and the theory of Seidel aberrations, we obtain three basic results. First, we derive a model for the lens PSF that (1) accounts for defocus and primary Seidel aberrations and (2) describes how light rays are bent by the lens. Second, we show that the problem of inferring depth and aberration coefficients from the blur kernel of just one pixel has three degrees of freedom in general. As such it cannot be solved unambiguously. Third, we show that these degrees of freedom can be eliminated by inferring scaled aberration coefficients and depth from the blur kernel at multiple pixels in a single photo (at least three). These theoretical results suggest that single-photo aberration \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:1qzjygNMrQYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Computational imaging of the electric grid",
            "Publication year": 2021,
            "Publication url": "https://patents.google.com/patent/US20210274085A1/en",
            "Abstract": "A method comprising receiving data representing light intensity values corresponding to a flicker pattern of a reference light source powered by an alternating current (AC); operating a controllable illumination source, based, at least in part, on said data; capturing, using an imaging device, a sequence of images of a scene illuminated, at least in part, by said controllable illumination source; estimating an intensity value for at least one pixel in said array, correspondingly in each of said images in said sequence of images; and determining a temporal point in said flicker pattern of said reference light source, based, at least in part, on said estimating.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:Lpa4s8qvUTIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Light Transport Analysis for 3D Photography",
            "Publication year": 2007,
            "Publication url": "https://www.computer.org/csdl/proceedings-article/3dim/2007/29390337/12OmNqOffAc",
            "Abstract": "While 3D photography research has enjoyed tremendous success in recent years, many everyday objects and materials are still difficult or impossible to capture in 3D. An important stumbling block is that typical algorithms do not consider the effects of light transport, ie, the sequence of bounces, refractions and scattering events that may occur when light interacts with an object. This puts objects with transparent materials or highly-reflective surfaces (clear plastic, crystal, liquids, polished metal, etc.) outside the reach of current 3D scanning techniques. To overcome these limitations, we have been investigating algorithms that explicitly analyze the light transport process caused by such objects [1-3]. These algorithms rely on 2D photos taken from multiple views and reconstruct the individual 3D path (s) that light must have traced in order to reach each pixel. Despite the apparent intractability of this endeavor, our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:1sJd4Hv_s6UC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "The effect of pinned photodiode shape on time-of-flight demodulation contrast",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7879108/",
            "Abstract": "An empirical investigation on improving the pinned photodiode (PPD) demodulation contrast by tailoring the geometry of the device is presented. Results of this TCAD simulation-based study are used to develop a structure especially suited for time-of-flight applications. In order to obtain a fair comparison between various PPD shapes, a square structure is adopted as a benchmark and all subsequent PPD geometries use the same process parameters. Five different PPD shapes are compared: 1) nominal square-shaped PPD; 2) triangular PPD; 3) constant-field PPD; 4) L-shaped constant-field PPD; and 5) proposed PPD. Device physics simulations are undertaken and the speed of each structure is evaluated on the basis of its demodulation contrast. It is shown that triangular and constant-field PPDs can provide significant improvement compared with a conventional square-shaped PPD, however they still lack \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:YanO1q2l3soC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Time-Constrained Photography Supplementary material",
            "Publication year": 2009,
            "Publication url": "https://www.academia.edu/download/30609293/hasinoff-timeconstrained-2009-supp.pdf",
            "Abstract": "Lattice-focal camera [15]. We used an idealized analytic formula for the lattice-focal camera [15], ignoring discretization effects; in practice the DOF may not be evenly spanned by an integer square number of lens subsquares. The formula also assumes that the spectrum of a given lens instance matches well its expectation over a random selection of subsquare focal lengths.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:4pF9x-cDGsoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reconstructing the surface of inhomogeneous transparent scenes by scatter-trace photography",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4408882/",
            "Abstract": "We present a new method for reconstructing the exterior surface of a complex transparent scene with inhomogeneous interior (e.g., multiple interfaces, reflective or painted interiors, etc). Our approach involves capturing images of the scene from one or more viewpoints while moving a proximal light source to a 2D or 3D set of positions. This gives a 2D (or 3D) dataset per pixel, called the scatter trace. The key idea of our approach is that even though light transport within a transparent scene's interior can be exceedingly complex, the scatter trace of each pixel has a highly-constrained geometry that (1) reveals the contribution of direct surface reflection, and (2) leads to a simple \"scatter- trace stereo\" algorithm for computing the local geometry of the exterior surface (depth and surface normals). We present 3D reconstruction results for a variety of scenes that exhibit complex light transport phenomena.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:M05iB0D1s5AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Dual-Tap Pipelined-Code-Memory Coded-Exposure-Pixel CMOS Image Sensor for Multi-Exposure Single-Frame Computational Imaging",
            "Publication year": 2019,
            "Publication url": "https://cris.fbk.eu/handle/11582/317672",
            "Abstract": "Modern computational photography applications such as 3D sensing, gesture analysis, and robotic navigation drive the growing need for programmability, or coding, of the camera exposure at the individual-pixel level. Unlike conventional cameras, which record all light incident onto a pixel, the emerging class of coded-exposure-pixel (CEP) cameras can be programmed to selectively detect only some of that light [1] or, better, sort all of the light [2, 3], depending on the pixel code. In conjunction with a concurrently coded illumination, this enables a wide range of new coded multi-exposure single-readout-frame imaging capabilities at video rates. This work demonstrates such an image sensor where multiple pixel-wise-coded exposures, or subframes, are accumulated during one video frame.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:hXZnTIgIr50C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Optical computing for fast light transport analysis",
            "Publication year": 2010,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.307.3151&rep=rep1&type=pdf",
            "Abstract": "We present a general framework for analyzing the transport matrix of a real-world scene at full resolution, without capturing many photos. The key idea is to use projectors and cameras to directly acquire eigenvectors and the Krylov subspace of the unknown transport matrix. To do this, we implement Krylov subspace methods partially in optics, by treating the scene as a \u201cblack box subroutine\u201d that enables optical computation of arbitrary matrix-vector products. We describe two methods\u2014optical Arnoldi to acquire a lowrank approximation of the transport matrix for relighting; and optical GMRES to invert light transport. Our experiments suggest that good quality relighting and transport inversion are possible from a few dozen low-dynamic range photos, even for scenes with complex shadows, caustics, and other challenging lighting effects.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:dfsIfKJdRG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "The geometry of first-returning photons for non-line-of-sight imaging",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2017/html/Tsai_The_Geometry_of_CVPR_2017_paper.html",
            "Abstract": "Non-line-of-sight (NLOS) imaging utilizes the full 5D light transient measurements to reconstruct scenes beyond the camera's field of view. Mathematically, this requires solving an elliptical tomography problem that unmixes the shape and albedo from spatially-multiplexed measurements of the NLOS scene. In this paper, we propose a new approach for NLOS imaging by studying the properties of first-returning photons from three-bounce light paths. We show that the times of flight of first-returning photons are dependent only on the geometry of the NLOS scene and each observation is almost always generated from a single NLOS scene point. Exploiting these properties, we derive a space carving algorithm for NLOS scenes. In addition, by assuming local planarity, we derive an algorithm to localize NLOS scene points in 3D and estimate their surface normals. Our methods do not require either the full transient measurements or solving the hard elliptical tomography problem. We demonstrate the effectiveness of our methods through simulations as well as real data captured from a SPAD sensor.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:ve7iT2ZEuL4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "High resolution photography with an RGB-infrared camera",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7168367/",
            "Abstract": "A convenient solution to RGB-Infrared photography is to extend the basic RGB mosaic with a fourth filter type with high transmittance in the near-infrared band. Unfortunately, applying conventional demosaicing algorithms to RGB-IR sensors is not possible for two reasons. First, the RGB and near-infrared image are differently focused due to different refractive indices of each band. Second, manufacturing constraints introduce crosstalk between RGB and IR channels. In this paper we propose a novel image formation model for RGB-IR cameras that can be easily calibrated, and propose an efficient algorithm that jointly addresses three restoration problems - channel deblurring, channel separation and pixel demosaicing - using quadratic image regularizers. We also extend our algorithm to handle more general regularizers and pixel saturation. Experiments show that our method produces sharp, full-resolution images \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:dj1AAMDQi3QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Imaging system with synchronized dynamic control of directable beam light source and reconfigurably masked photo-sensor",
            "Publication year": 2019,
            "Publication url": "https://patents.google.com/patent/US10359277B2/en",
            "Abstract": "An energy optimized imaging system that includes a light source that has the ability to illuminate specific pixels in a scene, and a sensor that has the ability to capture light with specific pixels of its sensor matrix, temporally synchronized such that the sensor captures light only when the light source is illuminating pixels in the scene.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:lL5f5cZgq8MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Photometric stereo via discrete hypothesis-and-test search",
            "Publication year": 2020,
            "Publication url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Enomoto_Photometric_Stereo_via_Discrete_Hypothesis-and-Test_Search_CVPR_2020_paper.html",
            "Abstract": "In this paper, we consider the problem of estimating surface normals of a scene with spatially varying, general BRDFs observed by a static camera under varying, known, distant illumination. Unlike previous approaches that are mostly based on continuous local optimization, we cast the problem as a discrete hypothesis-and-test search problem over the discretized space of surface normals. While a naive search requires a significant amount of time, we show that the expensive computation block can be precomputed in a scene-independent manner, resulting in accelerated inference for new scenes. It allows us to perform a full search over the finely discretized space of surface normals to determine the globally optimal surface normal for each scene point. We show that our method can accurately estimate surface normals of scenes with spatially varying different reflectances in a reasonable amount of time.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:u1IMlZnpNFQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Plenoptic image editing",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/article/10.1023/A:1016046923611",
            "Abstract": "This paper presents a new class of interactive image editing operations designed to maintain consistency between multiple images of a physical 3D scene. The distinguishing feature of these operations is that edits to any one image propagate automatically to all other images as if the (unknown) 3D scene had itself been modified. The modified scene can then be viewed interactively from any other camera viewpoint and under different scene illuminations. The approach is useful first as a power-assist that enables a user to quickly modify many images by editing just a few, and second as a means for constructing and editing image-based scene representations by manipulating a set of photographs. The approach works by extending operations like image painting, scissoring, and morphing so that they alter a scene's plenoptic function in a physically-consistent way, thereby affecting scene appearance from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:hMod-77fHWUC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Transparent and reflective scene reconstruction",
            "Publication year": 2008,
            "Publication url": "https://www.ics.forth.gr/eg2008/docs/stars/ReflectiveSceneReconstruction.pdf",
            "Abstract": "We provide a state of the art report on reconstruction methods for transparent and reflective scenes. While the 3D acquisition of opaque surfaces with lambertian reflectance is a well-studied problem, transparent, refractive, reflective and potentially dynamic scenes pose challenging problems for acquisition systems. The proposed State of the Art report will review and categorize the important literature in this exciting field. Despite tremendous interest in object digitization, the acquisition of digital models of transparent or reflective objects is far from being a solved problem. On the other hand, real-world data is in high demand for applications like object modeling, preservation of historic artifacts and input to data driven modeling techniques, to name just a few.This STAR aims at compiling the first comprehensive review of existing approaches for transparent and reflective object acquisition. The authors' goal is to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:2SFquFhkCoYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Optimal structured light a la carte",
            "Publication year": 2018,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Mirdehghan_Optimal_Structured_Light_CVPR_2018_paper.html",
            "Abstract": "We consider the problem of automatically generating sequences of structured-light patterns for active stereo triangulation of a static scene. Unlike existing approaches that use predetermined patterns and reconstruction algorithms tied to them, we generate patterns on the fly in response to generic specifications: number of patterns, projector-camera arrangement, workspace constraints, spatial frequency content, etc. Our pattern sequences are specifically optimized to minimize the expected rate of correspondence errors under those specifications for an unknown scene, and are coupled to a sequence-independent algorithm for per-pixel disparity estimation. To achieve this, we derive an objective function that is easy to optimize and follows from first principles within a maximum-likelihood framework. By minimizing it, we demonstrate automatic discovery of pattern sequences, in under three minutes on a laptop, that can outperform state-of-the-art triangulation techniques.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:HevVnK7dagcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "5.5 dual-tap pipelined-code-memory coded-exposure-pixel cmos image sensor for multi-exposure single-frame computational imaging",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8662326/",
            "Abstract": "Modern computational photography applications such as 3D sensing, gesture analysis, and robotic navigation drive the growing need for programmability, or coding, of the camera exposure at the individual-pixel level. Unlike conventional cameras, which record all light incident onto a pixel, the emerging class of coded-exposure-pixel (CEP) cameras can be programmed to selectively detect only some of that light [1] or, better, sort all of the light [2, 3], depending on the pixel code. In conjunction with a concurrently coded illumination, this enables a wide range of new coded multi-exposure single-readout-frame imaging capabilities at video rates. This work demonstrates such an image sensor where multiple pixel-wise-coded exposures, or subframes, are accumulated during one video frame.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:S_Qw7xXuMuIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Evaluating the memory system behavior of smartphone workloads",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6893198/",
            "Abstract": "Modern smartphones comprise several processing and input/output units that communicate mostly through main memory. As a result, memory represents a critical performance bottleneck for smartphones. This work 1  introduces a set of emerging workloads for smartphones and characterizes the performance of several memory controller policies and address-mapping schemes for those workloads. The workloads include high-resolution video conferencing, computer vision algorithms such as upper-body detection and feature extraction, computational photography techniques such as high dynamic range imaging, and web browsing. This work also considers combinations of these workloads that represent possible use cases of future smartphones such as detecting and focusing on people or other objects in live video. While some of these workloads have been characterized before, this is the first work that studies \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:pZ2CosqRuhkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Augmenting Reality Using Affine Object Representations",
            "Publication year": 2001,
            "Publication url": "https://www.taylorfrancis.com/chapters/edit/10.1201/9780585383590-11/augmenting-reality-using-affine-object-representations-james-vallino-kiriakos-kutulakos",
            "Abstract": "An augmented reality system is a system that creates a view of a real scene that visually incorporates into the scene computer-generated images of three-dimensional (3D) virtual objects. As the user of such a system moves about the real scene the virtual objects appear as if they actually exist in the scene. One motivation for augmenting reality in this way is to enhance the performance of real-world tasks. The performance requirements for an augmented reality system are:(1) merge images of 3D virtual objects with images of the real environment,(2) generate a consistent view of those objects from all views of the real scene, and (3) perform these operations in real time to be interactive with the user. Augmented reality can be compared to the more commonly known virtual reality. Virtual reality systems immerse a user in an environment that is completely computer generated. Augmented reality systems, on the other \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:SP6oXDckpogC",
            "Publisher": "CRC"
        },
        {
            "Title": "Method and system for pixel-wise imaging",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US20200204748A1/en",
            "Abstract": "There is provided a method and system for pixel-wise imaging of a scene. The method including: receiving a pixel-wise pattern, the pixel-wise pattern including a masking value for each pixel in an array of pixels of an image sensor; producing an electronic signal at each pixel when such pixel is exposed to light received from the scene; and directing the electronic signal at each pixel to one or more collection nodes associated with such pixel based on the respective masking value, the one or more collection nodes each capable of integrating the received electronic signal.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:rwEhk56xNqMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dynamic Refraction Stereo",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5708153/",
            "Abstract": "In this paper we consider the problem of reconstructing the 3D position and surface normal of points on an unknown, arbitrarily-shaped refractive surface. We show that two viewpoints are sufficient to solve this problem in the general case, even if the refractive index is unknown. The key requirements are 1) knowledge of a function that maps each point on the two image planes to a known 3D point that refracts to it, and 2) light is refracted only once. We apply this result to the problem of reconstructing the time-varying surface of a liquid from patterns placed below it. To do this, we introduce a novel \u201cstereo matching\u201d criterion called refractive disparity, appropriate for refractive scenes, and develop an optimization-based algorithm for individually reconstructing the position and normal of each point projecting to a pixel in the input views. Results on reconstructing a variety of complex, deforming liquid surfaces suggest \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:nb7KW1ujOQ8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Homogeneous codes for energy-efficient illumination and imaging",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2766897",
            "Abstract": "Programmable coding of light between a source and a sensor has led to several important results in computational illumination, imaging and display. Little is known, however, about how to utilize energy most effectively, especially for applications in live imaging. In this paper, we derive a novel framework to maximize energy efficiency by \"homogeneous matrix factorization\" that respects the physical constraints of many coding mechanisms (DMDs/LCDs, lasers, etc.). We demonstrate energy-efficient imaging using two prototypes based on DMD and laser illumination. For our DMD-based prototype, we use fast local optimization to derive codes that yield brighter images with fewer artifacts in many transport probing tasks. Our second prototype uses a novel combination of a low-power laser projector and a rolling shutter camera. We use this prototype to demonstrate never-seen-before capabilities such as (1) capturing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:ZJ-noXUx9mkC",
            "Publisher": "ACM"
        },
        {
            "Title": "Time-constrained photography",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5459269/",
            "Abstract": "Capturing multiple photos at different focus settings is a powerful approach for reducing optical blur, but how many photos should we capture within a fixed time budget? We develop a framework to analyze optimal capture strategies balancing the tradeoff between defocus and sensor noise, incorporating uncertainty in resolving scene depth. We derive analytic formulas for restoration error and use Monte Carlo integration over depth to derive optimal capture strategies for different camera designs, under a wide range of photographic scenarios. We also derive a new upper bound on how well spatial frequencies can be preserved over the depth of field. Our results show that by capturing the optimal number of photos, a standard camera can achieve performance at the level of more complex computational cameras, in all but the most demanding of cases. We also show that computational cameras, although specifically \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:3s1wT3WcHBgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Learned feature embeddings for non-line-of-sight imaging and recognition",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3414685.3417825",
            "Abstract": "Objects obscured by occluders are considered lost in the images acquired by conventional camera systems, prohibiting both visualization and understanding of such hidden objects. Non-line-of-sight methods (NLOS) aim at recovering information about hidden scenes, which could help make medical imaging less invasive, improve the safety of autonomous vehicles, and potentially enable capturing unprecedented high-definition RGB-D data sets that include geometry beyond the directly visible parts. Recent NLOS methods have demonstrated scene recovery from time-resolved pulse-illuminated measurements encoding occluded objects as faint indirect reflections. Unfortunately, these systems are fundamentally limited by the quartic intensity fall-off for diffuse scenes. With laser illumination limited by eye-safety limits, recovery algorithms must tackle this challenge by incorporating scene priors. However, existing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:djcsc3XHdKAC",
            "Publisher": "ACM"
        },
        {
            "Title": "Approximate N-view stereo",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-45054-8_5",
            "Abstract": "This paper introduces a new multi-view reconstruction problem called approximate N-view stereo. The goal of this problem is to recover a one-parameter family of volumes that are increasingly tighter supersets of an unknown, arbitrarily-shaped 3D scene. By studying 3D shapes that reproduce the input photographs up to a special image transformation called a shuffle transformation, we prove that (1) these shapes can be organized hierarchically into nested supersets of the scene, and (2) they can be computed using a simple algorithm called Approximate Space Carving that is provably-correct for arbitrary discrete scenes (i.e., for unknown, arbitrarily-shaped Lambertian scenes that are defined by a finite set of voxels and are viewed from N arbitrarily-distributed viewpoints inside or around them). The approach is specifically designed to attack practical reconstruction problems, including (1) recovering shape \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:M3NEmzRMIkIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Transport-aware imaging",
            "Publication year": 2015,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9376/937606/Transport-aware-imaging/10.1117/12.2085305.short",
            "Abstract": "Conventional cameras record all light falling on their sensor regardless of the path that light followed to get there. In this paper we give an overview of a new family of computational cameras that offers many more degrees of freedom. These cameras record just a fraction of the light coming from a controllable source, based on the actual 3D light path followed. Photos and live video captured this way offer an unconventional view of everyday scenes in which the effects of scattering, refraction and other phenomena can be selectively blocked or enhanced, visual structures that are too subtle to notice with the naked eye can become apparent, and object appearance can depend on depth. We give an overview of the basic theory behind these cameras and their DMD-based implementation, and discuss three applications: (1) live indirect-only imaging of complex everyday scenes, (2) reconstructing the 3D shape of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:3jqAvCjcdfEC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "Linear sequence-to-sequence alignment",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4731269/",
            "Abstract": "In this paper, we consider the problem of estimating the spatiotemporal alignment between N unsynchronized video sequences of the same dynamic 3D scene, captured from distinct viewpoints. Unlike most existing methods, which work for N = 2 and rely on a computationally intensive search in the space of temporal alignments, we present a novel approach that reduces the problem for general N to the robust estimation of a single line in R N . This line captures all temporal relations between the sequences and can be computed without any prior knowledge of these relations. Considering that the spatial alignment is captured by the parameters of fundamental matrices, an iterative algorithm is used to refine simultaneously the parameters representing the temporal and spatial relations between the sequences. Experimental results with real-world and synthetic sequences show that our method can accurately align \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:D03iK_w7-QYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Dual-tap Computational Photography Image Sensor with Per-pixel Pipelined Digital Memory for Intra-frame Coded Multi-exposure",
            "Publication year": 2019,
            "Publication url": "https://cris.fbk.eu/handle/11582/319504",
            "Abstract": "A coded-exposure-pixel image sensor for computational imaging applications is presented. Each frame exposure time is divided into  subframes. Within each subframe, each pixel sorts photo-generated charge into two charge taps depending on that pixel\u2019s 1-bit binary code.  global updates of arbitrary pixel-wise codes are implemented in each frame to enable  short global pixel-specific subexposures within one frame. To make these subexposures global, two latches per pixel are utilized in a pipelined fashion. The code for the next subframe is loaded into latch 1 in a row parallel fashion, while the code for the current subframe is being applied by latch 2 globally for photo-generated charge sorting during the current subexposure. A  image sensor prototype with  pixel pitch has been fabricated in a  CMOS image sensor (CIS) technology. The image sensor has been demonstrated in two computational photography applications, each using only a single frame of a video: 1) computing both albedo (a measure of reflectivity) and 3-D depth maps by means of structured-light imaging and 2) computing surface normals (3-D orientations) map by means of photometric stereo imaging. These demonstrations experimentally validate some of the unique capabilities of this computational image sensor, such as accurate 3-D visual scene reconstruction using only one camera, while maintaining its native specifications: the full spatial resolution and the maximum frame rate.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:dBzKUGQurMsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A theory of refractive and specular 3d shape by light-path triangulation",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/article/10.1007/s11263-007-0049-9",
            "Abstract": " We investigate the feasibility of reconstructing an arbitrarily-shaped specular scene (refractive or mirror-like) from one or more viewpoints. By reducing shape recovery to the problem of reconstructing individual 3D light paths that cross the image plane, we obtain three key results. First, we show how to compute the depth map of a specular scene from a single viewpoint, when the scene redirects incoming light just once. Second, for scenes where incoming light undergoes two refractions or reflections, we show that three viewpoints are sufficient to enable reconstruction in the general case. Third, we show that it is impossible to reconstruct individual light paths when light is redirected more than twice. Our analysis assumes that, for every point on the image plane, we know at least one 3D point on its light path. This leads to reconstruction algorithms that rely on an \u201cenvironment matting\u201d procedure to establish \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:g5m5HwL7SMYC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Auto-Tuning Structured Light by Optical Stochastic Gradient Descent",
            "Publication year": 2020,
            "Publication url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Auto-Tuning_Structured_Light_by_Optical_Stochastic_Gradient_Descent_CVPR_2020_paper.html",
            "Abstract": "We consider the problem of optimizing the performance of an active imaging system by automatically discovering the illuminations it should use, and the way to decode them. Our approach tackles two seemingly incompatible goals:(1)\" tuning\" the illuminations and decoding algorithm precisely to the devices at hand---to their optical transfer functions, non-linearities, spectral responses, image processing pipelines---and (2) doing so without modeling or calibrating the system; without modeling the scenes of interest; and without prior training data. The key idea is to formulate a stochastic gradient descent (SGD) optimization procedure that puts the actual system in the loop: projecting patterns, capturing images, and calculating the gradient of expected reconstruction error. We apply this idea to structured-light triangulation to\" auto-tune\" several devices---from smartphones and laser projectors to advanced computational cameras. Our experiments show that despite being model-free and automatic, optical SGD can boost system 3D accuracy substantially over state-of-the-art coding schemes.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:Z7R3Ocg27JUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A layer-based restoration framework for variable-aperture photography",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4408898/",
            "Abstract": "We present variable-aperture photography, a new method for analyzing sets of images captured with different aperture settings, with all other camera parameters fixed. We show that by casting the problem in an image restoration framework, we can simultaneously account for defocus, high dynamic range exposure (HDR), and noise, all of which are confounded according to aperture. Our formulation is based on a layered decomposition of the scene that models occlusion effects in detail. Recovering such a scene representation allows us to adjust the camera parameters in post-capture, to achieve changes in focus setting or depth-of-field\u2014with all results available in HDR. Our method is designed to work with very few input images: we demonstrate results from real sequences obtained using the three-image \"aperture bracketing\" mode found on consumer digital SLR cameras.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:70eg2SAEIzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A theory of inverse light transport",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1544888/",
            "Abstract": "In this paper we consider the problem of computing and removing interreflections in photographs of real scenes. Towards this end, we introduce the problem of inverse light transport - given a photograph of an unknown scene, decompose it into a sum of n-bounce images, where each image records the contribution of light that bounces exactly n times before reaching the camera. We prove the existence of a set of interreflection cancelation operators that enable computing each n-bounce image by multiplying the photograph by a matrix. This matrix is derived from a set of \"impulse images\" obtained by probing the scene with a narrow beam of light. The operators work under unknown and arbitrary illumination, and exist for scenes that have arbitrary spatially-varying BRDFs. We derive a closed-form expression for these operators in the Lambertian case and present experiments with textured and untextured \u2026",
            "Abstract entirety": 0,
            "Author pub id": "bQJWJPYAAAAJ:YFjsv_pBGBYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Primal-dual coding to probe light transport.",
            "Publication year": 2012,
            "Publication url": "https://www.cs.toronto.edu/~kyros/pubs/12.sig.pdc.pdf",
            "Abstract": "We present primal-dual coding, a photography technique that enables direct fine-grain control over which light paths contribute to a photo. We achieve this by projecting a sequence of patterns onto the scene while the sensor is exposed to light. At the same time, a second sequence of patterns, derived from the first and applied in lockstep, modulates the light received at individual sensor pixels. We show that photography in this regime is equivalent to a matrix probing operation in which the elements of the scene\u2019s transport matrix are individually re-scaled and then mapped to the photo. This makes it possible to directly acquire photos in which specific light transport paths have been blocked, attenuated or enhanced. We show captured photos for several scenes with challenging light transport effects, including specular inter-reflections, caustics, diffuse inter-reflections and volumetric scattering. A key feature of primal-dual coding is that it operates almost exclusively in the optical domain: our results consist of directly-acquired, unprocessed RAW photos or differences between them.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:tOudhMTPpwUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Coded two-bucket cameras for computer vision",
            "Publication year": 2018,
            "Publication url": "http://openaccess.thecvf.com/content_ECCV_2018/html/Mian_Wei_Coded_Two-Bucket_Cameras_ECCV_2018_paper.html",
            "Abstract": "We introduce coded two-bucket (C2B) imaging, a new operating principle for computational sensors with applications in active 3D shape estimation and coded-exposure imaging. A C2B sensor modulates the light arriving at each pixel by controlling which of the pixel's two\" buckets\" should integrate it. C2B sensors output two images per video frame---one per bucket---and allow rapid, fully-programmable, per-pixel control of the active bucket. Using these properties as a starting point, we (1) develop an image formation model for these sensors,(2) couple them with programmable light sources to acquire illumination mosaics, ie, images of a scene under many different illumination conditions whose pixels have been multiplexed onto the sensor plane and acquired in one shot, and (3) show how to process illumination mosaics to acquire time-varying depth or normal maps of dynamic scenes at the sensor's native resolution. We present the first experimental demonstration of these capabilities, using a fully functional C2B camera prototype. Key to this prototype is a C2B sensor that was designed by us, fabricated in a standard CMOS imaging technology, and demonstrated for the first time in this paper.",
            "Abstract entirety": 1,
            "Author pub id": "bQJWJPYAAAAJ:pcWPcJyQGiUC",
            "Publisher": "Unknown"
        }
    ]
}]