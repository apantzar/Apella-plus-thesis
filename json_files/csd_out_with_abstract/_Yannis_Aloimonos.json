[{
    "name": " Yannis Aloimonos",
    "romanize name": " Yannis Aloimonos",
    "School-Department": " Computer Science",
    "University": "University of Maryland",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 1297,
    "Scholar name": "Aloimonos",
    "Scholar id": "7QmEsOwAAAAJ",
    "Affiliation": "Professor of Computer Science, University of Maryland, Director Computer Vision Laboratory",
    "Citedby": 14495,
    "Interests": [
        "Computer Vision",
        "Humanoid Robotics",
        "Robot Perception",
        "Cognitive Systems"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=7QmEsOwAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "The influence of shape on image correspondence",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1335418/",
            "Abstract": "We examine the implications of shape on the process of finding dense correspondence and half-occlusions for a stereo pair of images. The desired property of the depth map is that it should be a piecewise continuous function which is consistent with the images and which has the minimum number of discontinuities. To zeroeth order, piecewise continuity becomes piecewise constancy. Using this approximation, we first discuss an approach for dealing with such a fronto-parallel shapeless world, and the problems involved therein. We then introduce horizontal and vertical slant to create a first order approximation to piecewise continuity. We highlight the fact that a horizontally slanted surface (ie. having depth variation in the direction of the separation of the two cameras) appears horizontally stretched in one image as compared to the other image. Thus, while corresponding two images, N pixels on a scanline in one \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:VLnqNzywnoUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Forecasting action through contact representations from first person video",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9340014/",
            "Abstract": "Human visual understanding of action is reliant on anticipation of contact as is demonstrated by pioneering work in cognitive science. Taking inspiration from this, we introduce representations and models centered on contact, which we then use in action prediction and anticipation. We annotate a subset of the EPIC Kitchens dataset to include time-to-contact between hands and objects, as well as segmentations of hands and objects. Using these annotations we train the Anticipation Module, a module producing Contact Anticipation Maps and Next Active Object Segmentations - novel low-level representations providing temporal and spatial characteristics of anticipated near future action. On top of the Anticipation Module we apply Egocentric Object Manipulation Graphs (Ego-OMG), a framework for action anticipation and prediction. Ego-OMG models longer term temporal semantic relations through the use of a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:txeM2kYbVNMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Combining Visual Learning with a Generic Cognitive Model for Appliance Representation",
            "Publication year": 2017,
            "Publication url": "https://core.ac.uk/download/pdf/157767954.pdf#page=54",
            "Abstract": "For robots to become ubiquitous in every home, it is imperative for them to be able to safely and intuitively interact with objects that humans use on a daily basis. To allow for such operation, we propose a cognitive model that will allow robots to recognize and work with common appliances, such as microwaves, refrigerators, dishwashers, and other similar equipment, found in everyday scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:s85pQhAUCrAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A language for human action",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4198245/",
            "Abstract": "Human-centered computing (HCC) involves conforming computer technology to humans while naturally achieving human-machine interaction. In a human-centered system, the interaction focuses on human requirements, capabilities, and limitations. These anthropocentric systems also focus on the consideration of human sensory-motor skills in a wide range of activities. This ensures that the interface between artificial agents and human users accounts for perception and action in a novel interaction paradigm. In turn, this leads to behavior understanding through cognitive models that allow content description and, ultimately, the integration of real and virtual worlds. Our work focuses on building a language that maps to the lower-level sensory and motor languages and to the higher-level natural language. An empirically demonstrated human activity language provides sensory-motor-grounded representations for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:D03iK_w7-QYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Detection of manipulation action consequences (mac)",
            "Publication year": 2013,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Yang_Detection_of_Manipulation_2013_CVPR_paper.html",
            "Abstract": "The problem of action recognition and human activity has been an active research area in Computer Vision and Robotics. While full-body motions can be characterized by movement and change of posture, no characterization, that holds invariance, has yet been proposed for the description of manipulation actions. We propose that a fundamental concept in understanding such actions, are the consequences of actions. There is a small set of fundamental primitive action consequences that provides a systematic high-level classification of manipulation actions. In this paper a technique is developed to recognize these action consequences. At the heart of the technique lies a novel active tracking and segmentation method that monitors the changes in appearance and topological structure of the manipulated object. These are then used in a visual semantic graph (VSG) based procedure applied to the time sequence of the monitored object to recognize the action consequence. We provide a new dataset, called Manipulation Action Consequences (MAC 1.0), which can serve as testbed for other studies on this topic. Several experiments on this dataset demonstrates that our method can robustly track objects and detect their deformations and division during the manipulation. Quantitative tests prove the effectiveness and efficiency of the method.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:kVjdVfd2voEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Minimalist Vision for Navigation: Martin Herman, Marilyn Nashman, Tsai-Hong Hong, Henry Schneiderman, David Coombs, Gin-Shu Young, Daniel Raviv, and Albert J. Wavering",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=14220443046228507215&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:SrKkpNFED5gC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "0-mms: Zero-shot multi-motion segmentation with a monocular event camera",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9561755/",
            "Abstract": "Segmentation of moving objects in dynamic scenes is a key process in scene understanding for navigation tasks. Classical cameras suffer from motion blur in such scenarios rendering them effete. On the contrary, event cameras, because of their high temporal resolution and lack of motion blur, are tailor-made for this problem. We present an approach for monocular multi-motion segmentation, which combines bottom-up feature tracking and top-down motion compensation into a unified pipeline, which is the first of its kind to our knowledge. Using the events within a time-interval, our method segments the scene into multiple motions by splitting and merging. We further speed up our method by using the concept of motion propagation and cluster keyslices.The approach was successfully evaluated on both challenging real-world and synthetic scenarios from the EV-IMO, EED, and MOD datasets and outperformed the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:TGkaJS32XoUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Detecting independent motion: The statistics of temporal continuity",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/868679/",
            "Abstract": "We consider a problem central in aerial visual surveillance applications; detection and tracking of small, independently moving objects in long and noisy video sequences. We directly use spatiotemporal image intensity gradient measurements to compute an exact model of background motion. This allows the creation of accurate mosaics over many frames, and the definition of a constraint violation function which acts as an indicator of independent motion. A novel temporal integration method maintains confidence measures over long subsequences without computing the optic flow, requiring object models, or using a Kalman filter. The mosaic acts as a stable feature frame, allowing precise localization of the independently moving objects. We present a statistical analysis of the effects of image noise on the constraint violation measure and find a good match between the predicted probability distribution function and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:ZHo1McVdvXMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Approximate matching of digital point sets using a novel angular tree",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4378391/",
            "Abstract": "Matching and analysis of patterns or shapes in the digital plane are of utmost importance in various problems of computer vision and pattern recognition. A digital point set is such a pattern that corresponds to an object in the digital plane. Although there exist several data structures that can be employed for Approximate Point Set Pattern Matching (APSPM) in the real domain, they require substantial modification to support algorithms in the digital domain. To bridge this gap, a novel data structure called \"angular treerdquo is proposed, targeting an efficient and error-controllable circular range query in the digital plane. The farthest pair of points may be used as the starting correspondence between the pattern set and the background set. Several classical discrete structures and methodologies of computational geometry, as well as some topological features of circles/discs in digital geometry, have been used in tandem \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:LgRImbQfgY4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multiple view image reconstruction: A harmonic approach",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4270310/",
            "Abstract": "This paper presents a new constraint connecting the signals in multiple views of a surface. The constraint arises from a harmonic analysis of the geometry of the imaging process and it gives rise to a new technique for multiple view image reconstruction. Given several views of a surface from different positions, fundamentally different information is present in each image, owing to the fact that cameras measure the incoming light only after the application of a low-pass filter. Our analysis shows how the geometry of the imaging is connected to this filtering. This leads to a technique for constructing a single output image containing all the information present in the input images.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:bnK-pcrLprsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Visual Navigation Using Fast Content-Based Retrieval: John J. Weng, Shaoyun Chen, and Thomas S. Huang",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=11277163057932899768&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:oqD4_j7ulsYC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Embedding high-level information into low level vision: Efficient object search in clutter",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6630566/",
            "Abstract": "The ability to search visually for objects of interest in cluttered environments is crucial for robots performing tasks in a multitude of environments. In this work, we propose a novel visual search algorithm that integrates high-level information of the target object - specifically its size and shape, with a recently introduced visual operator that rapidly clusters potential edges based on their coherence in belonging to a possible object. The output is a set of fixation points that indicate the potential location of the target object in the image. The proposed approach outperforms purely bottom-up approaches - saliency maps of Itti et al. [15], and kernel descriptors of Bo et al. [2], over two large datasets of objects in clutter collected using an RGB-Depth camera.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:cK4Rrx0J3m0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Computational Tactile Flow for Anthropomorphic Grippers",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1903.08248",
            "Abstract": "Grasping objects requires tight integration between visual and tactile feedback. However, there is an inherent difference in the scale at which both these input modalities operate. It is thus necessary to be able to analyze tactile feedback in isolation in order to gain information about the surface the end-effector is operating on, such that more fine-grained features may be extracted from the surroundings. For tactile perception of the robot, inspired by the concept of the tactile flow in humans, we present the computational tactile flow to improve the analysis of the tactile feedback in robots using a Shadow Dexterous Hand. In the computational tactile flow model, given a sequence of pressure values from the tactile sensors, we define a virtual surface for the pressure values and define the tactile flow as the optical flow of this surface. We provide case studies that demonstrate how the computational tactile flow maps reveal information on the direction of motion and 3D structure of the surface, and feedback regarding the action being performed by the robot.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:ndLnGcHYRF0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning the spatial semantics of manipulation actions through preposition grounding",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7139371/",
            "Abstract": "In this paper, we introduce an abstract representation for manipulation actions that is based on the evolution of the spatial relations between involved objects. Object tracking in RGBD streams enables straightforward and intuitive ways to model spatial relations in 3D space. Reasoning in 3D overcomes many of the limitations of similar previous approaches, while providing significant flexibility in the desired level of abstraction. At each frame of a manipulation video, we evaluate a number of spatial predicates for all object pairs and treat the resulting set of sequences (Predicate Vector Sequences, PVS) as an action descriptor. As part of our representation, we introduce a symmetric, time-normalized pairwise distance measure that relies on finding an optimal object correspondence between two actions. We experimentally evaluate the method on the classification of various manipulation actions in video, performed at \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:Hck25ST_3aIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "PRGFlow: Unified SWAP\u2010aware deep global optical flow for aerial robot navigation",
            "Publication year": 2021,
            "Publication url": "https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ell2.12274",
            "Abstract": "Global optical flow estimation is the foundation stone for obtaining odometry which is used to enable aerial robot navigation. However, such a method has to be of low latency and high robustness whilst also respecting the size, weight, area and power (SWAP) constraints of the robot. A combination of cameras coupled with inertial measurement units (IMUs) has proven to be the best combination in order to obtain such low latency odometry on resource\u2010constrained aerial robots. Recently, deep learning approaches for visual inertial fusion have gained momentum due to their high accuracy and robustness. However, an equally noteworthy benefit for robotics of these techniques are their inherent scalability (adaptation to different sized aerial robots) and unification (same method works on different sized aerial robots). To this end, we present a deep learning approach called PRGFlow for obtaining global optical flow and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:qCpRzq7zkD8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Integration of visual and inertial information for egomotion: a stochastic approach",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1642007/",
            "Abstract": "We present a probabilistic framework for visual correspondence, inertial measurements and egomotion. First, we describe a simple method based on Gabor filters to produce correspondence probability distributions. Next, we generate a noise model for inertial measurements. Probability distributions over the motions are then computed directly from the correspondence distributions and the inertial measurements. We investigate combining the inertial and visual information for a single distribution over the motions. We find that with smaller amounts of correspondence information, fusion of the visual data with the inertial sensor results in much better egomotion estimation. This is essentially because inertial measurements decrease the \"translation-rotation\" ambiguity. However, when more correspondence information is used, this ambiguity is reduced to such a degree that the inertial measurements provide negligible \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:_B80troHkn4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Navigational Preliminaries: Cornelia Fermuller",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=10033724040334726578&hl=en&oi=scholarr",
            "Abstract": "This book defines the emerging field of Active Perception which calls for studying perception coupled with action. It is devoted to technical problems related to the design and analysis of intelligent systems possessing perception such as the existing biological organisms and the\" seeing\" machines of the future. Since the appearance of the first technical results on active vision, researchers began to realize that perception--and intelligence in general--is not transcendental and disembodied. It is becoming clear that in the effort to build intelligent visual systems, consideration must be given to the fact that perception is intimately related to the physiology of the perceiver and the tasks that it performs. This viewpoint--known as Purposive, Qualitative, or Animate Vision--is the natural evolution of the principles of Active Vision. The seven chapters in this volume present various aspects of active perception, ranging from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:yIeBiWEAh44C",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Action Attribute Detection from Sports Videos with Contextual Constraints.",
            "Publication year": 2013,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.587.8103&rep=rep1&type=pdf",
            "Abstract": "In this paper, we are interested in detecting action attributes from sports videos for event understanding and video analysis. Action attribute is a middle layer between low level motion features and high level action classes, which includes various motion patterns of human limbs and bodies and the interaction between human and objects. Successfully detecting action attributes provides a richer video description that facilitates many other important tasks, such action classification, video understanding, automatic video transcript, etc.A naive approach to deal with this challenging problem is to train a classifier for each attribute and then use them to detect attributes in novel videos independently. However, this independence assumption is often too strong, and as we show in our experiments, produces a large number of false positives in practice. We propose a novel approach that incorporates the contextual constraints for activity attribute detection. The temporal contexts within an attribute and the co-occurrence contexts between different attributes are modelled by a factorial conditional random field, which encourages agreement between different time points and attributes. The effectiveness of our methods are clearly illustrated by the experimental evaluations.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:O0nohqN1r9EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Signals on Pencils of Lines",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4409110/",
            "Abstract": "This paper proposes the \"epipolar pencil transformation\" (EPT). This is a tool for comparing the signals in different images, with no use of feature detection, yet taking advantage of the constraints given by epipolar geometry. The idea is to develop a descriptor for each point, summarizing the signals on the pencil of lines intersecting at that point. To compute the EPT, first find compact descriptors for each line, then combine these appropriately for each pencil. Given the EPT for two images, computing the epipolar geometry reduces to a closest pairs problem- select one pencil from each set such that the L 1  distance (in descriptor space) is minimized. By this reduction to a high dimensional closest pairs problem, recent advances in computational geometry can be used to efficiently identify the best global solution. This technique is robust, as each potential solution is evaluated by comparing the signals for all the lines \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:edDO8Oi4QzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Revisiting active perception",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/article/10.1007/s10514-017-9615-3",
            "Abstract": "Despite the recent successes in robotics, artificial intelligence and computer vision, a complete artificial agent necessarily must include active perception. A multitude of ideas and methods for how to accomplish this have already appeared in the past, their broader utility perhaps impeded by insufficient computational power or costly hardware. The history of these ideas, perhaps selective due to our perspectives, is presented with the goal of organizing the past literature and highlighting the seminal contributions. We argue that those contributions are as relevant today as they were decades ago and, with the state of modern computational tools, are poised to find new life in the robotic perception systems of the next decade.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:1tZ8xJnm2c8C",
            "Publisher": "Springer US"
        },
        {
            "Title": "Detection and segmentation of 2d curved reflection symmetric structures",
            "Publication year": 2015,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Teo_Detection_and_Segmentation_ICCV_2015_paper.html",
            "Abstract": "Symmetry, as one of the key components of Gestalt theory, provides an important mid-level cue that serves as input to higher visual processes such as segmentation. In this work, we propose a complete approach that links the detection of curved reflection symmetries to produce symmetry-constrained segments of structures/regions in real images with clutter. For curved reflection symmetry detection, we leverage on patch-based symmetric features to train a Structured Random Forest classifier that detects multiscaled curved symmetries in 2D images. Next, using these curved symmetries, we modulate a novel symmetry-constrained foreground-background segmentation by their symmetry scores so that we enforce global symmetrical consistency in the final segmentation. This is achieved by imposing a pairwise symmetry prior that encourages symmetric pixels to have the same labels over a MRF-based representation of the input image edges, and the final segmentation is obtained via graph-cuts. Experimental results over four publicly available datasets containing annotated symmetric structures: 1) SYMMAX-300, 2) BSD-Parts, 3) Weizmann Horse and 4) NY-roads demonstrate the approach's applicability to different environments with state-of-the-art performance.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:hSRAE-fF4OAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Morpheyes: Variable baseline stereo for quadrotor navigation",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9561116/",
            "Abstract": "Morphable design and depth-based visual control are two upcoming trends leading to advancements in the field of quadrotor autonomy. Stereo-cameras have struck the perfect balance of weight and accuracy of depth estimation but suffer from the problem of depth range being limited and dictated by the baseline chosen at design time. In this paper, we present a framework for quadrotor navigation based on a stereo camera system whose baseline can be adapted on-the-fly. We present a method to calibrate the system at a small number of discrete baselines and interpolate the parameters for the entire baseline range. We present an extensive theoretical analysis of calibration and synchronization errors. We showcase three different applications of such a system for quadrotor navigation: (a) flying through a forest, (b) flying through an unknown shaped/location static/dynamic gap, and (c) accurate 3D pose detection \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:aDl3D7KC1E4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Direct Motion Perception: Cornelia Ferm\u00fcller and Yiannis Aloimonos",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=9887236613643992995&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:lK9BDNCuzFgC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Image transformations and blurring",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4531746/",
            "Abstract": "Since cameras blur the incoming light during measurement, different images of the same surface do not contain the same information about that surface. Thus, in general, corresponding points in multiple views of a scene have different image intensities. While multiple-view geometry constrains the locations of corresponding points, it does not give relationships between the signals at corresponding locations. This paper offers an elementary treatment of these relationships. We first develop the notion of \"idealrdquo and \"realrdquo images, corresponding to, respectively, the raw incoming light and the measured signal. This framework separates the filtering and geometric aspects of imaging. We then consider how to synthesize one view of a surface from another; if the transformation between the two views is affine, it emerges that this is possible if and only if the singular values of the affine matrix are positive. Next, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:hkOj_22Ku90C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Appreciation to IJCV Reviewers of 2016",
            "Publication year": 2017,
            "Publication url": "https://search.proquest.com/openview/acb3e96b74f6981e3dce984ad4ea4494/1?pq-origsite=gscholar&cbl=1456341",
            "Abstract": "For helping us deliver timely decisions to our authors, the Editors-in-Chief and Publisher would like to thank the following individuals that contributed reviews between October 1, 2015 and October 1, 2016. We applaud your efforts and dedication to the community.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:LGlY6t8CeOMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "View-invariant modeling and recognition of human actions using grammars",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-70932-9_9",
            "Abstract": "In this paper, we represent human actions as sentences generated by a language built on atomic body poses or phonemes. The knowledge of body pose is stored only implicitly as a set of silhouettes seen from multiple viewpoints; no explicit 3D poses or body models are used, and individual body parts are not identified. Actions and their constituent atomic poses are extracted from a set of multiview multiperson video sequences by an automatic keyframe selection process, and are used to automatically construct a probabilistic context-free grammar (PCFG), which encodes the syntax of the actions. Given a new single viewpoint video, we can parse it to recognize actions and changes in viewpoint simultaneously. Experimental results are provided.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:NaGl4SEjCO4C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "1 Visual Navigation: Flies, Bees, and UGV's?",
            "Publication year": 2013,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=m_7edh_hN1MC&oi=fnd&pg=PA1&dq=info:Gvi3aI46980J:scholar.google.com&ots=QYdGufHQqL&sig=nsZ-g-L4Z9Fm6_lQz5uniYqiwJI",
            "Abstract": "Recent developments in empirical and computational disciplines that study the problem of visual perception have advanced a great deal our understanding of the mechanisms underlying the process of visual navigation. Visual navigation amounts to the control of sensory mediated movement and encompasses a wide range of capabilities, ranging from low-level ones related to kinetic stabilization to highlevel ones related to the ability of a system to acquire a memory of a place or location and recognize it (homing). This book consists of articles in several disciplines representing major developments in some area of navigation, and it is divided into three parts. The \ufb01rst part (chapters 2 and 3) describes advances in understanding how biological systems deal with navigation problems, and in particular how insects process images for the purpose of successfully moving around in their world. The second part (chapters 4, 5 and 6) is devoted to recent theoretical developments that will in\ufb02uence the design of autonomous systems and future basic research in the \ufb01eld. The third part (chapters 7 through 12) describes the application of the theoretical developments to the design of autonomous visual navigation systems operating in various environments. This introduction brie\ufb02y examines the problem of visual navigation from several perspectives, provides a description of basic research questions, and shows how the different chapters relate to different questions and to each other. The treatment is, for the most part, of a computational nature, stressing geometry, statistics, computational techniques, and signal processing. A system that successfully \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:QsKbpXNoaWkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Event-based moving object detection and tracking",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8593805/",
            "Abstract": "Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are ideally suited for real-time motion analysis. The unique properties encompassed in the readings of such sensors provide high temporal resolution, superior sensitivity to light and low latency. These properties provide the grounds to estimate motion efficiently and reliably in the most sophisticated scenarios, but these advantages come at a price - modern event-based vision sensors have extremely low resolution, produce a lot of noise and require the development of novel algorithms to handle the asynchronous event stream. This paper presents a new, efficient approach to object tracking with asynchronous cameras. We present a novel event stream representation which enables us to utilize information about the dynamic (temporal)component of the event stream. The 3D geometry of the event stream is approximated with a parametric \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:wLxue7F8ec0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Understanding Noise Sensitivity in Structure From Motion: Kostas Daniilidis and Minas E. Spetsakis",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=8097612107175234724&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:oursBaop5wYC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Joint direct estimation of 3D geometry and 3D motion using spatio temporal gradients",
            "Publication year": 2021,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0031320320305628",
            "Abstract": "Conventional image-motion based methods for structure from motion first compute optical flow, then solve for the 3D motion parameters based on the epipolar constraint, and finally recover the 3D geometry of the scene. However, errors in optical flow due to regularization can lead to large errors in 3D motion and structure. This paper investigates whether performance and consistency can be improved by avoiding optical flow estimation in the early stages of the structure-from-motion pipeline, and it proposes a new direct method based on image gradients (normal flow) only. Our main idea lies in a reformulation of the positive-depth constraint \u2013 the basis for estimating egomotion from normal flow \u2013 as a continuous piecewise differentiable function, which allows the use of well-known minimization techniques to solve for 3D motion. The 3D motion estimate is then refined and structure estimated adding a regularization \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:sJPMR1oEGYQC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "A dataset for visual navigation with neuromorphic methods",
            "Publication year": 2016,
            "Publication url": "https://www.frontiersin.org/articles/10.3389/fnins.2016.00049/full",
            "Abstract": "Standardized benchmarks in Computer Vision have greatly contributed to the advance of approaches to many problems in the field. If we want to enhance the visibility of event-driven vision and increase its impact, we will need benchmarks that allow comparison among different neuromorphic methods as well as comparison to Computer Vision conventional approaches. We present datasets to evaluate the accuracy of frame-free and frame-based approaches for tasks of visual navigation. Similar to conventional Computer Vision datasets, we provide synthetic and real scenes, with the synthetic data created with graphics packages, and the real data recorded using a mobile robotic platform carrying a dynamic and active pixel vision sensor (DAVIS) and an RGB+Depth sensor. For both datasets the cameras move with a rigid motion in a static scene, and the data includes the images, events, optic flow, 3D camera motion, and the depth of the scene, along with calibration procedures. Finally, we also provide simulated event data generated synthetically from well-known frame-based optical flow datasets.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:ghEM2AJqZyQC",
            "Publisher": "Frontiers"
        },
        {
            "Title": "Active Segmentation: A New Approach",
            "Publication year": 2011,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=Hq2_gogd2lkC&oi=fnd&pg=PA25&dq=info:bYDJKkbHEVMJ:scholar.google.com&ots=e3SjNMB_9o&sig=pBIWPRq4e-SP5Xzf4ww9HCkUIII",
            "Abstract": "2.1 INTRODUCTION Segmenting a scene (or image) into regions is an important step in visual processing. The regions are more discriminative than the individual pixels and fewer in number than the total number of pixels. This makes regions better suited and computationally less expensive to use for high-level visual processing like tracking, recognizing objects, and three-dimensional (3D) reconstruction. But to make segmentation an essential first step of vision algorithms, the segmentation algorithm needs to be consistent in its output and should not require any user input. Let us give a state-of-the-art example to explain what it means for a segmentation algorithm to be fully automatic and consistent. We consider only a single image here shown in Figure 2.1 a, but our analysis is not restricted to single images only. Most segmentation algorithms amount to statistical (a)(b)(c)",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:MDX3w3dAD3YC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "Artificial Intelligence: Theories, Models and Applications: 5th Hellenic Conference on AI, SETN 2008, Syros, Greece, October 2-4, 2008, Proceedings",
            "Publication year": 2008,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=u1NsCQAAQBAJ&oi=fnd&pg=PP2&dq=info:xqFk8GeCOf8J:scholar.google.com&ots=l3pCgny5C-&sig=EHqx1qJLkEiRuBTnLnj2F1Vn3Rc",
            "Abstract": "Artificial intelligence (AI) is a dynamic field that is constantly expanding into new application areas, discovering new research challenges and facilitating the devel-ment of innovative products. Today\u2019s information overload and rapid technological advancement raise needs for effective management of the complexity and heteroge-ity of knowledge, for intelligent and adaptable man\u2013machine interfaces and for pr-ucts and applications that can learn and take decisions by themselves. Although the mystery of human-level intelligence has just started to be uncovered in various int-disciplinary fields, AI is inspired by the respective scientific areas to explore certain theories and models that will provide the methods and techniques to design and-velop human-centered applications that address the above-mentioned needs. This volume contains papers selected for presentation at the 5th Hellenic Conference on Artificial Intelligence (SETN 2008), the official meeting of the Hellenic Society for Artificial Intelligence (EETN). Previous conferences were held at the University of Piraeus (1996), at the Aristotle University of Thessaloniki (2002), at the University of the Aegean (2004) and at the Institute of Computer Science at FORTH (Foundation for Research and Technology-Hellas) and the University of Crete (2006).",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:27LrP4qxOz0C",
            "Publisher": "Springer"
        },
        {
            "Title": "Complete calibration of a multi-camera network",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/853820/",
            "Abstract": "We describe a calibration procedure for a multi-camera rig. Consider a large number of synchronized cameras arranged in some space, for example, on the walls of a room looking inwards. It is not necessary for all the cameras to have a common field of view, as long as every camera is connected to every other camera through common fields of view. Switching off the lights and waving a wand with an LED at the end of it, we can capture a very large set of point correspondences (corresponding points are captured at the same time stamp). The correspondences are then used in a large, nonlinear eigenvalue minimization routine whose basis is the epipolar constraint. The eigenvalue matrix encapsulates all points correspondences between every pair of cameras in a way that minimizing the smallest eigenvalue results in the projection matrices, to within a single perspective transformation. In a second step, given \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:IaI1MmNe2tcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Visual Servoing from 2-D Image Cues: Daniel Raviv and Martin Herman",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=18185435627148595468&hl=en&oi=scholarr",
            "Abstract": "This book defines the emerging field of Active Perception which calls for studying perception coupled with action. It is devoted to technical problems related to the design and analysis of intelligent systems possessing perception such as the existing biological organisms and the\" seeing\" machines of the future. Since the appearance of the first technical results on active vision, researchers began to realize that perception--and intelligence in general--is not transcendental and disembodied. It is becoming clear that in the effort to build intelligent visual systems, consideration must be given to the fact that perception is intimately related to the physiology of the perceiver and the tasks that it performs. This viewpoint--known as Purposive, Qualitative, or Animate Vision--is the natural evolution of the principles of Active Vision. The seven chapters in this volume present various aspects of active perception, ranging from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:tBlTYpvFGQIC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "A lightweight camera sensor network operating on symbolic information",
            "Publication year": 2006,
            "Publication url": "https://www.academia.edu/download/34027256/dsc06_yale.pdf",
            "Abstract": "This paper provides an overview of the research aspects of our DSC06 demonstration. We present a new camera sensor network for behavior recognition. Two new technologies are explored, biologically inspired address-event image sensors and sensory grammars. This paper explains how these two technologies are used together and reports of the current status of our prototyping effort. The application of the resulting system in assisted living is also described.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:An6A6Jpfc1oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "NudgeSeg: Zero-Shot Object Segmentation by Repeated Physical Interaction",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2109.13859",
            "Abstract": "Recent advances in object segmentation have demonstrated that deep neural networks excel at object segmentation for specific classes in color and depth images. However, their performance is dictated by the number of classes and objects used for training, thereby hindering generalization to never seen objects or zero-shot samples. To exacerbate the problem further, object segmentation using image frames rely on recognition and pattern matching cues. Instead, we utilize the 'active' nature of a robot and their ability to 'interact' with the environment to induce additional geometric constraints for segmenting zero-shot samples. In this paper, we present the first framework to segment unknown objects in a cluttered scene by repeatedly 'nudging' at the objects and moving them to obtain additional motion cues at every step using only a monochrome monocular camera. We call our framework NudgeSeg. These motion cues are used to refine the segmentation masks. We successfully test our approach to segment novel objects in various cluttered scenes and provide an extensive study with image and motion segmentation methods. We show an impressive average detection rate of over 86% on zero-shot objects.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:rqnDXT1GswoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Gapflyt: Active vision based minimalist structure-less gap detection for quadrotor flight",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8371216/",
            "Abstract": "Although quadrotors, and aerial robots in general, are inherently active agents, their perceptual capabilities in literature so far have been mostly passive in nature. Researchers and practitioners today use traditional computer vision algorithms with the aim of building a representation of general applicability: a 3-D reconstruction of the scene. Using this representation, planning tasks are constructed and accomplished to allow the quadrotor to demonstrate autonomous behavior. These methods are inefficient as they are not task driven and such methodologies are not utilized by flying insects and birds. Such agents have been solving the problem of navigation and complex control for ages without the need to build a 3-D map and are highly task driven. In this letter, we propose this framework of bioinspired perceptual design for quadrotors. We use this philosophy to design a minimalist sensorimotor framework for a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:OzeSX8-yOCQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Border ownership assignment in real images",
            "Publication year": 2015,
            "Publication url": "https://jov.arvojournals.org/article.aspx?articleid=2433871",
            "Abstract": "We explored whether spectral based cues, such as extremal edges, are useful for border ownership classification in real images, and we analyzed how different environments: indoor vs. outdoor affect the prediction. Our algorithmic approach is based on a random forest classifier using spectral and Gestalt-like texture grouping features. The classifier detects in real-time the points in the image, which are likely borders and assigns their border ownership. Training was achieved using annotated images. The spectral cues were obtained by performing a principal component analysis over clusters of edge patches, and then re projecting the input image using the top four principal components (PCs). Global relations were implemented using a novel semi-global Gestalt detector of closure, spiral, radial and hyperbolic patterns. Inference over a 481x321 image takes about 0.1 s using commodity hardware in Matlab. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:sA9dB-pw3HoC",
            "Publisher": "The Association for Research in Vision and Ophthalmology"
        },
        {
            "Title": "HAL: human activity language",
            "Publication year": 2008,
            "Publication url": "https://jov.arvojournals.org/article.aspx?articleid=2136640",
            "Abstract": "We propose a linguistic approach to model human activity. This approach is able to address several problems related to action interpretation in a single framework. The Human Activity Language (HAL) consists of kinetology, morphology, and syntax. Kinetology, the phonology of human movement, finds basic primitives for human motion (segmentation) and associates them with symbols (symbolization). The input is measurements of human movement in 3D (signals), as for example produced by motion capture systems. This way, kinetology provides a non-arbitrary grounded symbolic representation for human movement that allows synthesis, analysis, and symbolic manipulation. The morphology of a human action is related to the inference of essential parts of the movement (morpho-kinetology) and its structure (morpho-syntax). In order to learn the morphemes and their structure, we present a grammatical inference \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:s9ia6_kGH2AC",
            "Publisher": "The Association for Research in Vision and Ophthalmology"
        },
        {
            "Title": "Designing Visual Systems: Purposive Navigation: Yiannis Alojmonos, Ehud Rivlin and Liuqjng Huang",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=6595192905597247268&hl=en&oi=scholarr",
            "Abstract": "This book defines the emerging field of Active Perception which calls for studying perception coupled with action. It is devoted to technical problems related to the design and analysis of intelligent systems possessing perception such as the existing biological organisms and the\" seeing\" machines of the future. Since the appearance of the first technical results on active vision, researchers began to realize that perception--and intelligence in general--is not transcendental and disembodied. It is becoming clear that in the effort to build intelligent visual systems, consideration must be given to the fact that perception is intimately related to the physiology of the perceiver and the tasks that it performs. This viewpoint--known as Purposive, Qualitative, or Animate Vision--is the natural evolution of the principles of Active Vision. The seven chapters in this volume present various aspects of active perception, ranging from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:uAPFzskPt0AC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Observability of 3D motion",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/article/10.1023/A:1008177429387",
            "Abstract": "This paper examines the inherent difficulties in observing 3D rigid motion from image sequences. It does so without considering a particular estimator. Instead, it presents a statistical analysis of all the possible computational models which can be used for estimating 3D motion from an image sequence. These computational models are classified according to the mathematical constraints that they employ and the characteristics of the imaging sensor (restricted field of view and full field of view). Regarding the mathematical constraints, there exist two principles relating a sequence of images taken by a moving camera. One is the \u201cepipolar constraint,\u201d applied to motion fields, and the other the \u201cpositive depth\u201d constraint, applied to normal flow fields. 3D motion estimation amounts to optimizing these constraints over the image. A statistical modeling of these constraints leads to functions which are studied with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:ldfaerwXgEUC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Learning manipulation actions from unconstrained videos",
            "Publication year": 2016,
            "Publication url": "https://patents.google.com/patent/US20160221190A1/en",
            "Abstract": "Various systems may benefit from computer learning. For example, robotics systems may benefit from learning actions, such as manipulation actions, from unconstrained videos. A method can include processing a set of video images to obtain a collection of semantic entities. The method can also include processing the semantic entities to obtain at least one visual sentence from the set of video images. The method can further include deriving an action plan for a robot from the at least one visual sentence. The method can additionally include implementing the action plan by the robot. The processing the set of video images, the processing semantic entities, and the deriving the action plan can be computer implemented.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:-6RzNnnwWf8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spatial and spatio-temporal characterization of movement for the analysis of actions and actors",
            "Publication year": 2011,
            "Publication url": "https://drum.lib.umd.edu/handle/1903/12123",
            "Abstract": "Movement data is high-dimensional but often redundant, meaning there is certainly a lower dimensional subspace that spans most of the body configurations within an action performance. We propose that one such representation can be achieved through a decomposition method that explores the existence of key configurations and temporal correlations of those configurations that are typical of action matrices. The approach is compatible with computational models of motor synergies based on matrix factorizations, and it builds upon a method that was earlier proposed in the context of biological motion perception. Our experiments show that vertical jump trials collected from children and young adults can be consistently reconstructed from the resulting representation. We also observe that a subset of that same representation suggests differences among populations of jumpers based on their trials, which serves to illustrate the potential of the method as a tool to analyze both actions and actors.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:xtoqd-5pKcoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Visual Navigation: Flies, Bees, and UGV\u2019s: Yiannis Aloimonos",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=16908112373910821576&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:gV6rEsy15s0C",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "A sensory grammar for inferring behaviors in sensor networks",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1127777.1127817",
            "Abstract": "The ability of a sensor network to parse out observable activities into a set of distinguishable actions is a powerful feature that can potentially enable many applications of sensor networks to everyday life situations. In this paper we introduce a framework that uses a hierarchy of Probabilistic Context Free Grammars (PCFGs) to perform such parsing. The power of the framework comes from the hierarchical organization of grammars that allows the use of simple local sensor measurements for reasoning about more macroscopic behaviors. Our presentation describes how to use a set of phonemes to construct grammars and how to achieve distributed operation using a messaging model. The proposed framework is flexible. It can be mapped to a network hierarchy or can be applied sequentially and across the network to infer behaviors as they unfold in space and time. We demonstrate this functionality by inferring \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:pqnbT2bcN3wC",
            "Publisher": "ACM"
        },
        {
            "Title": "Introduction: Active Vision Revisited: yiannis Aloimonos",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=4811837590226820726&hl=en&oi=scholarr",
            "Abstract": "This book defines the emerging field of Active Perception which calls for studying perception coupled with action. It is devoted to technical problems related to the design and analysis of intelligent systems possessing perception such as the existing biological organisms and the\" seeing\" machines of the future. Since the appearance of the first technical results on active vision, researchers began to realize that perception--and intelligence in general--is not transcendental and disembodied. It is becoming clear that in the effort to build intelligent visual systems, consideration must be given to the fact that perception is intimately related to the physiology of the perceiver and the tasks that it performs. This viewpoint--known as Purposive, Qualitative, or Animate Vision--is the natural evolution of the principles of Active Vision. The seven chapters in this volume present various aspects of active perception, ranging from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:qsWQJNntlusC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Symbolic representation and learning with hyperdimensional computing",
            "Publication year": 2020,
            "Publication url": "https://www.frontiersin.org/articles/10.3389/frobt.2020.00063/full",
            "Abstract": "It has been proposed that Machine Learning techniques can benefit from symbolic representations and reasoning systems. We describe a method in which the two can be combined in a natural and direct way by use of Hyperdimensional Vectors and Hyperdimensional Computing. By using hashing neural networks to produce binary vector representations of images, we show how hyperdimensional vectors can be constructed such that vector-symbolic inference arises naturally out of their output. We design the Hyperdimensional Inference Layer (HIL) to facilitate this process and evaluate its performance compared to baseline hashing networks. In addition to this, we show that separate network outputs can directly be fused at the vector symbolic level within HILs to improve performance and robustness of the overall model. Furthermore, to the best of our knowledge, this is the first instance in which meaningful hyperdimensional representations of images are created on real data, while still maintaining hyperdimensionality.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:VRfTbSk87rEC",
            "Publisher": "Frontiers"
        },
        {
            "Title": "NudgeSeg: Zero-Shot Object Segmentation by Repeated Physical Interaction",
            "Publication year": 2021,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2021arXiv210913859D/abstract",
            "Abstract": "Recent advances in object segmentation have demonstrated that deep neural networks excel at object segmentation for specific classes in color and depth images. However, their performance is dictated by the number of classes and objects used for training, thereby hindering generalization to never seen objects or zero-shot samples. To exacerbate the problem further, object segmentation using image frames rely on recognition and pattern matching cues. Instead, we utilize the'active'nature of a robot and their ability to'interact'with the environment to induce additional geometric constraints for segmenting zero-shot samples. In this paper, we present the first framework to segment unknown objects in a cluttered scene by repeatedly'nudging'at the objects and moving them to obtain additional motion cues at every step using only a monochrome monocular camera. We call our framework NudgeSeg. These motion \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:CMvovTBb2okC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The SB-ST decomposition in the study of Developmental Coordination Disorder",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7346131/",
            "Abstract": "To deal with redundancy and high-dimensionality that are typical of movement data, we propose to decompose action matrices in two decoupled steps: first, we discover a set of key postures, that is, vectors corresponding to key relationships among degrees of freedom (like angles between body parts) which we call spatial basis (SB) and second, we impose a parametric model to the spatio-temporal (ST) profiles of each SB vector. These two steps constitute the SB-ST decomposition of an action: SB vectors represent the key postures, their ST profiles represent trajectories of these postures and ST parameters express how these postures are being controlled and coordinated. SB-ST shares elements in common with computational models of motor synergies and biological motion perception, and it relates to human manifold models that are popular in machine learning. We showcase the method by applying SB \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:QsaTk4IG4EwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2106.15045",
            "Abstract": "The rapid rise of accessibility of unmanned aerial vehicles or drones pose a threat to general security and confidentiality. Most of the commercially available or custom-built drones are multi-rotors and are comprised of multiple propellers. Since these propellers rotate at a high-speed, they are generally the fastest moving parts of an image and cannot be directly \"seen\" by a classical camera without severe motion blur. We utilize a class of sensors that are particularly suitable for such scenarios called event cameras, which have a high temporal resolution, low-latency, and high dynamic range. In this paper, we model the geometry of a propeller and use it to generate simulated events which are used to train a deep neural network called EVPropNet to detect propellers from the data of an event camera. EVPropNet directly transfers to the real world without any fine-tuning or retraining. We present two applications of our network: (a) tracking and following an unmarked drone and (b) landing on a near-hover drone. We successfully evaluate and demonstrate the proposed approach in many real-world experiments with different propeller shapes and sizes. Our network can detect propellers at a rate of 85.1% even when 60% of the propeller is occluded and can run at upto 35Hz on a 2W power budget. To our knowledge, this is the first deep learning-based solution for detecting propellers (to detect drones). Finally, our applications also show an impressive success rate of 92% and 90% for the tracking and landing tasks respectively.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:sgsej9ZJWHMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The Ouchi illusion as an artifact of biased flow estimation",
            "Publication year": 2000,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0042698999001625",
            "Abstract": "A pattern by Ouchi has the surprising property that small motions can cause illusory relative motion between the inset and background regions. The effect can be attained with small retinal motions or a slight jiggling of the paper and is robust over large changes in the patterns, frequencies and boundary shapes. In this paper, we explain that the cause of the illusion lies in the statistical difficulty of integrating local one-dimensional motion signals into two-dimensional image velocity measurements. The estimation of image velocity generally is biased, and for the particular spatial gradient distributions of the Ouchi pattern the bias is highly pronounced, giving rise to a large difference in the velocity estimates in the two regions. The computational model introduced to describe the statistical estimation of image velocity also accounts for the findings of psychophysical studies with variations of the Ouchi pattern and for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:EUQCXRtRnyEC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Learning to Recognize Objects in Images Using Anisotropic Nonparametric Kernels.",
            "Publication year": 2010,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=viYYT8hAnqIC&oi=fnd&pg=PA163&dq=info:LEUC5RgfmXsJ:scholar.google.com&ots=RMkyDtU0YW&sig=4l6RKfdg8G-hVukzY7Ec221k5NU",
            "Abstract": "We present a system that makes use of image context to perform pixellevel segmentation for many object classes simultaneously. The system finds approximate nearest neighbors from the training set for a (biologically plausible) feature patch surrounding each pixel. It then uses locally adaptive anisotropic Gaussian kernels to find the shape of the class manifolds embedded in the highdimensional space of the feature patches, in order to find the most likely label for the pixel. An iterative technique allows the system to make use of scene context information to refine its classification. Like humans, the system is able to quickly make use of new information without going through a lengthy training phase. The system provides insight into a possible mechanism for infants to quickly learn to recognize all of the classes they are presented with simultaneously, rather than having to be trained explicitly on a few classes like standard image classification algorithms.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:S16KYo8Pm5AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cilantro: A lean, versatile, and efficient library for point cloud data processing",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3240508.3243655",
            "Abstract": "We introduce Cilantro, an open-source C++ library for geometric and general-purpose point cloud data processing. The library provides functionality that covers low-level point cloud operations, spatial reasoning, various methods for point cloud segmentation and generic data clustering, flexible algorithms for robust or local geometric alignment, model fitting, as well as powerful visualization tools. To accommodate all kinds of workflows, Cilantro is almost fully templated, and most of its generic algorithms operate in arbitrary data dimension. At the same time, the library is easy to use and highly expressive, promoting a clean and concise coding style. Cilantro is highly optimized, has a minimal set of external dependencies, and supports rapid development of performant point cloud processing software in a wide variety of contexts.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:sfnaS5RM6jYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "2016 Index",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7776998/",
            "Abstract": "This index covers all technical items\u2014papers, correspondence, reviews, etc.\u2014that appeared in this periodical during 2014\u20132016, and items from previous years that were commented upon or corrected in 2014\u20132016. Departments and other items may also be covered if they have been judged to have archival value.The Author Index contains the primary entry for each item, listed under the first author\u2019s name. The primary entry includes the coauthors\u2019 names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author\u2019s name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:W2uZP3ddy8sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Visual scene interpretation as a dialogue between vision and language",
            "Publication year": 2011,
            "Publication url": "https://www.aaai.org/ocs/index.php/WS/AAAIW11/paper/viewPaper/3989",
            "Abstract": "We present a framework for semantic visual scene interpretation in a system with vision and language. In this framework the system consists of two modules, a language module and a vision module that communicate with each other in a form of a dialogue to actively interpret the scene. The language module is responsible for obtaining domain knowledge from linguistic resources and reasoning on the basis of this knowledge and the visual input. It iteratively creates questions that amount to an attention mechanism for the vision module which in turn shifts its focus to selected parts of the scene and applies selective segmentation and feature extraction. As a formalism for optimizing this dialogue we use information theory. We demonstrate the framework on the problem of recognizing a static scene from its objects and show preliminary results for the problem of human activity recognition from video. Experiments demonstrate the effectiveness of the active paradigm in introducing attention and additional constraints into the sensing process.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:9pM33mqn1YgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Applications of Nonmetric Vision to Some Visually Guided Robotics Tasks: Lug Robert, Cyril Zeller, Olivier Faugeras, and Martial Heb\u00e9rt",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=5381397276171451702&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:ukw-9cB-YDkC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "2009 Index IEEE Transactions on Pattern Analysis and Machine Intelligence Vol. 31",
            "Publication year": 2008,
            "Publication url": "https://www.computer.org/csdl/journal/tp/2009/12/tpami09/13rRUxC0SXk",
            "Abstract": "This index covers all technical items\u2014papers, correspondence, reviews, etc.\u2014that appeared in this periodical during 2009, and items from previous years that were commented upon or corrected in 2009. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author\u2019s name. The primary entry includes the coauthors\u2019 names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author\u2019s name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:yqoGN6RLRZoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A probabilistic notion of correspondence and the epipolar constraint",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4155708/",
            "Abstract": "We present a probabilistic framework for correspondence and egomotion. First, we suggest computing probability distributions of correspondence. This has the advantage of being robust to points subject to the aperture effect and repetitive structure, while giving up no information at feature points. Additionally, correspondence probability distributions can be computed for every point in the scene. Next, we generate a probability distribution over the motions, from these correspondence probability distributions, through a probabilistic notion of the epipolar constraint. Finding the maximum in this distribution is shown to be a generalization of least-squared epipolar minimization. We will show that because our technique allows so much correspondence information to be extracted, more accurate ego- motion estimation is possible.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Learning parallel grammar systems for a human activity language",
            "Publication year": 2006,
            "Publication url": "https://drum.lib.umd.edu/handle/1903/4012",
            "Abstract": "We have empirically discovered that the space of human actions has a linguistic structure. This is a sensory-motor space consisting of the evolution of the joint angles of the human body in movement. The space of human activity has its own phonemes, morphemes, and sentences. In kinetology, the phonology of human movement, we define atomic segments (kinetemes) that are used to compose human activity. In this paper, we present a morphological representation that explicitly contains the subset of actuators responsible for the activity, the synchronization rules modeling coordination among these actuators, and the motion pattern performed by each participating actuator. We model a human action with a novel formal grammar system, named Parallel Synchronous Grammar System (PSGS), adapted from Parallel Communicating Grammar Systems (PCGS). We propose a heuristic PArallel Learning (PAL) algorithm for the automatic inference of a PSGS. Our algorithm is used in the learning of human activity. Instead of a sequence of sentences, the input is a single string for each actuator in the body. The algorithm infers the components of the grammar system as a subset of actuators, a CFG grammar for the language of each component, and synchronization rules. Our framework is evaluated with synthetic data and real motion data from a large scale motion capture database containing around 200 different actions corresponding to verbs associated with voluntary observable movement. On synthetic data, our algorithm achieves 100% success rate with a noise level up to 7%.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:evX43VCCuoAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Computer vision and natural language processing: recent approaches in multimedia and robotics",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3009906",
            "Abstract": "Integrating computer vision and natural language processing is a novel interdisciplinary field that has received a lot of attention recently. In this survey, we provide a comprehensive introduction of the integration of computer vision and natural language processing in multimedia and robotics applications with more than 200 key references. The tasks that we survey include visual attributes, image captioning, video captioning, visual question answering, visual retrieval, human-robot interaction, robotic actions, and robot navigation. We also emphasize strategies to integrate computer vision and natural language processing models as a unified theme of distributional semantics. We make an analog of distributional semantics in computer vision and natural language processing as image embedding and word embedding, respectively. We also present a unified view for the field and propose possible future directions.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:-yGd096yOn8C",
            "Publisher": "ACM"
        },
        {
            "Title": "Robust Nonlinear Control-Based Trajectory Tracking for Quadrotors Under Uncertainty",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9295321/",
            "Abstract": "This letter presents a modified robust integral of signum error (RISE) nonlinear control method, for quadrotor trajectory tracking and control. The proposed control algorithm tracks trajectories of varying speeds, uncertainties and disturbance magnitudes. The control law presented achieves asymptotic regulation of the quadrotor states in the presence of parametric uncertainties and disturbances. To achieve the results, first the quadrotor UAV dynamics are derived in a strict form. Then, a robust state feedback control is developed in both the position and attitude loop respectively. A detailed Lyapunov-based stability analysis is provided which proves the proposed control method theoretically guarantees asymptotic regulation of the quadrotor states. To illustrate the performance of the proposed control method, comparative numerical simulation results are provided, which demonstrate an improved performance under \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:RXiHnyRawswC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Detecting and Counting Oysters",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2105.09758",
            "Abstract": "Oysters are an essential species in the Chesapeake Bay living ecosystem. Oysters are filter feeders and considered the vacuum cleaners of the Chesapeake Bay that can considerably improve the Bay's water quality. Many oyster restoration programs have been initiated in the past decades and continued to date. Advancements in robotics and artificial intelligence have opened new opportunities for aquaculture. Drone-like ROVs with high maneuverability are getting more affordable and, if equipped with proper sensory devices, can monitor the oysters. This work presents our efforts for videography of the Chesapeake bay bottom using an ROV, constructing a database of oysters, implementing Mask R-CNN for detecting oysters, and counting their number in a video by tracking them.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:SpPTWFSNUtQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Investigating the'co-ordination'in developmental coordination disorder",
            "Publication year": 2011,
            "Publication url": "https://lirias.kuleuven.be/1922342?limo=0",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:Xl6nMSl579sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Guest Editorial: Deep Learning and Robotics",
            "Publication year": 2021,
            "Publication url": "https://scholar.google.com/scholar?cluster=9515389348511179516&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:w2UhwfzvF0QC",
            "Publisher": "WILEY"
        },
        {
            "Title": "Occlusions in motion processing",
            "Publication year": 2004,
            "Publication url": "https://www.academia.edu/download/41531001/Occlusions_in_Motion_Processing20160124-13506-g8gvxg.pdf",
            "Abstract": "The instantaneous identification of independently moving objects in a video is particularly difficult if the camera itself is moving, since the motion field on the image is created by the combined effect of camera motion (egomotion), structure (depth), and the independent motion of scene entities. For a camera with a restricted field of view undergoing a small motion between frames, there exists in general a set of 3D motions (translation, rotation) compatible with the observed flow field, even if only small amounts of noise are present. We address the instantaneous motion segmentation problem in the presence of this ambiguity in obtaining exact solutions for interframe 3D motion. We show that moving objects can be classified based on the amount of information required for their detection: if separable clusters of solutions exist, motion based clustering alone will suffice for detecting the simplest class of moving objects. If only a single cluster is found, the positive depth (structure) constraint can help us identify a second class of moving objects. A more difficult third class of moving objects is found by detecting conflicts between occlusions and structure from motion. Occlusions can not only reduce the ambiguity in 3D motion estimation but also help us identify an important class of moving objects. We underscore the observation that occlusions must not merely be identified, they must also be \u2018filled\u2019, so that ordinal depth may be deduced.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:umqufdRvDiIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Manipulation action tree bank: A knowledge resource for humanoids",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7041483/",
            "Abstract": "Our premise is that actions of manipulation are represented at multiple levels of abstraction. At the high level a grammatical structure represents symbolic information (objects, actions, tools, body parts) and their interaction in a temporal sequence, and at lower levels the symbolic quantities are grounded in perception. In this paper we create symbolic high-level representations in the form of manipulation action tree banks, which are parsed from annotated action corpora. A context free grammar provides the grammatical description for the creation of the semantic trees. Experiments conducted on the tree banks show that they allow to 1) generate so-called visual semantic graphs (VSGs), 2) compare the semantic distance between steps of activities and 3) discover the underlying semantic space of an activity. We believe that tree banks are an effective and practical way to organize semantic structures of manipulation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:DkZNVXde3BIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Numerical Simulation Analysis with FLAC3D on Retrofitting and Strengthening of Existing Deep Foundation Pit",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7263669/",
            "Abstract": "Taking a reinforcement engineering about existing foundation pit as the background, the FLAC 3D  is used to simulate the process of construction and pile-anchor supporting system. During the simulation Mohr-Coulomb model is used and contact elements are applied to the interfaces between anchor and soil. According to the measured data and parametric analysis results, the soil parameters are calculated with FLAC 3D . By using these parameters, the existing supporting deformation under original scheme is calculated. The reinforcement scheme is adopted on the basis of calculation results. The analysis results of the deformation, that were calculated by this model, were generally in agreement with the measured data. Furthermore, the agreement shows that the FLAC 3D  model and analytical method are reliable. And the results of FLAC 3D  numerical simulations can satisfy the need of engineering, so it can \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:GFxP56DSvIMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Deformation and Viewpoint Invariant Color Histograms.",
            "Publication year": 2006,
            "Publication url": "http://users.cecs.anu.edu.au/~jdomke/papers/2006bmvc.pdf",
            "Abstract": "We develop a theoretical basis for creating color histograms that are invariant under deformation or changes in viewpoint. The gradients in different color channels weight the influence of a pixel on the histogram so as to cancel out the changes induced by deformations. Experiments show these histograms to be invariant under a variety of distortions and changes in viewpoint.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:5ugPr518TE4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Planning and Navigation in Stochastic Environments: Thomas Dean and Jean-Luc Marion",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=5104097481995492938&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:4n0clTBhZ78C",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Vision, Action, and Navigation in Animals: Takahashi Hamada",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=3629637114779308255&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:RfUwGJFMQ-0C",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Similarity learning and generalization with limited data: A reservoir computing approach",
            "Publication year": 2018,
            "Publication url": "https://www.hindawi.com/journals/complexity/2018/6953836/",
            "Abstract": "We investigate the ways in which a machine learning architecture known as Reservoir Computing learns concepts such as \u201csimilar\u201d and \u201cdifferent\u201d and other relationships between image pairs and generalizes these concepts to previously unseen classes of data. We present two Reservoir Computing architectures, which loosely resemble neural dynamics, and show that a Reservoir Computer (RC) trained to identify relationships between image pairs drawn from a subset of training classes generalizes the learned relationships to substantially different classes unseen during training. We demonstrate our results on the simple MNIST handwritten digit database as well as a database of depth maps of visual scenes in videos taken from a moving camera. We consider image pair relationships such as images from the same class; images from the same class with one image superposed with noise, rotated 90\u00b0, blurred, or scaled; images from different classes. We observe that the reservoir acts as a nonlinear filter projecting the input into a higher dimensional space in which the relationships are separable; i.e., the reservoir system state trajectories display different dynamical patterns that reflect the corresponding input pair relationships. Thus, as opposed to training in the entire high-dimensional reservoir space, the RC only needs to learns characteristic features of these dynamical patterns, allowing it to perform well with very few training examples compared with conventional machine learning feed-forward techniques such as deep learning. In generalization tasks, we observe that RCs perform significantly better than state-of-the-art, feed-forward, pair \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:R-LXmdHK_14C",
            "Publisher": "Hindawi"
        },
        {
            "Title": "Learning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception",
            "Publication year": 2019,
            "Publication url": "https://robotics.sciencemag.org/content/4/30/eaaw6736.abstract",
            "Abstract": "The hallmark of modern robotics is the ability to directly fuse the platform\u2019s perception with its motoric ability\u2014the concept often referred to as \u201cactive perception.\u201d Nevertheless, we find that action and perception are often kept in separated spaces, which is a consequence of traditional vision being frame based and only existing in the moment and motion being a continuous entity. This bridge is crossed by the dynamic vision sensor (DVS), a neuromorphic camera that can see the motion. We propose a method of encoding actions and perceptions together into a single space that is meaningful, semantically informed, and consistent by using hyperdimensional binary vectors (HBVs). We used DVS for visual perception and showed that the visual component can be bound with the system velocity to enable dynamic world perception, which creates an opportunity for real-time navigation and obstacle avoidance. Actions \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:Azgs6IHzeyYC",
            "Publisher": "Science Robotics"
        },
        {
            "Title": "Leadership in orchestra emerges from the causal relationships of movement kinematics",
            "Publication year": 2012,
            "Publication url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0035757",
            "Abstract": "Non-verbal communication enables efficient transfer of information among people. In this context, classic orchestras are a remarkable instance of interaction and communication aimed at a common aesthetic goal: musicians train for years in order to acquire and share a non-linguistic framework for sensorimotor communication. To this end, we recorded violinists' and conductors' movement kinematics during execution of Mozart pieces, searching for causal relationships among musicians by using the Granger Causality method (GC). We show that the increase of conductor-to-musicians influence, together with the reduction of musician-to-musician coordination (an index of successful leadership) goes in parallel with quality of execution, as assessed by musical experts' judgments. Rigorous quantification of sensorimotor communication efficacy has always been complicated and affected by rather vague qualitative methodologies. Here we propose that the analysis of motor behavior provides a potentially interesting tool to approach the rather intangible concept of aesthetic quality of music and visual communication efficacy.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:dBIO0h50nwkC",
            "Publisher": "Public Library of Science"
        },
        {
            "Title": "Lightnet: A versatile, standalone matlab-based environment for deep learning",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2964284.2973791",
            "Abstract": "LightNet is a lightweight, versatile, purely Matlab-based deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:qE4H1tSSYIIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Synergistic methods for using language in robotics",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2393091.2393109",
            "Abstract": "This paper presents an overview of our work on integrating language with vision to endow robots with the ability of complex scene understanding. We propose and motivate the Vision-Action-Language loop as a form of cognitive dialogue that enables us to integrate current tools in linguistics, vision and AI. We present several experimental results of preliminary implementation and discuss future research directions that we view as crucial for developing the cognitive robots of the future.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:mNrWkgRL2YcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "SpikeMS: Deep Spiking Neural Network for Motion Segmentation",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2105.06562",
            "Abstract": "Spiking Neural Networks (SNN) are the so-called third generation of neural networks which attempt to more closely match the functioning of the biological brain. They inherently encode temporal data, allowing for training with less energy usage and can be extremely energy efficient when coded on neuromorphic hardware. In addition, they are well suited for tasks involving event-based sensors, which match the event-based nature of the SNN. However, SNNs have not been as effectively applied to real-world, large-scale tasks as standard Artificial Neural Networks (ANNs) due to the algorithmic and training complexity. To exacerbate the situation further, the input representation is unconventional and requires careful analysis and deep understanding. In this paper, we propose \\textit{SpikeMS}, the first deep encoder-decoder SNN architecture for the real-world large-scale problem of motion segmentation using the event-based DVS camera as input. To accomplish this, we introduce a novel spatio-temporal loss formulation that includes both spike counts and classification labels in conjunction with the use of new techniques for SNN backpropagation. In addition, we show that \\textit{SpikeMS} is capable of \\textit{incremental predictions}, or predictions from smaller amounts of test data than it is trained on. This is invaluable for providing outputs even with partial input data for low-latency applications and those requiring fast predictions. We evaluated \\textit{SpikeMS} on challenging synthetic and real-world sequences from EV-IMO, EED and MOD datasets and achieving results on a par with a comparable ANN method, but using potentially 50 times less \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:sbeIDTyQOFgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "EV-IMO: Motion segmentation dataset and learning pipeline for event cameras",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8968520/",
            "Abstract": "We present the first event-based learning approach for motion segmentation in indoor scenes and the first event-based dataset - EV-IMO- which includes accurate pixel-wise motion masks, egomotion and ground truth depth. Our approach is based on an efficient implementation of the SfM learning pipeline using a low parameter neural network architecture on event data. In addition to camera egomotion and a dense depth map, the network estimates independently moving object segmentation at the pixel-level and computes per-object 3D translational velocities of moving objects. We also train a shallow network with just 40k parameters, which is able to compute depth and egomotion. Our EV-IMO dataset features 32 minutes of indoor recording with up to 3 fast moving objects in the camera field of view. The objects and the camera are tracked using a VICON \u00ae  motion capture system. By 3D scanning the room and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:_tF6a-HnqWAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Active visual segmentation",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5989830/",
            "Abstract": "Attention is an integral part of the human visual system and has been widely studied in the visual attention literature. The human eyes fixate at important locations in the scene, and every fixation point lies inside a particular region of arbitrary shape and size, which can either be an entire object or a part of it. Using that fixation point as an identification marker on the object, we propose a method to segment the object of interest by finding the \u201coptimal\u201d closed contour around the fixation point in the polar space, avoiding the perennial problem of scale in the Cartesian space. The proposed segmentation process is carried out in two separate steps: First, all visual cues are combined to generate the probabilistic boundary edge map of the scene; second, in this edge map, the \u201coptimal\u201d closed contour around a given fixation point is found. Having two separate steps also makes it possible to establish a simple feedback \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:VN7nJs4JPk0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Eye design in the plenoptic space of light rays",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1238623/",
            "Abstract": "Natural eye designs are optimized with regard to the tasks the eye-carrying organism has to perform for survival. This optimization has been performed by the process of natural evolution over many millions of years. Every eye captures a subset of the space of light rays. The information contained in this subset and the accuracy to which the eye can extract the necessary information determines an upper limit on how well an organism can perform a given task. In this work we propose a new methodology for camera design. By interpreting eyes as sample patterns in light ray space we can phrase the problem of eye design in a signal processing framework. This allows us to develop mathematical criteria for optimal eye design, which in turn enables us to build the best eye for a given task without the trial and error phase of natural evolution. The principle is evaluated on the task of 3D ego-motion estimation.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:Br1UauaknNIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Combining knowledge and reasoning through probabilistic soft logic for image puzzle solving",
            "Publication year": 2018,
            "Publication url": "https://par.nsf.gov/servlets/purl/10094209",
            "Abstract": "The uncertainty associated with human perception is often reduced by one\u2019s extensive prior experience and knowledge. Current datasets and systems do not emphasize the necessity and benefit of using such knowledge. In this work, we propose the task of solving a genre of image-puzzles (\u201cimage riddles\u201d) that require both capabilities involving visual detection (including object, activity recognition) and, knowledge-based or commonsense reasoning. Each puzzle involves a set of images and the question \u201cwhat word connects these images?\u201d. We compile a dataset of over 3k riddles where each riddle consists of 4 images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an automatic evaluation metric to track future progress. Our task bears similarity with the commonly known IQ tasks such as analogy solving, sequence filling that are often used to test intelligence. We develop a Probabilistic Reasoning-based approach that utilizes commonsense knowledge about words and phrases to answer these riddles with a reasonable accuracy. Our approach achieves some promising results for these riddles and provides a strong baseline for future attempts. We make the entire dataset and related materials publicly available to the community (bit. ly/22f9Ala).",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:qmtmRrLr0tkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Robust computation of intrinsic images from multiple cues",
            "Publication year": 2014,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=ZuFRAwAAQBAJ&oi=fnd&pg=PA115&dq=info:qW4R7S72rDUJ:scholar.google.com&ots=5kcdGKB9PS&sig=FG9SYujo3KXR2QsvH6lRn1ECvgQ",
            "Abstract": "All the terms in the above de\ufb01nition are well defined, with the excep-tion of the term understand. What is really the meaning of understand with respect to this problem? There have basically been two approaches to this question in computer vision: reconstruction and recognition (Figure 2.1). The reconstruction school attempts to reconstruct the physical pa-rameters of the visual world, such as the depth or orientation of surfaces, the boundaries of objects, the direction of light sources and the like. The recognition school aims for the recognition or description of objects, and studies processes whose end product is some piece of behavior like a decision or a motion. Both schools have strong ties with psychology and neuroscience, and it seems likely that both schools will merge into a new one that may \ufb01nd an answer to the vision problem. Physical parameters derived from vision can basically be classi\ufb01ed in two categories: retinotopic and nonretinotopic. Nonretinotopic ones can be divided into global features (such as ego motion or light source direction) and objects and relations (Ballard, 1984). Retinotopic parameters are spatially indexed at every image point. Retinotopic parameters (shape,",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:dhFuZR0502QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Rotations, Lines, and Multilinear Constraints",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=11777159898303052056&hl=en&oi=scholarr",
            "Abstract": "We introduce a new constraint based on local shape and rotation only, which we call the prismatic line constraint. We derive this constraint on images of parallel lines in the context of a reformulation of the standard computer vision constraints in a way that does not combine the rotation and translation into one tensor, but instead keeps them separate. This separation allows us to consider local shape as distinguished from depth; this can allow for novel recognition, egomotion, and navigation algorithms. In addition, using this reformulation we show that the epipolar and trifocal constraints can be reduced to epipolar and trifocal constraints on image lines only.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:foquWX3nUaYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Pattern and 3D Vision of Insects: Adrian Horridge",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=2581452770740179259&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:cNe27ouKFcQC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "The minimalist grammar of action",
            "Publication year": 2012,
            "Publication url": "https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2011.0123",
            "Abstract": "Language and action have been found to share a common neural basis and in particular a common \u2018syntax\u2019, an analogous hierarchical and compositional organization. While language structure analysis has led to the formulation of different grammatical formalisms and associated discriminative or generative computational models, the structure of action is still elusive and so are the related computational models. However, structuring action has important implications on action learning and generalization, in both human cognition research and computation. In this study, we present a biologically inspired generative grammar of action, which employs the structure-building operations and principles of Chomsky's Minimalist Programme as a reference model. In this grammar, action terminals combine hierarchically into temporal sequences of actions of increasing complexity; the actions are bound with the involved tools \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:5awf1xo2G04C",
            "Publisher": "The Royal Society"
        },
        {
            "Title": "Seeing behind the scene: Using symmetry to reason about objects in cluttered environments",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8593822/",
            "Abstract": "Symmetry is a common property shared by the majority of man-made objects. This paper presents a novel bottom-up approach for segmenting symmetric objects and recovering their symmetries from 3D pointclouds of natural scenes. Candidate rotational and reflectional symmetries are detected by fitting symmetry axes/planes to the geometry of the smooth surfaces extracted from the scene. Individual symmetries are used as constraints for the foreground segmentation problem that uses symmetry as a global grouping principle. Evaluation on a challenging dataset shows that our approach can reliably segment objects and extract their symmetries from incomplete 3D reconstructions of highly cluttered scenes, outperforming state-of-the-art methods by a wide margin.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:nPTYJWkExTIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Salientdso: Bringing attention to direct sparse odometry",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8671715/",
            "Abstract": "Although cluttered indoor scenes have a lot of useful high-level semantic information which can be used for mapping and localization, most visual odometry (VO) algorithms rely on the usage of geometric features such as points, lines, and planes. Lately, driven by this idea, the joint optimization of semantic labels and estimating odometry has gained popularity in the robotics community. This joint optimization method is accurate but is generally very slow. At the same time, in the vision community, direct and sparse approaches for VO have stricken the right balance between speed and accuracy. We merge the successes of these two communities and present a preprocessing method to incorporate semantic information in the form of visual saliency to direct sparse odometry (DSO)-a highly successful direct sparse VO algorithm. We also present a framework to filter the visual saliency based on scene parsing. Our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:ijdKiLOsEJMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Polydioptric camera design and 3d motion estimation",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1211483/",
            "Abstract": "Most cameras used in computer vision applications are still based on the pinhole principle inspired by our own eyes. It has been found though that this is not necessarily the optimal image formation principle for processing visual information using a machine. We describe how to find the optimal camera for 3D motion estimation by analyzing the structure of the space formed by the light rays passing through a volume of space. Every camera corresponds to a sampling pattern in light ray space, thus the question of camera design can be rephrased as finding the optimal sampling pattern with regard to a given task. This framework suggests that large field-of-view multi-perspective (polydioptric) cameras are the optimal image sensors for 3D motion estimation. We conclude by proposing design principles for polydioptric cameras and describe an algorithm for such a camera that estimates its 3D motion in a scene \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:bFI3QPDXJZMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Real-Time Distributed Algorithms for Visual and Battlefield Reasoning",
            "Publication year": 2006,
            "Publication url": "https://apps.dtic.mil/sti/citations/ADA456930",
            "Abstract": "Information is key to the success of the next generation battlefield. There is a critical need to determine, in real-time, what the enemy is doing, and to interpret that information in the context of past related events. In this project we examined two aspects of this issue development of a high-level task definition language for tasking a network of sensors to carry out given objectives, and interpreting recounted events so that past related scenarios could be automatically identified from a case database.Descriptors:",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:r0BpntZqJG4C",
            "Publisher": "MARYLAND UNIV COLLEGE PARK OFFICE OF RESEARCH ADMINISTRATION AND ADVANCEMENT"
        },
        {
            "Title": "Improving annotation quality in gene ontology by mining cross-ontology weighted association rules",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6999374/",
            "Abstract": "The Gene Ontology (GO) is the major resource of annotations for genes and proteins. Despite the presence of large efforts to avoid errors and inconsistencies, some unreliabilities are still present. In particular electronically inferred annotations are more unreliable than manual ones and their number is growing. Thus, the need for an accurate evaluation of annotations in an automatic way arises. In the past, some approaches for improving annotation consistencies have been proposed using association rule mining to discover hidden relationships among GO terms. However such approaches consider all the GO terms equally, while GO terms have different Information Content, i.e. different relevance. Consequently we designed a novel algorithm, (GO-WAR), Mining Weighted Association Rules from GO, that is based on the extraction of weighted association rules considering the IC of terms. We evaluated our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:-95Q15plzcUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Visual segmentation of \u201csimple\u201d objects for robots",
            "Publication year": 2012,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=9-HxCwAAQBAJ&oi=fnd&pg=PA217&dq=info:NN1zxFTX0LUJ:scholar.google.com&ots=09WAH07A1m&sig=UJ4H9xIBlL7U49tDLHWljE5N2XA",
            "Abstract": "The ability to automatically segment a \u201csimple\u201d object of any size from its background is important for an active agent (eg a robot) to interact effectively in the real world. Recently, we proposed an algorithm [14] to segment a \u201csimple\u201d object in a scale invariant manner given a point anywhere inside that object. However, we did not provide a strategy to select a point inside a \u201csimple\u201d object in [14]. In this paper, we propose a new system that automatically selects the points inside different \u201csimple\u201d objects in the scene, carries out the segmentation process for the selected points, and outputs only the regions corresponding to the \u201csimple\u201d objects in the scene. The proposed attention mechanism for the segmentation problem utilizes, for the first time, the concept of border ownership [17].I. MOTIVATION Object perception is as important a capability as navigation for robots to interact with their surroundings. Without object perception, robots would not have any dynamic target to navigate to, except the static ones such as the doors in a room or a fixed location given by the Global Positioning System (GPS). Object perception helps robots identify objects of interest. The robots then use their navigation capability to reach those objects. In other words, object perception and navigation are complementary capabilities. However, unlike navigation, object perception is not well defined. In fact, the exact definition of an object and what constitutes perception are both open questions. Since object perception is an intricate phenomenon involving not only visual inputs but also other cognitive modalities, we define a minimal form of object perception as knowing the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:CYCckWUYoCcC",
            "Publisher": "MIT Press"
        },
        {
            "Title": "The statistics of optical flow",
            "Publication year": 2001,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1077314200909007",
            "Abstract": "When processing image sequences some representation of image motion must be derived as a first stage. The most often used representation is the optical flow field, which is a set of velocity measurements of image patterns. It is well known that it is very difficult to estimate accurate optical flow at locations in an image which correspond to scene discontinuities. What is less well known, however, is that even at the locations corresponding to smooth scene surfaces, the optical flow field often cannot be estimated accurately.Noise in the data causes many optical flow estimation techniques to give biased flow estimates. Very often there is consistent bias: the estimate tends to be an underestimate in length and to be in a direction closer to the majority of the gradients in the patch. This paper studies all three major categories of flow estimation methods\u2014gradient-based, energy-based, and correlation methods, and it \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:J_g5lzvAfSwC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Network deconvolution",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1905.11926",
            "Abstract": "Convolution is a central operation in Convolutional Neural Networks (CNNs), which applies a kernel to overlapping regions shifted across the image. However, because of the strong correlations in real-world image data, convolutional kernels are in effect re-learning redundant data. In this work, we show that this redundancy has made neural network training challenging, and propose network deconvolution, a procedure which optimally removes pixel-wise and channel-wise correlations before the data is fed into each layer. Network deconvolution can be efficiently calculated at a fraction of the computational cost of a convolution layer. We also show that the deconvolution filters in the first layer of the network resemble the center-surround structure found in biological neurons in the visual regions of the brain. Filtering with such kernels results in a sparse representation, a desired property that has been missing in the training of neural networks. Learning from the sparse representation promotes faster convergence and superior results without the use of batch normalization. We apply our network deconvolution operation to 10 modern neural network models by replacing batch normalization within each. Extensive experiments show that the network deconvolution operation is able to deliver performance improvement in all cases on the CIFAR-10, CIFAR-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:I8rxH6phXEkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using a minimal action grammar for activity understanding in the real world",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6385483/",
            "Abstract": "There is good reason to believe that humans use some kind of recursive grammatical structure when we recognize and perform complex manipulation activities. We have built a system to automatically build a tree structure from observations of an actor performing such activities. The activity trees that result form a framework for search and understanding, tying action to language. We explore and evaluate the system by performing experiments over a novel complex activity dataset taken using synchronized Kinect and SR4000 Time of Flight cameras. Processing of the combined 3D and 2D image data provides the necessary terminals and events to build the tree from the bottom-up. Experimental results highlight the contribution of the action grammar in: 1) providing a robust structure for complex activity recognition over real data and 2) disambiguating interleaved activities from within the same sequence.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:artPoR2Yc-kC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Following instructions by imagining and reaching visual goals",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2001.09373",
            "Abstract": "While traditional methods for instruction-following typically assume prior linguistic and perceptual knowledge, many recent works in reinforcement learning (RL) have proposed learning policies end-to-end, typically by training neural networks to map joint representations of observations and instructions directly to actions. In this work, we present a novel framework for learning to perform temporally extended tasks using spatial reasoning in the RL framework, by sequentially imagining visual goals and choosing appropriate actions to fulfill imagined goals. Our framework operates on raw pixel images, assumes no prior linguistic or perceptual knowledge, and learns via intrinsic motivation and a single extrinsic reward signal measuring task completion. We validate our method in two environments with a robot arm in a simulated interactive 3D environment. Our method outperforms two flat architectures with raw-pixel and ground-truth states, and a hierarchical architecture with ground-truth states on object arrangement tasks.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:aEW5N-EHWIMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A holistic approach to structure from motion",
            "Publication year": 2006,
            "Publication url": "https://drum.lib.umd.edu/handle/1903/3807",
            "Abstract": "This dissertation investigates the general structure from motion problem. That is, how to compute in an unconstrained environment  3D scene structure, camera motion and moving objects from  video sequences. We present a framework which uses concatenated feed-back loops to overcome the main difficulty in  the structure from motion problem: the chicken-and-egg dilemma between scene segmentation and structure recovery.  The idea is that we compute structure and motion in stages by gradually computing 3D scene information of increasing complexity and using  processes which operate on increasingly large spatial image areas. Within this framework, we developed three modules. First, we introduce a new constraint for the estimation of shape using image features from multiple views. We analyze this constraint and show that noise leads to unavoidable mis-estimation of the shape, which also predicts the erroneous shape perception in human. This insight provides a clear argument for the need for feed-back loops. Second, a novel constraint on shape is developed which allows us to connect multiple frames in the estimation of camera motion by matching only small image patches. Third, we present a texture descriptor for matching areas of extended sizes. The advantage of this texture descriptor, which is based on fractal geometry, lies in its invariance to any smooth mapping (Bi-Lipschitz transform) including changes of viewpoint, illumination and surface distortion.  Finally, we apply our framework to the problem of  super-resolution imaging.   We use the 3D motion estimation together with a novel wavelet-based reconstruction scheme to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:4hFrxpcac9AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unblock: Interactive Perception for Decluttering",
            "Publication year": 2021,
            "Publication url": "https://drum.lib.umd.edu/handle/1903/27850",
            "Abstract": "Novel segmentation algorithms can easily identify objects that are occludedor partially occluded, however in highly cluttered scenes the degree of occlusion is so high that some objects may not be visible to a static camera. In these scenarios, humans use action to change the configuration of the environment, elicit more information through perception, process the information before taking the next action. Reinforcement learning models this behavior, however unlike humans, the phase where perception data is understood is not included, as images are directly used as observations. The aim of this thesis is to establish a novel method that indirectly uses perception data for reinforcement learning to address the task of decluttering a scene using a static camera.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:yJjnfzR0HrkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Shadow free segmentation in still images using local density measure",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6831803/",
            "Abstract": "Over the last decades several approaches were introduced to deal with cast shadows in background subtraction applications. However, very few algorithms exist that address the same problem for still images. In this paper we propose a figure ground segmentation algorithm to segment objects in still images affected by shadows. Instead of modeling the shadow directly in the segmentation process our approach works actively by first segmenting an object and then testing the resulting boundary for the presence of shadows and resegmenting again with modified segmentation parameters. In order to get better shadow boundary detection results we introduce a novel image preprocessing technique based on the notion of the image density map. This map improves the illumination invariance of classical filter-bank based texture description methods. We demonstrate that this texture feature improves shadow detection \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:kF1pexMAQbMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A spherical eye from multiple cameras (makes better models of the world)",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/990525/",
            "Abstract": "The paper describes an imaging system that has been designed specifically for the purpose of recovering egomotion and structure from video. The system consists of six cameras in a network arranged so that they sample different parts of the visual sphere. This geometric configuration has provable advantages compared to small field of view cameras for the estimation of the system's own motion and consequently the estimation of shape models from the individual cameras. The reason is that inherent ambiguities of confusion between translation and rotation disappear. We provide algorithms for the calibration of the system and 3D motion estimation. The calibration is based on a new geometric constraint that relates the images of lines parallel in space to the rotation between the cameras. The 3D motion estimation uses a constraint relating structure directly to image gradients.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:f2IySw72cVMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Grounded Representations for Sensory-Motor Learning: A Linguistic Approach",
            "Publication year": 2006,
            "Publication url": "https://escholarship.org/content/qt3vq4n4g2/qt3vq4n4g2.pdf",
            "Abstract": "We have empirically discovered that the space of human actions has a linguistic structure. This is a sensory-motor space consisting of the evolution of the joint angles of the human body in movement. The space of human activity has its own phonemes, morphemes, words (nouns, verbs, adjectives, adverbs), and sentences formed by syntax. This has a number of implications for the grounding problem and cognition in general. We present a Human Activity Language (HAL) for symbolic manipulation of visual and motor information. The embodiment of the language serves as the interface between visual perception and the motor subsystem. The visuo-motor language is defined using a linguistic approach. In phonology, we define basic atomic segments that are used to compose human activity. In morphology, we study how visuo-motor phonemes are combined to form strings representing human activity and to generate a higher-level morphological grammar. In syntax, we present a model for visuo-motor sentence construction where the subject corresponds to the active joints (noun) modified by a posture (adjective). A verbal phrase involves the representation of the human activity (verb) and timing coordination among different joints (adverb).",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:t-hv7AR41mYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fast 2d border ownership assignment",
            "Publication year": 2015,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Teo_Fast_2D_Border_2015_CVPR_paper.html",
            "Abstract": "A method for efficient border ownership assignment in 2D images is proposed. Leveraging on recent advances using Structured Random Forests (SRF) for boundary detection, we impose a novel border ownership structure that detects both boundaries and border ownership at the same time. Key to this work are features that predict ownership cues from 2D images. To this end, we use several different local cues: shape, spectral properties of boundary patches, and semi-global grouping cues that are indicative of perceived depth. For shape, we use HoG-like descriptors that encode local curvature (convexity and concavity). For spectral properties, such as extremal edges, we first learn an orthonormal basis spanned by the top K eigenvectors via PCA over common types of contour tokens. For grouping, we introduce a novel mid-level descriptor that captures patterns near edges and indicates ownership information of the boundary. Experimental results over a subset of the Berkeley Segmentation Dataset (BSDS) and the NYU Depth V2 dataset show that our method's performance exceeds current state-of-the-art multi-stage approaches that use more complex features.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:jSAVyFp_754C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Topology-aware non-rigid point cloud registration",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8827963/",
            "Abstract": "In this paper, we introduce a non-rigid registration pipeline for unorganized point clouds that may be topologically different. Standard warp field estimation algorithms, even under robust, discontinuity-preserving regularization, produce erratic motion estimates on boundaries associated with \u2018close-to-open\u2019 topology changes. We overcome this limitation by exploiting backward motion: in the opposite direction, a \u2018close-to-open\u2019 event becomes \u2018open-to-close\u2019, which is by default handled correctly. Our approach relies on a general, topology-agnostic warp field estimation algorithm, similar to those employed in recent dynamic reconstruction systems from RGB-D input. We improve motion estimation on boundaries associated with topology changes in an efficient post-processing phase. Based on both forward and (inverted) backward warp hypotheses, we explicitly detect regions of the deformed geometry that undergo \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:jlhcAiayVhoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Unsupervised Learning of Dense Optical Flow, Depth and Egomotion with Event-Based Sensors",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9341224/",
            "Abstract": "We present an unsupervised learning pipeline for dense depth, optical flow and egomotion estimation for autonomous driving applications, using the event-based output of the Dynamic Vision Sensor (DVS) as input. The backbone of our pipeline is a bioinspired encoder-decoder neural network architecture - ECN. To train the pipeline, we introduce a covariance normalization technique which resembles the lateral inhibition mechanism found in animal neural systems.Our work is the first monocular pipeline that generates dense depth and optical flow from sparse event data only, and is able to transfer from day to night scenes without any additional training. The network works in self-supervised mode and has just 150k parameters. We evaluate our pipeline on the MVSEC self driving dataset and present results for depth, optical flow and and egomotion estimation. Thanks to the efficient design, we are able to achieve \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:vnF2_uLGgtgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Grasping in the Dark: Compliant Grasping using Shadow Dexterous Hand and BioTac Tactile Sensor",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2011.00712",
            "Abstract": "When it comes to grasping and manipulating objects, the human hand is the benchmark based on which we design and model grasping strategies and algorithms. The task of imitating human hand in robotic end-effectors, especially in scenarios where visual input is limited or absent, is an extremely challenging one. In this paper we present an adaptive, compliant grasping strategy using only tactile feedback. The proposed algorithm can grasp objects of varying shapes, sizes and weights without having a priori knowledge of the objects. The proof of concept algorithm presented here uses classical control formulations for closed-loop grasping. The algorithm has been experimentally validated using a Shadow Dexterous Hand equipped with BioTac tactile sensors. We demonstrate the success of our grasping policies on a variety of objects, such as bottles, boxes and balls.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:J3LtWjKFLicC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards a watson that sees: Language-guided action recognition for robots",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6224589/",
            "Abstract": "For robots of the future to interact seamlessly with humans, they must be able to reason about their surroundings and take actions that are appropriate to the situation. Such reasoning is only possible when the robot has knowledge of how the World functions, which must either be learned or hard-coded. In this paper, we propose an approach that exploits language as an important resource of high-level knowledge that a robot can use, akin to IBM's Watson in Jeopardy!. In particular, we show how language can be leveraged to reduce the ambiguity that arises from recognizing actions involving hand-tools from video data. Starting from the premise that tools and actions are intrinsically linked, with one explaining the existence of the other, we trained a language model over a large corpus of English newswire text so that we can extract this relationship directly. This model is then used as a prior to select the best tool and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:9c2xU6iGI7YC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Evenly cascaded convolutional networks",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8622196/",
            "Abstract": "We introduce Evenly Cascaded convolutional Network (ECN), a neural network taking inspiration from the cascade algorithm of wavelet analysis. ECN employs two feature streams - a low-level and high-level steam. At each layer these streams interact, such that low-level features are modulated using advanced perspectives from the high-level stream. ECN is evenly structured through resizing feature map dimensions by a consistent ratio, which removes the burden of ad-hoc specification of feature map dimensions. ECN produces easily interpretable features maps, a result whose intuition can be understood in the context of scale-space theory. We demonstrate that ECN's design facilitates the training process through providing easily trainable shortcuts. We report new state-of-the-art results for small networks, without the need for additional treatment such as pruning or compression - a consequence of ECN's \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:iyewoVqAXLQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Calibration of a multicamera network",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4624334/",
            "Abstract": "With the advent of laboratories containing dozens of cameras, and the possibility of laboratories containing hundreds of cameras, the question of how to calibrate all the cameras has become pressing. While it is certainly possible to calibrate these networks in a labor intensive manner, a simple, stable, and accurate calibration method is still needed. This paper presents such a method, based on textures printable on a laser printer and mounted on a board. We will show what the problems with the current methods are, and show how these problems can be overcome with a novel use of a trilinear constraint related to the vanishing point constraint, which we call the primsatic line constraint. High accuracy with little user effort is achieved with this method.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:-mN3Mh-tlDkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Studying human behavior from infancy: on the acquisition of infant postural data",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6982990/",
            "Abstract": "The study of human behavior using infants as target subjects is very attractive since these individuals are minimally affected by cultural background and display the fastest rates of evolving cognition and physique, opening possibilities to longitudinal but relatively short-term research. Naturally, important customers of infant movement data are healthcare practitioners and scientists at the cutting edge of the understanding of human development and related disorders, in particular Autism Spectrum Disorder (ASD). Here we provide evidence that, as opposed to the current practice, these studies demand non-invasive instrumentation to measure movement, so the right paradigm to obtain the data will most likely depend on computer vision based pose estimation. By surveying the interdisciplinary literature on infant motion capture, we show that, up to now, very little has been done to consider infant data in vision, and no \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:mWEH9CqjF64C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Cluttered scene segmentation using the symmetry constraint",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7487376/",
            "Abstract": "Although modern object segmentation algorithms can deal with isolated objects in simple scenes, segmenting non-convex objects in cluttered environments remains a challenging task. We introduce a novel approach for segmenting unknown objects in partial 3D pointclouds that utilizes the powerful concept of symmetry. First, 3D bilateral symmetries in the scene are detected efficiently by extracting and matching surface normal edge curves in the pointcloud. Symmetry hypotheses are then used to initialize a segmentation process that finds points of the scene that are consistent with each of the detected symmetries. We evaluate our approach on a dataset of 3D pointcloud scans of tabletop scenes. We demonstrate that the use of the symmetry constraint enables our approach to correctly segment objects in challenging configurations and to outperform current state-of-the-art approaches.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:0aBXIfxlw9sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Detecting independent 3D movement",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/3-540-28247-5_12.pdf",
            "Abstract": "Motion segmentation is the problem of finding parts of the scene which possess independent 3D motion (such as people, animals or other objects like vehicles). This process is conceptually straightforward if the camera is stationary, and is often referred to as background subtraction. However, if the camera itself is also moving, then the problem becomes more complicated, since the image motion is generated by the combined effects of camera motion, structure and the motion of the independently moving objects. Isolating the contribution of each of these three factors is needed to solve the more general independent motion problem, which involves motion segmentation (finding the moving objects) and also finding their 3D motion. In this chapter, we shall restrict ourselves to the problem of finding moving objects only and not worry about finding their 3D motion. In the beginning, we present our philosophy that visual \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:9Nmd_mFXekcC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Eyes from eyes",
            "Publication year": 2001,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:K3LRdlH-MEoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Metaconcepts: isolating context in word embeddings",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8695402/",
            "Abstract": "Word embeddings are commonly used to measure word-level semantic similarity in text, especially in direct word-to-word comparisons. However, the relationships between words in the embedding space are often viewed as approximately linear and concepts comprised of multiple words are a sort of linear combination. In this paper, we demonstrate that this is not generally true and show how the relationships can be better captured by leveraging the topology of the embedding space. We propose a technique for directly computing new vectors representing multiple words in a way that naturally combines them into a new, more consistent space where distance better correlates to similarity. We show that this technique works well for natural language, even when it comprises multiple words, on a simple task derived from WordNet synset descriptions and examples of words. Thus, the generated vectors better represent \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:7Frjd3zlGBUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Neural self talk: Image understanding via continuous questioning and answering",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1512.03460",
            "Abstract": "In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:-DxkuPiZhfEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Shape and the stereo correspondence problem",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/article/10.1007/s11263-005-3672-3",
            "Abstract": "We examine the implications of shape on the process of finding dense correspondence and half-occlusions for a stereo pair of images. The desired property of the disparity map is that it should be a piecewise continuous function which is consistent with the images and which has the minimum number of discontinuities. To zeroth order, piecewise continuity becomes piecewise constancy. Using this approximation, we first discuss an approach for dealing with such a fronto-parallel shapeless world, and the problems involved therein. We then introduce horizontal and vertical slant to create a first order approximation to piecewise continuity. In particular, we emphasize the following geometric fact: a horizontally slanted surface (i.e., having depth variation in the direction of the separation of the two cameras) will appear horizontally stretched in one image as compared to the other image. Thus, while \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:35N4QoGY0k4C",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Evdodgenet: Deep dynamic obstacle dodging with event cameras",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9196877/",
            "Abstract": "Dynamic obstacle avoidance on quadrotors requires low latency. A class of sensors that are particularly suitable for such scenarios are event cameras. In this paper, we present a deep learning based solution for dodging multiple dynamic obstacles on a quadrotor with a single event camera and on-board computation. Our approach uses a series of shallow neural networks for estimating both the ego-motion and the motion of independently moving objects. The networks are trained in simulation and directly transfer to the real world without any fine-tuning or retraining. We successfully evaluate and demonstrate the proposed approach in many real-world experiments with obstacles of different shapes and sizes, achieving an overall success rate of 70% including objects of unknown shape and a low light testing scenario. To our knowledge, this is the first deep learning \u2013 based solution to the problem of dynamic \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:U0iAMwwPxtsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A Deep 2-Dimensional Dynamical Spiking Neuronal Network for Temporal Encoding trained with STDP",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2009.00581",
            "Abstract": "The brain is known to be a highly complex, asynchronous dynamical system that is highly tailored to encode temporal information. However, recent deep learning approaches to not take advantage of this temporal coding. Spiking Neural Networks (SNNs) can be trained using biologically-realistic learning mechanisms, and can have neuronal activation rules that are biologically relevant. This type of network is also structured fundamentally around accepting temporal information through a time-decaying voltage update, a kind of input that current rate-encoding networks have difficulty with. Here we show that a large, deep layered SNN with dynamical, chaotic activity mimicking the mammalian cortex with biologically-inspired learning rules, such as STDP, is capable of encoding information from temporal data. We argue that the randomness inherent in the network weights allow the neurons to form groups that encode the temporal data being inputted after self-organizing with STDP. We aim to show that precise timing of input stimulus is critical in forming synchronous neural groups in a layered network. We analyze the network in terms of network entropy as a metric of information transfer. We hope to tackle two problems at once: the creation of artificial temporal neural systems for artificial intelligence, as well as solving coding mechanisms in the brain.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:XDrR66g3YHsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An Approximate Linear Approach for the Fundamental Matrix Computation",
            "Publication year": 2003,
            "Publication url": "https://drum.lib.umd.edu/handle/1903/4015",
            "Abstract": "We introduce a new robust approach for the computation of the fundamental matrix taking into account the intrinsic errors (uncertainty) involved in the discretization process. The problem is modeled as an approximate equation system and reduced to a linear programming form. This approach is able to compute the solution set instead of trying to compute only a single vertex of the solution polyhedron as in previous approaches. Therefore, our algorithm is a robust generalization of the eight-point algorithm. The exact solution computation feasibility is proved for some pure translation motions, depending only on the distribution of the discretization errors. However, a single exact solution for the fundamental matrix is not feasible for pure rotation cases. The feasibility of an exact solution is decided according to an error distance between a nontrivial exact solution and the faces of the solution set.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:tYavs44e6CUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A computational theory for life-long learning of semantics",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-97676-1_21",
            "Abstract": "Semantic vectors are learned from data to express semantic relationships between elements of information, for the purpose of solving and informing downstream tasks. Other models exist that learn to map and classify supervised data. However, the two worlds of learning rarely interact to inform one another dynamically, whether across types of data or levels of semantics, in order to form a unified model. We explore the research problem of learning these vectors and propose a framework for learning the semantics of knowledge incrementally and online, across multiple mediums of data, via binary vectors. We discuss the aspects of this framework to spur future research on this approach and problem.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:ji7lAbPyDbYC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Object detection apparatus, object detection method and object detection program",
            "Publication year": 2012,
            "Publication url": "https://patents.google.com/patent/US8300887B2/en",
            "Abstract": "An object detection apparatus and method for accurately detecting a movable object in a region around a vehicle from a time series of images obtained through a camera mounted on the vehicle by eliminating the influence of the movement of the camera through simple processing, and a program for making a computer execute processing in the apparatus. The object detection apparatus has a feature point extraction unit which extracts a feature point contained in a feature region of each image in the time series of images obtained through a camera mounted on the vehicle, a correspondence degree computation unit which computes the degree of correspondence for each pair of the feature points, wherein one of the feature points in the each pair is each of one or more of the feature points extracted by the feature point extraction unit from one of two images taken by the camera at different times, and another of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:FPJr55Dyh1AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automated mouse behavior recognition using VGG features and LSTM networks",
            "Publication year": 2016,
            "Publication url": "https://homepages.inf.ed.ac.uk/rbf/VAIB16PAPERS/vaibkramida.pdf",
            "Abstract": "We present a mouse behavior classification method using a recurrent neural network with the long short-term memory (LSTM) model. The experimental hardware used to collect the data is a custom mouse cage with four stereo-camera pairs in each wall. Using as input the different videos, our computational method employs a so-called end-to-end learning approach: visual features from pre-trained convolutional neural networks are extracted from each image frame, and used to train a customized LSTM-based model in weakly-supervised fashion, to recognize different behaviors of the mouse in the videos. Future extensions of the system will incorporate 3D feature information from the stereo cameras and online classification functionality.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:MIg0yeAD4ggC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Semantic clusters combined with kinematics: The case of english and modern greek motion verbs",
            "Publication year": 2014,
            "Publication url": "https://www.degruyter.com/document/doi/10.2478/9788376560762.p318/html",
            "Abstract": "We combine corpus driven linguistic knowledge with experimentally obtained sensorimotor data in an effort to better specify the minimum conceptual representation of a motion event that distinguishes it from all other events, which still are largely presented in a vague and not objectively calculated quantitative method. We use American English and Modern Greek data as a case study, in order to focus on the clustering of motor actions and its correspondence to previous linguistic classifications of both languages.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:QYdC8u9Cj1oC",
            "Publisher": "De Gruyter Open Poland"
        },
        {
            "Title": "Attribute-based transfer learning for object categorization with zero/one training example",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-15555-0_10",
            "Abstract": "This paper studies the one-shot and zero-shot learning problems, where each object category has only one training example or has no training example at all. We approach this problem by transferring knowledge from known categories (a.k.a source categories) to new categories (a.k.a target categories) via object attributes. Object attributes are high level descriptions of object categories, such as color, texture, shape, etc. Since they represent common properties across different categories, they can be used to transfer knowledge from source categories to target categories effectively. Based on this insight, we propose an attribute-based transfer learning framework in this paper. We first build a generative attribute model to learn the probabilistic distributions of image features for each attribute, which we consider as attribute priors. These attribute priors can be used to (1) classify unseen images of target \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:35r97b3x0nAC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Grounding Concrete Motion Concepts with a Linguistic Framework",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-87881-0_1",
            "Abstract": "We have empirically discovered that the space of human actions has a linguistic framework. This is a sensorimotor space consisting of the evolution of the joint angles of the human body in movement. The space of human activity has its own phonemes, morphemes, and sentences formed by syntax. This has implications for the grounding of concrete motion concepts. We present a Human Activity Language (HAL) for symbolic non-arbitrary representation of visual and motor information. In phonology, we define basic atomic segments that are used to compose human activity. We introduce the concept of a kinetological system and propose basic properties for such a system: compactness, view-invariance, reproducibility, and reconstructivity. In morphology, we extend sequential language learning to incorporate associative learning with our parallel learning approach. Parallel learning solves the problem of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:uWiczbcajpAC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Spatio-temporal analysis of human faces using multi-resolution subdivision surfaces",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-45404-7_9",
            "Abstract": "We demonstrate a method to automatically extract spatio-temporal descriptions of human faces from synchronized and calibrated multi-view sequences. The head is modeled by a time-varying multi-resolution subdivision surface that is fitted to the observed person using spatio-temporal multi-view stereo information, as well as contour constraints. The stereo data is utilized by computing the normalized correlation between corresponding spatio-temporal image trajectories of surface patches, while the contour information is determined using incremental background subtraction. We globally optimize the shape of the spatio-temporal surface in a coarse-to-fine manner using the multiresolution structure of the subdivision mesh. The method presented incorporates the available image information in a unified framework and automatically reconstructs accurate spatio-temporal representations of complex non \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:Z5m8FVwuT1cC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Chromatic induction and perspective distortion",
            "Publication year": 2005,
            "Publication url": "https://jov.arvojournals.org/article.aspx?articleid=2131776",
            "Abstract": "A pattern presented by Robertson (1996), consisting of yellow and blue square waves with red squares superimposed on either the blue or yellow bands give rise to assimilation effects similar as in the White illusion. The red squares on the yellow bands appear more blue, and the red squares on the blue bands appear more yellow. Varying the position and orientation of the pattern in space changes the effect. For example, slanting the pattern or increasing the distance to the pattern in space increases the effect. But rotating the pattern by 90 degrees and slanting it, nearly eliminates the effect. These changes in color appearance can be explained as the result of averaging in receptive field of extended size.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:HtEfBTGE9r8C",
            "Publisher": "The Association for Research in Vision and Ophthalmology"
        },
        {
            "Title": "Landmark-Based Navigation and the Acquisition of Environmental Models: Edward M. Riseman, Allen R. Hanson, J. Ross Beveridge, Rakesh (TEDDY) Kumar, and Harpreet Sawhney",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=16620513501950745545&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:u3T1itk59dMC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Learning visual motion segmentation using event surfaces",
            "Publication year": 2020,
            "Publication url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Mitrokhin_Learning_Visual_Motion_Segmentation_Using_Event_Surfaces_CVPR_2020_paper.html",
            "Abstract": "Event-based cameras have been designed for scene motion perception-their high temporal resolution and spatial data sparsity converts the scene into a volume of boundary trajectories and allows to track and analyze the evolution of the scene in time. Analyzing this data is computationally expensive, and there is substantial lack of theory on dense-in-time object motion to guide the development of new algorithms; hence, many works resort to a simple solution of discretizing the event stream and converting it to classical pixel maps, which allows for application of conventional image processing methods. In this work we present a Graph Convolutional neural network for the task of scene motion segmentation by a moving camera. We convert the event stream into a 3D graph in (x, y, t) space and keep per-event temporal information. The difficulty of the task stems from the fact that unlike in metric space, the shape of an object in (x, y, t) space depends on its motion and is not the same across the dataset. We discuss properties of of the event data with respect to this 3D recognition problem, and show that our Graph Convolutional architecture is superior to PointNet++. We evaluate our method on the state of the art event-based motion segmentation dataset-EV-IMO and perform comparisons to a frame-based method proposed by its authors. Our ablation studies show that increasing the event slice width improves the accuracy, and how subsampling and edge configurations affect the network performance.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:3A3nxV7CjKIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The Language of Motion MoCap Ontology",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-17798-0_56",
            "Abstract": " We present a systematically organized MoCap Collection, especially designed to serve grounding language to action and provide linguistics with objectively measured method and parameters (among others path, manner, trajectory, direction), in order to solve long lasting theoretical questions, such as the minimum conceptual representation of verbal events, the binary nature of argument-adjunct, cross-lingual typologies, etc. We enriched the ontology with video data and avatars in various formats for behavioral studies. In the remaining of the paper, we present applications related to the present ontology. ",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:xm0LlTxljI0C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Hybrid Backpropagation Parallel Reservoir Networks",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2010.14611",
            "Abstract": "In many real-world applications, fully-differentiable RNNs such as LSTMs and GRUs have been widely deployed to solve time series learning tasks. These networks train via Backpropagation Through Time, which can work well in practice but involves a biologically unrealistic unrolling of the network in time for gradient updates, are computationally expensive, and can be hard to tune. A second paradigm, Reservoir Computing, keeps the recurrent weight matrix fixed and random. Here, we propose a novel hybrid network, which we call Hybrid Backpropagation Parallel Echo State Network (HBP-ESN) which combines the effectiveness of learning random temporal features of reservoirs with the readout power of a deep neural network with batch normalization. We demonstrate that our new network outperforms LSTMs and GRUs, including multi-layer \"deep\" versions of these networks, on two complex real-world multi-dimensional time series datasets: gesture recognition using skeleton keypoints from ChaLearn, and the DEAP dataset for emotion recognition from EEG measurements. We show also that the inclusion of a novel meta-ring structure, which we call HBP-ESN M-Ring, achieves similar performance to one large reservoir while decreasing the memory required by an order of magnitude. We thus offer this new hybrid reservoir deep learning paradigm as a new alternative direction for RNN learning of temporal or sequential data.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:PuOEWVtPfzwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An embodied tutoring system for literal vs. metaphorical concepts",
            "Publication year": 2018,
            "Publication url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02254/full",
            "Abstract": "\u2022 In this paper we combine motion captured data with linguistic notions in a game-like intelligent tutoring system, in order to help elementary school students to better differentiate literal from metaphorical uses of motion verbs, based on embodied information. In addition to the thematic goal, we intend to improve young students\u2019 attention and spatiotemporal memory, by presenting sensorimotor data experimentally collected from thirty two participants in our motion capturing labs. Furthermore, we examine the accomplishment of game\u2019s goals and compare them to curriculum\u2019s approach. Sixty nine elementary school students were randomly divided in two experimental groups (game and traditional) and one control group, which did not undergo any intervention. All groups were tested in pre and posttests. Even though the diagnostic pretests present a uniform picture, two way analysis of variance suggests that the experimental groups showed progress in posttests and, more specifically, game group remarkably progressed in the verbs/actions presented during the intervention. Furthermore, in the game condition the participants needed gradually shorter period of time to identify the avatar\u2019s actions. This finding was considered as a first indication of attentional and spatiotemporal memory\u2019s improvement, while the game\u2019s assistance features cultivated students\u2019 metacognitive perception.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:1r-w4gtu6w8C",
            "Publisher": "Frontiers"
        },
        {
            "Title": "Learning the semantics of manipulation action",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1512.01525",
            "Abstract": "In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs -calculus; (2) enable a probabilistic semantic parsing schema to learn the -calculus representation of manipulation action from an annotated action corpus of videos; (3) use (1) and (2) to develop a system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:qwy9JoKyICEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Motion segmentation using occlusions",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1432727/",
            "Abstract": "We examine the key role of occlusions in finding independently moving objects instantaneously in a video obtained by a moving camera with a restricted field of view. In this problem, the image motion is caused by the combined effect of camera motion (egomotion), structure (depth), and the independent motion of scene entities. For a camera with a restricted field of view undergoing a small motion between frames, there exists, in general, a set of 3D camera motions compatible with the observed flow field even if only a small amount of noise is present, leading to ambiguous 3D motion estimates. If separable sets of solutions exist, motion-based clustering can detect one category of moving objects. Even if a single inseparable set of solutions is found, we show that occlusion information can be used to find ordinal depth, which is critical in identifying a new class of moving objects. In order to find ordinal depth \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:70eg2SAEIzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robotics: science and systems VII",
            "Publication year": 2012,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=Ziy81kH3KfUC&oi=fnd&pg=PR5&dq=info:SXSKyac6S6AJ:scholar.google.com&ots=Z-cxrV1l2S&sig=B5kZ9tppeDK5CZGB2IPvwVTD7Dk",
            "Abstract": "Papers from a flagship conference reflect the latest developments in the field, including work in such rapidly advancing areas as human-robot interaction and formal methods. Robotics: Science and Systems VII spans a wide spectrum of robotics, bringing together researchers working on the algorithmic or mathematical foundations of robotics, robotics applications, and analysis of robotics systems. This volume presents the proceedings of the seventh annual Robotics: Science and Systems conference, held in 2011 at the University of Southern California. The papers presented cover a wide range of topics in robotics, spanning mechanisms, kinematics, dynamics and control, human-robot interaction and human-centered systems, distributed systems, mobile systems and mobility, manipulation, field robotics, medical robotics, biological robotics, robot perception, and estimation and learning in robotic systems. The conference and its proceedings reflect not only the tremendous growth of robotics as a discipline but also the desire in the robotics community for a flagship event at which the best of the research in the field can be presented.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:m4fbC6XIj1kC",
            "Publisher": "MIT Press"
        },
        {
            "Title": "Deepiu: an architecture for image understanding",
            "Publication year": 2016,
            "Publication url": "http://www.public.asu.edu/~cbaral/papers/acs2016.pdf",
            "Abstract": "Image Understanding is fundamental to systems that need to extract contents and infer concepts from images. In this paper, we develop an architecture for understanding images, through which a system can recognize the content and the underlying concepts of an image and, reason and answer questions about both using a visual module, a reasoning module, and a commonsense knowledge base. In this architecture, visual data combines with background knowledge and; iterates through visual and reasoning modules to answer questions about an image or to generate a textual description of an image. We first provide motivations of such a Deep Image Understanding architecture and then, we describe the necessary components it should include. We also introduce our own preliminary implementation of this architecture and empirically show how this more generic implementation compares with a recent end-to-end Neural approach on specific applications. We address the knowledge-representation challenge in such an architecture by representing an image using a directed labeled graph (called Scene Description Graph). Our implementation uses generic visual recognition techniques and commonsense reasoning1 to extract such graphs from images. Our experiments show that the extracted graphs capture the syntactic and semantic content of an image with reasonable accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:SAZ1SQo2q1kC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Measuring 1st order stretchwith a single filter",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4517758/",
            "Abstract": "We analytically develop a filter that is able to measure the linear stretch of the transformation around a point, and present results of applying it to real signals. We show that this method is a real-time alternative solution for measuring local signal transformations. Experimentally, this method can accurately measure stretch, however, it is sensitive to shift.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:w1MjKQ0l0TYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Computational video",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/article/10.1007/s00371-003-0200-8",
            "Abstract": "The past few years have witnessed a number of fundamental developments in the processing of perceptual data. This, in conjunction with advances in technology, has led to a dramatic increase in a variety of applications ranging from video manipulation to virtual reality. More specifically, application of geometry, statistics, control theory, physics and distributed processing to visual input opens new avenues and creates new methodologies for a variety of important, novel areas. Examples include video editing, teleimmersion and virtual reality, three-dimensional video and photography, synthetic worlds and the synergistic mixture of graphics with computer vision.In video editing one needs to alter the content of the video by deleting or inserting particular objects; this requires recovering and maintaining relationships between different coordinate systems. If the camera that captured the video segment of interest was \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:gsN89kCJA0AC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Center for Automation Research. University of Maryland, College Park",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=7503365792862775681&hl=en&oi=scholarr",
            "Abstract": "Motion segmentation is the problem of finding parts of the scene which possess independent 3D motion (such as people, animals or other objects like vehicles). This process is conceptually straightforward if the camera is stationary, and is often referred to as background subtraction. However, if the camera itself is also moving, then the problem becomes more complicated, since the image motion is generated by the combined effects of camera motion, structure and the motion of the independently moving objects. Isolating the contribution of each of these three factors is needed to solve the more general independent motion problem. which involves motion segmentation (finding the moving objects) and also finding their 3D motion. In this chapter, we shall restrict ourselves to the problem of finding moving objects only and not worry about finding their 3D motion. In the beginning, we present our philosophy that visual problems such as motion segmentation are inextricably linked with other problems in vision, and must be approached with a compositional outlook which attempts to solve multiple problems. simultaneously. This is followed by a brief review of existing algorithms which detect independently moving objects. The main body of this chapter presents our approach to motion segmentation1 which classifies moving objects and demonstrates that motion segmentation is compositional and is not about motion alone, but can also utilize information from sources such as occlusions to detect a wider array of moving objects.1 This chapter is based on our paper which is due to appear in the IEEE Transactions on Pattern Analysis and Machine \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:tHtfpZlB6tUC",
            "Publisher": "Springer Science & Business Media"
        },
        {
            "Title": "Active Perception",
            "Publication year": 2001,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/047134608X.W5515",
            "Abstract": "The sections in this article are",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:URolC5Kub84C",
            "Publisher": "John Wiley & Sons, Inc."
        },
        {
            "Title": "2014 Index",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6960942/",
            "Abstract": "This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the last 3 years, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:L24QuVWYgZ0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning shift-invariant sparse representation of actions",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5539977/",
            "Abstract": "A central problem in the analysis of motion capture (MoCap) data is how to decompose motion sequences into primitives. Ideally, a description in terms of primitives should facilitate the recognition, synthesis, and characterization of actions. We propose an unsupervised learning algorithm for automatically decomposing joint movements in human motion capture (MoCap) sequences into shift-invariant basis functions. Our formulation models the time series data of joint movements in actions as a sparse linear combination of short basis functions (snippets), which are executed (or \u201cactivated\u201d) at different positions in time. Given a set of MoCap sequences of different actions, our algorithm finds the decomposition of MoCap sequences in terms of basis functions and their activations in time. Using the tools of L 1  minimization, the procedure alternately solves two large convex minimizations: Given the basis functions, a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:UHK10RUVsp4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Vision During Action: Extracting Contact and Motion from Manipulation Videos\u2014Toward Parsing Human Activity",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-46732-6_9",
            "Abstract": "When we physically interact with our environment using our hands, we touch objects and force them to move: contact and motion are defining properties of manipulation. In this paper, we present an active, bottom-up method for the detection of actor\u2013object contacts and the extraction of moved objects and their motions in RGBD videos of manipulation actions. At the core of our approach lies non-rigid registration: we continuously warp a point cloud model of the observed scene to the current video frame, generating a set of dense 3D point trajectories. Under loose assumptions, we employ simple point cloud segmentation techniques to extract the actor and subsequently detect actor\u2013environment contacts based on the estimated trajectories. For each such interaction, using the detected contact as an attention mechanism, we obtain an initial motion segment for the manipulated object by clustering trajectories \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:kUhpeDhEZMUC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Vision During Action: Giulio Sandin!, Francesca Gandolfo, Enrico Grossoand Massimo Tistarelli",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=15896072920649172301&hl=en&oi=scholarr",
            "Abstract": "This book defines the emerging field of Active Perception which calls for studying perception coupled with action. It is devoted to technical problems related to the design and analysis of intelligent systems possessing perception such as the existing biological organisms and the\" seeing\" machines of the future. Since the appearance of the first technical results on active vision, researchers began to realize that perception--and intelligence in general--is not transcendental and disembodied. It is becoming clear that in the effort to build intelligent visual systems, consideration must be given to the fact that perception is intimately related to the physiology of the perceiver and the tasks that it performs. This viewpoint--known as Purposive, Qualitative, or Animate Vision--is the natural evolution of the principles of Active Vision. The seven chapters in this volume present various aspects of active perception, ranging from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:EBV337fEn3EC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Network Deconvolution",
            "Publication year": 2019,
            "Publication url": "https://par.nsf.gov/biblio/10287573",
            "Abstract": "Convolution is a central operation in Convolutional Neural Networks (CNNs), which applies a kernel to overlapping regions shifted across the image. However, because of the strong correlations in real-world image data, convolutional kernels are in effect re-learning redundant data. In this work, we show that this redundancy has made neural network training challenging, and propose network deconvolution, a procedure which optimally removes pixel-wise and channel-wise correlations before the data is fed into each layer. Network deconvolution can be efficiently calculated at a fraction of the computational cost of a convolution layer. We also show that the deconvolution filters in the first layer of the network resemble the center-surround structure found in biological neurons in the visual regions of the brain. Filtering with such kernels results in a sparse representation, a desired property that has been missing in the training of neural networks. Learning from the sparse representation promotes faster convergence and superior results without the use of batch normalization. We apply our network deconvolution operation to 10 modern neural network models by replacing batch normalization within each. Extensive experiments show that the network deconvolution operation is able to deliver performance improvement in all cases on the CIFAR-10, CIFAR-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:AFmTUeZ1pmEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An Embodied Intelligent Tutor for Literal Concepts' Recognition.",
            "Publication year": 2018,
            "Publication url": "https://cogsci.mindmodeling.org/2018/papers/0476/",
            "Abstract": "e combine motion captured data with linguistic notions in a game-like intelligent tutoring system, in order to help elementary school students to better differentiate literal from metaphorical uses of motion verbs, based on embodied information. In addition to the thematic goal, we intend to improve young students\u2019 attention and spatiotemporal memory, by presenting sensorimotor data experimentally collected in our motion capturing labs. Furthermore, we examine the accomplishment of game\u2019s goals and compare it to curriculum\u2019s approach. Sixty nine elementary school students were randomly divided in two experimental groups (game and traditional) and one control group. Two way analysis of variance suggests that the experimental groups showed progress in posttests, with game group showing remarkable progress especially in the verbs/actions presented during the intervention. This finding was \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:6VlyvFCUEfcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deep Reservoir Networks with Learned Hidden Reservoir Weights using Direct Feedback Alignment",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2010.06209",
            "Abstract": "Deep Reservoir Computing has emerged as a new paradigm for deep learning, which is based around the reservoir computing principle of maintaining random pools of neurons combined with hierarchical deep learning. The reservoir paradigm reflects and respects the high degree of recurrence in biological brains, and the role that neuronal dynamics play in learning. However, one issue hampering deep reservoir network development is that one cannot backpropagate through the reservoir layers. Recent deep reservoir architectures do not learn hidden or hierarchical representations in the same manner as deep artificial neural networks, but rather concatenate all hidden reservoirs together to perform traditional regression. Here we present a novel Deep Reservoir Network for time series prediction and classification that learns through the non-differentiable hidden reservoir layers using a biologically-inspired backpropagation alternative called Direct Feedback Alignment, which resembles global dopamine signal broadcasting in the brain. We demonstrate its efficacy on two real world multidimensional time series datasets.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:I858iXPj1OkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A roadmap to the integration of early visual modules",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s11263-006-8890-9.pdf",
            "Abstract": "By examining the problem of image correspondence (binocular stereo and optical flow) and its relationship with other modules such as segmentation, shape and depth estimation, occlusion detection, and local signal processing, we argue that early visual modules are entangled in chicken-and-egg relationships, and unraveling these necessitates a compositional approach. In this paper, we present compositional algorithms which can match images containing slanted surfaces and images having different contrast, while simultaneously solving other problems as part of the same process. Ultimately, our goal is to motivate the application of the compositional approach to unify many other early visual modules. Experimental results have been presented on a large variety of stereo and motion images, including images with contrast mismatch and images containing untextured slanted surfaces.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:fPk4N6BV_jEC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Contour detection and characterization for asynchronous event sensors",
            "Publication year": 2015,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Barranco_Contour_Detection_and_ICCV_2015_paper.html",
            "Abstract": "The bio-inspired, asynchronous event-based dynamic vision sensor records temporal changes in the luminance of the scene at high temporal resolution. Since events are only triggered at significant luminance changes, most events occur at the boundary of objects and their parts. The detection of these contours is an essential step for further interpretation of the scene. This paper presents an approach to learn the location of contours and their border ownership using Structured Random Forests on event-based features that encode motion, timing, texture, and spatial orientations. The classifier integrates elegantly information over time by utilizing the classification results previously computed. Finally, the contour detection and boundary assignment are demonstrated in a layer-segmentation of the scene. Experimental results demonstrate good performance in boundary detection and segmentation.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:v6i8RKmR8ToC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Discovering a language for human activity",
            "Publication year": 2005,
            "Publication url": "https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-05/FS05-05-010.pdf",
            "Abstract": "We present a roadmap to a Human Activity Language (HAL) for symbolic manipulation of visual and motor information in a sensory-motor system model. The visual perception subsystem translates a visual representation of action into our visuo-motor language. One instance of this perception process could be achieved by a Motion Capture system. We captured almost 90 different human actions in order to have empirical data that could validate and support our embodied language for movement and activity. The embodiment of the language serves as the interface between visual perception and the motor subsystem. The visuomotor language is defined using a linguistic approach. In phonology, we define basic atomic segments that are used to compose human activity. Phonological rules are modeled as a finite automaton. In morphology, we study how visuomotor phonemes are combined to form strings representing human activity and to generate a higher-level morphological grammar. This compact grammar suggests the existence of lexical units working as visuo-motor subprograms. In syntax, we present a model for visuo-motor sentence construction where the subject corresponds to the active joints (noun) modified by a posture (adjective). A verbal phrase involves the representation of the human activity (verb) and timing coordination among different joints (adverb).",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:CHSYGLWDkRkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The Manipulation Action Grammar: A Key to Intelligent Robots",
            "Publication year": 2016,
            "Publication url": "https://smartech.gatech.edu/handle/1853/54719",
            "Abstract": "Humanoid robots will need to learn the actions that humans perform. They will need to recognize these actions when they see them and they will need to perform these actions themselves. In this presentation, it is proposed that this learning task can be achieved using the manipulation grammar.  Context-free grammars have been in fashion in linguistics because they provide a simple and precise mechanism for describing the methods by which phrases in some natural language are built from smaller blocks. Also, the basic recursive structure of natural languages, the way in which clauses nest inside other clauses, and the way in which lists of adjectives and adverbs are followed by nouns and verbs, is described exactly. Similarly, for manipulation actions, every complex activity is built from smaller blocks involving hands and their movements, as well as objects, tools and the monitoring of their state. Thus, interpreting a \u201cseen\u201d action is like understanding language, and executing an action from knowledge in memory is like producing language. Several experiments will be shown interpreting human actions in the arts and crafts or assembly domain, through a parsing of the visual input, on the basis of the manipulation grammar. This parsing, in order to be realized, requires a network of visual processes that attend to objects and tools, segment them and recognize them, track the moving objects and hands, and monitor the state of objects to calculate goal completion. These processes will also be explained and we will conclude with demonstrations of robots learning how to perform tasks by watching videos of relevant human activities.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:isU91gLudPYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spatio-temporal stereo using multi-resolution subdivision surfaces",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/article/10.1023/A:1014597925429",
            "Abstract": "We present a method to automatically extract spatio-temporal descriptions of moving objects from synchronized and calibrated multi-view sequences. The object is modeled by a time-varying multi-resolution subdivision surface that is fitted to the image data using spatio-temporal multi-view stereo information, as well as contour constraints. The stereo data is utilized by computing the normalized correlation between corresponding spatio-temporal image trajectories of surface patches, while the contour information is determined using incremental segmentation of the viewing volume into object and background. We globally optimize the shape of the spatio-temporal surface in a coarse-to-fine manner using the multi-resolution structure of the subdivision mesh. The method presented incorporates the available image information in a unified framework and automatically reconstructs accurate spatio-temporal \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:M05iB0D1s5AC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "2000 reviewers list",
            "Publication year": 2001,
            "Publication url": "https://www.computer.org/csdl/journal/tp/2001/01/i0091/13rRUy0qnHp",
            "Abstract": "2000 Reviewers List Page 1 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE \nINTELLIGENCE, VOL. 23, NO. 1, JANUARY 2001 91 2000 Reviewers List \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \n\u2726 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 A Acton, Scott T. Aggarwal, Jake K. Ahuja, Narendra Akiyama, Koichiro \nAloimonos, Yiannis Alvarez, Luis Amin, Adnan Amini, Amir Amir, Arnon Amit, Yali Anandan, \nP. Ando, Shigeru Angelopoulou, Elli Asada, Minoru Astrom, Kalle Atkeson, Chris G. Avidan, \nShai Avi-Itzhak, Benjamin Ayer, Serge B Baird, Henry S. Baker, Simon Balasubramanian, \nRamprasad Bangham, Andrew Barrett, William Bartlett, Marian Stewart Basford, Kaye Basu, \nMitra Bayro, Eduardo Beardsley, Paul Belhumeur, Peter N. Ben-Arie, Jezekiel Benediktson, \nJon Bergen, Jim Berthilsson, Rikard Beucher, Serge Beveridge, J. Ross Beymer, David \nBhanu, Bir Bharath, A. Bhattacharya, Prabir Biehl, Michael Bigun, Josef Bishop, M. Bishop, -'\u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:IUKN3-7HHlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An experimental study of color-based segmentation algorithms based on the mean-shift concept",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-15552-9_37",
            "Abstract": "We point out a difference between the original mean-shift formulation of Fukunaga and Hostetler and the common variant in the computer vision community, namely whether the pairwise comparison is performed with the original or with the filtered image of the previous iteration. This leads to a new hybrid algorithm, called Color Mean Shift, that roughly speaking, treats color as Fukunaga\u2019s algorithm and spatial coordinates as Comaniciu\u2019s algorithm. We perform experiments to evaluate how different kernel functions and color spaces affect the final filtering and segmentation results, and the computational speed, using the Berkeley and Weizmann segmentation databases. We conclude that the new method gives better results than existing mean shift ones on four standard comparison measures ( improvement on RAND and BDE measures respectively for color images), with slightly higher running times \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:tKAzc9rXhukC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "2004 Reviewers List",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1359761/",
            "Abstract": "Reviewers list Page 1 A Sadegh Abbasi Mohamed Abdelmottaleb Keiichi Abe Driss \nAboutajdine Kannan Achan Bernard Achermann Scott T. Acton Lourdes Agapito Gaurav \nAggarwal Manoj Aggarwal Sung Joon Ahn Timo Ahonen Narendra Ahuja Mayer Aladjem Olivier \nAlata Fuad Alkoot Peter Allen Andres Almansa Giovanni Aloisio Ethem Alpaydin Luis Alvarez \nGuiseppe Amato Juan Amengual Adnan Amin Amir Amini Yali Amit Georgios Anagnostopoulos \nShigeru Ando Aya Aner-Wolf Elli Angelopoulou Jes\u00fas Angulo L\u00f3pez Adnan Ansar Andras \nAntos Mirko Appel Helder Araujo Klaus Arbter Okan Arikan Tetsuo\u2019 Asano Ognian Asparoukhov \nKalle Astrom Jonas August Shai Avidan Yannis Avrithis B Alireza Bab-Hadiashar Francis Bach \nAndrew Bagdanov Claus Bahlmann Donald Bailey Robert Bailey Henry Baird Simon Baker \nYoganand Balagurunathan Nikhil Balakrishnan M. Balasubramanian Dana H. Ballard S. S. -\u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:BAanoTsO0WEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Visual navigation: from biological systems to unmanned ground vehicles",
            "Publication year": 2013,
            "Publication url": "https://www.taylorfrancis.com/books/mono/10.4324/9780203774243/visual-navigation-yiannis-aloimonos",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:LkGwnXOMwfcC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "What can i do around here? deep functional scene understanding for cognitive robots",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7989535/",
            "Abstract": "For robots that have the capability to interact with the physical environment through their end effectors, understanding the surrounding scenes is not merely a task of image classification or object recognition. To perform actual tasks, it is critical for the robot to have a functional understanding of the visual scene. Here, we address the problem of localization and recognition of functional areas in an arbitrary indoor scene, formulated as a two-stage deep learning based detection pipeline. A new scene functionality testing-bed, which is compiled from two publicly available indoor scene datasets, is used for evaluation. Our method is evaluated quantitatively on the new dataset, demonstrating the ability to perform efficient recognition of functional areas from arbitrary indoor scenes. We also demonstrate that our detection model can be generalized to novel indoor scenes by cross validating it with images from two different \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:kJDgFkosVoMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A bug\u2019s-eye view",
            "Publication year": 2020,
            "Publication url": "http://users.umiacs.umd.edu/~fer/postscript/science-robotics-bugseye.pdf",
            "Abstract": "Biorobotics aims to mimic the characteristics of living organisms to enable new robot designs. Insects have been a popular focus of the field because of their amazing sensing and actuation capabilities. However, manufacturing miniaturized actuators at low weight and power is a very challenging task. Solutions to this problem will enable the design of microrobots that can efficiently move and perceive. Writing in this issue of Science Robotics, Iyer et al.(1) report a low-power rotatable vision system for live insects and insect-scale robots. Their system is lightweight (248 mg), can be controlled wirelessly, and is able to record images to a Bluetooth device. A key innovation is the use of piezo actuators as capacitors with low leakage; this enables the actuators to hold their state without power input. The other components are a customdesigned lens and antenna and off-the-shelf sensor and Bluetooth chips connected via a custom interface. The vision system is coupled with an accelerometer that triggers image capture only when the insect moves. This low-power configuration enabled operational times of up to 6 hours when the system was attached to freely walking beetles (Fig. 1A), allowing the insect\u2019s daily routine to be observed, thus potentially demonstrating new opportunities for the field of entomology. The researchers\u2019 vision system was also mounted on a microrobot to record images at 1 frame/s (Fig 1B), demonstrating that it is now feasible to equip insect-scale robots with what scientists and engineers have named \u201cActive Vision\u201d(2\u20134). An active observer is one that manipulates the parameters of the sensory apparatus (focal length, field of view \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:g8uWPOAv7ggC",
            "Publisher": "Science Robotics"
        },
        {
            "Title": "Appreciation to IJCV Reviewers",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s11263-015-0871-4.pdf",
            "Abstract": "For helping us deliver timely decisions to our authors, the Editors-in-Chief and Publisher would like to thank the following individuals that contributed reviews between October 1, 2014 and October 1, 2015. We applaud your efforts and dedication to the community.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:CNPyR2KL9-0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "The cognitive dialogue: A new model for vision implementing common sense reasoning",
            "Publication year": 2015,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0262885614001656",
            "Abstract": "We propose a new model for vision, where vision is part of an intelligent system that reasons. To achieve this we need to integrate perceptual processing with computational reasoning and linguistics. In this paper we present the basics of this formalism.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:pS0ncopqnHgC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Stereo correspondence with slanted surfaces: critical implications of horizontal slant",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1315082/",
            "Abstract": "We examine the stereo correspondence problem in the presence of slanted scene surfaces. In particular we highlight a previously overlooked geometric fact: a horizontally slanted surface (i.e. having depth variation in the direction of the separation of the two cameras) will appear horizontally stretched in one image as compared to the other image. Thus, while corresponding two images, N pixels on a scanline in one image may correspond to a different number of pixels M in the other image. This leads to three important modifications to existing stereo algorithms: (a) due to unequal sampling, intensity matching metrics such as the popular Birchfield-Tomasi procedure must be modified, (b) unequal numbers of pixels in the two images must be allowed to correspond to each other, and (c) the uniqueness constraint, which is often used for detecting occlusions, must be changed to a 3D uniqueness constraint. This paper \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:g5m5HwL7SMYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Moving obstacle detection using cameras for driver assistance system",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5548125/",
            "Abstract": "Moving obstacles have potentially higher risks of collision than stationary obstacles in traffic. Therefore, it is meaningful to detect moving obstacles by sensors equipped on a car for drive assistance applications. We propose two algorithms to detect moving obstacles using camera(s) depending on the relative motion between cameras and obstacles. Since camera(s) moves along with the subjective car, it is challenging to find actually moving obstacles in image sequences. The first algorithm identifies moving obstacle regions in images by checking conflicts between image motion and epipolar constraint when obstacles move in different direction from cameras' motion. The second algorithm identifies moving obstacle regions in images by finding disparity differences between stereo and motion especially when obstacles move in same direction as cameras' motion. Experiments show not only qualitative performance \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:86PQX7AUzd4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "2017 ICCV challenge: Detecting symmetry in the wild",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_ICCV_2017_workshops/w24/html/Funk_2017_ICCV_Challenge_ICCV_2017_paper.html",
            "Abstract": "Motivated by various new applications of computational symmetry in computer vision and in an effort to advance machine perception of symmetry in the wild, we organize the third international symmetry detection challenge at ICCV 2017 after the CVPR 2011/2013 symmetry detection competitions. Our goal is to gauge the progress in computational symmetry with continuous benchmarking of both new algorithms and datasets, as well as more polished validation methodology. Different from previous years, this time we expand our training/testing data sets to include 3D data, and establish the most comprehensive and largest annotated datasets for symmetry detection to date; we also expand the types of symmetries to include densely-distributed and medial-axis-like symmetries; furthermore, we establish a challenge-and-paper dual track mechanism where both algorithms and articles on symmetry-related research are solicited. In this report, we provide a detailed summary of our evaluation methodology for each type of symmetry detection algorithm validated. We demonstrate and analyze quantified detection results in terms of precision-recall curves and F-measures for all algorithms evaluated. We also offer a short survey of the paper-track submissions accepted for our 2017 symmetry challenge.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:nqdriD65xNoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Structure from motion: Beyond the epipolar constraint",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/article/10.1023/A:1008132107950",
            "Abstract": "The classic approach to structure from motion entails a clear separation between motion estimation and structure estimation and between two-dimensional (2D) and three-dimensional (3D) information. For the recovery of the rigid transformation between different views only 2D image measurements are used. To have available enough information, most existing techniques are based on the intermediate computation of optical flow which, however, poses a problem at the locations of depth discontinuities. If we knew where depth discontinuities were, we could (using a multitude of approaches based on smoothness constraints) accurately estimate flow values for image patches corresponding to smooth scene patches; but to know the discontinuities requires solving the structure from motion problem first. This paper introduces a novel approach to structure from motion which addresses the processes of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:abG-DnoFyZgC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "2015 Index",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7331690/",
            "Abstract": "This index covers all technical items\u2014papers, correspondence, reviews, etc.\u2014that appeared in this periodical during 2013\u20132015, and items from previous years that were commented upon or corrected in 2013\u20132015. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors\u2019 names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author\u2019s name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:In6cVmBjs0IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Appreciation to IJCV Reviewers",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s11263-014-0797-2.pdf",
            "Abstract": "For helping us deliver timely decisions to our authors, the Editors-in-Chief and Publisher would like to thank the following individuals that contributed reviews between October 1st, 2013 and October 1st, 2014. We applaud your efforts and dedication to the community.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:YlPif8NxrbYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Affordance detection of tool parts from geometric features",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7139369/",
            "Abstract": "As robots begin to collaborate with humans in everyday workspaces, they will need to understand the functions of tools and their parts. To cut an apple or hammer a nail, robots need to not just know the tool's name, but they must localize the tool's parts and identify their functions. Intuitively, the geometry of a part is closely related to its possible functions, or its affordances. Therefore, we propose two approaches for learning affordances from local shape and geometry primitives: 1) superpixel based hierarchical matching pursuit (S-HMP); and 2) structured random forests (SRF). Moreover, since a part can be used in many ways, we introduce a large RGB-Depth dataset where tool parts are labeled with multiple affordances and their relative rankings. With ranked affordances, we evaluate the proposed methods on 3 cluttered scenes and over 105 kitchen, workshop and garden tools, using ranked correlation and a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:JTqpx9DYBaYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "View-invariant identification of pose sequences for action recognition",
            "Publication year": 2004,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.6884&rep=rep1&type=pdf",
            "Abstract": "In this paper, we represent human actions as short sequences of atomic body poses. The knowledge of body pose is stored only implicitly as a set of silhouettes seen from multiple viewpoints; no explicit 3D poses or body models are used, and individual body parts are not identified. Actions and their constituent atomic poses are extracted from a set of multiview multiperson video sequences by an automatic keyframe selection process, and are used to construct a Hidden Markov Model. Given new single viewpoint sequences, we can recognize key pose sequences and changes in viewpoint simultaneously. Recognized pose sequences can potentially be parsed to find actions, if the actions are described by a grammar in which atomic pose pairs act as terminal symbols. We provide experimental results under constant viewpoint and changing viewpoint conditions.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:BJrgspguQaEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Detecting reflectional symmetries in 3d data through symmetrical fitting",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_ICCV_2017_workshops/w24/html/Ecins_Detecting_Reflectional_Symmetries_ICCV_2017_paper.html",
            "Abstract": "Symmetry is ubiquitous in both natural and man-made environments. It reveals redundancies in the structure of the world around us and thus can be used in a variety of visual processing tasks. This paper presents a simple and robust approach to detecting symmetric objects and extract-ing their symmetries from three-dimensional data. Given a 3D mesh of an object, a set of candidate symmetries are proposed first and are then refined, so that they reflect the complete mesh onto itself. We show how our method can be used to detect symmetric objects in scenes consisting of syn-thetic 3D models, as well as 3D scans of real environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:X0DADzN9RKwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Acknowledgement to Reviewers of Robotics in 2014",
            "Publication year": 2015,
            "Publication url": "https://www.mdpi.com/2218-6581/4/1/23/pdf",
            "Abstract": "The statements, opinions and data contained in the journal Robotics are solely those of the individual authors and contributors and not of the publisher and the editor (s). MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:SnGPuo6Feq8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scene statistics and 3D surface perception",
            "Publication year": 2010,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.1661&rep=rep1&type=pdf",
            "Abstract": "The inference of depth information from single images is typically performed by devising models of image formation based on the physics of light interaction and then inverting these models to solve for depth. Once inverted, these models are highly underconstrained, requiring many assumptions such as Lambertian surface reflectance, smoothness of surfaces, uniform albedo, or lack of cast shadows. Little is known about the relative merits of these assumptions in real scenes. A statistical understanding of the joint distribution of real images and their underlying 3D structure would allow us to replace these assumptions and simplifications with probabilistic priors based on real scenes. Furthermore, statistical studies may uncover entirely new sources of information that are not obvious from physical models. Real scenes are affected by many regularities in the environment, such as the natural geometry of objects, the arrangements of objects in space, natural distributions of light, and regularities in the position of the observer. Few current computer vision algorithms for 3D shape inference make use of these trends. Despite the potential usefulness of statistical models and the growing success of statistical methods in vision, few studies have been made into the statistical relationship between images and range (depth) images. Those studies that have examined this relationship in nature have uncovered meaningful and exploitable statistical trends in real scenes which may be useful for designing new algorithms in surface inference, and also for understanding how humans perceive depth in real scenes [32, 18, 46]. In this chapter, we will highlight some \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:RJOyoaXV5v8C",
            "Publisher": "London, UK: Chapman & Hall"
        },
        {
            "Title": "Does the grasp type reveal action intention?",
            "Publication year": 2015,
            "Publication url": "https://jov.arvojournals.org/article.aspx?articleid=2434263",
            "Abstract": "The hand grasp type provides visual information about a person\u2019s intended action. We conducted an experiment to evaluate how accurately humans can judge basic manipulation action categories from static images and how much image information they need to make this judgment. We classified manipulation actions into three basic categories, namely \u2018power action\u2019(including actions, such as lifting, cutting, hitting with force),\u2018precision action\u2019(such as playing an instrument, writing, or stirring with a straw) and \u2018casual movement\u2019(such as showcasing, posing, or gesturing). Images were shown in sequences of three: the first one showing the bare hands segmented from background, the second one showing square patches around the hands, and the third showing the full image. Mechanical Turkers were shown 39 images, and asked to make a judgment on the action category. 57 subjects participated, of which 48 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:SIv7DqKytYAC",
            "Publisher": "The Association for Research in Vision and Ophthalmology"
        },
        {
            "Title": "Robot learning manipulation action plans by \u201cwatching\u201d unconstrained videos from the world wide web",
            "Publication year": 2015,
            "Publication url": "https://ojs.aaai.org/index.php/AAAI/article/view/9671",
            "Abstract": "In order to advance action generation and creation in robots beyond simple learned schemas we need computational tools that allow us to automatically interpret and represent human actions. This paper presents a system that learns manipulation action plans by processing unconstrained videos from the World Wide Web. Its goal is to robustly generate the sequence of atomic actions of seen longer actions in video in order to acquire knowledge for robots. The lower level of the system consists of two convolutional neural network (CNN) based recognition modules, one for classifying the hand grasp type and the other for object recognition. The higher level is a probabilistic manipulation action grammar based parsing module that aims at generating visual sentences for robot manipulation. Experiments conducted on a publicly available unconstrained video dataset show that the system is able to learn manipulation actions by``watching''unconstrained videos with high accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:rTD5ala9j4wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The Argus eye: a new imaging system designed to facilitate robotic tasks of motion",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1371606/",
            "Abstract": "This article describes an imaging system that has been designed to facilitate robotic tasks of motion. The system consists of a number of cameras in a network, arranged so that they sample different parts of the visual sphere. This geometric configuration has provable advantages compared to small field of view cameras for the estimation of the system's own motion and, consequently, the estimation of shape models from the individual cameras. The reason is, inherent ambiguities of confusion between translation and rotation disappear. Pairs of cameras may also be arranged in multiple stereo configurations, which provide additional advantages for segmentation. Algorithms for the calibration of the system and the three-dimensional (3-D) motion estimation are provided.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:F2UWTTQJPOcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "PerMIS\u201912: Synergistic Methods for using Language in Robotics",
            "Publication year": 2012,
            "Publication url": "http://www.umiacs.umd.edu/~cteo/public-shared/permis12_synergisticmethods.pdf",
            "Abstract": "A model of a reasoning process that involves the Visual Executive (VE) and Language Executive (LE). VE provides: observations, low-level feature extraction. LE provides: constraints, plausibility of observations from VE. Each iteration of the \u201cdialog\u201d seeks to optimize a global function related to the task: eg scene/object/action recognition, object segmentation.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:ifOnle78iJkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A cognitive system for understanding human manipulation actions",
            "Publication year": 2014,
            "Publication url": "http://legacydirs.umiacs.umd.edu/~fer/postscript/acs-paper-2014.pdf",
            "Abstract": "This paper describes the architecture of a cognitive system that interprets human manipulation actions from perceptual information (image and depth data) and that includes interacting modules for perception and reasoning. Our work contributes to two core problems at the heart of action understanding:(a) the grounding of relevant information about actions in perception (the perception-action integration problem), and (b) the organization of perceptual and high-level symbolic information for interpreting the actions (the sequencing problem). At the high level, actions are represented with the Manipulation Action Grammar, a context-free grammar that organizes actions as a sequence of sub events. Each sub event is described by the hand, movements, objects and tools involved, and the relevant information about these factors is obtained from biologicallyinspired perception modules. These modules track the hands and objects, and they recognize the hand grasp, objects and actions using attention, segmentation, and feature description. Experiments on a new data set of manipulation actions show that our system extracts the relevant visual information and semantic representation. This representation could further be used by the cognitive agent for reasoning, prediction, and planning.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:EPG8bYD4jVwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the importance of consistency in training deep neural networks",
            "Publication year": 2017,
            "Publication url": "https://arxiv.org/abs/1708.00631",
            "Abstract": "We explain that the difficulties of training deep neural networks come from a syndrome of three consistency issues. This paper describes our efforts in their analysis and treatment. The first issue is the training speed inconsistency in different layers. We propose to address it with an intuitive, simple-to-implement, low footprint second-order method. The second issue is the scale inconsistency between the layer inputs and the layer residuals. We explain how second-order information provides favorable convenience in removing this roadblock. The third and most challenging issue is the inconsistency in residual propagation. Based on the fundamental theorem of linear algebra, we provide a mathematical characterization of the famous vanishing gradient problem. Thus, an important design principle for future optimization and neural network design is derived. We conclude this paper with the construction of a novel contractive neural network.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:_AeoHAGD03cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Principles of Animate Vision: Dana H. Ballard and Christopher M. Brown",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=274958570053694780&hl=en&oi=scholarr",
            "Abstract": "This book defines the emerging field of Active Perception which calls for studying perception coupled with action. It is devoted to technical problems related to the design and analysis of intelligent systems possessing perception such as the existing biological organisms and the\" seeing\" machines of the future. Since the appearance of the first technical results on active vision, researchers began to realize that perception--and intelligence in general--is not transcendental and disembodied. It is becoming clear that in the effort to build intelligent visual systems, consideration must be given to the fact that perception is intimately related to the physiology of the perceiver and the tasks that it performs. This viewpoint--known as Purposive, Qualitative, or Animate Vision--is the natural evolution of the principles of Active Vision. The seven chapters in this volume present various aspects of active perception, ranging from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:V_vSwabWVtYC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "From images to sentences through scene description graphs using commonsense reasoning and knowledge",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1511.03292",
            "Abstract": "In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning is applied on (a) detections obtained from existing perception methods on given images, (b) a \"commonsense\" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:g5Ck-dwhA_QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "\u4eba\u5de5\u667a\u80fd: \u7406\u8bba\u4e0e\u5b9e\u8df5",
            "Publication year": 2004,
            "Publication url": "https://scholar.google.com/scholar?cluster=16746868931809938536&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:YsrPvlHIBpEC",
            "Publisher": "\u7535\u5b50\u5de5\u4e1a\u51fa\u7248\u793e"
        },
        {
            "Title": "Reports of the AAAI 2011 Conference Workshops",
            "Publication year": 2012,
            "Publication url": "https://ojs.aaai.org/index.php/aimagazine/article/view/2390",
            "Abstract": "The AAAI-11 workshop program was held Sunday and Monday, August 7\u201318, 2011, at the Hyatt Regency San Francisco in San Francisco, California USA. The AAAI-11 workshop program included 15 workshops covering a wide range of topics in artificial intelligence. The titles of the workshops were Activity Context Representation: Techniques and Languages; Analyzing Microtext; Applied Adversarial Reasoning and Risk Modeling; Artificial Intelligence and Smarter Living: The Conquest of Complexity; AI for Data Center Management and Cloud Computing; Automated Action Planning for Autonomous Mobile Robots; Computational Models of Natural Argument; Generalized Planning; Human Computation; Human-Robot Interaction in Elder Care; Interactive Decision Theory and Game Theory; Language-Action Tools for Cognitive Artificial Agents: Integrating Vision, Action and Language; Lifelong Learning; Plan, Activity, and Intent Recognition; and Scalable Integration of Analytics and Visualization. This article presents short summaries of those events.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:HIFyuExEbWQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Contour motion estimation for asynchronous event-driven cameras",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6895239/",
            "Abstract": "This paper compares image motion estimation with asynchronous event-based cameras to Computer Vision approaches using as input frame-based video sequences. Since dynamic events are triggered at significant intensity changes, which often are at the border of objects, we refer to the event-based image motion as \u201ccontour motion.\u201d Algorithms are presented for the estimation of accurate contour motion from local spatio-temporal information for two camera models: the dynamic vision sensor (DVS), which asynchronously records temporal changes of the luminance, and a family of new sensors which combine DVS data with intensity signals. These algorithms take advantage of the high temporal resolution of the DVS and achieve robustness using a multiresolution scheme in time. It is shown that, because of the coupling of velocity and luminance information in the event distribution, the image motion estimation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:SGW5VrABaM0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "MGMM: multiresolution Gaussian mixture models for computer vision",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/905305/",
            "Abstract": "Introduces a generalisation of scale-space and pyramids, which combines statistical modelling with a spatial representation. The representation uses the familiar concept of multiple resolutions, but applied to a Gaussian mixture representation of the image-hence the title MGMM. It is shown that MGMM can approximate any probability density. Examples show how MGMM can be applied to problems such as segmentation and motion analysis.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:P7Ujq4OLJYoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "From Visual Homing to Object Recognition: Randal C. Nelson",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=9816395741684338587&hl=en&oi=scholarr",
            "Abstract": "All biological systems with vision move about their environments and successfully perform many tasks. The same capabilities are needed in the world of robots. To that end, recent results in empirical fields that study insects and primates, as well as in theoretical and applied disciplines that design robots, have uncovered a number of the principles of navigation. To offer a unifying approach to the situation, this book brings together ideas from zoology, psychology, neurobiology, mathematics, geometry, computer science, and engineering. It contains theoretical developments that will be essential in future research on the topic--especially new representations of space with less complexity than Euclidean representations possess. These representations allow biological and artificial systems to compute from images in order to successfully deal with their environments.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:e84hm74t-eoC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Grasp type revisited: A modern perspective on a classical feature for vision",
            "Publication year": 2015,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2015/html/Yang_Grasp_Type_Revisited_2015_CVPR_paper.html",
            "Abstract": "The grasp type provides crucial information about human action. However, recognizing the grasp type in unconstrained scenes is challenging because of the large variations in appearance, occlusions and geometric distortions. In this paper, first we present a convolutional neural network to classify functional hand grasp types. Experiments on a public static scene hand data set validate good performance of the presented method. Then we present two applications utilizing grasp type classification:(a) inference of human action intention and (b) fine level manipulation action segmentation. Experiments on both tasks demonstrate the usefulness of grasp type as a cognitive feature for computer vision. This study shows that the grasp type is a powerful symbolic representation for action understanding, and thus opens new avenues for future research.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:rbm3iO8VlycC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Creating virtual spatial audio via scientific computing and computer vision",
            "Publication year": 2000,
            "Publication url": "https://escholarship.org/uc/item/1g10f7pk",
            "Abstract": "Creating Virtual Spatial Audio Via Scientific Computing and Computer Vision Skip to main \ncontent eScholarship Open Access Publications from the University of California Search \neScholarship Refine Search All of eScholarship This Series Institute for Data Analysis and \nVisualization UC Davis Deposit ManageSubmissions Menu Unit Home About Contact us Policies \nRSS eScholarship UC Davis Institute for Data Analysis and Visualization IDAV Publications \nDownload PDF Main PDF Share EmailFacebookTwitter Creating Virtual Spatial Audio Via \nScientific Computing and Computer Vision 2000 Author(s): Duraiswami, R. Duda, Richard O \nAlgazi, Ralph Davis, L. Gumerov, A. Liu, QH Shamma, S. Elman, H. Chellappa, R. Aloimonos, Y. \nRaveendra, ST et al. ... Main Content Metrics Author & Article Info Main Content Download PDF \nto View View Larger Thumbnails Document Outline Attachments Previous Next Highlight to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:x21FZCSn4ZoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Supplementary Material: Using a Minimal Action Grammar for Activity Understanding in the Real World",
            "Publication year": 2012,
            "Publication url": "http://www.umiacs.umd.edu/research/POETICON/umd_complex_activities/data/suppl_material_v1.pdf",
            "Abstract": "This document provides the supplementary material and links for our IROS 2012 paper submission number 90.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:TaaCk18tZOkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning hand movements from markerless demonstrations for humanoid tasks",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7041476/",
            "Abstract": "We present a framework for generating trajectories of the hand movement during manipulation actions from demonstrations so the robot can perform similar actions in new situations. Our contribution is threefold: 1) we extract and transform hand movement trajectories using a state-of-the-art markerless full hand model tracker from Kinect sensor data; 2) we develop a new bio-inspired trajectory segmentation method that automatically segments complex movements into action units, and 3) we develop a generative method to learn task specific control using Dynamic Movement Primitives (DMPs). Experiments conducted both on synthetic data and real data using the Baxter research robot platform validate our approach.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:eO3_k5sD8BwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "6 DIRECT MOTION PERCEPTION",
            "Publication year": 2013,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=m_7edh_hN1MC&oi=fnd&pg=PA135&dq=info:hmjFzpblZYUJ:scholar.google.com&ots=QYdGufHQtO&sig=ZkLvZ9YSRJSsJfl1N7jKjTgfgbg",
            "Abstract": "The question of how three-dimensional motion can be understood from visual cues has interested scientists in empirical as well as computational and engineering disciplines for many years. In computational Vision most research has been rooted in the View that awareness of the world is indirect. As a consequence motion perception has been treated as an in~ ferential process which involves the estimation of retinal motion \ufb01elds and their interpretation in the form of optimization procedures. In this chapter it is shown that the perception Of motion can also be realized in a direct way by detecting invariant patterns in the two-dimensional spatiotemporal image representation. Thus, this chapter presents a computational theory of some of the ideas set forth by Gibson in the theory of direct perception, in particular the ideas Of transformational invariants. The basis of the theory lies in a global structure inherent in image motion \ufb01elds due to rigid motion. This structure is independent of the scene in view and manifests itself as geometric entities (areas and contours) whose locations on the image encode 3D motion information. The geometric results presented give rise to constraints that can form the ba-sis for a variety Of of algorithms for the recovery Of visual information from multiple views. Some of the constraints introduced are based solely on the use of the Sign of flow measurements. In order to give theoretical signi\ufb01cance to these results, it is also shown that information in the sign of the flow is almost always suf\ufb01cient to recover 3D motion uniquely.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:MLfJN-KU85MC",
            "Publisher": "Psychology Press"
        },
        {
            "Title": "Publications in preprint",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=12149961439267964012&hl=en&oi=scholarr",
            "Abstract": "Graduate Research Assistant at Center for Automation Research, University of Maryland, with Dr. Yiannis Aloimonos June 2012\u2013August 2012, June 2016\u2013present Working on manipulation action understanding with planning and linguistic information. Working on multimodal coreference resolutions (primary research interest) among images, text and videos. Working on semantic feedback in deep neural networks. Working on zero shot learning using word embeddings. Working on coreference guided word embeddings.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:unp9ATQDT5gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Talking heads: Introducing the tool of 3d motion fields in the study of action",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/897367/",
            "Abstract": "We demonstrate a method to complete three-dimensional (3D) motion fields on a face to serve as an intermediate representation for the study of actions. Twelve synchronized and calibrated cameras are positioned all around a talking person and observe its head in motion. We represent the head as a deformable mesh, which is fitted in a global optimization step to silhouette-contour and multi-camera stereo data derived from all images. The non-rigid displacement of the mesh from frame to frame, the 3D motion field, is determined from the normal flow information in all the images. We integrate these cues over time, thus producing a spatio-temporal representation of the talking head. Our ability to estimate 3D motion fields points to a new framework for the study of action. Using multicamera configurations we can estimate a sequence of evolving 3D motion fields representing specific actions. Then, by performing a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:uLbwQdceFCQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Comparing linguistic classifications with sensorimotor data of English and Greek verbs of motion",
            "Publication year": 2010,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.435.5274&rep=rep1&type=pdf#page=81",
            "Abstract": "We combine linguistic knowledge from corpus data with sensorimotor data obtained experimentally in an effort to better specify the minimum conceptual representation of a motion event that distinguishes it from all other events. Sensorimotor data are collected by measuring the performance of speakers of Modern Greek and American English. We focus on the clustering of motor actions and its correspondence to previous linguistic classifications of both languages.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:EkHepimYqZsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "2012 Index IEEE Transactions on Pattern Analysis and Machine Intelligence Vol. 34",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6353859/",
            "Abstract": "This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:nRpfm8aw39MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Corpus-guided sentence generation of natural images",
            "Publication year": 2011,
            "Publication url": "https://www.aclweb.org/anthology/D11-1041.pdf",
            "Abstract": "We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:XvxMoLDsR5gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Ray carving with gradients and motion",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/897375/",
            "Abstract": "Recent developments in camera and computer technology have made multiple-camera systems less expensive and more usable. Using such systems, we can generate 3D models of human activity for use in surveillance, as avatars, or for 3D effects generation. Some approaches to model generation are voxel coloring, space carving, silhouette intersection, and the combination of multiple stereo reconstructions. Our attempt to overcome various shortcomings of the above approaches has led to the use of image derivatives and motion to determine the shape and motion of the activity in view. Direct computations of the gradient directions and the image motion normal to the gradient provide the information to generate a 3D+motion model that is consistent with all the image data. Data structures encode visibility information from each of the cameras surrounding the scene, allowing efficient determination of the subsets \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:ZfRJV9d4-WMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Appreciation to IJCV Reviewers",
            "Publication year": 2013,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s11263-012-0606-8.pdf",
            "Abstract": "For helping us deliver timely decisions to our authors, the Editors-in-Chief and Publisher would like to thank the following individuals that contributed reviews between October, 2011 and September 30th, 2012. We applaud your efforts and dedication to the community.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:43bX7VzcjpAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Image Segmentation: The View from Inside the Image",
            "Publication year": 2010,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=KZj7tl2tRJsC&oi=fnd&pg=PA165&dq=info:h3kU_-FKDXIJ:scholar.google.com&ots=VBJRc3nrCQ&sig=NNcDBQeNVjJsvUVKAOz704rAOd4",
            "Abstract": "Humans observe and understand a scene/image through a series of \ufb01xations. Each \ufb01xation point lies inside a particular region of arbitrary shape and size in the scene which can either be an object or a part of it. We de\ufb01ne the basic segmentation problem as segmenting that region containing the \ufb01xation which is equivalent to \ufb01nding the enclosing contour\u2013a connected set of boundary edge fragments in the edge map of the scene\u2013around the \ufb01xation. This enclosing contour should be a depth boundary. In the \ufb01rst part of the chapter, we present a novel algorithm to \ufb01nd this bounding contour and achieves the segmentation of one object, given the \ufb01xation. The proposed segmentation framework combines monocular cues (color/intensity/texture) with stereo and/or motion, in a cue independent manner. Our approach is different from current approaches. While existing work attempts to segment the whole scene at once into many areas, we segment only one image region, speci\ufb01cally the one containing the \ufb01xation point. Experiments with real imagery collected by our active robot and from the known databases demonstrate the promise of the approach. After this description of a bottom up segmentation, in the second part of the chapter, we investigate the role of prior object knowledge in achieving segmentation. Clearly, segmentation and recognition have the form of chicken-egg problems. It appears as if in order and Memory of Professor King-Sun Fu, 165\u2013179. c 2010 River Publishers. All rights reserved.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:WZBGuue-350C",
            "Publisher": "River Publishers"
        },
        {
            "Title": "Evaluating dynamic software update safety using systematic testing",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6035725/",
            "Abstract": "Dynamic software updating (DSU) systems patch programs on the fly without incurring downtime. To avoid failures due to the updating process itself, many DSU systems employ timing restrictions. However, timing restrictions are theoretically imperfect, and their practical effectiveness is an open question. This paper presents the first significant empirical evaluation of three popular timing restrictions: activeness safety (AS), which prevents updates to active functions, con-freeness safety (CFS), which only allows modifications to active functions when doing so is provably type-safe, and manual identification of the event-handling loops during which an update may occur. We evaluated these timing restrictions using a series of DSU patches to three programs: OpenSSH, vsftpd, and ngIRCd. We systematically applied updates at each distinct update point reached during execution of a suite of system tests for these \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:aIdbFUkbNIkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Active segmentation with fixation",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5459254/",
            "Abstract": "The human visual system observes and understands a scene/image by making a series of fixations. Every \u201cfixation point\u201d lies inside a particular region of arbitrary shape and size in the scene which can either be an object or just a part of it. We define as a basic segmentation problem the task of segmenting that region containing the \u201cfixation point\u201d. Segmenting this region is equivalent to finding the enclosing contour - a connected set of boundary edge fragments in the edge map of the scene - around the fixation. We present here a novel algorithm that finds this bounding contour and achieves the segmentation of one object, given the fixation. The proposed segmentation framework combines monocular cues (color/intensity/texture) with stereo and/or motion, in a cue independent manner. We evaluate the performance of the proposed algorithm on challenging videos and stereo pairs. Although the proposed algorithm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:b0M2c_1WBrUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A New Framework for Multi-camera Structure from Motion",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-59802-9_10",
            "Abstract": "Models of real-world objects and actions for use in graphics, Virtual and augmented reality and related fields can only be obtained automatically through the use of visual data and particularly video. This paper introduces a new framework for integrating multiple synchronized videos of an object or a scene captured by a calibrated configuration of cameras into a unified space-time description. Given videos of a rigidly-moving object, it is a first step for dynamic descriptions and model building to recover the three-dimensional (3D) motion of the object which consists of a rotation and a translation at each instant. In this paper we extend the structure-from-motion framework for moving cameras in a static world to static multi-camera configurations consisting of large numbers of cameras observing a moving world. This new framework integrates the information from all the cameras into a single model. Using all the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:AvfA0Oy_GE0C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Active scene recognition with vision and language",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6126320/",
            "Abstract": "This paper presents a novel approach to utilizing high level knowledge for the problem of scene recognition in an active vision framework, which we call active scene recognition. In traditional approaches, high level knowledge is used in the post-processing to combine the outputs of the object detectors to achieve better classification performance. In contrast, the proposed approach employs high level knowledge actively by implementing an interaction between a reasoning module and a sensory module (Figure 1). Following this paradigm, we implemented an active scene recognizer and evaluated it with a dataset of 20 scenes and 100+ objects. We also extended it to the analysis of dynamic scenes for activity recognition with attributes. Experiments demonstrate the effectiveness of the active paradigm in introducing attention and additional constraints into the sensing process.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:vbGhcppDl1QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Active segmentation for robotics",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5354325/",
            "Abstract": "The semantic robots of the immediate future are robots that will be able to find and recognize objects in any environment. They need the capability of segmenting objects in their visual field. In this paper, we propose a novel approach to segmentation based on the operation of fixation by an active observer. Our approach is different from current approaches: while existing works attempt to segment the whole scene at once into many areas, we segment only one image region, specifically the one containing the fixation point. Furthermore, our solution integrates monocular cues (color, texture) with binocular cues (stereo disparities and optical flow). Experiments with real imagery collected by our active robot and from the known databases demonstrate the promise of the approach.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:nPT8s1NX_-sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Real-time shape retrieval for robotics using skip tri-grams",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5354738/",
            "Abstract": "The real time requirement is an additional constraint on many intelligent applications in robotics, such as shape recognition and retrieval using a mobile robot platform. In this paper, we present a scalable approach for efficiently retrieving closed contour shapes. The contour of an object is represented by piecewise linear segments. A skip Tri-Gram is obtained by selecting three segments in the clockwise order while allowing a constant number of segments to be \u00bfskipped\u00bf in between. The main idea is to use skip Tri-Grams of the segments to implicitly encode the distant dependency of the shape. All skip Tri-Grams are used for efficiently retrieving closed contour shapes without pairwise matching feature points from two shapes. The retrieval is at least an order of magnitude faster than other state-of-the-art algorithms. We score 80% in the Bullseye retrieval test on the whole MPEG 7 shape dataset. We further test the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:7T2F9Uy0os0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Active Segmentation",
            "Publication year": 2009,
            "Publication url": "https://www.academia.edu/download/41531015/Active_Segmentation20160124-3706-mnxr73.pdf",
            "Abstract": "The human visual system observes and understands a scene/image by making a series of fixations. Every fixation point lies inside a particular region of arbitrary shape and size in the scene which can either be an object or just a part of it. We define as a basic segmentation problem the task of segmenting that region containing the fixation point. Segmenting the region containing the fixation is equivalent to finding the enclosing contour-a connected set of boundary edge fragments in the edge map of the scene-around the fixation. This enclosing contour should be a depth boundary.We present here a novel algorithm that finds this bounding contour and achieves the segmentation of one object, given the fixation. The proposed segmentation framework combines monocular cues (color/intensity/texture) with stereo and/or motion, in a cue independent manner. The semantic robots of the immediate future will be able to use this algorithm to automatically find objects in any environment. The capability of automatically segmenting objects in their visual field can bring the visual processing to the next level. Our approach is different from current approaches. While existing work attempts to segment the whole scene at once into many areas, we segment only one image region, specifically the one containing the fixation point. Experiments with real imagery collected by our active robot and from the known databases 1 demonstrate the promise of the approach.",
            "Abstract entirety": 1,
            "Author pub id": "7QmEsOwAAAAJ:WC23djZS0W4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Who killed the directed model?",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4587817/",
            "Abstract": "Prior distributions are useful for robust low-level vision, and undirected models (e.g. Markov Random Fields) have become a central tool for this purpose. Though sometimes these priors can be specified by hand, this becomes difficult in large models, which has motivated learning these models from data. However, maximum likelihood learning of undirected models is extremely difficult- essentially all known methods require approximations and/or high computational cost. Conversely, directed models are essentially trivial to learn from data, but have not received much attention for low-level vision. We compare the two formalisms of directed and undirected models, and conclude that there is no a priori reason to believe one better represents low-level vision quantities. We formulate two simple directed priors, for natural images and stereo disparity, to empirically test if the undirected formalism is superior. We find in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "7QmEsOwAAAAJ:eflP2zaiRacC",
            "Publisher": "IEEE"
        }
    ]
}]