[{
    "name": "\u039d\u03b5\u03bf\u03ba\u03bb\u03ae\u03c2 \u03a0\u03bf\u03bb\u03c5\u03b6\u03ce\u03c4\u03b7\u03c2",
    "romanize name": "Neoklis Polyzotis",
    "School-Department": "\u0395\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03c2 \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "University of California Santa Cruz",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 5718,
    "Scholar name": "Neoklis Polyzotis",
    "Scholar id": "vfIriy4AAAAJ",
    "Affiliation": "Databricks",
    "Citedby": 6718,
    "Interests": [
        "Databases",
        "Machine Learning"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=vfIriy4AAAAJ&hl=en",
    "Publications": [
        {
            "Title": "XML processing in DHT networks",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4497469/",
            "Abstract": "We study the scalable management of XML data in P2P networks based on distributed hash tables (DHTs). We identify performance limitations in this context, and propose an array of techniques to lift them. First, we adapt the DHT platform's index store and communication primitives to the needs of massive data processing. Second, we introduce a distributed hierarchical index and associated efficient algorithms to speed up query processing. Third, we present an innovative, XML-specific flavor of Bloom filters, to reduce data transfers entailed by query processing. Our approach is fully implemented in the KadoP system, used in a real-life software manufacturing application. Our experiments demonstrate the benefits of the proposed techniques.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:ufrVoPGSRksC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Seedb: Efficient data-driven visualization recommendations to support visual analytics",
            "Publication year": 2015,
            "Publication url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4714568/",
            "Abstract": "Data analysts often build visualizations as the first step in their analytical workflow. However, when working with high-dimensional datasets, identifying visualizations that show relevant or desired trends in data can be laborious. We propose S ee DB, a visualization recommendation engine to facilitate fast visual analysis: given a subset of data to be studied, S ee DB intelligently explores the space of visualizations, evaluates promising visualizations for trends, and recommends those it deems most \u201cuseful\u201d or \u201cinteresting\u201d. The two major obstacles in recommending interesting visualizations are (a) scale: evaluating a large number of candidate visualizations while responding within interactive time scales, and (b) utility: identifying an appropriate metric for assessing interestingness of visualizations. For the former, S ee DB introduces pruning optimizations to quickly identify high-utility visualizations and sharing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:maZDTaKrznsC",
            "Publisher": "NIH Public Access"
        },
        {
            "Title": "Production Machine Learning Pipelines: Empirical Analysis and Optimization Opportunities",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3448016.3457566",
            "Abstract": "Machine learning (ML) is now commonplace, powering data-driven applications in various organizations. Unlike the traditional perception of ML in research, ML production pipelines are complex, with many interlocking analytical components beyond training, whose sub-parts are often run multiple times on overlapping subsets of data. However, there is a lack of quantitative evidence regarding the lifespan, architecture, frequency, and complexity of these pipelines to understand how data management research can be used to make them more efficient, effective, robust, and reproducible. To that end, we analyze the provenance graphs of 3000 production ML pipelines at Google, comprising over 450,000 models trained, spanning a period of over four months, in an effort to understand the complexity and challenges underlying production ML. Our analysis reveals the characteristics, components, and topologies of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:wbdj-CoPYUoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deco: A system for declarative crowdsourcing",
            "Publication year": 2012,
            "Publication url": "http://ilpubs.stanford.edu:8090/1034/",
            "Abstract": "Deco is a system that enables declarative crowdsourcing: answering SQL queries posed over data gathered from the crowd as well as existing relational data. Deco implements a novel push-pull hybrid execution model in order to support a flexible data model and a precise query semantics, while coping with the combination of latency, monetary cost, and uncertainty of crowdsourcing. We demonstrate Deco using two crowdsourcing platforms: Amazon Mechanical Turk and an in-house platform, to show how Deco provides a convenient means of collecting and querying crowdsourced data.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:5nxA0vEk-isC",
            "Publisher": "Stanford InfoLab"
        },
        {
            "Title": "INUM+: A leaner, more accurate and more efficient fast what-if optimizer",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6547426/",
            "Abstract": "INUM is a what-if optimization technique that efficiently estimates the cost of optimal query plans under hypothetical index configurations and can thus serve as a fast alternative to conventional what-if optimization. In this paper we introduce three crucial enhancements to INUM: a principled method to handle query plans with Nested-Loop Join (NLJ) operators (to improve estimation accuracy); a method to reduce the time to preprocess a query in the workload (to reduce setup latency); and, a method to prune the amount of information stored per query (to improve estimation efficiency). We demonstrate experimentally that these improvements make INUM 5x faster and improve median estimation accuracy by 79%. Our work extends significantly the scope of workloads and tuning problems to which INUM can be applied.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:lSLTfruPkqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On-line index selection for shifting workloads",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4401029/",
            "Abstract": "This paper introduces COLT (continuous on-line tuning), a novel framework that continuously monitors the workload of a database system and enriches the existing physical design with a set of effective indices. The key idea behind COLT is to gather performance statistics at different levels of detail and to carefully allocate profiling resources to the most promising candidate configurations. Moreover, COLT uses effective heuristics to self-regulate its own performance, lowering its overhead when the system is well tutted and being more aggressive when the workload shifts and it becomes necessary to re-tune the system. We describe an implementation of the proposed framework in the PostgreSQL database system and evaluate its performance experimentally. Our results validate the effectiveness of COLT and demonstrate its ability to modify the system configuration in response to changes in the query load.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:hqOjcs7Dif8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fractional XSketch Synopses for XML Databases",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-30081-6_14",
            "Abstract": "A key step in the optimization of declarative queries over XML data is estimating the selectivity of path expressions, ie, the number of elements reached by a specific navigation pattern through the XML data graph. Recent studies have introduced XSketch structural graph synopses as an effective, space-efficient tool for the compile-time estimation of complex path-expression selectivities over graph-structured, schema-less XML data. Briefly, XSketch es exploit localized graph stability and well-founded statistical assumptions to accurately approximate the path and branching distribution in the underlying XML data graph. Empirical results have demonstrated the effectiveness of XSketch summaries over real-life and synthetic data sets, and for a variety of path-expression workloads. In this paper, we introduce fractional XSketch es (f XSketches) a simple, yet intuitive and very effective generalization of the basic XSketch \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:hFOr9nPyWt4C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Matchup: Autocompletion for mashups",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4812552/",
            "Abstract": "A mashup is a Web application that integrates data, computation and GUI provided by several systems into a unique tool. The concept originated from the understanding that the number of applications available on the Web and the need for combining them to meet user requirements, are growing very rapidly. This demo presents MatchUp, a system that supports rapid, on-demand, intuitive development of mashups, based on a novel autocompletion mechanism. The key observation guiding the development of MatchUp is that mashups developed by different users typically share common characteristics; they use similar classes of mashup components and glue them together in a similar manner. MatchUp exploits these similarities to predict, given a user's partial mashup specification, what are the most likely potential completions (missing components and connection between them) for the specification. Using a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:mVmsd5A6BfQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "From Data to Models and Back",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3399579.3399868",
            "Abstract": "Production ML is more than writing the code for the trainer. It requires processes and tooling that enable a larger team to share, track, analyze, and monitor not only on the code for ML but also on the artifacts (Datasets, Models,...) that are manipulated and generated in these production ML pipelines.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:V3AGJWp-ZtQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Semi-automatic index tuning: Keeping dbas in the loop",
            "Publication year": 2010,
            "Publication url": "https://arxiv.org/abs/1004.1249",
            "Abstract": "To obtain good system performance, a DBA must choose a set of indices that is appropriate for the workload. The system can aid in this challenging task by providing recommendations for the index configuration. We propose a new index recommendation technique, termed semi-automatic tuning, that keeps the DBA \"in the loop\" by generating recommendations that use feedback about the DBA's preferences. The technique also works online, which avoids the limitations of commercial tools that require the workload to be known in advance. The foundation of our approach is the Work Function Algorithm, which can solve a wide variety of online optimization problems with strong competitive guarantees. We present an experimental analysis that validates the benefits of semi-automatic tuning in a wide variety of conditions.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:M3ejUd6NZC8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Skyline query processing over joins",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1989323.1989332",
            "Abstract": "This paper addresses the problem of efficiently computing the skyline set of a relational join. Existing techniques either require to access all tuples of the input relations or demand specialized multi-dimensional access methods to generate the skyline join result. To avoid these inefficiencies, we introduce the novel SFSJ algorithm that fuses the identification of skyline tuples with the computation of the join. SFSJ is able to compute the correct skyline set by accessing only a subset of the input tuples, ie, it has the property of early termination. SFSJ employs standard access methods for reading the input tuples and is readily implementable in an existing database system. Moreover, it can be used in pipelined execution plans, as it generates the skyline tuples progressively. Additionally, we formally analyze the performance of SFSJ and propose a novel strategy for accessing the input tuples that is proven to be optimal for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:_kc_bZDykSQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Divergent physical design tuning for replicated databases",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2213836.2213843",
            "Abstract": "We introduce divergent designs as a novel tuning paradigm for database systems that employ replication. A divergent design installs a different physical configuration (eg, indexes and materialized views) with each database replica, specializing replicas for different subsets of the workload. At runtime, queries are routed to the subset of the replicas configured to yield the most efficient execution plans. When compared to uniformly designed replicas, divergent replicas can potentially execute their subset of the queries significantly faster, and their physical configurations could be initialized and maintained (updated) in less time. However, the specialization of divergent replicas limits the ability to load-balance the workload at runtime.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:k_IJM867U9cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tuple Graph Synopses for Relational Data Sets",
            "Publication year": 2006,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.9501&rep=rep1&type=pdf",
            "Abstract": "This paper introduces the Tuple Graph (TuG) synopses, a new class of data summaries that enable accurate selectivity estimates for complex relational queries. The proposed summarization framework adopts a \u201csemi-structured\u201d view of the relational database, modeling a relational data set as a graph of tuples and join queries as graph traversals respectively. The key idea is to approximate the structure of the induced data graph in a concise synopsis, and to estimate the selectivity of a query by performing the corresponding traversal over the summarized graph. We detail the TuG synopsis model that is based on this novel approach, and we describe an efficient and scalable construction algorithm for building accurate TuGs within a specific storage budget. We validate the performance of TuGs with an extensive experimental study on real-life and synthetic data sets. Our results verify the effectiveness of TuGs in generating accurate selectivity estimates for complex join queries, and demonstrate their benefits over existing summarization techniques.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:u_35RYKgDlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Seedb: towards automatic query result visualizations",
            "Publication year": 2014,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.695.8183&rep=rep1&type=pdf",
            "Abstract": "Data analysts operating on large volumes of data often rely on visualizations to interpret the results of queries. However, finding the right visualization given a query of interest is a laborious and time-consuming task. We propose SEEDB, a visualization recommendation engine that aims to partially automate this task: given a query, SEEDB intelligently explores the space of all possible visualizations, evaluates promising visualizations, and automatically recommends the most \u201cinteresting\u201d or \u201cuseful\u201d visualizations to the analyst. We present two types of optimizations for SEEDB: sharing-based optimizations, that try to share as much computation as possible, and pruning-based optimizations, that try to avoid as much computation on not-so-useful visualizations as possible, and show that these optimizations may be applied to implementations of SEEDB over a relational row store database and a column store database. We demonstrate how our optimizations lead to multiple orders of magnitude speedup on SEEDB on both types of databases, while not significantly impacting accuracy. We further demonstrate via a user study that SEEDB returns interesting and useful visualizations. Overall, SEEDB represents significant progress towards the challenging and important task of automated visualization generation.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:ldfaerwXgEUC",
            "Publisher": "Tech. rep., Technical Report, data-people. cs. illinois. edu/seedb-tr. pdf"
        },
        {
            "Title": "Searching shared content in communities with the data ring",
            "Publication year": 2009,
            "Publication url": "https://hal.inria.fr/inria-00429489/",
            "Abstract": "Information ubiquity has created a large crowd of users (most notably scientists), who could employ DBMS technology to process and share their data more effectively. Still, this user base prefers to keep its data in files that can be easily managed by applications such as spreadsheets, rather than deal with the complexity and rigidity of modern database systems. In this paper, we propose a vision for enabling non-experts, such as scientists, to build content sharing communities in a true database fashion: declaratively. The proposed infrastructure, called the data ring, enables users to manage and share their data with minimal effort; the user points to the data that should be shared, and the data ring becomes responsible for automatically indexing the data (to make it accessible), replicating it (for availability), and reorganizing its physical storage (for better query performance). We outline the salient features of our proposal, and outline research challenges that must be addressed in order to realize this vision.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:vV6vV6tmYwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Human-assisted graph search: it's okay to ask questions",
            "Publication year": 2011,
            "Publication url": "https://arxiv.org/abs/1103.3102",
            "Abstract": "We consider the problem of human-assisted graph search: given a directed acyclic graph with some (unknown) target node(s), we consider the problem of finding the target node(s) by asking an omniscient human questions of the form \"Is there a target node that is reachable from the current node?\". This general problem has applications in many domains that can utilize human intelligence, including curation of hierarchies, debugging workflows, image segmentation and categorization, interactive search and filter synthesis. To our knowledge, this work provides the first formal algorithmic study of the optimization of human computation for this problem. We study various dimensions of the problem space, providing algorithms and complexity results. Our framework and algorithms can be used in the design of an optimizer for crowd-sourcing platforms such as Mechanical Turk.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:qjMakFHDy7sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Autocompletion for mashups",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/1687627.1687689",
            "Abstract": "A mashup is a Web application that integrates data, computation and UI elements provided by several components into a single tool. The concept originated from the understanding that there is an increasing number of applications available on the Web and a growing need to combine them in order to meet user requirements. This paper presents MatchUp, a system that supports rapid, on-demand, intuitive development of mashups, based on a novel autocompletion mechanism. The key observation guiding the development of MatchUp is that mashups developed by different users typically share common characteristics; they use similar classes of mashup components and glue them together in a similar manner. MatchUp exploits these similarities to recommend useful completions (missing components and connections between them) for a user's partial mashup specification. The user is presented with a ranking of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:Y0pCki6q_DkC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "XSKETCH synopses for XML data graphs",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1166074.1166082",
            "Abstract": "Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows (1) path navigation and branching through the label structure of the XML data graph, and (2) predicates on the values of specific path/branch nodes, in order to reach the desired data elements. Clearly, optimizing such queries requires approximating the result cardinality of the referenced paths and hence hinges on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over the base XML data. In this article, we introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:YsMSGLbcyi4C",
            "Publisher": "ACM"
        },
        {
            "Title": "MISO: souping up big data query processing with a multistore system",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2588555.2588568",
            "Abstract": "Multistore systems utilize multiple distinct data stores such as Hadoop's HDFS and an RDBMS for query processing by allowing a query to access data and computation in both stores. Current approaches to multistore query processing fail to achieve the full potential benefits of utilizing both systems due to the high cost of data movement and loading between the stores. Tuning the physical design of a multistore, ie, deciding what data resides in which store, can reduce the amount of data movement during query processing, which is crucial for good multistore performance. In this work, we provide what we believe to be the first method to tune the physical design of a multistore system, by focusing on which store to place data. Our method, called MISO for MultISstore Online tuning, is adaptive, lightweight, and works in an online fashion utilizing only the by-products of query processing, which we term as opportunistic \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:4TOpqqG69KYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The rank join problem",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-19668-3_10",
            "Abstract": "In the rank join problem, we are given a set of relations and a scoring function, and the goal is to return the K join results with the highest scores. It is often the case in practice that the inputs may be accessed in ranked order and the scoring function is monotonic. These conditions allow for efficient algorithms that solve the rank join problem without reading all of the input. In this chapter, we review recent efforts in the development and analysis of such rank join algorithms. First, we present some theoretical results that state the inherent complexity of the rank join problem and essentially reveal that any rank join algorithm has to trade off between I/O efficiency and computational efficiency. We then review a specific rank join algorithm that adjusts this trade-off at runtime, depending on the data and the scoring function, in order to strike a balance between I/O overhead and computation.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:a0OBvERweLwC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Data validation for machine learning",
            "Publication year": 2019,
            "Publication url": "https://scholar.google.com/scholar?cluster=3987869554833945546&hl=en&oi=scholarr",
            "Abstract": "Machine learning is a powerful tool for gleaning knowledge from massive amounts of data. While a great deal of machine learning research has focused on improving the accuracy and efficiency of training and inference algorithms, there is less attention in the equally important problem of monitoring the quality of data fed to machine learning. The importance of this problem is hard to dispute: errors in the input data can nullify any benefits on speed and accuracy for training and inference. This argument points to a data-centric approach to machine learning that treats training and serving data as an important production asset, on par with the algorithm and infrastructure used for learning.In this paper, we tackle this problem and present a data validation system that is designed to detect anomalies specifically in data fed into machine learning pipelines. This system is deployed in production as an integral part of TFX \u2026Machine learning is a powerful tool for gleaning knowledge from massive amounts of data. While a great deal of machine learning research has focused on improving the accuracy and efficiency of training and inference algorithms, there is less attention in the equally important problem of monitoring the quality of data fed to machine learning. The importance of this problem is hard to dispute: errors in the input data can nullify any benefits on speed and accuracy for training and inference. This argument points to a data-centric approach to machine learning that treats training and serving data as an important production asset, on par with the algorithm and infrastructure used for learning.In this paper, we tackle this problem and present a data validation system that is designed to detect anomalies specifically in data fed into machine learning pipelines. This system is deployed in production as an integral part of TFX \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:sSrBHYA8nusC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Goods: Organizing google's datasets",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2882903.2903730",
            "Abstract": "Enterprises increasingly rely on structured datasets to run their businesses. These datasets take a variety of forms, such as structured files, databases, spreadsheets, or even services that provide access to the data. The datasets often reside in different storage systems, may vary in their formats, may change every day. In this paper, we present GOODS, a project to rethink how we organize structured datasets at scale, in a setting where teams use diverse and often idiosyncratic ways to produce the datasets and where there is no centralized system for storing and querying them. GOODS extracts metadata ranging from salient information about each dataset (owners, timestamps, schema) to relationships among datasets, such as similarity and provenance. It then exposes this metadata through services that allow engineers to find datasets within the company, to monitor datasets, to annotate them in order to enable \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:ns9cj8rnVeAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Opportunities for data management research in the era of horizontal AI/ML",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/3352063.3352149",
            "Abstract": "AI/ML is becoming a horizontal technology: its application is expanding to more domains, and its integration touches more parts of the technology stack. Given the strong dependence of ML on data, this expansion creates a new space for applying data management techniques. At the same time, the deeper integration of ML in the technology stack provides more touch points where ML can be used in data management systems and vice versa.In this panel, we invite researchers working in this domain to discuss this emerging world and its implications on data-management research. Among other topics, the discussion will touch on the opportunities for interesting research, how we can interact with other communities, what is the core expertise we bring to the table, and how we can conduct and evaluate this research effectively within our own community. The goal of the panel is to nudge the community to appreciate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:BrmTIyaxlBUC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Odyssey: a multistore system for evolutionary analytics",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/pdf/10.14778/2536222.2536249",
            "Abstract": "We present a data analytics system, Odyssey, that is being developed at NEC Labs in collaboration with NEC\u2019s commercial business units and our academic collaborators. The design principles of the system are based on the business requirements identified through extensive surveys and communications with the practitioners and customers. Most notable high-level requirements are: 1) The analytics system should be able to effectively use both structured and unstructured data sources. 2) Business requirements are not captured in a single simple metric but combination of metrics, such as value of data, performance, monetary costs, and they are dynamic. The system should manage data by observing constantly changing metric values. 3) Time-to-insight is very important, so the system should enable immediate exploratory querying of data without heavy prerequisite processes. 4) The system should efficiently \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:isC4tDSrTZIC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Data lifecycle challenges in production machine learning: a survey",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3299887.3299891",
            "Abstract": "Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:tOudhMTPpwUC",
            "Publisher": "ACM"
        },
        {
            "Title": "SeeDB: supporting visual analytics with data-driven recommendations",
            "Publication year": 2015,
            "Publication url": "http://i.stanford.edu/~adityagp/seedb-tr.pdf",
            "Abstract": "Data analysts often build visualizations as the first step in their analytical workflow. However, when working with high-dimensional datasets, identifying visualizations that show relevant or desired trends in data can be laborious. We propose SEEDB, a visualization recommendation engine to facilitate fast visual analysis: given a subset of data to be studied, SEEDB intelligently explores the space of visualizations, evaluates promising visualizations for trends, and recommends those it deems most \u201cuseful\u201d or \u201cinteresting\u201d. The two major obstacles in recommending interesting visualizations are (a) scale: dealing with a large number of candidate visualizations and evaluating all of them in parallel, while responding within interactive time scales, and (b) utility: identifying an appropriate metric for assessing interestingness of visualizations. For the former, SEEDB introduces pruning optimizations to quickly identify high-utility visualizations and sharing optimizations to maximize sharing of computation across visualizations. For the latter, as a first step, we adopt a deviation-based metric for visualization utility, while indicating how we may be able to generalize it to other factors influencing utility. We implement SEEDB as a middleware layer that can run on top of any DBMS. Our experiments show that our framework can identify interesting visualizations with high accuracy. Our optimizations lead to multiple orders of magnitude speedup on relational row and column stores and provide recommendations at interactive time scales. Finally, we demonstrate via a user study the effectiveness of our deviation-based utility metric and the value of recommendations in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:2P1L_qKh6hAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The repeatability experiment of SIGMOD 2008",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1374780.1374791",
            "Abstract": "SIGMOD 2008 was the first database conference that offered to test submitters' programs against their data to verify the experiments published. This paper discusses the rationale for this effort, the community's reaction, our experiences, and advice for future similar efforts.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:kNdYIx-mwKoC",
            "Publisher": "ACM"
        },
        {
            "Title": "Validating Data and Models in Continuous ML pipelines",
            "Publication year": 2021,
            "Publication url": "https://research.google/pubs/pub50721/",
            "Abstract": "Production ML is more than writing the code for the trainer. It requires processes and tooling that enable a larger team to share, track, analyze, and monitor not only the code for ML but also the artifacts (Datasets, Models,...) that are manipulated and generated in these production ML pipelines.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:olpn-zPbct0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Crowdscreen: Algorithms for filtering data with humans",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2213836.2213878",
            "Abstract": "Given a large set of data items, we consider the problem of filtering them based on a set of properties that can be verified by humans. This problem is commonplace in crowdsourcing applications, and yet, to our knowledge, no one has considered the formal optimization of this problem.(Typical solutions use heuristics to solve the problem.) We formally state a few different variants of this problem. We develop deterministic and probabilistic algorithms to optimize the expected cost (ie, number of questions) and expected error. We experimentally show that our algorithms provide definite gains with respect to other strategies. Our algorithms can be applied in a variety of crowdsourcing scenarios and can form an integral part of any query processor that uses human computation.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:d1gkVwhDpl0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "An automated, yet interactive and portable DB designer",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1807167.1807314",
            "Abstract": "Tuning tools attempt to configure a database to achieve optimal performance for a given workload. Selecting an optimal set of physical structures is computationally hard since it involves searching a vast space of possible configurations. Commercial DBMSs offer tools that can address this problem. The usefulness of such tools, however, is limited by their dependence on greedy heuristics, the need for a-priori (offline) knowledge of the workload, and lack of an optimal materialization schedule to get the best out of suggested design features. Moreover, the open source DBMSs do not provide any automated tuning tools.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:TFP_iSt0sucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Searching a file system using inferred semantic links",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1083356.1083372",
            "Abstract": "We describe Eureka, a file system search engine that takes into account the inherent relationships among files in order to improve the rankings of search results. The key idea is to automatically infer semantic links within the file system, and use the structure of the links to determine the importance of different files and essentially bias the result rankings. We discuss the inference of semantic links and describe the design of the Eureka search engine.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:YOwf2qJgpHMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "TuG synopses for approximate query answering",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1508857.1508860",
            "Abstract": "This article introduces the Tuple Graph (TuG) synopses, a new class of data summaries that enable accurate approximate answers for complex relational queries. The proposed summarization framework adopts a \u201csemi-structured\u201d view of the relational database, modeling a relational data set as a graph of tuples and join queries as graph traversals, respectively. The key idea is to approximate the structure of the induced data graph in a concise synopsis, and to approximate the answer to a query by performing the corresponding traversal over the summarized graph. We detail the (TuG) synopsis model that is based on this novel approach, and we describe an efficient and scalable construction algorithm for building accurate (TuG) within a specific storage budget. We validate the performance of (TuG) with an extensive experimental study on real-life and synthetic datasets. Our results verify the effectiveness of (TuG \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:hC7cP41nSMkC",
            "Publisher": "ACM"
        },
        {
            "Title": "Asking the right questions in crowd data sourcing",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6228183/",
            "Abstract": "Crowd-based data sourcing is a new and powerful data procurement paradigm that engages Web users to collectively contribute information. In this work, we target the problem of gathering data from the crowd in an economical and principled fashion. We present Ask It!, a system that allows interactive data sourcing applications to effectively determine which questions should be directed to which users for reducing the uncertainty about the collected data. Ask It! uses a set of novel algorithms for minimizing the number of probing (questions) required from the different users. We demonstrate the challenge and our solution in the context of a multiple-choice question game played by the ICDE'12 attendees, targeted to gather information on the conference's publications, authors and colleagues.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:Zph67rFs4hoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Processing of rank joins in highly distributed systems",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6228118/",
            "Abstract": "In this paper, we study efficient processing of rank joins in highly distributed systems, where servers store fragments of relations in an autonomous manner. Existing rank-join algorithms exhibit poor performance in this setting due to excessive communication costs or high latency. We propose a novel distributed rank-join framework that employs data statistics, maintained as histograms, to determine the subset of each relational fragment that needs to be fetched to generate the top-k join results. At the heart of our framework lies a distributed score bound estimation algorithm that produces sufficient score bounds for each relation, that guarantee the correctness of the rank-join result set, when the histograms are accurate. Furthermore, we propose a generalization of our framework that supports approximate statistics, in the case that the exact statistical information is not available. An extensive experimental study \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:blknAaTinKkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Speculative Query Processing.",
            "Publication year": 2003,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.11.8541&rep=rep1&type=pdf",
            "Abstract": "Speculation is an every day phenomenon whereby one acts in anticipation of particular conditions that are likely to hold in the future. Computer science research has seen many successfull applications of speculation: modern processors, for example, speculate on the run-time properties of a program and decide to pre-execute instructions accordingly. We draw inspiration from these techniques and introduce speculation to query processing. Our approach is based on a visual query interface that monitors the construction of a query and takes advantage of the user \u2018think time\u2019. In particular, based on the features of the partial query specified at any point, the interface prepares the database by issuing asynchronous manipulations to it that are likely to make the final query (or even queries further into the future) more efficient. Furthermore, the interface applies machine learning techniques on past user actions and builds a user-behavior model that guides speculation and deals with future uncertainty. We formalize speculative query processing as an optimization problem and derive algebraic properties of the corresponding cost model that are sufficient to address the complexities of the particular optimization. We have implemented our framework on top of an existing commercial database system and have evaluated its effectiveness experimentally, with actual user traces. Our results show that speculation outperforms normal query processing, reducing query execution time by an average of 35% and achieving performance improvements of more than 90% on certain queries.Permission to copy without fee all or part of this material is granted provided \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:mB3voiENLucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Predictable performance and high query concurrency for data analytics",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-011-0221-2",
            "Abstract": "Conventional data warehouses employ the query-at-a-time model, which maps each query to a distinct physical plan. When several queries execute concurrently, this model introduces contention and thrashing, because the physical plans\u2014unaware of each other\u2014compete for access to the underlying I/O and computation resources. As a result, while modern systems can efficiently optimize and evaluate a single complex data analysis query, their performance suffers significantly and can be highly erratic when multiple complex queries run at the same time. We present in this paper Cjoin, a new design that substantially improves throughput in large-scale data analytics systems processing many concurrent join queries. In contrast to the conventional query-at-a-time model our approach employs a single physical plan that shares I/O, computation, and tuple storage across all in-flight join queries. We use an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:RHpTSmoSYBkC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Report on the 10th international workshop on web information and data management (WIDM)",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1145/1670598.1670607",
            "Abstract": "The 10th ACM International Workshop on Web Information and Data Management (WIDM 2008) was held in Napa Valley, California, USA, in conjunction with the 17th International Conference on Information and Knowledge Management (CIKM), on October 30, 2008. Continuing the tradition of the previous WIDM workshops, the main objective of the workshop was to bring together researchers, industrial practitioners, and developers to study how Web information can be extracted, stored, analyzed, and processed to provide useful knowledge to the end users for various advanced database applications.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:3s1wT3WcHBgC",
            "Publisher": "ACM"
        },
        {
            "Title": "The Data Ring: Community Content Sharing.",
            "Publication year": 2007,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.2157&rep=rep1&type=pdf",
            "Abstract": "Information ubiquity has created a large crowd of users (most notably scientists), who could employ DBMS technology to process and share their data more effectively. Still, this user base prefers to keep its data in files that can be easily managed by applications such as spreadsheets, rather than deal with the complexity and rigidity of modern database systems.In this paper, we propose a vision for enabling non-experts, such as scientists, to build content sharing communities in a true database fashion: declaratively. The proposed infrastructure, called the data ring, enables users to manage and share their data with minimal effort; the user points to the data that should be shared, and the data ring becomes responsible for automatically indexing the data (to make it accessible), replicating it (for availability), and reorganizing its physical storage (for better query performance). We outline the salient features of our proposal, and outline research challenges that must be addressed in order to realize this vision.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:dhFuZR0502QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automated data slicing for model validation: A big data-AI integration approach",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8713886/",
            "Abstract": "As machine learning systems become democratized, it becomes increasingly important to help users easily debug their models. However, current data tools are still primitive when it comes to helping users trace model performance problems all the way to the data. We focus on the particular problem of slicing data to identify subsets of the validation data where the model performs poorly. This is an important problem in model validation because the overall model performance can fail to reflect that of the smaller subsets, and slicing allows users to analyze the model performance on a more granular-level. Unlike general techniques (e.g., clustering) that can find arbitrary slices, our goal is to find interpretable slices (which are easier to take action compared to arbitrary subsets) that are problematic and large. We propose  $\\mathsf{Slice Finder}$ , which is an interactive framework for identifying such slices using \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:VOx2b1Wkg3QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Metadata Service to Enable Display of Rich Artifacts in Machine Learning Pipelines",
            "Publication year": 2020,
            "Publication url": "https://www.tdcommons.org/dpubs_series/3850/",
            "Abstract": "Cloud-based machine learning (ML) platforms enable ML practitioners to build, rebuild, and serve multiple machine learning models in production environments. Proper ML metadata tracking and management is important to enable large-scale experimentation and to provide traceability and verifiability for modern production ML. This disclosure describes a ML metadata service to manage the lifecycle of metadata consumed and produced by ML pipelines. The ML metadata service enables logging detailed metadata as artifacts, capturing metadata as typed artifacts, and capturing a ML pipeline in an intuitive workflow graph. The metadata service enables provision of a ML dashboard that displays visualizations of a ML workflow along with the relevant metadata for each type of entity and supports queries for models and/or datasets that meet specific criteria.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:bnK-pcrLprsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Graph-based synopses for relational selectivity estimation",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1142473.1142497",
            "Abstract": "This paper introduces the Tuple Graph (TUG) synopses, a new class of data summaries that enable accurate selectivity estimates for complex relational queries. The proposed summarization framework adopts a\" semi-structured\" view of the relational database, modeling a relational data set as a graph of tuples and join queries as graph traversals respectively. The key idea is to approximate the structure of the induced data graph in a concise synopsis, and to estimate the selectivity of a query by performing the corresponding traversal over the summarized graph. We detail the TUG synopsis model that is based on this novel approach, and we describe an efficient and scalable construction algorithm for building accurate TUGs within a specific storage budget. We validate the performance of TUGs with an extensive experimental study on real-life and synthetic data sets. Our results verify the effectiveness of TUGs in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:MXK_kJrjxJIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Answering queries using humans, algorithms and databases",
            "Publication year": 2011,
            "Publication url": "http://ilpubs.stanford.edu:8090/986/",
            "Abstract": "For some problems, human assistance is needed in addition to automated (algorithmic) computation. In sharp contrast to existing data management approaches, where human input is either ad-hoc or is never used, we describe the design of the first declarative language involving human-computable functions, standard relational operators, as well as algorithmic computation. We consider the challenges involved in optimizing queries posed in this language, in particular, the tradeoffs between uncertainty, cost and performance, as well as combination of human and algorithmic evidence. We believe that the vision laid out in this paper can act as a road-map for a new area of data management research where human computation is routinely used in data analytics.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:UeHWp8X0CEIC",
            "Publisher": "Stanford InfoLab"
        },
        {
            "Title": "Managing Google's data lake: an overview of the Goods system.",
            "Publication year": 2016,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1053.1683&rep=rep1&type=pdf",
            "Abstract": "For most large enterprises today, data constitutes their core asset, along with code and infrastructure. For most enterprises, the amount of data that they produce internally has exploded in recent years. At the same time, in many cases, engineers and data scientists do not use centralized data-management systems and end up creating what became known as a data lake\u2014a collection of datasets that often are not well organized or not organized at all and where one needs to \u201cfish\u201d for useful datasets. In this paper, we describe our experience building and deploying GOODS, a system to manage Google\u2019s internal data lake. GOODS crawls Google\u2019s infrastructure and builds a catalog of discovered datasets, including structured files, databases, spreadsheets, and even services that provide access to the data. GOODS extracts metadata about datasets in a post-hoc way: engineers continue to generate and organize datasets in the same way that they have before, and GOODS provides value without disrupting teams\u2019 practices. The technical challenges that we had to address resulted both from the scale and heterogeneity of Google\u2019s data lake and from our decision to extract metadata in a post-hoc manner. We believe that many of the lessons that we learned are applicable to building large-scale enterprise-level data-management systems in general.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:_xSYboBqXhAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A scalable, predictable join operator for highly concurrent data warehouses",
            "Publication year": 2009,
            "Publication url": "https://infoscience.epfl.ch/record/139392",
            "Abstract": "Conventional data warehouses employ the query-at-a-time model, which maps each query to a distinct physical plan. When several queries execute concurrently, this model introduces contention, because the physical plans\u2014unaware of each other\u2014compete for access to the underlying I/O and computation resources. As a result, while modern systems can efficiently optimize and evaluate a single complex data analysis query, their performance suffers significantly when multiple complex queries run at the same time. We describe an augmentation of traditional query engines that improves join throughput in large-scale concurrent data warehouses. In contrast to the conventional query-at-a-time model, our approach employs a single physical plan that can share I/O, computation, and tuple storage across all in-flight join queries. We use an \u201calwayson\u201d pipeline of non-blocking operators, coupled with a controller that continuously examines the current query mix and performs run-time optimizations. Our design allows the query engine to scale gracefully to large data sets, provide predictable execution times, and reduce contention. In our empirical evaluation, we found that our prototype outperforms conventional commercial systems by an order of magnitude for tens to hundreds of concurrent queries.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:LkGwnXOMwfcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Trends in rank join",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-19668-3_13",
            "Abstract": "This chapter reports the main findings of a panel that was moderated by Davide Martinenghi in which Ihab Ilyas, Neoklis Polyzotis, and Marco Tagliasacchi shared their thoughts with the attendees of the Second SeCo Workshop regarding current and future issues related to what was presented during the session on rank join. The topics touched upon during the discussion regarded relevance for Search Computing, pertinence of optimization, multi-way joins, approximate answers, and uncertainty.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:J_g5lzvAfSwC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Querie: Collaborative database exploration",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6515114/",
            "Abstract": "Interactive database exploration is a key task in information mining. However, users who lack SQL expertise or familiarity with the database schema face great difficulties in performing this task. To aid these users, we developed the QueRIE system for personalized query recommendations. QueRIE continuously monitors the user's querying behavior and finds matching patterns in the system's query log, in an attempt to identify previous users with similar information needs. Subsequently, QueRIE uses these \u201csimilar\u201d users and their queries to recommend queries that the current user may find interesting. In this work we describe an instantiation of the QueRIE framework, where the active user's session is represented by a set of query fragments. The recorded fragments are used to identify similar query fragments in the previously recorded sessions, which are in turn assembled in potentially interesting queries for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:e5wmG9Sq2KIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Consistent Multi-Cloud {AI} Lifecycle Management with Kubeflow",
            "Publication year": 2019,
            "Publication url": "https://s.usenix.org/acton/ct/2452/p-0012/Bct/-/-/ct13_0/1/l?sid=TV2%3AqrnFTiiYl",
            "Abstract": "The full Proceedings published by USENIX for the conference are available for download below. Individual papers can also be downloaded from the presentation page. Copyright to the individual works is retained by the author [s].",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:fQNAKQ3IYiAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards a workload for evolutionary analytics",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2486767.2486773",
            "Abstract": "Emerging data analysis involves the ingestion and exploration of new data sets, application of complex functions, and frequent query revisions based on observing prior query answers. We call this new type of analysis evolutionary analytics and identify its properties. This type of analysis is not well represented by current benchmark workloads. In this paper, we present a workload and identify several metrics to test system support for evolutionary analytics. Along with our metrics, we present methodologies for running the workload that capture this analytical scenario.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:JV2RwH3_ST0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "SEEDB: automatically generating query visualizations",
            "Publication year": 2014,
            "Publication url": "https://dspace.mit.edu/handle/1721.1/100959",
            "Abstract": "Data analysts operating on large volumes of data often rely on visualizations to interpret the results of queries. However, finding the right visualization for a query is a laborious and time-consuming task. We demonstrate SeeDB, a system that partially automates this task: given a query, SeeDB explores the space of all possible visualizations, and automatically identifies and recommends to the analyst those visualizations it finds to be most \"interesting\" or \"useful\". In our demonstration, conference attendees will see SeeDB in action for a variety of queries on multiple real-world datasets.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:qUcmZB5y_30C",
            "Publisher": "Association for Computing Machinery (ACM)"
        },
        {
            "Title": "Optimal algorithms for evaluating rank joins in database systems",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1670243.1670249",
            "Abstract": "In the rank join problem, we are given a set of relations and a scoring function, and the goal is to return the join results with the top k scores. It is often the case in practice that the inputs may be accessed in ranked order and the scoring function is monotonic. These conditions allow for efficient algorithms that solve the rank join problem without reading all of the input. In this article, we present a thorough analysis of such rank join algorithms. A strong point of our analysis is that it is based on a more general problem statement than previous work, making it more relevant to the execution model that is employed by database systems. One of our results indicates that the well-known HRJN algorithm has shortcomings, because it does not stop reading its input as soon as possible. We find that it is NP-hard to overcome this weakness in the general case, but cases of limited query complexity are tractable. We prove the latter \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:-f6ydRqryjwC",
            "Publisher": "ACM"
        },
        {
            "Title": "Rita: An index-tuning advisor for replicated databases",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2791347.2791376",
            "Abstract": "Given a replicated database, a divergent design tunes the indexes in each replica differently in order to specialize it for a specific subset of the workload. Empirical studies have shown that this specialization brings significant performance gains compared to the common practice of having the same indexes in all replicas. However, reaping the benefits of divergent designs requires the development of new tuning tools for database administrators, and the existing tools unfortunately suffer from severe shortcomings: they assume a fixed number of replicas and a known workload distribution, and ignore the possibility of replica failures and the subsequent effect on load imbalance.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:g5m5HwL7SMYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Data infrastructure for machine learning",
            "Publication year": 2018,
            "Publication url": "https://systemsandml.org/Conferences/doc/2018/9.pdf",
            "Abstract": "Data quality is critical for effective machine learning, and this makes data a first-class citizen in the context of machine learning, on par with algorithms, software, and infrastructure. As a result, machine-learning platforms need to support data analysis and validation in a principled manner, throughout the lifecycle of the machine learning process. This paper reviews the data infrastructure we built at Google to address these challenges in the context of large-scale production machine learning pipelines.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:WbkHhVStYXYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Optimal top-k query evaluation for weighted business processes",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/1920841.1920960",
            "Abstract": "A Business Process (BP for short) consists of a set of activities that achieve some business goal when combined in a flow. Among all the (maybe infinitely many) possible execution flows of a BP, analysts are often interested in identifying flows that are \"most important\", according to some weight metric. This paper studies the following problem: given a specification of such a BP, a weighting function over BP execution flows, a query, and a number k, identify the k flows with the highest weight among those satisfying the query. We provide here, for the first time, a provably optimal algorithm for identifying the top-k weighted flows of a given BP, and use it for efficient top-k query evaluation.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:iH-uZ7U-co4C",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Kaizen: a semi-automatic index advisor",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2213836.2213932",
            "Abstract": "Index tuning; ie, selecting indexes that are appropriate for the workload to obtain good system performance, is a crucial task for database administrators. Administrators rely on automated index advisors for this task, but existing advisors work either offline, requiring a-priori knowledge of the workload, or online, taking the administrator out of the picture and assuming total control of the index tuning task. Semi-automatic index tuning is a new paradigm that achieves a middle ground: the advisor analyzes the workload online and provides recommendations tailored to the current workload, and the administrator is able to provide feedback to refine future recommendations. In this demonstration we present Kaizen, an index tuning tool that implements semi-automatic tuning.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:35N4QoGY0k4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deco: declarative crowdsourcing",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2396761.2398421",
            "Abstract": "Crowdsourcing enables programmers to incorporate\" human computation\" as a building block in algorithms that cannot be fully automated, such as text analysis and image recognition. Similarly, humans can be used as a building block in data-intensive applications--providing, comparing, and verifying data used by applications. Building upon the decades-long success of declarative approaches to conventional data management, we use a similar approach for data-intensive applications that incorporate humans. Specifically, declarative queries are posed over stored relational data as well as data computed on-demand from the crowd, and the underlying system orchestrates the computation of query answers.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:_FxGoFyzp5QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Validating Data and Models in Continuous ML pipelines",
            "Publication year": 2021,
            "Publication url": "http://sites.computer.org/debull/A21mar/A21MAR-CD.pdf#page=44",
            "Abstract": "Production ML is more than writing the code for the trainer. It requires processes and tooling that enable a larger team to share, track, analyze, and monitor not only the code for ML but also the artifacts (Datasets, Models,...) that are manipulated and generated in these production ML pipelines. In this paper we describe the tools we developed at Google for the analysis and validation of two of the most important types of artifacts: Datasets and Models. These tools are currently deployed in production at Google and other large organizations. Our approach is heavily inspired by well-known principles of data-management systems. Ultimately, we want to enable users to trust their data and models, and understand how data properties affect the quality of the generated ML models.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:5ugPr518TE4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Very Large Databases",
            "Publication year": 2007,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470050118.ecse443",
            "Abstract": "In this article, we provide an overview of date reduction and approximation methods for massive databases and discuss some of the issues that develop from different types of data, large data volumes, and applications\u2010specific requirements.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:RGFaLdJalmkC",
            "Publisher": "John Wiley & Sons, Inc."
        },
        {
            "Title": "Scaling datalog for machine learning on big data",
            "Publication year": 2012,
            "Publication url": "https://arxiv.org/abs/1203.0160",
            "Abstract": "In this paper, we present the case for a declarative foundation for data-intensive machine learning systems. Instead of creating a new system for each specific flavor of machine learning task, or hardcoding new optimizations, we argue for the use of recursive queries to program a variety of machine learning systems. By taking this approach, database query optimization techniques can be utilized to identify effective execution plans, and the resulting runtime plans can be executed on a single unified data-parallel query processing engine. As a proof of concept, we consider two programming models--Pregel and Iterative Map-Reduce-Update---from the machine learning domain, and show how they can be captured in Datalog, tuned for a specific task, and then compiled into an optimized physical plan. Experiments performed on a large computing cluster with real data demonstrate that this declarative approach can provide very good performance while offering both increased generality and programming ease.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:KlAtU1dfN6UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A benchmark for online index selection",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4812595/",
            "Abstract": "Online approaches to physical design tuning have received considerable attention in the recent literature, with a focus on the problem of online index selection. However, it is difficult to draw conclusions on the relative merits of the proposed techniques, as they have been evaluated in isolation using different methodologies. In this paper, we make two concrete contributions to address this issue. First, we propose a benchmark for evaluating the performance of an online tuning algorithm in a principled fashion. Second, using the benchmark, we present a comparison of two representative online tuning algorithms that are implemented in the same database system. The results provide interesting insights on the behavior of these algorithms and validate the usefulness of the proposed benchmark.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:HDshCWvjkbEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Statistical synopses for XML data",
            "Publication year": 2003,
            "Publication url": "https://search.proquest.com/openview/c2d3c5549c52b792a226f2cb3cbaeac1/1?pq-origsite=gscholar&cbl=18750&diss=y",
            "Abstract": "The effective optimization of declarative queries over XML data hinges upon the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for the operators of an execution plan. This thesis introduces the XSketch synopses, a novel family of statistical summaries, that allow effective selectivity estimation for common XML query constructs such as path expressions and twig queries. The abstract XSketch framework is defined in terms of three components:(a) an abstract summarization model which augments a basic graph synopsis with more detailed distribution information,(b) a methodology for computing selectivity estimates based on the recorded information and a set of statistical assumptions, and (c) a generic construction algorithm that takes directly into account the assumptions of the estimation framework in order to construct an accurate summary for a limited space budget \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:CHSYGLWDkRkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "From Data to Models and Back",
            "Publication year": 2020,
            "Publication url": "https://research.google/pubs/pub49419/",
            "Abstract": "Production ML is more than writing the code for the trainer. It requires processes and tooling that enable a larger team to share, track, analyze, and monitor not only on the code for ML but also on the artifacts (Datasets, Models,...) that are manipulated and generated in these production ML pipelines. In this paper we describe the tools we developed at Google for the analysis and validation of two of the most important types of artifacts: Datasets and Models. These tools are currently deployed in production at Google and other large organizations. Our approach is heavily inspired by well-known principles of data-management systems. Ultimately, we want to enable users to trust their data and models, and understand how data properties affect the quality of the generated ML models.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:1qzjygNMrQYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Selectivity-based partitioning: A divide-and-union paradigm for effective query optimization",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1099554.1099730",
            "Abstract": "Modern query optimizers select an efficient join ordering for a physical execution plan based essentially on the average join selectivity factors among the referenced tables. In this paper, we argue that this\" monolithic\" approach can miss important opportunities for the effective optimization of relational queries. We propose selectivity-based partitioning, a novel optimization paradigm that takes into account the join correlations among relation fragments in order to essentially enable multiple (and more effective) join orders for the evaluation of a single query. In a nutshell, the basic idea is to carefully partition a relation according to the selectivities of the join operations, and subsequently rewrite the query as a union of constituent queries over the computed partitions. We provide a formal definition of the related optimization problem and derive properties that characterize the set of optimal solutions. Based on our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:L8Ckcad2t8MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "XML Selectivity Estimation",
            "Publication year": 2009,
            "Publication url": "https://nyuscholars.nyu.edu/en/publications/xml-selectivity-estimation",
            "Abstract": "XML Selectivity Estimation \u2014 NYU Scholars Skip to main navigation Skip to search Skip to \nmain content NYU Scholars Logo Help & FAQ Home Profiles Research Units Research \nOutput Search by expertise, name or affiliation XML Selectivity Estimation M. Ramanath, \nJuliana Freire, A. Polyzotis Urban Initiative Research output: Chapter in Book/Report/Conference \nproceeding \u203a Entry for encyclopedia/dictionary Overview Original language English (US) Title \nof host publication Encyclopedia of Database Systems Editors L. Liu, T. Ozsu State Published \n- 2009 Cite this APA Standard Harvard Vancouver Author BIBTEX RIS Ramanath, M., Freire, \nJ., & Polyzotis, A. (2009). XML Selectivity Estimation. In L. Liu, & T. Ozsu (Eds.), Encyclopedia \nof Database Systems XML Selectivity Estimation. / Ramanath, M. ; Freire, Juliana; Polyzotis, A. \n. Encyclopedia of Database Systems. ed. / L. Liu; T. Ozsu. 2009. Research output: Chapter ///\u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:xtRiw3GOFMkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Continuous Training for Production {ML} in the TensorFlow Extended ({TFX}) Platform",
            "Publication year": 2019,
            "Publication url": "https://www.usenix.org/conference/opml19/presentation/baylor",
            "Abstract": "Large organizations rely increasingly on continuous ML pipelines in order to keep machine-learned models continuously up-to-date with respect to data. In this scenario, disruptions in the pipeline can increase model staleness and thus degrade the quality of downstream services supported by these models. In this paper we describe the operation of continuous pipelines in the Tensorflow Extended (TFX) platform that we developed and deployed at Google. We present the main mechanisms in TFX to support this type of pipelines in production and the lessons learned from the deployment of the platform internally at Google.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:LPZeul_q3PIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Private Database Synthesis for Outsourced System Evaluation.",
            "Publication year": 2011,
            "Publication url": "http://ceur-ws.org/Vol-749/paper10.pdf",
            "Abstract": "The goal of this paper is to permit secure outsourced system evaluation. We propose a method for generating synthetic databases and obfuscating a workload of queries in order to protect the sensitive information present in the database. The synthetic database and workload can be used by a third party to accurately carry out performance tuning, index selection, or other system evaluation tasks. As a result, an untrusted third party can evaluate whether a new technology would benefit the data owner without the risk of a privacy breach. Our approach is to employ state-of-the-art privacy mechanisms to compute the sufficient statistics of a statistical model of the true database. These statistics are safe to release, so a third party can then use them to generate one or more synthetic databases to be used as a surrogate for the true database.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:M3NEmzRMIkIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Map-reduce extensions and recursive queries",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1951365.1951367",
            "Abstract": "We survey the recent wave of extensions to the popular map-reduce systems, including those that have begun to address the implementation of recursive queries using the same computing environment as map-reduce. A central problem is that recursive tasks cannot deliver their output only at the end, which makes recovery from failures much more complicated than in map-reduce and its nonrecursive extensions. We propose several algorithmic ideas for efficient implementation of recursions in the map-reduce environment and discuss several alternatives for supporting recovery from failures without restarting the entire job.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:eQOLeE2rZwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Machine learning on big data",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6544913/",
            "Abstract": "Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:aqlVkmm33-oC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimal crowd-powered rating and filtering algorithms",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/2732939.2732942",
            "Abstract": "We focus on crowd-powered filtering, i.e., filtering a large set of items using humans. Filtering is one of the most commonly used building blocks in crowdsourcing applications and systems. While solutions for crowd-powered filtering exist, they make a range of implicit assumptions and restrictions, ultimately rendering them not powerful enough for real-world applications. We describe two approaches to discard these implicit assumptions and restrictions: one, that carefully generalizes prior work, leading to an optimal, but often-times intractable solution, and another, that provides a novel way of reasoning about filtering strategies, leading to a sometimes suboptimal, but efficiently computable solution (that is asymptotically close to optimal). We demonstrate that our techniques lead to significant reductions in error of up to 30% for fixed cost over prior work in a novel crowdsourcing application: peer evaluation in online \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:ZeXyd9-uunAC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Post-hoc management of datasets",
            "Publication year": 2019,
            "Publication url": "https://patents.google.com/patent/US10417439B2/en",
            "Abstract": "Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating a catalog for multiple datasets, the method comprising accessing multiple extant data sets, the extant data sets including data sets that are independently generated and structurally dissimilar; organizing the data sets into collections, each data set in each collection belonging to the collection based on collection data associated with the data set; for each collection of data sets: determining, from a subset of the data sets that belong to the collection, metadata that describe the data sets that belong to the collection, wherein the metadata does not include the collection data, and attributing, to other data sets in the collection, the metadata determined from the subset of data sets; and generating, from the collections of data sets and the determined metadata, a catalog for the multiple datasets.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:SP6oXDckpogC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Efficient techniques for crowdsourced top-k lists",
            "Publication year": 2016,
            "Publication url": "https://ojs.aaai.org/index.php/HCOMP/article/view/13281",
            "Abstract": "We propose techniques that obtain top-k lists of items out of larger itemsets, using human workers to perform comparisons among items. An example application is to short-list a large set of college applications using advanced students as workers. A method that obtains crowdsourced top-k lists has to address several challenges of crowdsourcing: there are constraints in the total number of tasks due to monetary or practical reasons; tasks posted to workers have an inherent limitation on their size; obtaining results from human workers has high latency; workers may disagree on their judgments for the same items or provide wrong results on purpose; and, there can be varying difficulty among tasks of the same size. We describe novel efficient techniques and explore their tolerance to adversarial behavior and the tradeoffs among different measures of performance (latency, expense and quality of results). We empirically evaluate the proposed techniques using simulations as well as real crowds in Amazon Mechanical Turk. A randomized variant of the proposed algorithms achieves significant budget saves, especially for very large itemsets and large top-k lists, with negligible risk of lowering the quality of the output.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:1sJd4Hv_s6UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "AQAX: a system for approximate XML query answers",
            "Publication year": 2006,
            "Publication url": "https://www.academia.edu/download/31984213/p1159-spiegel.pdf",
            "Abstract": "On-line, interactive exploration of large databases becomes prohibitively expensive as the size of the database grows. Approximate query answering offers a cost-effective solution to this problem, by enabling the fast generation of approximate results based on concise data summaries. We apply this paradigm in the context of XML databases, where the increased complexity of data and queries amplifies the challenges behind interactive exploration. We have developed an on-line XML exploration system, termed AQAX that relies on accurate XML summaries in order to enable the rapid exploration of large data sets. To effectively support the exploration of semi-structured query answers, our system employs a tight coupling between the main query processor and the graphical clients that visualize the results. This demonstration will showcase the functionality of our system and the effectiveness of approximate query answering in the context of XML databases.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:R3hNpaxXUhUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Best Newcomer Award: Evaluating rank joins with optimal costs",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1376916.1452516",
            "Abstract": "Best Newcomer Award: Evaluating rank joins with optimal costs | Proceedings of the twenty-seventh \nACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems ACM Digital \nLibrary Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in \nRegister Advanced Search Journals Magazines Proceedings Books SIGs Conferences \nPeople More Search ACM Digital Library SearchSearch Advanced Search mod Conference \nProceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesMODProceedingsPODS \n'08Best Newcomer Award: Evaluating rank joins with optimal costs ARTICLE Best \nNewcomer Award: Evaluating rank joins with optimal costs Share on Authors: Carl \nSchneider Search about this author , Neoklis Polyzotis Search about this author Authors Info \n& Affiliations Publication: PODS '08: Proceedings of the twenty-seventh ACM SIGMOD--on .: \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:70eg2SAEIzsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Web information management with access control",
            "Publication year": 2011,
            "Publication url": "https://hal.inria.fr/inria-00595200/",
            "Abstract": "We investigate the problem of sharing private information on the Web, where the information is hosted on different machines that may use different access control and distribution schemes. We introduce a distributed knowledge-base model, termed WebdamExchange, that comprises logical statements for specifying data, access control, distribution and knowledge about other peers. The statements can be communicated, replicated, queried, and updated, while keeping track of time and provenance. This unified base allows applications to reason declaratively about what data is accessible, where it resides, and how to retrieve it securely.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:ZHo1McVdvXMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Robust and efficient algorithms for rank join evaluation",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1559845.1559890",
            "Abstract": "In the rank join problem we are given a relational join R 1 x R 2 and a function that assigns numeric scores to the join tuples, and the goal is to return the tuples with the highest score. This problem lies at the core of processing top-k SQL queries, and recent studies have introduced specialized operators that solve the rank join problem by accessing only a subset of the input tuples. A desirable property for such operators is instance-optimality, ie, their I/O cost should remain within a factor of the optimal for different inputs. However, a recent theoretical study has shown that existing rank join operators are not instance-optimal even though they have been shown to perform well empirically. The same study proposed the PBRJ RR over FR operator that was proved to be instance-optimal, but its performance was not tested empirically and in fact it was hinted that its complexity can be high. Thus, the following important \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:8k81kl-MbHgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exploiting opportunistic physical design in large-scale data analytics",
            "Publication year": 2013,
            "Publication url": "https://arxiv.org/abs/1303.6609",
            "Abstract": "Large-scale systems, such as MapReduce and Hadoop, perform aggressive materialization of intermediate job results in order to support fault tolerance. When jobs correspond to exploratory queries submitted by data analysts, these materializations yield a large set of materialized views that typically capture common computation among successive queries from the same analyst, or even across queries of different analysts who test similar hypotheses. We propose to treat these views as an opportunistic physical design and use them for the purpose of query optimization. We develop a novel query-rewrite algorithm that addresses the two main challenges in this context: how to search the large space of rewrites, and how to reason about views that contain UDFs (a common feature in large-scale data analytics). The algorithm, which provably finds the minimum-cost rewrite, is inspired by nearest-neighbor searches in non-metric spaces. We present an extensive experimental study on real-world datasets with a prototype data-analytics system based on Hive. The results demonstrate that our approach can result in dramatic performance improvements on complex data-analysis queries, reducing total execution time by an average of 61% and up to two orders of magnitude.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:hMod-77fHWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Max algorithms in crowdsourcing environments",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2187836.2187969",
            "Abstract": "Our work investigates the problem of retrieving the maximum item from a set in crowdsourcing environments. We first develop parameterized families of max algorithms, that take as input a set of items and output an item from the set that is believed to be the maximum. Such max algorithms could, for instance, select the best Facebook profile that matches a given person or the best photo that describes a given restaurant. Then, we propose strategies that select appropriate max algorithm parameters. Our framework supports various human error and cost models and we consider many of them for our experiments. We evaluate under many metrics, both analytically and via simulations, the tradeoff between three quantities:(1) quality,(2) monetary cost, and (3) execution time. Also, we provide insights on the effectiveness of the strategies in selecting appropriate max algorithm parameters and guidelines for choosing max \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:roLk4NBRz8UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sidr: Efficient structure-aware intelligent data routing in scihadoop",
            "Publication year": 2012,
            "Publication url": "https://users.soe.ucsc.edu/~carlosm/dev/publication/buck-ucsctr-12/buck-ucsctr-12.pdf",
            "Abstract": "MapReduce is a popular framework for distributed, parallel computation that has started to be used in domains quite different from the web applications for which it was designed, including the processing of big structured data, eg, scientific and financial data. Previous work on using MapReduce to process scientific data did not incorporate knowledge of this structure in its internal communications. We show that performance gains can be realized by leveraging knowledge of the structure of the data to minimize and localize communications between nodes, guarantee workload balance across processing nodes, ensure that Reduce tasks start as soon as possible, and create balanced, contiguous output. We implemented these improvements in SciHadoop, a version of the open-source Hadoop MapReduce framework designed for structured scientific data. Our results show total query execution time reductions of up to 29% over SciHadoop with initial results available with only 6% of the query completed, and the resultant output is more efficiently organized, compared to Hadoop.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:NMxIlDl6LWMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cluster computing, recursion and datalog",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-24206-9_8",
            "Abstract": "The cluster-computing environment typified by Hadoop, the open-source implementation of map-reduce, is receiving serious attention as the way to execute queries and other operations on very large-scale data. Datalog execution presents several unusual issues for this enviroment. We discuss the best way to execute a round of seminaive evaluation on a computing cluster using the map-reduce. Using transitive closure as an example, we examine the cost of executing recursions in several different ways. Recursive processes such as evaluation of a recursive Datalog program do not fit the key map-reduce assumption that tasks deliver output only when they are completed. As a result, the resilience under compute-node failure that is a key element of the map-reduce framework is not supported for recursive programs. We discuss extensions to this framework that are suitable for executing recursive Datalog \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:4JMBOYKVnBMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Oracle workload intelligence",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2723372.2742791",
            "Abstract": "Analyzing and understanding the characteristics of the incoming workload is crucial in unraveling trends and tuning the performance of a database system. In this work, we present Oracle Workload Intelligence (WI), a tool for workload modeling and mining, as our attempt to infer the processes that generate a given workload. WI consists of two main functionalities. First, WI derives a model that captures the main characteristics of the workload without overfitting, which makes it likely to generalize well to unseen instances of the workload. Such a model provides insights into the most frequent code paths in the application that drives the workload, and also enables optimizations inside the database system that target sequences of query statements. Second, WI can compare the models of different snapshots of the workload to detect whether the workload has changed. Such changes might indicate new trends \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:HoB7MX3m0LUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Colt: continuous on-line tuning",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1142473.1142592",
            "Abstract": "The physical schema of a database plays a critical role in performance. Self-tuning is a cost-effective and elegant solution to optimize the physical configuration for the characteristics of the query load. Existing techniques operate in an off-line fashion, by choosing a fixed configuration that is tailored to a subset of the query load. The generated configurations therefore ignore any temporal patterns that may exist in the actual load submitted to the system. This demonstration introduces COLT (Continuous On-Line Tuning), a novel self-tuning framework that continuously monitors the incoming queries and adjusts the system configuration in order to maximize query performance. The key idea behind COLT is to gather performance statistics at different levels of detail and to carefully allocate profiling resources to the most promising candidate configurations. Moreover, COLT uses effective heuristics to regulate its own \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:W7OEmFMy1HYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tensorflow data validation: Data analysis and validation in continuous ml pipelines",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3318464.3384707",
            "Abstract": "Machine Learning (ML) research has primarily focused on improving the accuracy and efficiency of the training algorithms while paying much less attention to the equally important problem of understanding, validating, and monitoring the data fed to ML. Irrespective of the ML algorithms used, data errors can adversely affect the quality of the generated model. This indicates that we need to adopt a data-centric approach to ML that treats data as a first-class citizen, on par with algorithms and infrastructure which are the typical building blocks of ML pipelines. In this demonstration we showcase TensorFlow Data Validation (TFDV), a scalable data analysis and validation system for ML that we have developed at Google and recently open-sourced. This system is deployed in production as an integral part of TFX-an end-to-end machine learning platform at Google. It is used by hundreds of product teams at Google and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:mvPsJ3kp5DgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fusing data management services with file systems",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1713072.1713085",
            "Abstract": "File systems are the backbone of large-scale data processing for scientific applications. Motivated by the need to provide an extensible and flexible framework beyond the abstractions provided by API libraries for files to manage and analyze large-scale data, we are developing Damasc, an enhanced file system where rich data management services for scientific computing are provided as a native part of the file system.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Slice finder: Automated data slicing for model interpretability",
            "Publication year": 2018,
            "Publication url": "https://mlsys.org/Conferences/2019/doc/2018/8.pdf",
            "Abstract": "As machine learning (ML) systems become democratized, helping users easily debug their models becomes increasingly important. Yet current data tools are still primitive when it comes to helping users trace model performance problems all the way to the data. We focus on the particular problem of slicing data to identify subsets of the training data where the model performs poorly. Unlike general techniques (eg, clustering) that can find arbitrary slices, our goal is to find interpretable slices (which are easier to take action compared to arbitrary subsets) that are problematic and large. We propose Slice Finder, which is an interactive framework for identifying such slices using statistical techniques. The slices can be used for applications like diagnosing model fairness and fraud detection where describing slices that are interpretable to humans is necessary.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:eJXPG6dFmWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Statistical synopses for graph-structured XML databases",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/564691.564733",
            "Abstract": "Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (eg, XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, We introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:u5HHmVD_uO8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Declarative Systems for Large-Scale Machine Learning.",
            "Publication year": 2012,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.362.4961&rep=rep1&type=pdf",
            "Abstract": "In this article, we make the case for a declarative foundation for data-intensive machine learning systems. Instead of creating a new system for each specific flavor of machine learning task, or hardcoding new optimizations, we argue for the use of recursive queries to program a variety of machine learning algorithms. By taking this approach, database query optimization techniques can be utilized to identify effective execution plans, and the resulting runtime plans can be executed on a single unified data-parallel query processing engine.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:4DMP91E08xMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Data Validation for Machine Learning.",
            "Publication year": 2019,
            "Publication url": "https://mlsys.org/Conferences/2019/doc/2019/167.pdf",
            "Abstract": "Machine learning is a powerful tool for gleaning knowledge from massive amounts of data. While a great deal of machine learning research has focused on improving the accuracy and efficiency of training and inference algorithms, there is less attention in the equally important problem of monitoring the quality of data fed to machine learning. The importance of this problem is hard to dispute: errors in the input data can nullify any benefits on speed and accuracy for training and inference. This argument points to a data-centric approach to machine learning that treats training and serving data as an important production asset, on par with the algorithm and infrastructure used for learning.In this paper, we tackle this problem and present a data validation system that is designed to detect anomalies specifically in data fed into machine learning pipelines. This system is deployed in production as an integral part of TFX (Baylor et al., 2017)\u2013an end-to-end machine learning platform at Google. It is used by hundreds of product teams use it to continuously monitor and validate several petabytes of production data per day. We faced several challenges in developing our system, most notably around the ability of ML pipelines to soldier on in the face of unexpected patterns, schema-free data, or training/serving skew. We discuss these challenges, the techniques we used to address them, and the various design choices that we made in implementing the system. Finally, we present evidence from the system\u2019s deployment in production that illustrate the tangible benefits of data validation in the context of ML: early detection of errors, model-quality wins from using \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:XiVPGOgt02cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Approximate XML query answers",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1007568.1007599",
            "Abstract": "The rapid adoption of XML as the standard for data representation and exchange foreshadows a massive increase in the amounts of XML data collected, maintained, and queried over the Internet or in large corporate data-stores. Inevitably, this will result in the development of on-line decision support systems, where users and analysts interactively explore large XML data sets through a declarative query interface (eg, XQuery or XSLT). Given the importance of remaining interactive, such on-line systems can employ approximate query answers as an effective mechanism for reducing response time and providing users with early feedback. This approach has been successfully used in relational systems and it becomes even more compelling in the XML world, where the evaluation of complex queries over massive tree-structured data is inherently more expensive. In this paper, we initiate a study of approximate query \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:u-x6o8ySG0sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Slice finder: Automated data slicing for model validation",
            "Publication year": 2019,
            "Publication url": "https://research.google/pubs/pub47966/",
            "Abstract": "As machine learning (ML) systems become democratized, helping users easily debug their models becomes increasingly important. Yet current data tools are still primitive when it comes to helping users trace model performance problems all the way to the data. We focus on the particular prob-lem of slicing data to identify subsets of the training data where the model performs poorly. Unlike general techniques (eg, clustering) that can find arbitrary slices, our goal is to find interpretable slices (which are easier to take action com-pared to arbitrary subsets) that are problematic and large. We propose Slice Finder, which is an interactive framework for identifying such slices using statistical techniques. The slices can be used for applications like diagnosing model fair-ness and fraud detection where describing slices that are interpretable to humans is necessary.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:geHnlv5EZngC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cophy: A scalable, portable, and interactive index advisor for large workloads",
            "Publication year": 2011,
            "Publication url": "https://arxiv.org/abs/1104.3214",
            "Abstract": "Index tuning, i.e., selecting the indexes appropriate for a workload, is a crucial problem in database system tuning. In this paper, we solve index tuning for large problem instances that are common in practice, e.g., thousands of queries in the workload, thousands of candidate indexes and several hard and soft constraints. Our work is the first to reveal that the index tuning problem has a well structured space of solutions, and this space can be explored efficiently with well known techniques from linear optimization. Experimental results demonstrate that our approach outperforms state-of-the-art commercial and research techniques by a significant margin (up to an order of magnitude).",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:9ZlFYXVOiuMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Opportunistic physical design for big data analytics",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2588555.2610512",
            "Abstract": "Big data analytical systems, such as MapReduce, perform aggressive materialization of intermediate job results in order to support fault tolerance. When jobs correspond to exploratory queries submitted by data analysts, these materializations yield a large set of materialized views that we propose to treat as an opportunistic physical design. We present a semantic model for UDFs that enables effective reuse of views containing UDFs along with a rewrite algorithm that provably finds the minimum-cost rewrite under certain assumptions. An experimental study on real-world datasets using our prototype based on Hive shows that our approach can result in dramatic performance improvements.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:TQgYirikUcIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Benchmarking Online Index-Tuning Algorithms.",
            "Publication year": 2011,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.229.1824&rep=rep1&type=pdf",
            "Abstract": "The topic of index tuning has received considerable attention in the research literature. However, very few studies provide a comparative evaluation of the proposed index tuning techniques in the same environment and with the same experimental methodology. In this paper, we outline our efforts in this direction with the development of a performance benchmark for the specific problem of online index tuning. We describe the salient features of the benchmark, present some representative results on the evaluation of different index tuning techniques, and conclude with lessons we learned about implementing and running a benchmark for self tuning systems.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:BqipwSGYUEgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On-line Database Tuning",
            "Publication year": 2006,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.119.7219&rep=rep1&type=pdf",
            "Abstract": "This paper introduces Colt (Continuous On-Line Tuning), a novel self-tuning framework that continuously monitors the incoming queries and adjusts the system configuration in order to maximize query performance. The key idea behind Colt is to gather performance statistics at different levels of detail and to carefully allocate profiling resources to the most promising candidate configurations. Moreover, Colt uses effective heuristics to self-regulate its own performance, lowering its overhead when the system is well tuned and being more aggressive when the workload shifts and it becomes necessary to re-tune the system. We detail the design of the generic Colt system, and present its specialization to the important problem of selecting an effective set of indices for a relational query load. We describe an implementation of the proposed framework in the PostgreSQL database system and evaluate its performance experimentally. Our results validate the effectiveness of Colt in self-tuning a relational database, demonstrating its ability to modify the system configuration in response to changes in the query load. Moreover, Colt achieves performance improvements that are comparable to more expensive off-line techniques, thus verifying the potential of the on-line approach in the design of self-tuning systems.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:GnPB-g6toBAC",
            "Publisher": "Technical Report UCSC-CRL-06-07, UC Santa Cruz"
        },
        {
            "Title": "Meshing streaming updates with persistent data in an active data warehouse",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4441713/",
            "Abstract": "Active data warehousing has emerged as an alternative to conventional warehousing practices in order to meet the high demand of applications for up-to-date information. In a nutshell, an active warehouse is refreshed online and thus achieves a higher consistency between the stored information and the latest data updates. The need for online warehouse refreshment introduces several challenges in the implementation of data warehouse transformations, with respect to their execution time and their overhead to the warehouse processes. In this paper, we focus on a frequently encountered operation in this context, namely, the join of a fast stream 5\" of source updates with a disk-based relation R, under the constraint of limited memory. This operation lies at the core of several common transformations such as surrogate key assignment, duplicate detection, or identification of newly inserted tuples. We propose a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:WF5omc3nYNoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Depth estimation for ranking query optimization",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-008-0124-z",
            "Abstract": "A relational ranking query uses a scoring function to limit the results of a conventional query to a small number of the most relevant answers. The increasing popularity of this query paradigm has led to the introduction of specialized rank join operators that integrate the selection of top tuples with join processing. These operators access just \u201cenough\u201d of the input in order to generate just \u201cenough\u201d output and can offer significant speed-ups for query evaluation. The number of input tuples that an operator accesses is called the input depth of the operator, and this is the driving cost factor in rank join processing. This introduces the important problem of depth estimation, which is crucial for the costing of rank join operators during query compilation and thus for their integration in optimized physical plans. We introduce an estimation methodology, termed deep, for approximating the input depths of rank join \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:3fE2CSJIrl8C",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Seedb: Visualizing database queries efficiently",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/2732240.2732250",
            "Abstract": "Data scientists rely on visualizations to interpret the data returned by queries, but finding the right visualization remains a manual task that is often laborious. We propose a DBMS that partially automates the task of finding the right visualizations for a query. In a nutshell, given an input query Q, the new DBMS optimizer will explore not only the space of physical plans for Q, but also the space of possible visualizations for the results of Q. The output will comprise a recommendation of potentially \"interesting\" or \"useful\" visualizations, where each visualization is coupled with a suitable query execution plan. We discuss the technical challenges in building this system and outline an agenda for future research.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:IWHjjKOFINEC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Report on the 9th international workshop on web information and data management (WIDM 2007)",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1394251.1394258",
            "Abstract": "The 9th ACM International Workshop on Web Information and Data Management (WIDM 2007) was held in Lisbon, Portugal, in conjunction with the 16th International Conference on Information and Knowledge Management (CIKM), on November 9, 2007.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:M05iB0D1s5AC",
            "Publisher": "ACM"
        },
        {
            "Title": "Sidr: Structure-aware intelligent data routing in hadoop",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2503210.2503241",
            "Abstract": "The MapReduce framework is being extended for domains quite different from the web applications for which it was designed, including the processing of big structured data, eg, scientific and financial data. Previous work using MapReduce to process scientific data ignores existing structure when assigning intermediate data and scheduling tasks. In this paper, we present a method for incorporating knowledge of the structure of scientific data and executing query into the MapReduce communication model. Built in SciHadoop, a version of the Hadoop MapReduce framework for scientific data, SIDR intelligently partitions and routes intermediate data, allowing it to: remove Hadoop's global barrier and execute Reduce tasks prior to all Map tasks completing; minimize intermediate key skew; and produce early, correct results. SIDR executes queries up to 2.5 times faster than Hadoop and 37% faster than SciHadoop \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:O3NaXMp0MMsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "System for evolutionary analytics",
            "Publication year": 2015,
            "Publication url": "https://patents.google.com/patent/US9183253B2/en",
            "Abstract": "A system for evolutionary analytics supports three dimensions (analytical workflows, the users, and the data) by rewriting workflows to be more efficient by using answers materialized as part of previous workflow execution runs in the system.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:NhqRSupF_l8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Data Engineering",
            "Publication year": 2011,
            "Publication url": "https://www.academia.edu/download/21361968/debullpublished.pdf",
            "Abstract": "We give an account of the researches on context-aware information tailoring which are going on within the PEDiGREE1 group at Politecnico di Milano, starting from a foundational framework for the lifecycle of context-aware information systems, in which the system design and management activities consider context as an orthogonal, first-class citizen. The design-time and run-time activities involved in this life-cycle provide material for stimulating research, summarized in this paper.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:zA6iFVUQeVQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "High-concurrency query operator and method",
            "Publication year": 2012,
            "Publication url": "https://patents.google.com/patent/US8285709B2/en",
            "Abstract": "In one embodiment, a method includes concurrently executing a set of multiple queries, through a processor, to improve a resource usage within a data warehouse system. The method also includes permitting a group of users of the data warehouse system to simultaneously run a set of queries. In addition, the method includes applying a high-concurrency query operator to continuously optimize a large number of concurrent queries for a set of highly concurrent dynamic workloads.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:bEWYMUwI8FkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "DEEM 2019: Workshop on Data Management for End-to-End Machine Learning",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3299869.3323598",
            "Abstract": "The DEEM workshop brings together researchers and practitioners at the intersection of applied machine learning, data management and systems research, with the goal to discuss the arising data management issues in machine learning application scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:5Ul4iDaHHb8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "SciHadoop Semantic Compression",
            "Publication year": 2012,
            "Publication url": "https://users.soe.ucsc.edu/~carlosm/dev/publication/crume-ucsctr-12/crume-ucsctr-12.pdf",
            "Abstract": "Many scientific applications, when written in a MapReduce paradigm, naturally use grid coordinates as keys. Unfortunately, a straightforward representation of intermediate keys leads to an enormous amount of overhead. We show how grid coordinates can be stored compactly, yielding a significant reduction in data size. This is an important step in making MapReduce systems such as Hadoop more attractive for developers of scientific applications.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:UxriW0iASnsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "System for multi-store analytics execution environments with storage constraints",
            "Publication year": 2016,
            "Publication url": "https://patents.google.com/patent/US9477708B2/en",
            "Abstract": "Systems and methods are disclosed for managing a multi-store execution environment by applying opportunistic materialized views to improve workload performance and executing a plan on multiple database engines to increase query processing speed by leveraging unique capabilities of each engine by enabling stages of a query to execute on multiple engines, and by moving materialized views across engines.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:RYcK_YlVTxYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX)",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2010.02013",
            "Abstract": "Software Engineering, as a discipline, has matured over the past 5+ decades. The modern world heavily depends on it, so the increased maturity of Software Engineering was an eventuality. Practices like testing and reliable technologies help make Software Engineering reliable enough to build industries upon. Meanwhile, Machine Learning (ML) has also grown over the past 2+ decades. ML is used more and more for research, experimentation and production workloads. ML now commonly powers widely-used products integral to our lives. But ML Engineering, as a discipline, has not widely matured as much as its Software Engineering ancestor. Can we take what we have learned and help the nascent field of applied ML evolve into ML Engineering the way Programming evolved into Software Engineering [1]? In this article we will give a whirlwind tour of Sibyl [2] and TensorFlow Extended (TFX) [3], two successive end-to-end (E2E) ML platforms at Alphabet. We will share the lessons learned from over a decade of applied ML built on these platforms, explain both their similarities and their differences, and expand on the shifts (both mental and technical) that helped us on our journey. In addition, we will highlight some of the capabilities of TFX that help realize several aspects of ML Engineering. We argue that in order to unlock the gains ML can bring, organizations should advance the maturity of their ML teams by investing in robust ML infrastructure and promoting ML Engineering education. We also recommend that before focusing on cutting-edge ML modeling techniques, product leaders should invest more time in adopting interoperable ML \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:J-pR_7NvFogC",
            "Publisher": "Unknown"
        },
        {
            "Title": "XCluster synopses for structured XML content",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1617431/",
            "Abstract": "We tackle the difficult problem of summarizing the path/branching structure and value content of an XML database that comprises both numeric and textual values. We introduce a novel XML-summarization model, termed XCLUSTERs, that enables accurate selectivity estimates for the class of twig queries with numeric-range, substring, and textual IR predicates over the content of XML elements. In a nutshell, an XCLUSTER synopsis represents an effective clustering of XML elements based on both their structural and value-based characteristics. By leveraging techniques for summarizing XML-document structure as well as numeric and textual data distributions, our XCLUSTER model provides the first known unified framework for handling path/branching structure and different types of element values. We detail the XCLUSTER model, and develop a systematic framework for the construction of effective XCLUSTER \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Querie: A query recommender system supporting interactive database exploration",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5693465/",
            "Abstract": "This demonstration presents QueRIE, a recommender system that supports interactive database exploration. This system aims at assisting non-expert users of scientific databases by generating personalized query recommendations. Drawing inspiration from Web recommender systems, QueRIE tracks the querying behavior of each user and identifies potentially \u201cinteresting\u201d parts of the database related to the corresponding data analysis task by locating those database parts that were accessed by similar users in the past. It then generates and recommends the queries that cover those parts to the user.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:YFjsv_pBGBYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Identifying reliable workers swiftly",
            "Publication year": 2012,
            "Publication url": "http://ilpubs.stanford.edu:8090/1043/",
            "Abstract": "We study the evaluation and replacement of workers in a crowdsourcing system. We focus on evaluation based on result disagreement among workers, and on policies that replace low accuracy workers with fresh ones from a large pool. We study how long it takes the system to identify an active set of high accuracy workers, and the achieved accuracy of the selected workers. We study through simulations the dynamics of such a system, and we propose a rule of thumb that is helpful for selecting parameters of the evaluation/replacement algorithm.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:7PzlFSSx8tAC",
            "Publisher": "Stanford InfoLab"
        },
        {
            "Title": "Structure and value synopses for XML data graphs",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/B9781558608696500482",
            "Abstract": "This chapter proposes a novel XSKETCH graph synopsis model for eXtensible Markup Language (XML) data graphs with raw data values. All existing proposals for querying XML rely on a pattern-specification language that allows path navigation and branching through the label structure of the XML data graph, and predicates on the values of specific path/branch nodes in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile time selectivity estimates for complex path expressions over graph-structured XML data. XML is rapidly emerging as the new standard for data representation and exchange on the Internet. The simple, self-describing nature of the XML standard promises to enable a broad suite of next-generation Internet applications, ranging from intelligent Web searching and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:9yKSN-GCB0IC",
            "Publisher": "Morgan Kaufmann"
        },
        {
            "Title": "Sketch-based summarization of ordered XML streams",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4812433/",
            "Abstract": "In this paper, we tackle the problem of approximately answering a continuous aggregate query over an XML stream using limited memory. This problem is key in the development of tools for the on-line monitoring and analysis of streaming XML data, such as complex event streams, RSS feeds, or workflow traces. We introduce a novel technique that supports XML queries with any combination of the common XPath axes, namely, ancestor, descendant, parent, child, following, preceding, following-sibling, and preceding-sibling. At the heart of our approach lies an efficient transform that reduces a continuous XML query to an equi-join query over relational streams. We detail the transform and discuss its integration with randomized sketches as a basic mechanism to estimate the result of the XML query. We further enhance this mechanism with structural sieving, a technique that takes advantage of the XML data and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:j3f4tGmQtD8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Slice finder: Automated data slicing for model validation",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8731353/",
            "Abstract": "As machine learning (ML) systems become democratized, it becomes increasingly important to help users easily debug their models. However, current data tools are still primitive when it comes to helping users trace model performance problems all the way to the data. We focus on the particular problem of slicing data to identify subsets of the validation data where the model performs poorly. This is an important problem in model validation because the overall model performance can fail to reflect that of the smaller subsets, and slicing allows users to analyze the model performance on a more granular-level. Unlike general techniques (e.g., clustering) that can find arbitrary slices, our goal is to find interpretable slices (which are easier to take action compared to arbitrary subsets) that are large and problematic. We propose Slice Finder, which is an interactive framework for identifying such slices using statistical \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:Tiz5es2fbqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Selectivity estimation for XML twigs",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1320003/",
            "Abstract": "Twig queries represent the building blocks of declarative query languages over XML data. A twig query describes a complex traversal of the document graph and generates a set of element tuples based on the intertwined evaluation (i.e., join) of multiple path expressions. Estimating the result cardinality of twig queries or, equivalently, the number of tuples in such a structural (path-based) join, is a fundamental problem that arises in the optimization of declarative queries over XML. It is crucial, therefore, to develop concise synopsis structures that summarize the document graph and enable such selectivity estimates within the time and space constraints of the optimizer. We propose novel summarization and estimation techniques for estimating the selectivity of twig queries with complex XPath expressions over tree-structured data. Our approach is based on the XSKETCH model, augmented with new types of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:IjCSPb-OGe4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The QueRIE system for Personalized Query Recommendations.",
            "Publication year": 2011,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.229.1925&rep=rep1&type=pdf",
            "Abstract": "Interactive database exploration is a key task in information mining. However, users who lack SQL expertise or familiarity with the database schema face great difficulties in performing this task. To aid these users, we developed the QueRIE system for personalized query recommendations. QueRIE continuously monitors the user\u2019s querying behavior and finds matching patterns in the system\u2019s query log, in an attempt to identify previous users with similar information needs. Subsequently, QueRIE uses these \u201csimilar\u201d users and their queries to recommend queries that the current user may find interesting. We discuss the key components of QueRIE and describe empirical results based on actual user traces with the Sky Server database.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:qxL8FJ1GzNcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tfx: A tensorflow-based production-scale machine learning platform",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3097983.3098021",
            "Abstract": "Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components---a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:dshw04ExmUIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automated discovery of template patterns based on received server requests",
            "Publication year": 2014,
            "Publication url": "https://patents.google.com/patent/US8782219B2/en",
            "Abstract": "Described herein are methods for determining patterns based on requests received by a server. Based on the determined patterns, insight into the types of requests received by the server can be gained. Additionally, performance statistics and query statistics can be aggregated in a useful way. For example, performance statistics may be summarized for each determined pattern. One technique for determining patterns includes determining a sequence of template identifiers identifying templates that correspond to sub-sequences of requests in a sequence of server requests. A model may be created based on the sequence of template identifiers. Based on the model, template patterns may be determined. Template patterns may further be grouped into pattern clusters.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:rO6llkc54NcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving differentially private models with active learning",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1910.01177",
            "Abstract": "Broad adoption of machine learning techniques has increased privacy concerns for models trained on sensitive data such as medical records. Existing techniques for training differentially private (DP) models give rigorous privacy guarantees, but applying these techniques to neural networks can severely degrade model performance. This performance reduction is an obstacle to deploying private models in the real world. In this work, we improve the performance of DP models by fine-tuning them through active learning on public data. We introduce two new techniques - DIVERSEPUBLIC and NEARPRIVATE - for doing this fine-tuning in a privacy-aware way. For the MNIST and SVHN datasets, these techniques improve state-of-the-art accuracy for DP models while retaining privacy guarantees.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:eflP2zaiRacC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Index interactions in physical design tuning: modeling, analysis, and applications",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/1687627.1687766",
            "Abstract": "One of the key tasks of a database administrator is to optimize the set of materialized indices with respect to the current workload. To aid administrators in this challenging task, commercial DBMSs provide advisors that recommend a set of indices based on a sample workload. It is left for the administrator to decide which of the recommended indices to materialize and when. This decision requires some knowledge of how the indices benefit the workload, which may be difficult to understand if there are any dependencies or interactions among indices. Unfortunately, advisors do not provide this crucial information as part of the recommendation.Motivated by this shortcoming, we propose a framework and associated tools that can help an administrator understand the interactions within the recommended set of indices. We formalize the notion of index interactions and develop a novel algorithm to identify the interaction \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:ULOm3_A8WrAC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Evaluating rank joins with optimal cost",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1376916.1376924",
            "Abstract": "In the rank join problem, we are given a set of relations and a scoring function, and the goal is to return the join results with the top K scores. It is often the case in practice that the inputs may be accessed in ranked order and the scoring function is monotonic. These conditions allow for efficient algorithms that solve the rank join problem without reading all of the input. In this paper, we present a thorough analysis of such rank join algorithms. A strong point of our analysis is that it is based on a more general problem statement than previous work, making it more relevant to the execution model that is employed by database systems. One of our results indicates that the well known HRJN algorithm has shortcomings, because it does not stop reading its input as soon as possible. We find that it is NP-hard to overcome this weakness in the general case, but cases of limited query complexity are tractable. We prove the latter \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:Se3iqnhoufwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Web information management with access control.",
            "Publication year": 2011,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.229.3397&rep=rep1&type=pdf",
            "Abstract": "We investigate the problem of sharing private information on the Web, where the information is hosted on different machines that may use different access control and distribution schemes. We introduce a distributed knowledge-base model, termed WebdamExchange, that comprises logical statements for specifying data, access control, distribution and knowledge about other peers. The statements can be communicated, replicated, queried, and updated, while keeping track of time and provenance. This unified base allows applications to reason declaratively about what data is accessible, where it resides, and how to retrieve it securely.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:r0BpntZqJG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Data Ring: Let us turn the net into a database!",
            "Publication year": 2006,
            "Publication url": "https://www.researchgate.net/profile/Serge-Abiteboul/publication/221651302_Data_Ring_Let_Us_Turn_the_Net_into_a_Database/links/0c960537cb4fc292ea000000/Data-Ring-Let-Us-Turn-the-Net-into-a-Database.pdf",
            "Abstract": "Because of information ubiquity, one observes an important trend towards transferring information management tasks from database systems to networks. We introduce the notion of Data Ring that can be seen as a network version of a database or a content warehouse. A main goal is to achieve better performance for content management without requiring the acquisition of explicit control over information resources. We discuss the main traits of Data Rings and argue that Active XML provides an appropriate basis for such systems. The collaborating peers that form the Data Ring are autonomous, heterogeneous and their capabilities may greatly vary, eg, from a sensor to a large database. To support effectively this paradigm of loose integration, the Data Ring enforces a seamless transition between data and metadata and between explicit and intentional data. It does not distinguish between data provided by web pages and data provided by web services, between local (extensional) data and external data obtained via a Web service call. This is achieved using the Active XML technology that is based on exchanging XML documents with embedded service calls both for the logical and physical data model.A Brief Introduction to Active XML. An XML document is an unbounded, labeled, ordered tree, and web services are protocols for distributed computation. Active XML (AXML for short) extends the XML model by allowing documents to contain embedded calls to web services. An XML document thus contains both extensional information, ie, the specific XML structure, as well as intensional information from the results of service calls. AXML \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:pyW8ca7W8N0C",
            "Publisher": "SPRINGER-VERLAG"
        },
        {
            "Title": "Iterative mapreduce for large scale machine learning",
            "Publication year": 2013,
            "Publication url": "https://arxiv.org/abs/1303.3517",
            "Abstract": "Large datasets (\"Big Data\") are becoming ubiquitous because the potential value in deriving insights from data, across a wide range of business and scientific applications, is increasingly recognized. In particular, machine learning - one of the foundational disciplines for data analysis, summarization and inference - on Big Data has become routine at most organizations that operate large clouds, usually based on systems such as Hadoop that support the MapReduce programming paradigm. It is now widely recognized that while MapReduce is highly scalable, it suffers from a critical weakness for machine learning: it does not support iteration. Consequently, one has to program around this limitation, leading to fragile, inefficient code. Further, reliance on the programmer is inherently flawed in a multi-tenanted cloud environment, since the programmer does not have visibility into the state of the system when his or her program executes. Prior work has sought to address this problem by either developing specialized systems aimed at stylized applications, or by augmenting MapReduce with ad hoc support for saving state across iterations (driven by an external loop). In this paper, we advocate support for looping as a first-class construct, and propose an extension of the MapReduce programming paradigm called {\\em Iterative MapReduce}. We then develop an optimizer for a class of Iterative MapReduce programs that cover most machine learning techniques, provide theoretical justifications for the key optimization steps, and empirically demonstrate that system-optimized programs for significant machine learning tasks are competitive with state-of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:QIV2ME_5wuYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The case for learned index structures",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3183713.3196909",
            "Abstract": "Indexes are models: a\\btree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term\\em learned indexes. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show that our learned indexes can have significant advantages over traditional indexes. More importantly, we believe that the idea of replacing core components of a data management system through learned models has far \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:uWQEDVKXjbEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Managing Google\u2019s data lake: an overview of the Goods system",
            "Publication year": 2016,
            "Publication url": "https://research.google/pubs/pub47600/",
            "Abstract": "For most large enterprises today, data constitutes their core assets, along with code and infrastructure. Indeed, for most enterprises, the amount of data that they produce internally has exploded. At the same time, in many cases, engineers and data scientists do not use centralized data-management systems and and up creating what became known as a data lake\u2014a collection of datasets that often are not well organized or not organized at all and where one needs to \u201cfish\u201d for the useful datasets. In this paper, we describe our experience building and deploying Goods, a system to manage Google\u2019s internal data lake. Goods crawls Google\u2019s internal infrastructure and builds a catalog of discovered datasets, including structured files, databases, spreadsheets, or even services that provide access to the data. Goods extracts metadata about datasets in a post-hoc way: engineers continue to generate and organize \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:08ZZubdj9fEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Top-k queries over web applications",
            "Publication year": 2013,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-012-0303-9",
            "Abstract": "The core logic of web applications that suggest some particular service, such as online shopping, e-commerce etc., is typically captured by Business Processes (BPs). Among all the (maybe infinitely many) possible execution flows of a BP, analysts are often interested in identifying flows that are \u201cmost important\u201d, according to some weight metric. The goal of the present paper is to provide efficient algorithms for top-k query evaluation over the possible executions of Business Processes, under some given weight function. Unique difficulties in top-k analysis in this settings stem from (1) the fact that the number of possible execution flows of a given BP is typically very large, or even infinite in presence of recursion and (2) that the weights (e.g., likelihood, monetary cost, etc.) induced by actions performed during the execution (e.g., product purchase) may be inter-dependent (due to probabilistic dependencies \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:NaGl4SEjCO4C",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "XSketch synopses for XML",
            "Publication year": 2002,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.535.1044&rep=rep1&type=pdf",
            "Abstract": "All existing proposals for querying XML (eg, XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, we summarize our main results from our recent work on XS-KETCHes, a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data graph. To estimate the selectivities of complex path expressions over concise XSKETCH synopses, we develop an estimation framework that relies on appropriate statistical (uniformity and independence) assumptions to compensate for the lack of detailed distribution information. Given our estimation framework, We demonstrate that the problem of building an accuracy-optimal XSKETCH for a given amount of space is NP-hard, and propose an efficient heuristic algorithm based on greedy forward selection. Extensive experimental results with synthetic as well as real-life data sets verify the effectiveness of our approach. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured data and complex (branching) path expressions.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:pqnbT2bcN3wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Supporting streaming updates in an active data warehouse",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4221696/",
            "Abstract": "Active data warehousing has emerged as an alternative to conventional warehousing practices in order to meet the high demand of applications for up-to-date information. In a nutshell, an active warehouse is refreshed on-line and thus achieves a higher consistency between the stored information and the latest data updates. The need for on-line warehouse refreshment introduces several challenges in the implementation of data warehouse transformations, with respect to their execution time and their overhead to the warehouse processes. In this paper, we focus on a frequently encountered operation in this context, namely, the join of a fast stream S of source updates with a disk-based relation R, under the constraint of limited memory. This operation lies at the core of several common transformations, such as, surrogate key assignment, duplicate detection or identification of newly inserted tuples. We propose a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:zYLM7Y9cAGgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "SQL QueRIE recommendations",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/1920841.1921048",
            "Abstract": "This demonstration presents QueRIE, a recommender system that supports interactive database exploration. This system aims at assisting non-expert users of scientific databases by tracking their querying behavior and generating personalized query recommendations. The system is supported by two recommendation engines and the underlying recommendation algorithms. The first identifies potentially \"interesting\" parts of the database related to the corresponding data analysis task by locating those database parts that were accessed by similar users in the past. The second identifies structurally similar queries to the ones posted by the current user. Both approaches result in a recommendation set of SQL queries that is provided to the user to modify, or directly post to the database. The demonstrated system will enable users to query and get real-time recommendations from the SkyServer database, using user \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:0EnyYjriUFMC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Scihadoop: Array-based query processing in hadoop",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6114451/",
            "Abstract": "Hadoop has become the de facto platform for large-scale data analysis in commercial applications, and increasingly so in scientific applications. However, Hadoop's byte stream data model causes inefficiencies when used to process scientific data that is commonly stored in highly-structured, array-based binary file formats resulting in limited scalability of Hadoop applications in science. We introduce Sci- Hadoop, a Hadoop plugin allowing scientists to specify logical queries over array-based data models. Sci-Hadoop executes queries as map/reduce programs defined over the logical data model. We describe the implementation of a Sci-Hadoop prototype for NetCDF data sets and quantify the performance of five separate optimizations that address the following goals for several representative aggregate queries: reduce total data transfers, reduce remote reads, and reduce unnecessary reads. Two optimizations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Query recommendations for interactive database exploration",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-02279-1_2",
            "Abstract": "Relational database systems are becoming increasingly popular in the scientific community to support the interactive exploration of large volumes of data. In this scenario, users employ a query interface (typically, a web-based client) to issue a series of SQL queries that aim to analyze the data and mine it for interesting information. First-time users, however, may not have the necessary knowledge to know where to start their exploration. Other times, users may simply overlook queries that retrieve important information. To assist users in this context, we draw inspiration from Web recommender systems and propose the use of personalized query recommendations. The idea is to track the querying behavior of each user, identify which parts of the database may be of interest for the corresponding data analysis task, and recommend queries that retrieve relevant data. We discuss the main challenges in this \u2026",
            "Abstract entirety": 0,
            "Author pub id": "vfIriy4AAAAJ:2osOgNQ5qMEC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Data management challenges in production machine learning",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3035918.3054782",
            "Abstract": "The tutorial discusses data-management issues that arise in the context of machine learning pipelines deployed in production. Informed by our own experience with such largescale pipelines, we focus on issues related to understanding, validating, cleaning, and enriching training data. The goal of the tutorial is to bring forth these issues, draw connections to prior work in the database literature, and outline the open research questions that are not addressed by prior art.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:KxtntwgDAa4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "An overview of the deco system: data model and query language; query processing and optimization",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2430456.2430462",
            "Abstract": "Deco is a comprehensive system for answering declarative queries posed over stored relational data together with data obtained on-demand from the crowd. In this overview paper, we describe Deco's data model, query language, and system prototype, summarizing material from earlier papers. Deco's data model was designed to be general, flexible, and principled. Deco's query language extends SQL with simple constructs necessary for crowdsourcing, and has a precise semantics for arbitrary queries. Deco's query execution engine and cost-based query optimizer incorporate many novel techniques to address the limitations of traditional query processing techniques in the crowdsourcing setting. Query processing is guided by the objective of minimizing monetary cost and reducing latency.",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:Wp0gIr-vW9MC",
            "Publisher": "ACM"
        },
        {
            "Title": "A Machine Learning Approach to Databases Indexes",
            "Publication year": 2017,
            "Publication url": "http://alexbeutel.com/papers/mlsys2017_learned_indexes.pdf",
            "Abstract": "Databases rely on indexing data structures to efficiently perform many of their core operations. In order to look up all records in a particular range of keys, databases use a BTree-Index. In order to look up the record for a single key, databases use a Hash-Index. In order to check if a key exists, databases use a BitMap-Index (a bloom filter). These data structures have been studied and improved for decades, carefully tuned to best utilize each CPU cycle and cache available. However, they do not focus on leveraging the distribution of data they are indexing. In this paper, we demonstrate that these critical data structures are merely models, and can be replaced with more flexible, faster, and smaller machine learned neural networks. Further, we demonstrate how machine learned indexes can be combined with classic data structures to provide the guarantees expected of database indexes. Our initial results show, that we are able to outperform B-Trees by up to 44% in speed while saving over 2/3 of the memory. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs. 2",
            "Abstract entirety": 1,
            "Author pub id": "vfIriy4AAAAJ:p2g8aNsByqUC",
            "Publisher": "NIPS"
        }
    ]
}]