[{
    "name": "\u0391\u03b8\u03b1\u03bd\u03ac\u03c3\u03b9\u03bf\u03c2 \u039c\u03bf\u03c5\u03c7\u03c4\u03ac\u03c1\u03b7\u03c2",
    "romanize name": "Athanasios Mouchtaris",
    "School-Department": "\u0395\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03c2 \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "uoc",
    "Rank": "\u0391\u03bd\u03b1\u03c0\u03bb\u03b7\u03c1\u03c9\u03c4\u03ae\u03c2 \u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 19537,
    "Scholar name": "Athanasios Mouchtaris",
    "Scholar id": "CMNg4XUAAAAJ",
    "Affiliation": "Applied Science Manager at Amazon Alexa",
    "Citedby": 1756,
    "Interests": [
        "audio and speech processing"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=CMNg4XUAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Single-channel and multi-channel sinusoidal audio coding using compressed sensing",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5618549/",
            "Abstract": "Compressed sensing (CS) samples signals at a much lower rate than the Nyquist rate if they are sparse in some basis. In this paper, the CS methodology is applied to sinusoidally modeled audio signals. As this model is sparse by definition in the frequency domain (being equal to the sum of a small number of sinusoids), we investigate whether CS can be used to encode audio signals at low bitrates. In contrast to encoding the sinusoidal parameters (amplitude, frequency, phase) as current state-of-the-art methods do, we propose encoding few randomly selected samples of the time-domain description of the sinusoidal component (per signal segment). The potential of applying compressed sensing both to single-channel and multi-channel audio coding is examined. The listening test results are encouraging, indicating that the proposed approach can achieve comparable performance to that of state-of-the-art \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:W7OEmFMy1HYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "3D localization of multiple sound sources with intensity vector estimates in single source zones",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7362645/",
            "Abstract": "This work proposes a novel method for 3D direction of arrival (DOA) estimation based on the sound intensity vector estimation, via the encoding of the signals of a spherical microphone array from the space domain to the spherical harmonic domain. The sound intensity vector is estimated on detected single source zones (SSZs), where one source is dominant. A smoothed 2D histogram of these estimates reveals the DOA of the present sources and through an iterative process, accurate 3D DOA information can be obtained. The performance of the proposed method is demonstrated through simulations in various signal-to-noise ratio and reverberation conditions.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:yD5IFk8b50cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multiresolution source/filter model for low bitrate coding of spot microphone signals",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/content/pdf/10.1155/2008/624321.pdf",
            "Abstract": "A multiresolution source/filter model for coding of audio source signals (spot recordings) is proposed. Spot recordings are a subset of the multimicrophone recordings of a music performance, before the mixing process is applied for producing the final multichannel audio mix. The technique enables low bitrate coding of spot signals with good audio quality (above 3.0 perceptual grade compared to the original). It is demonstrated that this particular model separates the various microphone recordings of a multimicrophone recording into a part that mainly characterizes a specific microphone signal and a part that is common to all signals of the same recording (and can thus be omitted during transmission). Our interest in low bitrate coding of spot recordings is related to applications such as remote mixing and real-time collaboration of musicians who are geographically distributed. Using the proposed approach, it \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:SeFeTyx0c_EC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "Speech analysis and synthesis with a computationally efficient adaptive harmonic model",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7163319/",
            "Abstract": "Harmonic models have to be both precise and fast in order to represent the speech signal adequately and be able to process large amount of data in a reasonable amount of time. For these purposes, the full-band adaptive harmonic model (aHM) used by the adaptive iterative refinement (AIR) algorithm has been proposed in order to accurately model the perceived characteristics of a speech signal. Even though aHM-AIR is precise, it lacks the computational efficiency that would make its use convenient for large databases. The least squares (LS) solution used in the original aHM-AIR accounts for most of the computational load. In a previous paper, we suggested a peak picking (PP) approach as a substitution to the LS solution. In order to integrate the adaptivity scheme of aHM in the PP approach, an adaptive discrete Fourier transform (aDFT), whose frequency basis can fully follow the variations of the f 0  curve \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:fPk4N6BV_jEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robust text-independent speaker identification using short test and training sessions",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7096483/",
            "Abstract": "In this paper two methods for noise-robust text-independent speaker identification are described and compared against a baseline method for speaker identification based on the Gaussian Mixture Model (GMM). The two methods proposed in this paper are: (a) a statistical approach based on the Generalized Gaussian Density (GGD), and (b) a Sparse Representation Classification (SRC) method. The performance evaluation of each method is examined in a database containing twelve speakers. The main contribution of the paper is to investigate whether the SRC and GGD approaches can achieve robust speaker identification performance under noisy conditions using short duration testing and training data, in relevance to the baseline method. Our simulations indicate that the SRC approach significantly outperforms the other two methods under the short test and training sessions restriction, for all the signal-to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:aqlVkmm33-oC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A Subjective Evaluation on Mixtures of Crowdsourced Audio Recordings",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8081523/",
            "Abstract": "Exploiting correlations in the audio, several works in the past have demonstrated the ability to automatically match and synchronize User Generated Recordings (UGRs) of the same event. Considering a small number of synchronized UGRs, we formulate in this paper simple linear audio mixing approaches to combine the available audio content. We use data from two different public events to perform a comparative listening test with the goal to assess the potential of such mixtures in improving the listening experience of the captured event, as opposed to when each UGR is consumed individually. The results of the listening tests indicate that, even with just a small number of overlapping UGRs, the outcome of the mixing process gains higher preference in comparison to original UGRs played back individually.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:SP6oXDckpogC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Convolutional neural networks for video quality assessment",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1809.10117",
            "Abstract": "Video Quality Assessment (VQA) is a very challenging task due to its highly subjective nature. Moreover, many factors influence VQA. Compression of video content, while necessary for minimising transmission and storage requirements, introduces distortions which can have detrimental effects on the perceived quality. Especially when dealing with modern video coding standards, it is extremely difficult to model the effects of compression due to the unpredictability of encoding on different content types. Moreover, transmission also introduces delays and other distortion types which affect the perceived quality. Therefore, it would be highly beneficial to accurately predict the perceived quality of video to be distributed over modern content distribution platforms, so that specific actions could be undertaken to maximise the Quality of Experience (QoE) of the users. Traditional VQA techniques based on feature extraction and modelling may not be sufficiently accurate. In this paper, a novel Deep Learning (DL) framework is introduced for effectively predicting VQA of video content delivery mechanisms based on end-to-end feature learning. The proposed framework is based on Convolutional Neural Networks, taking into account compression distortion as well as transmission delays. Training and evaluation of the proposed framework are performed on a user annotated VQA dataset specifically created to undertake this work. The experiments show that the proposed methods can lead to high accuracy of the quality estimation, showcasing the potential of using DL in complex VQA scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:8AbLer7MMksC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Media content mixing apparatuses, methods and systems",
            "Publication year": 2018,
            "Publication url": "https://patents.google.com/patent/US20180358030A1/en",
            "Abstract": "In aspects, systems, methods, apparatuses and computer-readable storage media implementing embodiments for mixing audio content based on a plurality of user generated recordings (UGRs) are disclosed. In embodiments, the mixing comprises: receiving a plurality of UGRs, each UGR of the plurality of UGRs comprising at least audio content; determining a correlation between samples of audio content associated with at least two UGRs of the plurality of UGRs; generating one or more clusters comprising samples of the audio content identified as having a relationship based on the determined correlations; synchronizing, for each of the one or more clusters, the samples of the audio content to produce synchronized audio content for each of the one or more clusters, normalizing, for each of the one or more clusters, the synchronized audio content to produce normalized audio content; and mixing, for each of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:eflP2zaiRacC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multichannel audio synthesis by subband-based spectral conversion and parameter adaptation",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1395971/",
            "Abstract": "Multichannel audio can immerse a group of listeners in a seamless aural environment. Previously, we proposed a system capable of synthesizing the multiple channels of a virtual multichannel recording from a smaller set of reference recordings. This problem was termed multichannel audio resynthesis and the application was to reduce the excessive transmission requirements of multichannel audio. In this paper, we address the more general problem of multichannel audio synthesis, i.e., how to completely synthesize a multichannel audio recording from a specific stereophonic or monophonic recording, which would significantly enhance the recording's acoustic impression. We approach this problem by extending the model employed for the resynthesis problem. This is accomplished by adapting the resynthesis conversion parameters to the statistical properties of the recording that we wish to enhance. This \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multichannel audio coding using sinusoidal modelling and compressed sensing",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7096479/",
            "Abstract": "This paper explores the potential of applying compressed sensing (CS) to multichannel audio coding. In this context, we consider how sinusoidally-modelled multichannel audio signals might be encoded using compressed sensing, as opposed to directly encoding the sinusoidal parameters (amplitude, frequency, phase) as current state-of-the-art methods do. The results, obtained from listening tests using 80 sinusoids per frame with no residual noise signal, show that such a model can achieve equal or better performance to that of the state-of-the-art methods. Given that CS can lead to novel coding systems where the sampling and compression operations are combined into one low-complexity step, this can be considered as an important step towards applying the CS framework to audio coding applications.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:TFP_iSt0sucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Synthesis of enhanced audio from low bitrate compressed audio based on unit selection and statistical conversion methods",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5074820/",
            "Abstract": "Audio synthesis algorithms in general, are aimed at synthesizing audio based on target specifications or target descriptors with the requirement of natural-sounding results and correct content representation. However, work on synthesis algorithms in an audio quality enhancement context, is limited. In this paper, a new algorithm on audio synthesis is presented that attempts to improve the quality of compressed audio under the constraint that no specific information on the original, uncompressed signal is available. The methods employed here combine corpus-based audio synthesis techniques and the statistical spectral conversion framework adopted from speech conversion algorithms. The results show significant enhancement of a compressed audio signal under subband-specific and audio quality evaluation tests.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Semantic complexity in end-to-end spoken language understanding",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2008.02858",
            "Abstract": "End-to-end spoken language understanding (SLU) models are a class of model architectures that predict semantics directly from speech. Because of their input and output types, we refer to them as speech-to-interpretation (STI) models. Previous works have successfully applied STI models to targeted use cases, such as recognizing home automation commands, however no study has yet addressed how these models generalize to broader use cases. In this work, we analyze the relationship between the performance of STI models and the difficulty of the use case to which they are applied. We introduce empirical measures of dataset semantic complexity to quantify the difficulty of the SLU tasks. We show that near-perfect performance metrics for STI models reported in the literature were obtained with datasets that have low semantic complexity values. We perform experiments where we vary the semantic complexity of a large, proprietary dataset and show that STI model performance correlates with our semantic complexity measures, such that performance increases as complexity values decrease. Our results show that it is important to contextualize an STI model's performance with the complexity values of its training dataset to reveal the scope of its applicability.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:dTyEYWd-f8wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The RWTH/UPB/FORTH system combination for the 4th CHiME challenge evaluation",
            "Publication year": 2016,
            "Publication url": "https://www.researchgate.net/profile/Athanasios-Mouchtaris/publication/320584773_The_RWTHUPBFORTH_System_Combination_for_the_4th_CHiME_Challenge_Evaluation/links/59ef02ca0f7e9b0f76a976e7/The-RWTH-UPB-FORTH-System-Combination-for-the-4th-CHiME-Challenge-Evaluation.pdf",
            "Abstract": "This paper describes automatic speech recognition (ASR) systems developed jointly by RWTH, UPB and FORTH for the 1ch, 2ch and 6ch track of the 4th CHiME Challenge. In the 2ch and 6ch tracks the final system output is obtained by a Confusion Network Combination (CNC) of multiple systems. The Acoustic Model (AM) is a deep neural network based on Bidirectional Long Short-Term Memory (BLSTM) units. The systems differ by front ends and training sets used for the acoustic training. The model for the 1ch track is trained without any preprocessing. For each front end we trained and evaluated individual acoustic models. We compare the ASR performance of different beamforming approaches: a conventional superdirective beamformer [1] and an MVDR beamformer as in [2], where the steering vector is estimated based on [3]. Furthermore we evaluated a BLSTM supported Generalized Eigenvalue beamformer using NN-GEV [4]. The back end is implemented using RWTH\u2019s open-source toolkits RASR [5], RETURNN [6] and rwthlm [7]. We rescore lattices with a Long Short-Term Memory (LSTM) based language model. The overall best results are obtained by a system combination that includes the lattices from the system of UPB\u2019s submission [8]. Our final submission scored second in each of the three tracks of the 4th CHiME Challenge.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:xtRiw3GOFMkC",
            "Publisher": "Universit\u00e4tsbibliothek der RWTH Aachen"
        },
        {
            "Title": "Multichannel Audio Coding for Multimedia Services in Intelligent Environments",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-78502-6_5",
            "Abstract": "Audio is an integral component of multimedia services in intelligent environments. Use of multiple channels in audio capturing and rendering offers the advantage of recreating arbitrary acoustic environments, immersing the listener into the acoustic scene. On the other hand, multichannel audio contains a large degree of information which is highly demanding to transmit, especially for real-time applications. For this reason, a variety of compression methods have been developed for multichannel audio content. In this chapter, we initially describe the currently popular methods for multichannel audio compression. Low-bitrate encoding methods for multichannel audio have also been recently starting to attract interest, mostly towards extending MP3 audio coding to multichannel audio recordings, and these methods are also examined here. For synthesizing a truly immersive intelligent audio environment \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:Zph67rFs4hoC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Directional coding of audio using a circular microphone array",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6637656/",
            "Abstract": "We propose a real-time method for coding an acoustic environment based on estimating the Direction-of-Arrival (DOA) and reproducing it using an arbitrary loudspeaker configuration or headphones. We encode the sound field with the use of one audio signal and side-information. The audio signal can be further encoded with an MP3 encoder to reduce the bitrate. We investigate how such coding can affect the spatial impression and sound quality of spatial audio reproduction. Also, we propose a lossless efficient compression scheme for the side-information. Our method is compared with other recently proposed microphone array based methods for directional coding. Listening tests confirm the effectiveness of our method in achieving excellent reconstruction of the sound field while maintaining the sound quality at high levels.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:ZeXyd9-uunAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multiple sound source location estimation and counting in a wireless acoustic sensor network",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7336895/",
            "Abstract": "In this work, we consider the multiple sound source location estimation and counting problem in a wireless acoustic sensor network, where each sensor consists of a microphone array. Our method is based on inferring a location estimate for each frequency of the captured signals. A clustering approach-where the number of clusters (i.e., sound sources) is also an unknown parameter-is then employed to decide on the number of sources and their locations. The efficiency of our proposed method is evaluated through simulations and real recordings in scenarios with up to three simultaneous sound sources for different signal-to-noise ratios and reverberation times.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:D03iK_w7-QYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Capturing and reproducing spatial sound apparatuses, methods, and systems",
            "Publication year": 2018,
            "Publication url": "https://patents.google.com/patent/US10136239B1/en",
            "Abstract": "A processor-implemented method for capturing and reproducing spatial sound. The method includes: capturing a plurality of input signals using a plurality of sensors within a sound field; subjecting each input signal to a short-time Fourier transform to transform each signal into a transformed signal in the time-frequency domain; decomposing each of the transformed signals into a directional component and a diffuse component; optimizing beamformer weights using vector based amplitude panning to determine an optimal directivity pattern for the diffuse component of each transformed signal; constructing a set of diffuse sound channels using the diffuse components of the transformed signals and the optimized beamformer weights; constructing a set of directional sound channels using the directional components of the transformed signals; and reproducing the sound field by distributing the directional and diffuse \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:BrmTIyaxlBUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spatial sound characterization apparatuses, methods and systems",
            "Publication year": 2018,
            "Publication url": "https://patents.google.com/patent/US9955277B1/en",
            "Abstract": "A processor-implemented method for spatial sound characterization is described. In one implementation, each of a plurality of source signals detected by a plurality of sensing devices, is segmented into a plurality of time frames. For each time frame, time-frequency transform of the source signals is derived, an estimated number of sources and at least one estimated direction of arrival corresponding to each of the source signals is obtained. Further, source signals are extracted by spatial separation based at least on the estimated directions of arrival and the estimated number of sources, and separated source signals are processed to yield a reference signal and side information.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:B3FOqHPlNUQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Maximum component elimination in mixing of user generated audio recordings",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8122279/",
            "Abstract": "User generated content is gradually being recognized for its remarkable potential to enrich the professionally broadcasted content, but also as the means to provide acceptable quality audiovisual content for public events where professional coverage is absent. This potential is particularly interesting with respect to the audio modality, as a multitude of temporally overlapping User Generated audio Recordings (UGRs) may be utilized in order to provide a multichannel recording of the captured acoustic event. In this paper, we formulate a simple audio mixing approach called Maximum Component Elimination (MCE) to process a multiplicity of synchronized UGRs in a collaborative fashion. Operating in the Time-Frequency (TF) domain, MCE relies on the use of binary weights in order to selectively prevent certain TF components from individual UGRs to enter in the final mix. Results from a listening test indicate that the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:4fKUyHm3Qg0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A multichannel sinusoidal model applied to spot microphone signals for immersive audio",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4895273/",
            "Abstract": "In this paper, a multichannel version of the sinusoids plus noise model (also known as deterministic plus stochastic decomposition) is proposed and applied to spot microphone signals of a music recording. These are the recordings captured by the various microphones placed in a venue, before the mixing process produces the final multichannel audio mix. Coding these microphone signals makes them available to the decoder, allowing for interactive audio reproduction which is a necessary component in immersive audio applications. The proposed model uses a single reference audio signal in order to derive a noise signal per spot microphone. This noise signal can significantly enhance the sinusoidal representation of the corresponding spot signal. The reference can be one of the spot signals or a downmix, depending on the application. Thus, for a collection of multiple spot signals, only the reference is fully \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Adaptive modeling of synthetic nonstationary sinusoids",
            "Publication year": 2015,
            "Publication url": "https://repositorio.inesctec.pt/handle/123456789/5706",
            "Abstract": "Nonstationary oscillations are ubiquitous in music and speech, ranging from the fast transients in the attack of musical instruments and consonants to amplitude and frequency modulations in expressive variations present in vibrato and prosodic contours. Modeling nonstationary oscillations with sinusoids remains one of the most challenging problems in signal processing because the fit also depends on the nature of the underlying sinusoidal model. For example, frequency modulated sinusoids are more appropriate to model vibrato than fast transitions. In this paper, we propose to model nonstationary oscillations with adaptive sinusoids from the extended adaptive quasi-harmonic model (eaQHM).We generated synthetic nonstationary sinusoids with different amplitude and frequency modulations and compared the modeling performance of adaptive sinusoids estimated with eaQHM, exponentially damped sinusoids estimated with ESPRIT, and log-linear-amplitude quadratic-phase sinusoids estimated with frequency reassignment. The adaptive sinusoids from eaQHM outperformed frequency reassignment for all nonstationary sinusoids tested and presented performance comparable to exponentially damped sinusoids.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:tS2w5q8j5-wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Top-down strategies in parameter selection of sinusoidal modeling of audio",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5495954/",
            "Abstract": "Sinusoidal modeling of audio requires the model parameters to be selected by analyzing the original signal spectrum. This paper proposes two improvements in sinusoidal selection by considering how psychoacoustic masking curves can be calculated using a top-down strategy in certain situations. First, a non-iterative component selection method to be used in combination with an added residual signal is presented. Tests indicate computational gain and quality increase when the method is used with a noise-synthesized residual. Secondly, the estimation of the masking curve in binaural listening when signals are panned is considered. Tests show that knowledge of the degree of panning is beneficial when heavy panning is applied to simultaneously rendered audio object signals.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:M3ejUd6NZC8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A spectral conversion approach to single-channel speech enhancement",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4156209/",
            "Abstract": "In this paper, a novel method for single-channel speech enhancement is proposed, which is based on a spectral conversion feature denoising approach. Spectral conversion has been applied previously in the context of voice conversion, and has been shown to successfully transform spectral features with particular statistical properties into spectral features that best fit (with the constraint of a piecewise linear transformation) different target statistics. This spectral transformation is applied as an initialization step to two well-known single channel enhancement methods, namely the iterative Wiener filter (IWF) and a particular iterative implementation of the Kalman filter. In both cases, spectral conversion is shown here to provide a significant improvement as opposed to initializations using the spectral features directly from the noisy speech. In essence, the proposed approach allows for applying these two algorithms in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:qjMakFHDy7sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Evaluating how well filtered white noise models the residual from sinusoidal modeling of musical instrument sounds",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6701840/",
            "Abstract": "Nowadays, sinusoidal modeling commonly includes a residual obtained by the subtraction of the sinusoidal model from the original sound. This residual signal is often further modeled as filtered white noise. In this work, we evaluate how well filtered white noise models the residual from sinusoidal modeling of musical instrument sounds for several sinusoidal algorithms. We compare how well each sinusoidal model captures the oscillatory behavior of the partials by looking into how \u201cnoisy\u201d their residuals are. We performed a listening test to evaluate the perceptual similarity between the original residual and the modeled counterpart. Then we further investigate whether the result of the listening test can be explained by the fine structure of the residual magnitude spectrum. The results presented here have the potential to subsidize improvements on residual modeling.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:NMxIlDl6LWMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multilingual grapheme-to-phoneme conversion with byte representation",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9054696/",
            "Abstract": "Grapheme-to-phoneme (G2P) models convert a written word into its corresponding pronunciation and are essential components in automatic-speech-recognition and text-to-speech systems. Recently, the use of neural encoder-decoder architectures has substantially improved G2P accuracy for mono- and multi-lingual cases. However, most multilingual G2P studies focus on sets of languages that share similar graphemes, such as European languages. Multilingual G2P for languages from different writing systems, e.g. European and East Asian, remains an understudied area. In this work, we propose a multilingual G2P model with byte-level input representation to accommodate different grapheme systems, along with an attention-based Transformer architecture. We evaluate the performance of both character-level and byte-level G2P using data from multiple European and East Asian locales. Models using byte \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:Mojj43d5GZwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multichannel audio resynthesis based on a generalized Gaussian mixture model and cepstral smoothing",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1540208/",
            "Abstract": "Multichannel audio is an emerging technology with continuously increasing applications. Audio reproduction through multiple channels has the advantage of recreating the acoustic scene with unprecedented fidelity and of immersing the listener in an acoustic environment that is virtually indistinguishable from reality. However, one of the greatest challenges of this scheme is its high transmission requirements especially since accurate rendering through as many possible channels is the main purpose. This paper follows previous techniques on spectral conversion and a recently introduced concept called audio resynthesis. In audio resynthesis, a reference channel is transmitted and then used to recreate the remaining channels at the receiver. An alternative approach to audio resynthesis is presented based on the generalized Gaussian mixture model. This model incorporates most of the standard mixtures (Laplace \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:Se3iqnhoufwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Addressing the data-association problem for multiple sound source localization using DOA estimates",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7362644/",
            "Abstract": "In this paper, we consider the data association problem that arises when localizing multiple sound sources using direction of arrival (DOA) estimates from multiple microphone arrays. In such a scenario, the association of the DOAs across the arrays that correspond to the same source is unknown and must be found for accurate localization. We present an association algorithm that finds the correct DOA association to the sources based on features extracted for each source that we propose. Our method results in high association and localization accuracy in scenarios with missed detections, reverberation, and noise and outperforms other recently proposed methods.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:a0OBvERweLwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Perpendicular cross-spectra fusion for sound source localization with a planar microphone array",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7955064/",
            "Abstract": "Multiple sound source localization in reverberant environments stands as one of the most difficult challenges for many applications related to microphone array signal processing. In this paper, we describe perpendicular cross-spectra fusion (PCSF), a new direction-of-arrival (DOA) estimation algorithm, which utilizes an analytic formula for direction estimation in the time-frequency (TF) domain. Inherent to this technique is the presence of multiple direction estimation subsystems which operate in parallel, producing a multiplicity of candidate DOAs at each TF point. We define a metric of coherence, based on the property of divergence of the different DOA estimators, for assessing the reliability of different signal portions, so that only TF bins with a high quality of directional information are exploited for local DOA estimation. The resulting collection of local DOAs is provided as input to a recently proposed histogram \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:dshw04ExmUIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Quality Enhancement of Compressed Audio Based on Statistical Conversion",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/content/pdf/10.1155/2008/462830.pdf",
            "Abstract": "Most audio compression formats are based on the idea of low bit rate transparent encoding. As these types of audio signals are starting to migrate from portable players with inexpensive headphones to higher quality home audio systems, it is becoming evident that higher bit rates may be required to maintain transparency. We propose a novel method that enhances low bit rate encoded audio segments by applying multiband audio resynthesis methods in a postprocessing stage. Our algorithm employs the highly flexible Generalized Gaussian mixture model which offers a more accurate representation of audio features than the Gaussian mixture model. A novel residual conversion technique is applied which proves to significantly improve the enhancement performance without excessive overhead. In addition, both cepstral and residual errors are dramatically decreased by a feature-alignment scheme that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:pqnbT2bcN3wC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "Acoustic beamforming in front of a reflective plane",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8553103/",
            "Abstract": "In this paper, we consider the problem of beamforming with a planar microphone array placed in front of a wall of the room, so that the microphone array plane is perpendicular to that of the wall. While this situation is very likely to occur in a real life problem, the reflections introduced by the adjacent wall can be the cause of a serious mismatch between the actual acoustic paths and the traditionally employed free-field propagation model. We present an adaptation from the free-field to the so-called reflection-aware propagation model, that exploits an in-situ estimation of the complex and frequency-dependent wall reflectivity. Results presented in a real environment demonstrate that the proposed approach may bring significant improvements to the beamforming process compared to the free-field propagation model, as well as compared to other reflection-aware models that have been recently proposed.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:D_sINldO8mEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Speaker identification using sparsely excited speech signals and compressed sensing",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7096682/",
            "Abstract": "Compressed sensing samples signals at a much lower rate than the Nyquist rate if they are sparse in some basis. Using compressed sensing theory to reconstruct speech signals was recently proposed, assuming speech signals are sparse in the excitation domain if they are modelled using the source/filter model. In this paper, the compressed sensing theory for sparsely excited speech signals is applied to the specific problem of speaker identification, and is found to provide encouraging results using a number of measurements as low as half of the signal samples. In this manner, compressed sensing theory allows the use of less samples to achieve accurate identification, which in turn would be beneficial in several sensor network related applications. Additionally, enforcing sparsity on the excitation signal is shown to provide identification accuracy which is more robust to noise than using the noisy signal samples.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:r0BpntZqJG4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Encoding the sinusoidal model of an audio signal using compressed sensing",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5202459/",
            "Abstract": "In this paper, the compressed sensing (CS) methodology is applied to the harmonic part of sinusoidally-modeled audio signals. As this part of the model is sparse by definition in the frequency domain, we investigate how CS can be used to encode this signal at low bitrates, instead of encoding the sinusoidal parameters (amplitude, frequency, phase) as current state-of-the-art methods do. We extend our previous work by considering an improved system model, by comparing our model to other schemes, and exploring the effect of incorrectly reconstructed frames. We show that encouraging results can be obtained by our approach, although inferior at this point compared to state-of-the-art. Good performance is obtained using 24 bits per sinusoid as indicated by our listening tests.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:eQOLeE2rZwMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sparsity based robust speaker identification using a discriminative dictionary learning approach",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6811422/",
            "Abstract": "Speaker identification is a key component in many practical applications and the need of finding algorithms, which are robust under adverse noisy conditions, is extremely important. In this paper, the problem of text-independent speaker identification is studied in light of classification based on sparsity representation combined with a discriminative dictionary learning technique. Experimental evaluations on a small dataset reveal that the proposed method achieves a superior performance under short training sessions restrictions. In specific, the proposed method achieved high robustness for all the noisy conditions that were examined, when compared with a GMM universal background model (UBM-GMM) and sparse representation classification (SRC) approaches.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:hFOr9nPyWt4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Adaptive Modeling of Synthetic Nonstationary Sinusoids",
            "Publication year": 2015,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:XiSMed-E-HIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Parametric Spatial Audio Techniques in Teleconferencing and Remote Presence",
            "Publication year": 2017,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=4U03DwAAQBAJ&oi=fnd&pg=PA363&dq=info:DenoI81wLxAJ:scholar.google.com&ots=2L0G0mhCTB&sig=pwzZK3oLDGFecEUBkhKa8DxfFyU",
            "Abstract": "\uf6dc\uf63d.\uf63a BackgroundIn this section we briefly present the state of the art in parametric spatial audio techniques, emphasizing those methods that are based on uniform microphone arrays as the capturing device, since this is the device of interest for this chapter. The description provides the main research directions, and is by no means an exhaustive bibliographic review. See Chapter 4 for a broader introduction to the field. Directional audio coding (DirAC; Pulkki, 2007), see also Chapter 5, represents an important paradigm in the family of parametric approaches, providing an efficient description of spatial sound in terms of one or more audio signals and parametric side information, namely the DOA and the diffuseness of the sound. While originally designed for differential microphone signals, an adaptation of DirAC to compact planar microphone arrays with omnidirectional sensors has been described by Kuech et al.(2008) and Kallinger et al.(2008), and an adaptation to spaced microphone arrangements by Politis et al.(2015). In the same direction, the approach in Thiergart et al.(2011) presents an example of how the principles of parametric spatial audio can be exploited for the case of a linear microphone array. It is also relevant to mention at this point the pioneering work of binaural cue coding (BCC; Baumgarte and Faller, 2003; Faller and Baumgarte, 2003), due to its innovative approach of encoding a spatial audio scene using one or more audio signals plus side information, which is a coding philosophy followed by several of the methods encountered in this chapter. In the context of binaural reproduction, Cobos et al.(2010) presented an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:K3LRdlH-MEoC",
            "Publisher": "John Wiley & Sons"
        },
        {
            "Title": "Multichannel audio modeling and coding using a multiband source/filter model",
            "Publication year": 2005,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.4657&rep=rep1&type=pdf",
            "Abstract": "In this paper we propose a source/filter model for achieving low bitrate transmission of multichannel audio signals, in which the filter part corresponds to the specifics of each microphone information while the source part contains mostly the interchannel similarities. Using the appropriate filter for each channel and the source part of only one of the microphone signals, we can resynthesize a high quality approximation of each channel; thus, the filter part of each channel need only be encoded. Low datarates can be achieved in the order of few KBits/sec/channel focusing on applications such as remote mixing or distributed musicians collaboration.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:YsMSGLbcyi4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Linear predictive spectral coding and independent component analysis in identifying gasoline constituents using infrared spectroscopy",
            "Publication year": 2007,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0169743907001153",
            "Abstract": "An advanced spectral encoding method used in combination with independent component analysis (ICA) yields promising results in identifying refinery fractions contained in commercial gasoline mixtures based on infrared (IR) spectroscopy data. Previous work has shown how the signatures of the gasoline constituents can be recovered by solely relying on the IR spectra of their mixtures using ICA as a blind separation procedure. The present methodology encodes peak information from the spectra in linear predictive (LP) coefficients which are subsequently transformed into line spectrum frequencies (LSF). Such encoded spectra have a drastically reduced size (to 1/20 of the original size) while preserving the crucial peak information that characterizes each constituent. Source identification is then established by simply computing a Euclidean distance measure between the corresponding LSF of the gasoline \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:WF5omc3nYNoC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "3D localization of multiple audio sources utilizing 2D DOA histograms",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7760493/",
            "Abstract": "Steered response power (SRP) techniques have been well appreciated for their robustness and accuracy in estimating the direction of arrival (DOA) when a single source is active. However, by increasing the number of sources, the complexity of the resulting power map increases, making it challenging to localize the separate sources. In this work, we propose an efficient 2D histogram processing approach which is applied on the local DOA estimates, provided by SRP, and reveals the DOA of multiple audio sources in an iterative fashion. Driven by the results, we also apply the same methodology to local DOA estimates of a known subspace method and improve its accuracy. The performance of the presented algorithms is validated with numerical simulations and real measurements with a rigid spherical microphone array in different acoustical conditions: for multiple audio sources with different angular separations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:NhqRSupF_l8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Modeling and coding of spot microphone signals for immersive audio based on the sinusoidal model",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7080474/",
            "Abstract": "In this paper, the Sinusoids plus Noise Model (briefly SNM) is applied in a novel manner, in order to efficiently encode spot audio signals. These are the microphone recordings of a performance, before obtaining the multichannel mix, and are important for immersive audio applications since they can be used to provide interactivity. The SNM, as well as the SNM error spectral envelope, are extracted from each spot signal, providing a low-quality version of the signals. The main contribution of the paper corresponds to the use of a single audio reference signal which significantly enhances the quality of all the modeled spot signals. Reproduction of good quality and without loss of image width can be achieved using the proposed approach (above 4.0 perceptual grade for modeling and coding), by encoding a single audio (reference) signal, with side information per spot signal on the order of 19 kbps.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:hMod-77fHWUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Foreground suppression for capturing and reproduction of crowded acoustic environments",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7177930/",
            "Abstract": "Traditionally, sensor arrays and spatial filtering aim to enhance individual sources by suppressing ambient noise and reverberation. In this paper, the exactly opposite problem is examined, that of suppressing individual sources in favour of the ambient sound and of the whole acoustic scene in general. We consider a compact circular sensor array which is embedded in a crowded ambient acoustic environment and is at the same time prone to interference from directional speech originating from multiple nearby speakers. We propose a method for suppressing the undesired components and we compare its performance with two established approaches in spatial audio processing, namely, direct-to-diffuse decomposition and Primary-Ambient Extraction (PAE). Experimental results and a listening test which are presented illustrate the superiority of our method.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:rO6llkc54NcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "DOA estimation with histogram analysis of spatially constrained active intensity vectors",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7952211/",
            "Abstract": "The active intensity vector (AIV) is a common descriptor of the sound field. In microphone array processing, AIV is commonly approximated with beamforming operations and utilized as a direction of arrival (DOA) estimator. However, in its original form, it provides inaccurate estimates in sound field conditions where coherent sound sources are simultaneously active. In this work we utilize a higher order intensity-based DOA estimator on spatially-constrained regions (SCR) to overcome such limitations. We then apply 1-dimensional (1D) histogram processing on the noisy estimates for multiple DOA estimation. The performance of the estimator is shown with a 7-channel mobile microphone array, in reverberant conditions and under different signal-to-noise ratios.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:nb7KW1ujOQ8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A survey of sound source localization methods in wireless acoustic sensor networks",
            "Publication year": 2017,
            "Publication url": "https://www.hindawi.com/journals/wcmc/2017/3956282/",
            "Abstract": "Wireless acoustic sensor networks (WASNs) are formed by a distributed group of acoustic-sensing devices featuring audio playing and recording capabilities. Current mobile computing platforms offer great possibilities for the design of audio-related applications involving acoustic-sensing nodes. In this context, acoustic source localization is one of the application domains that have attracted the most attention of the research community along the last decades. In general terms, the localization of acoustic sources can be achieved by studying energy and temporal and/or directional features from the incoming sound at different microphones and using a suitable model that relates those features with the spatial location of the source (or sources) of interest. This paper reviews common approaches for source localization in WASNs that are focused on different types of acoustic features, namely, the energy of the incoming signals, their time of arrival (TOA) or time difference of arrival (TDOA), the direction of arrival (DOA), and the steered response power (SRP) resulting from combining multiple microphone signals. Additionally, we discuss methods not only aimed at localizing acoustic sources but also designed to locate the nodes themselves in the network. Finally, we discuss current challenges and frontiers in this field.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:UxriW0iASnsC",
            "Publisher": "Hindawi"
        },
        {
            "Title": "Exploiting the sparsity of the sinusoidal model using compressed sensing for audio coding",
            "Publication year": 2009,
            "Publication url": "https://hal.inria.fr/inria-00369613/",
            "Abstract": "Audio signals are represented via the sinusoidal model as a summation of a small number of sinusoids. This approach introduces sparsity to the audio signals in the frequency domain, which is exploited in this paper by applying Compressed Sensing (CS) to this sparse representation. CS allows sampling of signals at a much lower rate than the Nyquist rate if they are sparse in some basis. In this manner, a novel sinusoidal audio coding approach is proposed, which differs in philosophy from current state-of-the-art methods which encode the sinusoidal parameters (amplitude, frequency, phase) directly. It is shown here that encouraging results can be obtained by this approach, although inferior at this point compared to state-of-the-art. Several practical implementation issues are discussed, such as quantization of the CS samples, frequency resolution vs. coding gain, error checking, etc., and directions for future research in this framework are proposed.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:KlAtU1dfN6UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Adaptive sinusoidal modeling of percussive musical instrument sounds",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6811508/",
            "Abstract": "Percussive musical instrument sounds figure among the most challenging to model using sinusoids particularly due to the characteristic attack that features a sharp onset and transients. Attack transients present a highly nonstationary inharmonic behaviour that is very difficult to model with traditional sinusoidal models which use slowly varying sinusoids, commonly introducing an artifact known as pre-echo. In this work we use an adaptive sinusoidal model dubbed eaQHM to model percussive sounds from musical instruments such as plucked strings or percussion and investigate how eaQHM handles the sharp onsets and the nonstationary inharmonic nature of the attack transients. We show that adaptation renders a virtually perceptually identical sinusoidal representation of percussive sounds from different musical instruments, improving the Signal to Reconstruction Error Ratio (SRER) obtained with a traditional \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:M3NEmzRMIkIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multiple source location estimation on a dataset of real recordings in a wireless acoustic sensor network",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8547105/",
            "Abstract": "Recently, wireless acoustic sensor networks (WASNs) have received significant attention from the research community and a variety of methods have been proposed for numerous applications, such as location estimation and speech enhancement. The lack of publicly available datasets with signals recorded in WASNs, presents difficulties in obtaining consistent performance indicators across the different approaches. In this paper, we present and release a dataset of real recorded signals in an outdoor WASN comprised of four microphone arrays. Our dataset consists of several speakers recorded at various locations within the WASN and can be used for benchmarking purposes. We also present location estimation results using our real recorded dataset. Our results can serve as a baseline indicator of localization performance of single and multiple sources in a real environment.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:fQNAKQ3IYiAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sinusoidal spatial audio coding for low-bitrate binaural reproduction",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5495803/",
            "Abstract": "A binaural audio synthesis system based on sinusoidal modeling is proposed for spatial, low-bitrate audio coding utilized for example in teleconference applications. The system transmits monaural sinusoidal parameters of a downmix signal, from which the left and right binaural signals are synthesized according to the directional metadata at the receiver. Typical sinusoidal synthesis methods, as well as the effectiveness of a monaural frequency masking model, are evaluated in binaural context. Furthermore, a method for binaural noise residual synthesis and efficiency improvements for HRTF parameter acquisition are suggested. Tests utilizing speech signals indicate that sinusoidal modeling is an attractive technique for applications such as the proposed one.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:R3hNpaxXUhUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "and Ville Pulkki Title: DOA estimation with histogram analysis of spatially constrained intensity vectors",
            "Publication year": 2017,
            "Publication url": "https://scholar.google.com/scholar?cluster=16821338357987664787&hl=en&oi=scholarr",
            "Abstract": "The active intensity vector (AIV) is a common descriptor of the sound field. In microphone array processing, AIV is commonly approximated with beamforming operations and utilized as a direction of arrival (DOA) estimator. However, in its original form, it provides inaccurate estimates in sound field conditions where coherent sound sources are simultaneously active. In this work, we utilize a higher order intensitybased DOA estimator on spatially-constrained regions (SCR) to overcome such limitations. We then apply 1-dimensional (1D) histogram processing on the noisy estimates for multiple DOA estimation. The performance of the estimator is shown with a 7-channel microphone array, fitted on a rigid mobile-like device, in reverberant conditions and under different signal-to-noise ratios.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:tOudhMTPpwUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Combined software/hardware implementation of a filterbank front-end for speech recognition",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1579908/",
            "Abstract": "In this paper, a cost-effective implementation of a programmable filterbank front-end for speech recognition is presented. The objective has been to design a real-time bandpass filtering system with a filterbank of 16 filters, with analog audio input and analog output. The output consists of 16 analog signals, which are the envelopes of the filter outputs of the audio signal. These analog signals are then led to an analog neural computer, which performs the feature-based recognition task. One of the main objectives has been to allow the user to easily change the filter specifications without affecting the remaining system, thus a software implementation of the filterbank was preferred. In addition, the neural computer requires analog input. Therefore, we implemented the filterbank on a PC, with the input A/D and the output D/A performed by the PC stereo soundcard. Since multiple analog outputs are necessary for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:YOwf2qJgpHMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A multi-sensor approach for real-time detection and classification of impact sounds",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7362742/",
            "Abstract": "We present a method for real-time detection and classification of impact sounds - relying solely on spatial features - that exploits the difference in the location of each impacted structure. Using a compact sensor array, we formulate the classification problem in terms of an undetermined source separation process where we assume that the linear mixing model can be learned through a training phase. The recovered source amplitudes are exploited for estimating the source activity in time, and the detection and classification decisions are derived based on simple energy criteria. Experimental results with two sensors demonstrate the efficiency of the method in an application scenario which considers the use of a simple object as a real-time control interface for triggering a percussion synthesizer.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:cFHS6HbyZ2cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multiple sound source location estimation in wireless acoustic sensor networks using DOA estimates: The data-association problem",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8105842/",
            "Abstract": "In this paper, we consider the data-association problem for the localization of multiple sound sources in a wireless acoustic sensor network, where each node is a microphone array, using direction of arrival (DOA) estimates. The data-association problem arises because the central node that receives the multiple DOA estimates from the nodes cannot know to which source they belong. Hence, the DOAs from the different nodes that correspond to the same source must be found in order to perform accurate localization. We present a method to identify the correct association of DOAs to the sources and thus accurately estimate their locations. Our method results in high association and localization accuracy in realistic scenarios with missed detections, reverberation, noise, and moving sources and outperforms other recently proposed methods. It also incorporates a bitrate reduction scheme in order to keep the amount \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The ASPIRE Project\u2013Sensor Networks for Immersive Multimedia Environments",
            "Publication year": 2009,
            "Publication url": "http://users.ics.forth.gr/~tsakalid/PAPERS/2009-ERCIM.pdf",
            "Abstract": "Art, entertainment and education have always served as unique and demanding laboratories for information science and ubiquitous computing research. The ASPIRE project explores the fundamental challenges of deploying sensor networks for immersive multimedia, concentrating on multichannel audio capture, representation and transmission. The techniques developed in this project will help augment human auditory experience, interaction and perception, and will ultimately enhance the creative flexibility of audio artists and engineers by providing additional information for post-production and processing.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Low Bitrate Coding of Spot Audio Signals for Interactive and Immersive Audio Applications",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-68127-4_16",
            "Abstract": "In the last few years, a revolution has occurred in the area of consumer audio. Similarly to the transition from analog to digital sound that took place during the 80s, we have been experiencing the transition from 2-channel stereophonic sound to multichannel sound (e.g., 5.1 systems). Future audiovisual systems will not make distinctions regarding whether the user will be watching a movie or listening to a music recording; they are envisioned to offer a realistic experience to the user who will be immersed into the content, implying that the user will be able to interact with the content according to his will. In this paper, an encoding procedure is proposed, focusing on spot microphone signals, which is necessary for providing interactivity between the user and the environment. A model is proposed which achieves high-quality audio reproduction with side information for each spot microphone signal in the order of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:k_IJM867U9cC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Automatic matching and synchronization of user generated videos from a large scale sport event",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7952710/",
            "Abstract": "Exploiting correlations in the audio, several works in the past have demonstrated the ability to automatically match and synchronize User Generated Video (UGV) files of the same event. In this paper, we focus on the challenging acoustic environment of a large scale athletic event. We show that the chanting of the crowd produces an acoustic background common in the audio streams of different UGVs and we design a novel audio fingerprinting method for organizing the UGV collection based on that content. Results presented with recordings from a crowded football match demonstrate that the proposed approach provides significantly better audio matching performance in comparison to three of the most well known audio fingerprinting techniques.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:P5F9QuxV20EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The musinet project: Addressing the challenges in networked music performance systems",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7388002/",
            "Abstract": "This paper presents the progress in the MusiNet research project, which aims to provide a comprehensive architecture and a prototype implementation of a Networked Music Performance (NMP) system. We describe the Musinet client and server components, and the different approaches followed in our research effort in order to culminate in the most appropriate scheme in terms of delay and quality for the audio and video streams involved. We also describe the MusiNet user interface, which allows an integrated communication between the participants and the proposed NMP system.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:u_35RYKgDlwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Conditional vector quantization for voice conversion",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4218148/",
            "Abstract": "Voice conversion methods have the objective of transforming speech spoken by a particular source speaker, so that it sounds as if spoken by a different target speaker. The majority of voice conversion methods is based on transforming the short-time spectral envelope of the source speaker, based on derived correspondences between the source and target vectors using training speech data from both speakers. These correspondences are usually obtained by segmenting the spectral vectors of one or both speakers into clusters, using soft (GMM-based) or hard (VQ-based) clustering. Here, we propose that voice conversion performance can be improved by taking advantage of the fact that often the relationship between the source and target vectors is one-to-many. In order to illustrate this, we propose that a VQ approach namely constrained vector quantization (CVQ), can be used for voice conversion. Results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:Y0pCki6q_DkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Media content mixing apparatuses, methods and systems",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US10685667B1/en",
            "Abstract": "In aspects, systems, methods, apparatuses and computer-readable storage media implementing embodiments for mixing audio content based on a plurality of user generated recordings (UGRs) are disclosed. In embodiments, the mixing comprises: receiving a plurality of UGRs, each UGR of the plurality of UGRs comprising at least audio content; determining a correlation between samples of audio content associated with at least two UGRs of the plurality of UGRs; generating one or more clusters comprising samples of the audio content identified as having a relationship based on the determined correlations; synchronizing, for each of the one or more clusters, the samples of the audio content to produce synchronized audio content for each of the one or more clusters, normalizing, for each of the one or more clusters, the synchronized audio content to produce normalized audio content; and mixing, for each of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:tkaPQYYpVKoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spatially localized direction of arrival estimation",
            "Publication year": 2018,
            "Publication url": "https://www.researchgate.net/profile/Symeon-Delikaris-Manias/publication/325877680_Spatially_localized_direction_of_arrival_estimation/links/5b2a6c41aca27209f376656c/Spatially-localized-direction-of-arrival-estimation.pdf",
            "Abstract": "Summary Direction-of-arrival (DOA) estimation is a fundamental area of research in the field of array processing, as it applies in a variety of areas, including: sound field analysis, spatial sound reproduction and spatial filtering. There exists a plethora of DOA estimating algorithms, which all vary in terms of accuracy and computational complexity. Among the most popular approaches for DOA estimation are the steered-response power, time-delay based, active intensity based and subspace based methods. In this work, DOA estimation in spatially constrained areas is explored. A spatially constrained version of the active intensity vector is first formulated, which is then utilized for the DOA estimation. This work elaborates on this concept and demonstrates its advantages when compared to conventional approaches.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:sSrBHYA8nusC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Robust speaker identification using matrix completion under a missing data imputation framework",
            "Publication year": 2013,
            "Publication url": "http://www.csd.uoc.gr/~tzagarak/docs/MCsid_SPARS2013.pdf",
            "Abstract": "In this work, we examine the problem of noise robust speaker identification under short training sessions restrictions. In order to improve the identification performance, the effects of noise prior to identification must be alleviated. Towards this direction, we exploit matrix completion to recover the unreliable spectrographic data due to noise corruption. This is done by taking advantage of the low rank property of the speech magnitude STFT spectrogram.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:mB3voiENLucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Real-time localization of multiple audio sources in a wireless acoustic sensor network",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6952060/",
            "Abstract": "In this work we propose a grid-based method to estimate the location of multiple sources in a wireless acoustic sensor network, where each sensor node contains a microphone array and only transmits direction-of-arrival (DOA) estimates in each time interval, minimizing the transmissions to the central processing node. We present new work on modeling the DOA estimation error in such a scenario. Through extensive, realistic simulations, we show our method outperforms other state-of-the-art methods, in both accuracy and complexity. We present localization results of real recordings in an outdoor cell of a sensor network.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:M05iB0D1s5AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Synchronization Ambiguity in Audio Content Generated by Users Attending the Same Public Event",
            "Publication year": 2017,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:WbkHhVStYXYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bandwidth extension of low bitrate compressed audio based on statistical conversion",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5202445/",
            "Abstract": "Algorithmic and protocol constraints of most low bitrate compression schemes lead to audio signals of low bandwidth and, inevitably, of low perceptual audio quality. Audio bandwidth extension methods address this problem by reconstructing the high frequency spectrum of a degraded signal based on information from the low frequency part. In this work, a novel audio bandwidth extension method is presented in which high frequency reconstruction is achieved through statistical conversion between the low frequency spectrum of the compressed signal and the high frequency part of the uncompressed signal's spectrum. Even though no psychoacoustic model is used, quality evaluation tests show that the proposed method has similar performance to one of the most recent, state-of-the-art, bandwidth extension schemes.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:L8Ckcad2t8MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Effects of audio coding on ICA performance: An experimental study",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6648949/",
            "Abstract": "In this paper, we study the influence of lossy audio coding on the performance of Independent Component Analysis (ICA). In particular, we derive two compression scenarios from practical implementations. In the first case, we consider the situation when the sources are independently compressed, decompressed and then mixed. In the second, we consider the situation when mixtures of sources are jointly compressed. We experimentally show that the modification of the spectro-temporal diversity due to compression has almost no effect on the performance of ICA methods. We also show that the two tested stereo encoding strategies have a major effect on the performance of ICA, especially when the mixed signals are compressed at low bit rates. As these strategies have been extended to audio systems involving much more than two channels, our work suggests that ICA will not be able to successfully process the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:hC7cP41nSMkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Effcient multichannel audio resynthesis by subband-based spectral conversion",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7072210/",
            "Abstract": "Multichannel audio offers significant advantages for music reproduction that include the ability to provide better localization and envelopment, as well as reduced imaging distortion. On the other hand, multichannel audio is one of the most demanding media types in terms of transmission requirements. A novel architecture was previously proposed, allowing delivery of uncompressed multichannel audio over high-bandwidth communications networks. In most cases, however, bandwidth limitations prohibit transmission of multiple audio channels. In such cases, an alternative would be to transmit only one or two reference channels and recreate the rest of the channels at the receiving end. In this paper, we propose a system that is capable of synthesizing the required signals from a smaller set of signals recorded in a particular venue. These synthesized \u201cvirtual\u201d microphone signals can be used to produce multichannel \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:iH-uZ7U-co4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A spectral conversion approach to feature denoising and speech enhancement",
            "Publication year": 2005,
            "Publication url": "https://repository.upenn.edu/ese_papers/142/",
            "Abstract": "In this paper we demonstrate that spectral conversion can be successfully applied to the speech enhancement problem as a feature denoising method. The enhanced spectral features can be used in the context of the Kalman filter for estimating the clean speech signal. In essence, instead of estimating the clean speech features and the clean speech signal using the iterative Kalman filter, we show that is more efficient to initially estimate the clean speech features from the noisy speech features using spectral conversion (using a training speech corpus) and then apply the standard Kalman filter. Our results show an average improvement compared to the iterative Kalman filter that can reach 6 dB in the average segmental output Signal-to-Noise Ratio (SNR), in low input SNR's.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:O3NaXMp0MMsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Apparatuses, methods and systems for audio processing and transmission",
            "Publication year": 2015,
            "Publication url": "https://patents.google.com/patent/US9111525B1/en",
            "Abstract": "This disclosure details the implementation of apparatuses, methods and systems for audio processing and transmission. Some implementations of the system are configured to provide a method for encoding an arbitrary number of audio source signals using only a small amount of (transmitted or stored) information, while facilitating high-quality audio playback at the decoder side. Some implementations may be configured to implement, a parametric model for retaining the essential information of each source signal (side information). After the side information is extracted, the remaining information for all source signals may be summed to create a reference signal from which noise information for the original source signals may be reconstructed. The reference signal and the side information form the new collection of information to be transmitted or stored for subsequent decoding.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:dfsIfKJdRG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Breaking down the cocktail party: Capturing and isolating sources in a soundscape",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6952383/",
            "Abstract": "Spatial scene capture and reproduction requires extracting directional information from captured signals. Our previous work focused on directional coding of a sound scene using a single microphone array. In this paper, we investigate the benefits of using multiple microphone arrays, and extend our previous method by allowing arrays to cooperate during spatial feature extraction. We can thus render the sound scene using both direction and distance information and selectively reproduce specific \u201cspots\u201d of the captured sound scene.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:2P1L_qKh6hAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automating mixing of user-generated audio recordings from the same event",
            "Publication year": 2019,
            "Publication url": "http://www.aes.org/e-lib/online/browse.cfm?elib=20452",
            "Abstract": "When users attend the same public event, there may be multiple audiovisual recordings that are then posted on social media and websites. The availability of such a massive amount of user-generated recordings (UGR) has triggered new research directions related to the search, organization, and management of this content. And it has provided inspiration for new business models for content storage, retrieval, and consumption. The authors propose an approach to combine the available recordings based on a normalization step and a mixing step. The normalization step defines a fixed-with-time gain that is specific to each UGR. In the mixing step, a mechanism that reduces the master gain in accordance with the number of activated inputs at each time is employed. An approach called orthogonal mixing is presented, which is based on the assumption that the mixture components are mutually independent. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:wbdj-CoPYUoC",
            "Publisher": "Audio Engineering Society"
        },
        {
            "Title": "Sound source localization and isolation apparatuses, methods and systems",
            "Publication year": 2017,
            "Publication url": "https://patents.google.com/patent/US9549253B2/en",
            "Abstract": "A processor-implemented method for spatial sound localization and isolation is described. The method includes segmenting, via a processor, each of a plurality of source signals detected by a plurality of sensors, into a plurality of time frames. For each time frame, the method further includes obtaining, via a processor, a plurality of direction of arrival (DOA) estimates from the plurality of sensors, discretizing an area of interest into a plurality of grid points, calculating, via the processor, DOA at each of grid points, comparing, via the processor, the DOA estimates with the computed DOAs. If the number of sources is more than 1, the method includes obtaining via the processor, a plurality of combinations of DOA estimates, from amongst the plurality of combinations, estimating, via the processor, one or more initial candidate locations corresponding to each of the combinations, selecting location of the sources from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:zA6iFVUQeVQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Modeling spot microphone signals using the sinusoidal plus noise approach",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4392991/",
            "Abstract": "This paper focuses on high-fidelity multichannel audio coding based on an enhanced adaptation of the well-known sinusoidal plus noise model (SNM). Sinusoids cannot be used per se for high-quality audio modeling because they do not represent all the audible information of a recording. The noise part has also to be treated to avoid an artificial sounding resynthesis of the audio signal. Generally, the encoding process needs much higher bitrates for the noise part than the sinusoidal one. Our objective is to encode spot microphone signals using the SNM, by taking advantage of the interchannel similarities to achieve low bitrates. We demonstrate that for a given multichannel audio recording, the noise part for each spot microphone signal (before the mixing stage) can be obtained by using its noise envelope to transform the noise part of just one of the signals (the so-called \"reference signal\", which is fully encoded).",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Wireless acoustic sensor networks and applications",
            "Publication year": 2017,
            "Publication url": "https://www.hindawi.com/journals/wcmc/2017/1085290/",
            "Abstract": "Acoustic array processing is today an essential part of many applications involving the analysis of audio signals, such as hearing aids, hands-free devices, or immersive audio recording. While a number of acoustic sensing and processing systems have been proposed over the last decades, these have typically relied on high-throughput computing platforms and/or expensive microphone arrays. Although microphone arrays yield a higher performance than single-microphone systems, some limitations arise from the fact that the position of the microphones tend to be fixed, and all the signal processing tasks are performed on a centralized processor. The alternative is to use comparatively low-resource, distributed nodes with sensing devices and algorithms aimed at detecting, localizing, or characterizing acoustic events. The advantage of these systems is that the wireless, batterypowered nodes are less expensive and can be easily deployed in a wide range of environments. Moreover, as opposed to traditional microphone arrays that sample a sound field only locally, distributed acoustic sensing systems allow using many more sensors to cover a large area of interest. Signal processing and machine learning research for advanced acoustic systems of this type is giving birth to emerging technologies and services with a great exploitation potential. Current application domains such as smart cities and buildings, ambient assisted living, or habitat monitoring have already demonstrated the interest for acoustic-based solutions. Internet of Things (IoT) platforms and singleboard computers have substantially increased the capabilities of sensor networks \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:08ZZubdj9fEC",
            "Publisher": "Hindawi"
        },
        {
            "Title": "Compressive sensing in footstep sounds, hand tremors and speech using K-SVD dictionaries",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6622834/",
            "Abstract": "The application of Compressive Sensing is explored in three signal categories; footstep sounds, hand tremors and speech. An investigation of the reconstruction performance of various dictionaries is undertaken. It is demonstrated that these signal categories are reconstructed with higher SNR performance using K-SVD dictionaries than other fixed dictionaries. In particular, for footstep sounds and hand tremors, the K-SVD dictionaries outperform the fixed dictionaries; Discrete Cosine Transform (DCT), Wavelet Symlet with order 8 Transform and the union of DCT and Discrete Sine Transform. Moreover, in speech reconstruction, the use of a codebook of K-SVD dictionaries instead of a codebook of impulse response matrices improves performance.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:maZDTaKrznsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The role of time in music emotion recognition: Modeling musical emotions from time-varying music features",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-41248-6_10",
            "Abstract": "Music is widely perceived as expressive of emotion. However, there is no consensus on which factors in music contribute to the expression of emotions, making it difficult to find robust objective predictors for music emotion recognition (MER). Currently, MER systems use supervised learning to map non time-varying feature vectors into regions of an emotion space guided by human annotations. In this work, we argue that time is neglected in MER even though musical experience is intrinsically temporal. We advance that the temporal variation of music features rather than feature values should be used as predictors in MER because the temporal evolution of musical sounds lies at the core of the cognitive processes that regulate the emotional response to music. We criticize the traditional machine learning approach to MER, then we review recent proposals to exploit the temporal variation of music features to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:-f6ydRqryjwC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "GMM-Based Methods for Virtual Microphone Signal Synthesis",
            "Publication year": 2002,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:TQgYirikUcIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sound source characterization apparatuses, methods and systems",
            "Publication year": 2017,
            "Publication url": "https://patents.google.com/patent/US9554203B1/en",
            "Abstract": "A processor-implemented method for sound characterization is described. In one implementation, time-frequency transform of each of a plurality of sound signals from one or more sources, the sound signals being detected by a plurality of sensing devices, is derived. One or more single-source constant-time analysis zones based at least on correlation between the time-frequency transform signals from a pair of sensing devices are detected. At least one direction of arrival for each source in the detected single source analysis zones are detected. A histogram of the estimated directions of arrival is created and an estimate of a number of the sound sources and corresponding directions of arrival are generated based at least on the histogram.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:1sJd4Hv_s6UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Real-time multiple sound source localization and counting using a circular microphone array",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6557035/",
            "Abstract": "In this work, a multiple sound source localization and counting method is presented, that imposes relaxed sparsity constraints on the source signals. A uniform circular microphone array is used to overcome the ambiguities of linear arrays, however the underlying concepts (sparse component analysis and matching pursuit-based operation on the histogram of estimates) are applicable to any microphone array topology. Our method is based on detecting time-frequency (TF) zones where one source is dominant over the others. Using appropriately selected TF components in these \u201csingle-source\u201d zones, the proposed method jointly estimates the number of active sources and their corresponding directions of arrival (DOAs) by applying a matching pursuit-based approach to the histogram of DOA estimates. The method is shown to have excellent performance for DOA estimation and source counting, and to be highly \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:mVmsd5A6BfQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Enhanced multichannel audio resynthesis through residual processing and features alignment",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4284888/",
            "Abstract": "Multichannel audio refers to a widespread technology that enables audio rendering through multiple channels. Audio reproduction with multiple channels has the advantage of recreating the acoustic scene with unprecedented fidelity and of immersing the listener in an acoustic environment that is virtually indistinguishable from reality. However, one of the greatest challenges of multichannel audio is its high storage and transmission requirements especially since accurate rendering through as many possible channels is the main purpose. Audio resynthesis addresses this issue by enabling us to recreate a set of channels at the receiver end by transmitting only one source channel. We propose a new, enhanced, approach on multichannel audio resynthesis which involves a novel residual processing technique and a features alignment method that significantly increase the resynthesis accuracy. Our results show \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:4TOpqqG69KYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Real-time multiple sound source localization using a circular microphone array based on single-source confidence measures",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6288455/",
            "Abstract": "We propose a novel real-time adaptative localization approach for multiple sources using a circular array, in order to suppress the localization ambiguities faced with linear arrays, and assuming a weak sound source sparsity which is derived from blind source separation methods. Our proposed method performs very well both in simulations and in real conditions at 50% real-time.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:LkGwnXOMwfcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Non-parallel training for voice conversion by maximum likelihood constrained adaptation",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1325907/",
            "Abstract": "The objective of voice conversion methods is to modify the speech characteristics of a particular speaker in such manner, as to sound like speech by a different target speaker. Current voice conversion algorithms are based on deriving a conversion function by estimating its parameters through a corpus that contains the same utterances spoken by both speakers. Such a corpus, usually referred to as a parallel corpus, has the disadvantage that many times it is difficult or even impossible to collect. Here, we propose a voice conversion method that does not require a parallel corpus for training, i.e. the spoken utterances by the two speakers need not be the same, by employing speaker adaptation techniques to adapt to a particular pair of source and target speakers, the derived conversion parameters from a different pair of speakers. We show that adaptation reduces the error obtained when simply applying the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:d1gkVwhDpl0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A spectral conversion approach to the iterative Wiener filter for speech enhancement",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1394648/",
            "Abstract": "The iterative Wiener filter (IWF) for speech enhancement in additive noise is an effective and simple algorithm to implement. One of its main disadvantages is the lack of proper criteria for convergence, which has been shown to introduce severe degradation to the estimated clean signal. Here, an improvement of the IWF algorithm is proposed, when additional information is available for the signal to be enhanced. If a small amount of clean speech data is available, spectral conversion techniques can be applied for estimating the clean short-term spectral envelope of the speech signal from the noisy signal, with significant noise reduction. Our results show an average improvement compared to the original IWF that can reach 2 dB in the segmental output signal-to-noise ratio (SNR), in low input SNRs, which is perceptually significant.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:MXK_kJrjxJIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The MusiNet project: Towards unraveling the full potential of Networked Music Performance systems",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6878779/",
            "Abstract": "The MusiNet research project aims to provide a comprehensive architecture and a prototype implementation of a complete Networked Music Performance (NMP) system. In this paper we describe the current status of the project, focusing on critical decisions regarding the system's architecture and specifications, the low delay audio and video coding techniques to be employed, the media relay design, and the synchronous and asynchronous collaboration algorithms to be adopted.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:RYcK_YlVTxYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sinusoidal modeling of spot microphone signals based on noise transplantation for multichannel audio coding",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7099028/",
            "Abstract": "This paper focuses on high-fidelity multichannel audio modeling based on an enhanced adaptation of the well-known sinusoidal plus noise model (SNM). Sinusoids cannot be used per se for high-quality audio modeling because they do not represent all the audible information of a recording. The noise part has also to be treated to avoid an artificial sounding resynthesis of the audio signal. Generally, the encoding process needs much higher bitrates for the noise part than the sinusoidal one. Our objective is to model spot microphone signals using the SNM, by taking advantage of the interchannel similarities to achieve low bitrates. We demonstrate that for a given multichannel audio recording, the noise part for each spot microphone signal (microphone signals before the mixing stage) can be obtained from the noise part of one of the signals (reference, which is fully encoded), by transforming the reference noise \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "FANS: Fusing ASR and NLU for on-device SLU",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2111.00400",
            "Abstract": "Spoken language understanding (SLU) systems translate voice input commands to semantics which are encoded as an intent and pairs of slot tags and values. Most current SLU systems deploy a cascade of two neural models where the first one maps the input audio to a transcript (ASR) and the second predicts the intent and slots from the transcript (NLU). In this paper, we introduce FANS, a new end-to-end SLU model that fuses an ASR audio encoder to a multi-task NLU decoder to infer the intent, slot tags, and slot values directly from a given input audio, obviating the need for transcription. FANS consists of a shared audio encoder and three decoders, two of which are seq-to-seq decoders that predict non null slot tags and slot values in parallel and in an auto-regressive manner. FANS neural encoder and decoders architectures are flexible which allows us to leverage different combinations of LSTM, self-attention, and attenders. Our experiments show compared to the state-of-the-art end-to-end SLU models, FANS reduces ICER and IRER errors relatively by 30 % and 7 %, respectively, when tested on an in-house SLU dataset and by 0.86 % and 2 % absolute when tested on a public SLU dataset.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:7T2F9Uy0os0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Post-nonlinear sparse component analysis using single-source zones and functional data clustering",
            "Publication year": 2012,
            "Publication url": "https://arxiv.org/abs/1204.1085",
            "Abstract": "In this paper, we introduce a general extension of linear sparse component analysis (SCA) approaches to postnonlinear (PNL) mixtures. In particular, and contrary to the state-of-art methods, our approaches use a weak sparsity source assumption: we look for tiny temporal zones where only one source is active. We investigate two nonlinear single-source confidence measures, using the mutual information and a local linear tangent space approximation (LTSA). For this latter measure, we derive two extensions of linear single-source measures, respectively based on correlation (LTSA-correlation) and eigenvalues (LTSA-PCA). A second novelty of our approach consists of applying functional data clustering techniques to the scattered observations in the above single-source zones, thus allowing us to accurately estimate them.We first study a classical approach using a B-spline approximation, and then two approaches which locally approximate the nonlinear functions as lines. Finally, we extend our PNL methods to more general nonlinear mixtures. Combining single-source zones and functional data clustering allows us to tackle speech signals, which has never been performed by other PNL-SCA methods. We investigate the performance of our approaches with simulated PNL mixtures of real speech signals. Both the mutual information and the LTSA-correlation measures are better-suited to detecting single-source zones than the LTSA-PCA measure. We also find local-linear-approximation-based clustering approaches to be more flexible and more accurate than the B-spline one.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:j3f4tGmQtD8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Two open access datasets of user generated audio recordings",
            "Publication year": 2016,
            "Publication url": "https://scholar.google.com/scholar?cluster=9862472773037639522&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:Tiz5es2fbqcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Time-frequency methods for virtual microphone signal synthesis",
            "Publication year": 2001,
            "Publication url": "https://scholar.google.com/scholar?cluster=13616975585295928052&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:3fE2CSJIrl8C",
            "Publisher": "Audio Engineering Society; 1999"
        },
        {
            "Title": "Towards wireless acoustic sensor networks for location estimation and counting of multiple speakers in real-life conditions",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7953336/",
            "Abstract": "Speaker localization and counting in real-life conditions remains a challenging task. The computational burden, transmission usage and synchronization issues pose several limitations. Moreover, the physical characteristics of real speakers in terms of directivity pattern and orientation, as well as restrictions in the microphone array positioning, which commonly have to be placed close to walls, deteriorate the localization performance. In this paper, we propose a localization and counting method that accounts for the adjacent wall reflections and evaluate it using a dataset of real recorded signals of actual speakers that we collected. Our dataset is publicly available to foster further investigation towards localization in real-life scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:KxtntwgDAa4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Real-time multiple speaker DOA estimation in a circular microphone array based on matching pursuit",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6334205/",
            "Abstract": "We recently proposed an approach inspired by Sparse Component Analysis for real-time localisation of multiple sound sources using a circular microphone array. The method was based on identifying time-frequency zones where only one source is active, reducing the problem to single-source localisation in these zones. A histogram of estimated Directions of Arrival (DOAs) was formed and then processed to obtain improved DOA estimates, assuming that the number of sources was known. In this paper, we extend our previous work by proposing a new method for the final DOA estimations, that outperforms our previous method at lower SNRs and in the case of six simultaneous speakers. In keeping with the spirit of our previous work, the new method is very computationally efficient, facilitating its use in real-time systems.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:hqOjcs7Dif8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Capturing and reproducing spatial audio based on a circular microphone array",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1155/2013/718574",
            "Abstract": "This paper proposes a real-time method for capturing and reproducing spatial audio based on a circular microphone array. Following a different approach than other recently proposed array-based methods for spatial audio, the proposed method estimates the directions of arrival of the active sound sources on a per time-frame basis and performs source separation with a fixed superdirective beamformer, which results in more accurate modelling and reproduction of the recorded acoustic environment. The separated source signals are downmixed into one monophonic audio signal, which, along with side information, is transmitted to the reproduction side. Reproduction is possible using either headphones or an arbitrary loudspeaker configuration. The method is compared with other recently proposed array-based spatial audio methods through a series of listening tests for both simulated and real microphone array \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:isC4tDSrTZIC",
            "Publisher": "Hindawi"
        },
        {
            "Title": "CoDERT: Distilling Encoder Representations with Co-learning for Transducer-based Speech Recognition",
            "Publication year": 2021,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2021arXiv210607734V/abstract",
            "Abstract": "We propose a simple yet effective method to compress an RNN-Transducer (RNN-T) through the well-known knowledge distillation paradigm. We show that the transducer's encoder outputs naturally have a high entropy and contain rich information about acoustically similar word-piece confusions. This rich information is suppressed when combined with the lower entropy decoder outputs to produce the joint network logits. Consequently, we introduce an auxiliary loss to distill the encoder logits from a teacher transducer's encoder, and explore training strategies where this encoder distillation works effectively. We find that tandem training of teacher and student encoders with an inplace encoder distillation outperforms the use of a pre-trained and static teacher transducer. We also report an interesting phenomenon we refer to as implicit distillation, that occurs when the teacher and student encoders share the same \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:W5xh706n7nkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Analysis of emotional speech using an adaptive sinusoidal model",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6952538/",
            "Abstract": "Processing of emotional (or expressive) speech has gained attention over recent years in the speech community due to its numerous applications. In this paper, an adaptive sinusoidal model (aSM), dubbed extended adaptive Quasi-Harmonic Model - eaQHM, is employed to analyze emotional speech in accurate, robust, continuous, timevarying parameters (amplitude, frequency, and phase). It is shown that these parameters can adequately and accurately represent emotional speech content. Using a well known database of narrowband expressive speech (SUSAS) we show that very high Signal-to-Reconstruction-Error Ratio (SRER) values can be obtained, compared to the standard sinusoidal model (SM). Formal listening tests on a smaller wideband speech database show that the eaQHM outperforms SM from a perceptual resynthesis quality point of view. Finally, preliminary emotion classification tests show \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:ldfaerwXgEUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Foreground signal suppression apparatuses, methods, and systems",
            "Publication year": 2019,
            "Publication url": "https://patents.google.com/patent/US10178475B1/en",
            "Abstract": "A processor-implemented method for foreground signal suppression. The method includes: capturing a plurality of input signals using a plurality of sensors within a sound field; subjecting each input signal to a short-time Fourier transform to transform each signal into a plurality of non-overlapping subband regions; estimating the diffuseness of the sound field based on the plurality of input signals; decomposing each of the plurality of input signals into a diffuse component and a directional component based on the diffuseness estimate; applying a spatial analysis operation to filter the directional component of each of the plurality of input signals, wherein the spatial analysis operation includes applying a set of beamformers to the directional components to produce a plurality of beamformer signals; and processing the plurality of beamformer signals to decompose the signal into a foreground channel and a background \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:mvPsJ3kp5DgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Nonparallel training for voice conversion based on a parameter adaptation approach",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1621207/",
            "Abstract": "The objective of voice conversion algorithms is to modify the speech by a particular source speaker so that it sounds as if spoken by a different target speaker. Current conversion algorithms employ a training procedure, during which the same utterances spoken by both the source and target speakers are needed for deriving the desired conversion parameters. Such a (parallel) corpus, is often difficult or impossible to collect. Here, we propose an algorithm that relaxes this constraint, i.e., the training corpus does not necessarily contain the same utterances from both speakers. The proposed algorithm is based on speaker adaptation techniques, adapting the conversion parameters derived for a particular pair of speakers to a different pair, for which only a nonparallel corpus is available. We show that adaptation reduces the error obtained when simply applying the conversion parameters of one pair of speakers to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:u5HHmVD_uO8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "End-to-End Multi-Channel Transformer for Speech Recognition",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9414123/",
            "Abstract": "Transformers are powerful neural architectures that allow integrating different modalities using attention mechanisms. In this paper, we leverage the neural transformer architectures for multi-channel speech recognition systems, where the spectral and spatial information collected from different microphones are integrated using attention layers. Our multi-channel transformer network mainly consists of three parts: channel-wise self attention layers (CSA), cross-channel attention layers (CCA), and multi-channel encoder-decoder attention layers (EDA). The CSA and CCA layers encode the contextual relationship \"within\" and \"between\" channels and across time, respectively. The channel-attended outputs from CSA and CCA are then fed into the EDA layers to help decode the next token given the preceding ones. The experiments show that in a far-field in-house dataset, our method outperforms the baseline single \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:ZuybSZzF8UAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Post-nonlinear speech mixture identification using single-source temporal zones & curve clustering",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7074155/",
            "Abstract": "In this paper, we propose a method for estimating the nonlinearities which hold in post-nonlinear source separation. In particular and contrary to the state-of-art methods, our proposed approach uses a weak joint-sparsity sources assumption: we look for tiny temporal zones where only one source is active. This method is well suited to non-stationary signals such as speech. The main novelty of our work consists of using nonlinear single-source confidence measures and curve clustering. Such an approach may be seen as an extension of linear instantaneous sparse component analysis to post-nonlinear mixtures. The performance of the approach is illustrated with some tests showing that the nonlinear functions are estimated accurately, with mean square errors around 4e-5 when the sources are \u201cstrongly\u201d mixed.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:_kc_bZDykSQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Inverse filter design for immersive audio rendering over loudspeakers",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/845012/",
            "Abstract": "Immersive audio systems can be used to render virtual sound sources in three-dimensional (3-D) space around a listener. This is achieved by simulating the head-related transfer function (HRTF) amplitude and phase characteristics using digital filters. In this paper, we examine certain key signal processing considerations in spatial sound rendering over headphones and loudspeakers. We address the problem of crosstalk inherent in loudspeaker rendering and examine two methods for implementing crosstalk cancellation and loudspeaker frequency response inversion in real time. We demonstrate that it is possible to achieve crosstalk cancellation of 30 dB using both methods, but one of the two (the Fast RLS Transversal Filter Method) offers a significant advantage in terms of computational efficiency. Our analysis is easily extendable to nonsymmetric listening positions and moving listeners.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:u-x6o8ySG0sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Capturing and reproduction of a crowded sound scene using a circular microphone array",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7760533/",
            "Abstract": "Over the years, different spatial audio techniques have been proposed as the means to capture, encode and reproduce the spatial properties of acoustic fields, yet specific issues need to be modified each time in accordance to the type of microphone array used as well as with the technology used for reproduction. Using a circular array of omnidirectional microphones, we formulate in this paper a parametric and a non-parametric approach for capturing and reproduction of the crowded acoustic environment of a football stadium. A listening test performed reveals the advantages and disadvantages of each approach in connection to the particularities of the acoustic environment.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:b0M2c_1WBrUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Source counting in real-time sound source localization using a circular microphone array",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6250555/",
            "Abstract": "Recently, we proposed an approach inspired by Sparse Component Analysis for real-time localization of multiple sound sources using a circular microphone array. The method was based on identifying time-frequency zones where only one source is active, reducing the problem to single-source localization for these zones. A histogram of estimated Directions of Arrival (DOAs) was formed and then processed to obtain improved DOA estimates, assuming that the number of sources was known. In this paper, we extend our previous work by proposing three different methods for counting the number of sources by looking for prominent peaks in the derived histogram based on: (a) performing a peak search, (b) processing an LPC-smoothed version of the histogram, (c) employing a matching pursuit-based approach. The third approach is shown to perform very accurately in simulated reverberant conditions and additive \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:ufrVoPGSRksC",
            "Publisher": "IEEE"
        },
        {
            "Title": "End-to-end neural transformer based spoken language understanding",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2008.10984",
            "Abstract": "Spoken language understanding (SLU) refers to the process of inferring the semantic information from audio signals. While the neural transformers consistently deliver the best performance among the state-of-the-art neural architectures in field of natural language processing (NLP), their merits in a closely related field, i.e., spoken language understanding (SLU) have not beed investigated. In this paper, we introduce an end-to-end neural transformer-based SLU model that can predict the variable-length domain, intent, and slots vectors embedded in an audio signal with no intermediate token prediction architecture. This new architecture leverages the self-attention mechanism by which the audio signal is transformed to various sub-subspaces allowing to extract the semantic context implied by an utterance. Our end-to-end transformer SLU predicts the domains, intents and slots in the Fluent Speech Commands dataset with accuracy equal to 98.1 \\%, 99.6 \\%, and 99.6 \\%, respectively and outperforms the SLU models that leverage a combination of recurrent and convolutional neural networks by 1.4 \\% while the size of our model is 25\\% smaller than that of these architectures. Additionally, due to independent sub-space projections in the self-attention layer, the model is highly parallelizable which makes it a good candidate for on-device SLU.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:SdhP9T11ey4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Musical genre classification via generalized Gaussian and alpha-stable modeling",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1661251/",
            "Abstract": "This paper describes a novel methodology for automatic musical genre classification based on a feature extraction/statistical similarity measurement approach. First, we perform a 1-D wavelet decomposition of the music signal and we model the resulting subband coefficients using the generalized Gaussian density (GGD) and the alpha-stable distribution. Subsequently, the GGD and alpha-stable distribution parameters are estimated during the feature extraction step, while the similarity between two music signals is measured by employing the Kullback-Leibler divergence (KLD) between their corresponding estimated wavelet distributions. We evaluate the performance of the proposed methodology by using a dataset consisting of six different musical genre sets",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Localizing multiple audio sources from DOA estimates in a wireless acoustic sensor network",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6701872/",
            "Abstract": "In this work we propose a method to estimate the position of multiple sources in a wireless acoustic sensor network, where each sensor node only transmits direction-of-arrival (DOA) estimates each time interval, minimizing the transmissions to the processing node. Our method is based on the intersection of DOA estimates with outlier removal, and as such is very computationally efficient. We explore the performance of our method through extensive simulations and real measurements.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:NaGl4SEjCO4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A computationally efficient refinement of the fundamental frequency estimate for the adaptive harmonic model",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6853843/",
            "Abstract": "The full-band Adaptive Harmonic Model (aHM) can be used by the Adaptive Iterative Refinement (AIR) algorithm to accurately model the perceived characteristics of a speech recording. However, the Least Squares (LS) solution used in the current aHM-AIR makes the f 0  refinement in AIR time consuming, limiting the use of this algorithm for large databases. In this paper, a Peak Picking (PP) approach is suggested as a substitution to the LS solution. In order to integrate the adaptivity scheme of aHM in the PP approach, an adaptive Discrete Fourier Transform (aDFT) is also suggested in this paper, whose frequency basis can fully follow the frequency variations of the f 0  curve. Evaluations have shown an average time reduction of 5.5 times compared to the LS solution approach, while the quality of the resynthesis is preserved compared to the original aHM-AIR.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:35N4QoGY0k4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sparsification via compressed sensing for automatic speech recognition",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9413844/",
            "Abstract": "In order to achieve high accuracy for machine learning (ML) applications, it is essential to employ models with a large number of parameters. Certain applications, such as Automatic Speech Recognition (ASR), however, require real-time interactions with users, hence compelling the model to have as low latency as possible. Deploying large scale ML applications thus necessitates model quantization and compression, especially when running ML models on resource constrained devices. For example, by forcing some of the model weight values into zero, it is possible to apply zero-weight compression, which reduces both the model size and model reading time from the memory. In the literature, such methods are referred to as sparse pruning. The fundamental questions are when and which weights should be forced to zero, i.e. be pruned. In this work, we propose a compressed sensing based pruning (CSP \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:1yQoGdGgb4wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Immersive sound rendering using laser-based tracking",
            "Publication year": 2000,
            "Publication url": "http://www.aes.org/e-lib/browse.cfm?elib=9111",
            "Abstract": "In this paper we describe the underlying concepts behind the spatial sound renderer built at the University of Southern California\u2019s Immersive Audio Laboratory. In creating this sound rendering system, we were faced with three main challenges. First the rendering of sound using the Head-Related Transfer Functions, second the cancellation of the crosstalk terms and third the localization of the listener\u2019s ears. To deal with the spatial rendering sound we use a two-layer method of modeling the HRTF\u2019s. The first layer accurately reproduces the ITD\u2019s and IAD\u2019s, and the second layer reproduces the spectral characteristics of the HRTF\u2019s. A novel method for generating the required crosstalk cancellation filters as the listener moves was developed based on Low-Rank modeling. Using Karhunen-Loeve expansion we can interpolate among listener positions from a small number of HRTF measurements. Finally we present a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Audio Engineering Society"
        },
        {
            "Title": "Exploiting Large-scale Teacher-Student Training for On-device Acoustic Models",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2106.06126",
            "Abstract": "We present results from Alexa speech teams on semi-supervised learning (SSL) of acoustic models (AM) with experiments spanning over 3000 hours of GPU time, making our study one of the largest of its kind. We discuss SSL for AMs in a small footprint setting, showing that a smaller capacity model trained with 1 million hours of unsupervised data can outperform a baseline supervised system by 14.3% word error rate reduction (WERR). When increasing the supervised data to seven-fold, our gains diminish to 7.1% WERR; to improve SSL efficiency at larger supervised data regimes, we employ a step-wise distillation into a smaller model, obtaining a WERR of 14.4%. We then switch to SSL using larger student models in low data regimes; while learning efficiency with unsupervised data is higher, student models may outperform teacher models in such a setting. We develop a theoretical sketch to explain this behavior.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:JQOojiI6XY0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving narrowband DOA estimation of sound sources using the complex Watson distribution",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7760492/",
            "Abstract": "Narrowband direction-of-arrival (DOA) estimates for each time-frequency (TF) point offer a parametric spatial modeling of the acoustic environment which is very commonly used in many applications, such as source separation, dereverberation, and spatial audio. However, irrespective of the narrowband DOA estimation method used, many TF-points suffer from erroneous estimates due to noise and reverberation. We propose a novel technique to yield more accurate DOA estimates in the TF-domain, through statistical modeling of each TF-point with a complex Watson distribution. Then, instead of using the microphone array signals at a given TF-point to estimate the DOA, the maximum likelihood estimate of the mode vector of the distribution is used as input to the DOA estimation method. This approach results in more accurate DOA estimates and thus more accurate modeling of the acoustic environment, while it \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:abG-DnoFyZgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Virtual microphones for multichannel audio resynthesis",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/article/10.1155/S1110865703304032",
            "Abstract": "Multichannel audio offers significant advantages for music reproduction, including the ability to provide better localization and envelopment, as well as reduced imaging distortion. On the other hand, multichannel audio is a demanding media type in terms of transmission requirements. Often, bandwidth limitations prohibit transmission of multiple audio channels. In such cases, an alternative is to transmit only one or two reference channels and recreate the rest of the channels at the receiving end. Here, we propose a system capable of synthesizing the required signals from a smaller set of signals recorded in a particular venue. These synthesized \"virtual\" microphone signals can be used to produce multichannel recordings that accurately capture the acoustics of that venue. Applications of the proposed system include transmission of multichannel audio over the current Internet infrastructure and, as an extension of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:9yKSN-GCB0IC",
            "Publisher": "SpringerOpen"
        },
        {
            "Title": "Tempo Estimation Based on Linear Prediction and Perceptual Modelling.",
            "Publication year": 2011,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.466.1492&rep=rep1&type=pdf",
            "Abstract": "Many applications demand the automatic induction of the tempo of a musical excerpt. The tempo estimation systems follow a general scheme that consists of two main steps: the creation of a feature list and the detection of periodicities on this list. In this study, we propose a new method for the implementation of the first step, along with the addition of a final step that will enhance the tempo estimation procedure. The proposed method for the extraction of the feature list is based on Gammatone subspace analysis and Linear Prediction Error Filters (LPEFs). As a final step on the system, the application of a model that approximates the tempo perception by human listeners is proposed. The results of the evaluation indicate the proposed method compares favourably with other, state-of-the-art tempo estimation methods, using only one frame of the musical experts when most of the literature methods demand the processing of the whole piece.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Nonlinear blind mixture identification using local source sparsity and functional data clustering",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6250544/",
            "Abstract": "In this paper we propose several methods, using the same structure but with different criteria, for estimating the nonlinearities in nonlinear source separation. In particular and contrary to the state-of-art methods, our proposed approach uses a weak joint-sparsity sources assumption: we look for tiny temporal zones where only one source is active. This method is well suited to non-stationary signals such as speech. We extend our previous work to a more general class of nonlinear mixtures, proposing several nonlinear single-source confidence measures and several functional clustering techniques. Such approaches may be seen as extensions of linear instantaneous sparse component analysis to nonlinear mixtures. Experiments demonstrate the effectiveness and relevancy of this approach.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:4DMP91E08xMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Quantization Aware Training with Absolute-Cosine Regularization for Automatic Speech Recognition.",
            "Publication year": 2020,
            "Publication url": "http://www.interspeech2020.org/uploadfile/pdf/Wed-3-9-5.pdf",
            "Abstract": "Compression and quantization is important to neural networks in general and Automatic Speech Recognition (ASR) systems in particular, especially when they operate in real-time on resource-constrained devices. By using fewer number of bits for the model weights, the model size becomes much smaller while inference time is reduced significantly, with the cost of degraded performance. Such degradation can be potentially addressed by the so-called quantization-aware training (QAT). Existing QATs mostly take into account the quantization in forward propagation, while ignoring the quantization loss in gradient calculation during back-propagation. In this work, we introduce a novel QAT scheme based on absolute-cosine regularization (ACosR), which enforces a prior, quantization-friendly distribution to the model weights. We apply this novel approach into ASR task assuming a recurrent neural network transducer (RNN-T) architecture. The results show that there is zero to little degradation between floating-point, 8-bit, and 6-bit ACosR models. Weight distributions further confirm that in-training weights are very close to quantization levels when ACosR is applied.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:eq2jaN3J8jMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Packet loss concealment for multichannel audio using the multiband source/filter model",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4176735/",
            "Abstract": "We recently proposed a multichannel audio coding method using a multiband source/filter model, which results in a compact representation of the original recording. Our method can reproduce the original recording using only one audio channel and side information for the remaining channels in the order of 5 KBps/channel. Here, we examine packet loss concealment strategies for use within our model, so that we can derive a complete system for low-bitrate multichannel audio streaming through the Internet or wireless channels.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:kNdYIx-mwKoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Apparatuses, methods and systems for sparse sinusoidal audio processing and transmission",
            "Publication year": 2013,
            "Publication url": "https://patents.google.com/patent/US8489403B1/en",
            "Abstract": "The APPARATUSES, METHODS AND SYSTEMS FOR SPARSE SINUSOIDAL AUDIO PROCESSING AND TRANSMISSION (hereinafter \u201cSS-Audio\u201d) provides a platform for encoding and decoding audio signals based on a sparse sinusoidal structure. In one embodiment, the SS-Audio encoder may encode received audio inputs based on its sparse representation in the frequency domain and transmit the encoded and quantized bit streams. In one embodiment, the SS-Audio decoder may decode received quantized bit streams based on sparse reconstruction and recover the original audio input by reconstructing the sinusoidal parameters in the frequency domain.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:YFjsv_pBGBYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Virtual microphones for multichannel audio applications",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/869534/",
            "Abstract": "Multichannel audio can immerse a group of listeners in a seamless aural environment. Consumer media such as DVD and DVD-Audio allow the delivery of multiple (up to 10) channels of audio today. However, although there are thousands of music recordings available in two-channel stereo, only a handful have been recorded with multiple channels. We present a novel method for synthesizing multiple microphone signals from a smaller set of signals recorded in a particular venue. These synthesized \"virtual\" microphone signals can be used to produce multichannel recordings that accurately capture the acoustics of a particular venue. Our method uses linear prediction to generate a set of IIR filters that can be used to synthesize microphone signals at various locations within the hall, while preserving the correct acoustical characteristics of the venue. Applications of this method include transmission of multichannel \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:2osOgNQ5qMEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Instantaneous Detection and Classification of Impact Sound: Turning Simple Objects into Powerful Musical Control Interfaces.",
            "Publication year": 2014,
            "Publication url": "http://smc.afim-asso.org/smc-icmc-2014/papers/images/VOL_2/1178.pdf",
            "Abstract": "This paper demonstrates an approach for achieving instantaneous detection and classification of impact sounds that the user produces while interacting with simple daily objects. Using a single microphone, the system is trained to recognize the differences in the resonant behavior of a plastic bucket, a box made of paper and an empty bottle of beer, as these objects are struck at different locations. The method employs a first-nearest neighbour classifier which is based on simple spectral features extracted from a very short segment of the acoustic signal. Tests performed illustrate that classification rates above 90% may be achieved with a system response around 5 ms or even less. While still perfectible, the presented work illustrates the potential in creating a generic system which would enable the users to turn costless objects into powerful music controllers and percussive instruments into Hyper-instruments, by training the system to respond to their disposable instruments and audio equipment.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:g5m5HwL7SMYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Perceptually-Driven Scalable MDCT Enhancement of Compressed Audio Based on Statistical Conversion",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6123323/",
            "Abstract": "Many state-of-the-art audio codecs operating in a transform domain provide scalability as a core function by allowing to selectively subtract bits -- usually according to a nonperceptual criterion from the full bit rate data stream. This work presents a different, or even reverse, scalability approach in which a scalable codec can selectively add perceptually significant bits to a low bit rate data stream. The scalable enhancement algorithm presented here operates in the Modified Discrete Cosine Transform domain, which is popular among perceptual audio transform encoders, but its extension on other domains is straightforward. By exploiting the information of an existing low bit rate base layer, the algorithm adds perceptually significant data to the data stream according to a psycho acoustic model, and improves the audio quality at a fraction of the bit rate that would normally be required for the encoding or transmission of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Time-frequency and adaptive signal processing methods for immersive audio virtual acquisition and rendering",
            "Publication year": 2003,
            "Publication url": "https://search.proquest.com/openview/76e30083726a6ab9b71a1a8bc5fce88b/1?pq-origsite=gscholar&cbl=18750&diss=y",
            "Abstract": "This dissertation is concerned with the enhancement of an existing audio acquisition/reproduction system by both providing more loudspeakers for rendering (virtual rendering) as well as multiple input audio channels (virtual acquisition). By providing virtual microphones and virtual loudspeakers, the methods proposed here can transform a given microphone/loudspeaker setting into a truly immersive environment.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:CHSYGLWDkRkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "End-to-End Spoken Language Understanding for Generalized Voice Assistants",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2106.09009",
            "Abstract": "End-to-end (E2E) spoken language understanding (SLU) systems predict utterance semantics directly from speech using a single model. Previous work in this area has focused on targeted tasks in fixed domains, where the output semantic structure is assumed a priori and the input speech is of limited complexity. In this work we present our approach to developing an E2E model for generalized SLU in commercial voice assistants (VAs). We propose a fully differentiable, transformer-based, hierarchical system that can be pretrained at both the ASR and NLU levels. This is then fine-tuned on both transcription and semantic classification losses to handle a diverse set of intent and argument combinations. This leads to an SLU system that achieves significant improvements over baselines on a complex internal generalized VA dataset with a 43% improvement in accuracy, while still meeting the 99% accuracy benchmark on the popular Fluent Speech Commands dataset. We further evaluate our model on a hard test set, exclusively containing slot arguments unseen in training, and demonstrate a nearly 20% improvement, showing the efficacy of our approach in truly demanding VA scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:_Ybze24A_UAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Foreground Signal Suppression Apparatuses, Methods, and Systems",
            "Publication year": 2016,
            "Publication url": "https://patents.google.com/patent/US20160210957A1/en",
            "Abstract": "A processor-implemented method for foreground signal suppression. The method includes: capturing a plurality of input signals using a plurality of sensors within a sound field; subjecting each input signal to a short-time Fourier transform to transform each signal into a plurality of non-overlapping subband regions; estimating the diffuseness of the sound field based on the plurality of input signals; decomposing each of the plurality of input signals into a diffuse component and a directional component based on the diffuseness estimate; applying a spatial analysis operation to filter the directional component of each of the plurality of input signals, wherein the spatial analysis operation includes applying a set of beamformers to the directional components to produce a plurality of beamformer signals; and processing the plurality of beamformer signals to decompose the signal into a foreground channel and a background \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:bFI3QPDXJZMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multiband source/filter representation of multichannel audio for reduction of inter-channel redundancy",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7071067/",
            "Abstract": "In this paper we propose a model for multichannel audio recordings that can be utilized for revealing the underlying interchannel similarities. This is important for achieving low bitrates for multichannel audio and is especially suitable for applications when there is a large number of microphone signals to be transmitted (such as remote mixing or distributed musicians collaboration). Using this model, we can encode a multichannel audio signal using only one full audio channel and some side information in the order of few KBits/sec per channel, which can be used to decode the multiple channels at the receiving end. We apply objective and subjective measures in order to evaluate the performance of our method.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Streaming end-to-end bilingual asr systems with joint language identification",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2007.03900",
            "Abstract": "Multilingual ASR technology simplifies model training and deployment, but its accuracy is known to depend on the availability of language information at runtime. Since language identity is seldom known beforehand in real-world scenarios, it must be inferred on-the-fly with minimum latency. Furthermore, in voice-activated smart assistant systems, language identity is also required for downstream processing of ASR output. In this paper, we introduce streaming, end-to-end, bilingual systems that perform both ASR and language identification (LID) using the recurrent neural network transducer (RNN-T) architecture. On the input side, embeddings from pretrained acoustic-only LID classifiers are used to guide RNN-T training and inference, while on the output side, language targets are jointly modeled with ASR targets. The proposed method is applied to two language pairs: English-Spanish as spoken in the United States, and English-Hindi as spoken in India. Experiments show that for English-Spanish, the bilingual joint ASR-LID architecture matches monolingual ASR and acoustic-only LID accuracies. For the more challenging (owing to within-utterance code switching) case of English-Hindi, English ASR and LID metrics show degradation. Overall, in scenarios where users switch dynamically between languages, the proposed architecture offers a promising simplification over running multiple monolingual ASR models and an LID classifier in parallel.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:_B80troHkn4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "CoDERT: Distilling Encoder Representations with Co-learning for Transducer-based Speech Recognition",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2106.07734",
            "Abstract": "We propose a simple yet effective method to compress an RNN-Transducer (RNN-T) through the well-known knowledge distillation paradigm. We show that the transducer's encoder outputs naturally have a high entropy and contain rich information about acoustically similar word-piece confusions. This rich information is suppressed when combined with the lower entropy decoder outputs to produce the joint network logits. Consequently, we introduce an auxiliary loss to distill the encoder logits from a teacher transducer's encoder, and explore training strategies where this encoder distillation works effectively. We find that tandem training of teacher and student encoders with an inplace encoder distillation outperforms the use of a pre-trained and static teacher transducer. We also report an interesting phenomenon we refer to as implicit distillation, that occurs when the teacher and student encoders share the same decoder. Our experiments show 5.37-8.4% relative word error rate reductions (WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test sets.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:uLbwQdceFCQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Joint low-rank representation and matrix completion under a singular value thresholding framework",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6952420/",
            "Abstract": "Matrix completion is the process of estimating missing entries from a matrix using some prior knowledge. Typically, the prior knowledge is that the matrix is low-rank. In this paper, we present an extension of standard matrix completion that leverages prior knowledge that the matrix is low-rank and that the data samples can be efficiently represented by a fixed known dictionary. Specifically, we compute a low-rank representation of a data matrix with respect to a given dictionary using only a few observed entries. A novel modified version of the singular value thresholding (SVT) algorithm named joint low-rank representation and matrix completion SVT (J-SVT) is proposed. Experiments on simulated data show that the proposed J-SVT algorithm provides better reconstruction results compared to standard matrix completion.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:vV6vV6tmYwMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Phonetically Induced Subwords for End-to-End Speech Recognition",
            "Publication year": 2021,
            "Publication url": "https://assets.amazon.science/25/ae/5d36cc3843d1b906647b6b528c1b/phonetically-induced-subwords-for-end-to-end-speech-recognition.pdf",
            "Abstract": "End-to-end automatic speech recognition systems map a sequence of acoustic features to text. In modern systems, text is encoded to grapheme subwords which are generated by methods designed for text processing tasks and therefore don\u2019t model or take advantage of the statistics of the acoustic features. Here, we present a novel method for generating grapheme subwords that are derived from phoneme sequences, therefore capturing phonetical statistics. The phonetically induced subwords can be used for training and inference in any system that benefits from subwords, regardless of architecture and without the need of a pronunciation lexicon. We compare our method to other commonly used methods, which are based on text statistics or on text-phoneme correspondence and present experiments on CTC and RNN-T architectures, evaluating subword sets of different sizes. We find that our phonetically induced subwords can improve performance of RNN-T models with relative improvements of up to 15.21% compared to other subword methods.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:NJ774b8OgUMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multiresolution spectral conversion for multichannel audio resynthesis",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1035574/",
            "Abstract": "Multichannel audio is attracting rapidly increasing popularity in audio reproduction. In most cases, however, its transmission requirements are extremely demanding compared to the available bandwidth. One possible solution to this problem could be to transmit a reference channel and recreate the remaining channels at the receiving end. Such a method is proposed by taking advantage of spectral conversion techniques that have been successfully applied to speech processing. Applications of the proposed system include transmission of multichannel audio over the current Internet infrastructure and, as an extension of the methods proposed here, remastering of existing monophonic and stereophonic recordings for multichannel rendering.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:UeHWp8X0CEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Localizing multiple audio sources in a wireless acoustic sensor network",
            "Publication year": 2015,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0165168414003764",
            "Abstract": "In this work, we propose a grid-based method to estimate the location of multiple sources in a wireless acoustic sensor network, where each sensor node contains a microphone array and only transmits direction-of-arrival (DOA) estimates in each time interval, reducing the transmissions to the central processing node. We present new work on modeling the DOA estimation error in such a scenario. Through extensive, realistic simulations, we show that our method outperforms other state-of-the-art methods, in both accuracy and complexity. We also present localization results of real recordings in an outdoor cell of a sensor network.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:70eg2SAEIzsC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Tie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2011.09044",
            "Abstract": "End-to-end (E2E) spoken language understanding (SLU) systems can infer the semantics of a spoken utterance directly from an audio signal. However, training an E2E system remains a challenge, largely due to the scarcity of paired audio-semantics data. In this paper, we treat an E2E system as a multi-modal model, with audio and text functioning as its two modalities, and use a cross-modal latent space (CMLS) architecture, where a shared latent space is learned between the `acoustic' and `text' embeddings. We propose using different multi-modal losses to explicitly guide the acoustic embeddings to be closer to the text embeddings, obtained from a semantically powerful pre-trained BERT model. We train the CMLS model on two publicly available E2E datasets, across different cross-modal losses and show that our proposed triplet loss function achieves the best performance. It achieves a relative improvement of 1.4% and 4% respectively over an E2E model without a cross-modal space and a relative improvement of 0.7% and 1% over a previously published CMLS model using  loss. The gains are higher for a smaller, more complicated E2E dataset, demonstrating the efficacy of using an efficient cross-modal loss function, especially when there is limited E2E training data available.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:ye4kPcJQO24C",
            "Publisher": "Unknown"
        },
        {
            "Title": "3D DOA estimation of multiple sound sources based on spatially constrained beamforming driven by intensity vectors",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7471644/",
            "Abstract": "Sound source localization in three dimensions with microphone arrays is an active field of research, applicable in sound enhancement, source separation, and sound field analysis. In this contribution we propose a method for three dimensional multiple sound source localization in reverberant environments. We employ a spatially constrained steered response beamformer on a spherical sector centered at the direction of arrival (DOA) estimates of the intensity vector. Experiments are performed in both simulated and real acoustical environments with a spherical microphone array for multiple sound sources under different reverberation and signal-to-noise ratio (SNR) conditions. The performance of the proposed method is compared with our previously proposed work and a subspace method in the spherical harmonic domain. The results demonstrate a significant improvement in terms of localization accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:p2g8aNsByqUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Reconstruction of missing features based on a low-rank assumption for robust speaker identification",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6878778/",
            "Abstract": "Reconstruction of missing features promotes robustness in speaker recognition applications under noisy conditions. In this paper, we aim at enhancing the reliability of speech features for noise robust speaker identification under short training and testing sessions restrictions. Towards this direction, we apply a low-rank matrix recovery approach to reconstruct the unreliable spectrographic data due to noise corruption. This is performed by leveraging prior knowledge that the speech log-magnitude spectrotemporal representation is low-rank. Experiments on real speech data show that the proposed method improves the speaker identification accuracy especially for low signal-to-noise ratio (SNR) scenarios when compared with a sparse imputation approach.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:J_g5lzvAfSwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Joint ASR and language identification using RNN-T: An efficient approach to dynamic language switching",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9413734/",
            "Abstract": "Conventional dynamic language switching enables seamless multilingual interactions by running several monolingual ASR systems in parallel and triggering the appropriate downstream components using a standalone language identification (LID) service. Since this solution is neither scalable nor cost- and memory-efficient, especially for on-device applications, we propose end-to-end, streaming, joint ASR-LID architectures based on the recurrent neural network transducer framework. Two key formulations are explored: (1) joint training using a unified output space for ASR and LID vocabularies, and (2) joint training viewed as multi-task optimization. We also evaluate the benefit of using auxiliary language information obtained on-the-fly from an acoustic LID classifier. Experiments with the English-Hindi language pair show that: (a) multi-task architectures perform better overall, and (b) the best joint architecture \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:PR6Y55bgFSsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the Multichannel Sinusoidal Model for Coding Audio Object Signals",
            "Publication year": 2011,
            "Publication url": "https://www.aes.org/e-lib/online/browse.cfm?elib=15886",
            "Abstract": "This paper presents two improvements on a recently proposed multichannel sinusoidal modeling system for coding multiple audio object signals. The system includes extracting the sinusoidal components and an LPC envelope for each object signal, as well as transform coding of the residuals' downmix. The contributions of this paper are:(a) a psychoacoustic model for enabling the system to scale well with multiple object signals, and (b) an improved method to encode the common residual, tailored to the\" white\" nature of this signal. As a result, sound quality of 90% on the MUSHRA scale is obtained for 10 simultaneous object signals coded with a total rate of 150 kbit/s, while retaining the individual object parametric representations.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:RHpTSmoSYBkC",
            "Publisher": "Audio Engineering Society"
        },
        {
            "Title": "Maximum likelihood constrained adaptation for multichannel audio synthesis",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1197182/",
            "Abstract": "Multichannel audio environment can immerse a group of listeners in a seamless aural environment. Previously, we proposed a system capable of synthesizing the multiple channels of a virtual multichannel recording from a smaller set of reference recordings. This problem was termed multichannel audio resynthesis and the application was to reduce the excessive transmission requirements of multichannel audio. In this paper, we address the more general problem of multichannel audio synthesis, i.e. how to completely synthesize a multichannel audio recording from a specific stereophonic of monophonic recording, significantly enhancing the recording's quality. We approach this problem by extending the model employed for the resynthesis problem.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:0EnyYjriUFMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Evaluating the vulnerability of end-to-end automatic speech recognition models to membership inference attacks",
            "Publication year": 2021,
            "Publication url": "https://assets.amazon.science/48/f3/f8d1b62c4bf8bd63cf7a069eff24/evaluating-the-vulnerability-of-end-to-end-automation-speech-recognition-models-to-membership-inference-attacks.pdf",
            "Abstract": "Recent studies have shown that it may be possible to determine if a machine learning model was trained on a given data sample, using Membership Inference Attacks (MIA). In this paper we evaluate the vulnerability of state-of-the-art speech recognition models to MIA under black-box access. Using models trained with standard methods and public datasets, we demonstrate that without any knowledge of the target model\u2019s parameters or training data a MIA can successfully infer membership with precision and recall more than 60%. Furthermore, for utterances from about 39% of the speakers the precision is more than 75%, indicating that training data membership can be inferred more precisely for some speakers than others. While strong regularization reduces the overall accuracy of MIA to almost 50%, the attacker can still infer membership for utterances from 25% of the speakers with high precision. These results indicate that (1) speaker-level MIA success should be reported, along with overall accuracy, to provide a holistic view of the model\u2019s vulnerability and (2) conventional regularization is an inadequate defense against MIA. We believe that the insights gleaned from this study can direct future work towards more effective defenses.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:Fu2w8maKXqMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Full-band quasi-harmonic analysis and synthesis of musical instrument sounds with adaptive sinusoids",
            "Publication year": 2016,
            "Publication url": "https://www.mdpi.com/138594",
            "Abstract": "Sinusoids are widely used to represent the oscillatory modes of musical instrument sounds in both analysis and synthesis. However, musical instrument sounds feature transients and instrumental noise that are poorly modeled with quasi-stationary sinusoids, requiring spectral decomposition and further dedicated modeling. In this work, we propose a full-band representation that fits sinusoids across the entire spectrum. We use the extended adaptive Quasi-Harmonic Model (eaQHM) to iteratively estimate amplitude-and frequency-modulated (AM\u2013FM) sinusoids able to capture challenging features such as sharp attacks, transients, and instrumental noise. We use the signal-to-reconstruction-error ratio (SRER) as the objective measure for the analysis and synthesis of 89 musical instrument sounds from different instrumental families. We compare against quasi-stationary sinusoids and exponentially damped sinusoids. First, we show that the SRER increases with adaptation in eaQHM. Then, we show that full-band modeling with eaQHM captures partials at the higher frequency end of the spectrum that are neglected by spectral decomposition. Finally, we demonstrate that a frame size equal to three periods of the fundamental frequency results in the highest SRER with AM\u2013FM sinusoids from eaQHM. A listening test confirmed that the musical instrument sounds resynthesized from full-band analysis with eaQHM are virtually perceptually indistinguishable from the original recordings. View Full-Text",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:pyW8ca7W8N0C",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Normalization of Partly Overlapping Audio Recordings from the Same Event Based on Relative Signal Powers",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8461919/",
            "Abstract": "Exploiting correlations in the audio, several works in the past have demonstrated the ability to automatically match and synchronize user-generated video or audio files of the same event. Such tools solve for the unknown starting and ending time of each available recording along the event time-line and open the way for collaborative content production approaches. However, a source of difficulty for collaborative processing approaches related to audio is the fact that the different audio recordings may be available at significantly different signal levels. In this paper, we present a normalization approach to automatically define gains for all the recordings so that the variations in the signal levels among different recordings are suppressed. We show that normalization is trivial when all recordings share the same time support but the same process is non-trivial when the recordings partly overlap along time, especially if the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "CMNg4XUAAAAJ:geHnlv5EZngC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Context-Aware Transformer Transducer for Speech Recognition",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2111.03250",
            "Abstract": "End-to-end (E2E) automatic speech recognition (ASR) systems often have difficulty recognizing uncommon words, that appear infrequently in the training data. One promising method, to improve the recognition accuracy on such rare words, is to latch onto personalized/contextual information at inference. In this work, we present a novel context-aware transformer transducer (CATT) network that improves the state-of-the-art transformer-based ASR system by taking advantage of such contextual signals. Specifically, we propose a multi-head attention-based context-biasing network, which is jointly trained with the rest of the ASR sub-networks. We explore different techniques to encode contextual data and to create the final attention context vectors. We also leverage both BLSTM and pretrained BERT based models to encode contextual data and guide the network training. Using an in-house far-field dataset, we show that CATT, using a BERT based context encoder, improves the word error rate of the baseline transformer transducer and outperforms an existing deep contextual model by 24.2% and 19.4% respectively.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:2KloaMYe4IUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Development and evaluation of a digital MEMS microphone array for spatial audio",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7760321/",
            "Abstract": "We present the design of a digital microphone array comprised of MEMS microphones and evaluate its potential for spatial audio capturing and direction-of-arrival (DOA) estimation which is an essential part of encoding the soundscape. The device is a cheaper and more compact alternative to analog microphone arrays which require external - and usually expensive - analog-to-digital converters and sound cards. However, the performance of such digital arrays for DOA estimation and spatial audio acquisition has not been investigated. In this work, the efficiency of the digital array for spatial audio is evaluated and compared to a typical analog microphone array of the same geometry. Our results indicate that our digital array achieves the same performance as its analog counterpart, thus offering a cheaper and easily deployable device, suitable for spatial audio applications.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:EUQCXRtRnyEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Direction of arrival estimation in front of a reflective plane using a circular microphone array",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7760323/",
            "Abstract": "The presence of reflecting surfaces inside an enclosure is generally known to have an adverse effect in acoustic source localization and Direction of Arrival (DOA) estimation performance. In this paper, we focus on the problem of indoor multi-source DOA estimation along the horizontal plane, considering a circular sensor array which is placed just in front of one of the vertical walls of the room. We present a modification in the propagation model, which traditionally accounts for the direct path only, by incorporating also the contribution of the earliest reflection introduced by the adjacent vertical wall. Based on the traditional and the modified model, a Matched Filter and a Minimum Variance Distortionless Response beamformer are designed and tested for DOA estimation. Results with simulated and real data demonstrate the validity of the proposed model and its superiority in comparison to the traditional one.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:_xSYboBqXhAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multi-Channel Transformer Transducer for Speech Recognition",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2108.12953",
            "Abstract": "Multi-channel inputs offer several advantages over single-channel, to improve the robustness of on-device speech recognition systems. Recent work on multi-channel transformer, has proposed a way to incorporate such inputs into end-to-end ASR for improved accuracy. However, this approach is characterized by a high computational complexity, which prevents it from being deployed in on-device systems. In this paper, we present a novel speech recognition model, Multi-Channel Transformer Transducer (MCTT), which features end-to-end multi-channel training, low computation cost, and low latency so that it is suitable for streaming decoding in on-device speech recognition. In a far-field in-house dataset, our MCTT outperforms stagewise multi-channel models with transformer-transducer up to 6.01% relative WER improvement (WERR). In addition, MCTT outperforms the multi-channel transformer up to 11.62% WERR, and is 15.8 times faster in terms of inference speed. We further show that we can improve the computational cost of MCTT by constraining the future and previous context in attention computations.",
            "Abstract entirety": 1,
            "Author pub id": "CMNg4XUAAAAJ:kzcrU_BdoSEC",
            "Publisher": "Unknown"
        }
    ]
}]