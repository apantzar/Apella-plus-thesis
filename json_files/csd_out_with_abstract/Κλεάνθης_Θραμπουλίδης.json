[{
    "name": "\u039a\u03bb\u03b5\u03ac\u03bd\u03b8\u03b7\u03c2 \u0398\u03c1\u03b1\u03bc\u03c0\u03bf\u03c5\u03bb\u03af\u03b4\u03b7\u03c2",
    "romanize name": "Kleanthis Thrampoulidis",
    "School-Department": "\u0397\u03bb\u03b5\u03ba\u03c4\u03c1\u03bf\u03bb\u03cc\u03b3\u03c9\u03bd \u039c\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ce\u03bd & \u03a4\u03b5\u03c7\u03bd\u03bf\u03bb\u03bf\u03b3\u03af\u03b1\u03c2 \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "upatras",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 19589,
    "Scholar name": "Christos Thrampoulidis",
    "Scholar id": "X54nJqcAAAAJ",
    "Affiliation": "Assistant Professor, University of British Columbia, ECE Department",
    "Citedby": 1376,
    "Interests": [
        "data science",
        "signal processing",
        "optimization",
        "machine-learning"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=X54nJqcAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Fundamental limits of ridge-regularized empirical risk minimization in high dimensions",
            "Publication year": 2021,
            "Publication url": "http://proceedings.mlr.press/v130/taheri21a.html",
            "Abstract": "Despite the popularity of Empirical Risk Minimization (ERM) algorithms, a theory that explains their statistical properties in modern high-dimensional regimes is only recently emerging. We characterize for the first time the fundamental limits on the statistical accuracy of convex ridge-regularized ERM for inference in high-dimensional generalized linear models. For a stylized setting with Gaussian features and problem dimensions that grow large at a proportional rate, we start with sharp performance characterizations and then derive tight lower bounds on the estimation and prediction error. Our bounds provably hold over a wide class of loss functions, and, for any value of the regularization parameter and of the sampling ratio. Our precise analysis has several attributes. First, it leads to a recipe for optimally tuning the loss function and the regularization parameter. Second, it allows to precisely quantify the sub-optimality of popular heuristic choices, such as optimally-tuned least-squares. Third, we use the bounds to precisely assess the merits of ridge-regularization as a function of the sampling ratio. Our bounds are expressed in terms of the Fisher Information of random variables that are simple functions of the data distribution, thus making ties to corresponding bounds in classical statistics.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:hMsQuOkrut0C",
            "Publisher": "PMLR"
        },
        {
            "Title": "Sharp global convergence guarantees for iterative nonconvex optimization: A Gaussian process perspective",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2109.09859",
            "Abstract": "We consider a general class of regression models with normally distributed covariates, and the associated nonconvex problem of fitting these models from data. We develop a general recipe for analyzing the convergence of iterative algorithms for this task from a random initialization. In particular, provided each iteration can be written as the solution to a convex optimization problem satisfying some natural conditions, we leverage Gaussian comparison theorems to derive a deterministic sequence that provides sharp upper and lower bounds on the error of the algorithm with sample-splitting. Crucially, this deterministic sequence accurately captures both the convergence rate of the algorithm and the eventual error floor in the finite-sample regime, and is distinct from the commonly used \"population\" sequence that results from taking the infinite-sample limit. We apply our general framework to derive several concrete consequences for parameter estimation in popular statistical models including phase retrieval and mixtures of regressions. Provided the sample size scales near-linearly in the dimension, we show sharp global convergence rates for both higher-order algorithms based on alternating updates and first-order algorithms based on subgradient descent. These corollaries, in turn, yield multiple consequences, including: (a) Proof that higher-order algorithms can converge significantly faster than their first-order counterparts (and sometimes super-linearly), even if the two share the same population update and (b) Intricacies in super-linear convergence behavior for higher-order algorithms, which can be nonstandard (e.g., with exponent 3/2) and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:ClCfbGk0d_YC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Estimating structured signals in sparse noise: A precise noise sensitivity analysis",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7028545/",
            "Abstract": "We consider the problem of estimating a structured signal xo from linear, underdetermined and noisy measurements y = Ax 0  + z, in the presence of sparse noise z. A natural approach to recovering x 0 , that takes advantage of both the structure of xo and the sparsity of z is solving: x = arg min x  ||y - Ax|| 1  subject to f(x) \u2264 f(x 0 ) (constrained LAD estimator). Here, f is a convex function aiming to promote the structure of x 0 , say \u2113 1 -norm to promote sparsity or nuclear norm to promote low-rankness. We assume that the entries of A and the non-zero entries of z are i.i.d normal with variances 1 and \u03c3 2 , respectively. Our analysis precisely characterizes the asymptotic noise sensitivity ||x - x 0 || 2 2 /\u03c3 2  in the limit \u03c3 2  \u2192 0. We show analytically that the LAD method outperforms the more popular LASSO method when the noise is sparse. At the same time its performance is no more than \u03c0/2 times worse in the presence of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:t6usbXjVLHcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A simple bound on the ber of the map decoder for massive mimo systems",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8682440/",
            "Abstract": "The deployment of massive MIMO systems has revived much of the interest in the study of the large-system performance of multiuser detection systems. In this paper, we prove a non-trivial upper bound on the bit-error rate (BER) of the MAP detector for BPSK signal transmission and equal-power condition. In particular, our bound is approximately tight at high-SNR. The proof is simple and relies on Gordon's comparison inequality. Interestingly, we show that under the assumption that Gordon's inequality is tight, the resulting BER prediction matches that of the replica method under the replica symmetry (RS) ansatz. Also, we prove that, when the ratio of receive to transmit antennas exceeds 0.9251, the replica prediction matches the matched filter lower bound (MFB) at high-SNR. We corroborate our results by numerical evidence.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:zLWjf1WUPmwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Regret Bound for Safe Gaussian Process Bandit Optimization",
            "Publication year": 2020,
            "Publication url": "http://proceedings.mlr.press/v120/amani20a",
            "Abstract": "Many applications require a learner to make sequential decisions given uncertainty regarding both the system\u2019s payoff function and safety constraints. When learning algorithms are used in safety-critical systems, it is paramount that the learner\u2019s actions do not violate the safety constraints at any stage of the learning process. In this paper, we study a stochastic bandit optimization problem where the system\u2019s unknown payoff and constraint functions are sampled from Gaussian Processes (GPs). We develop a safe variant of the proposed algorithm by Srinivas et al.(2010), GP-UCB, called SGP-UCB, with necessary modifications to respect safety constraints at every round. Our most important contribution is to derive the first sub-linear regret bounds for this problem.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:L7CI7m0gUJcC",
            "Publisher": "PMLR"
        },
        {
            "Title": "Isotropically random orthogonal matrices: Performance of lasso and minimum conic singular values",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7282516/",
            "Abstract": "Recently, the precise performance of the Generalized LASSO algorithm for recovering structured signals from compressed noisy measurements, obtained via i.i.d. Gaussian matrices, has been characterized. The analysis is based on a framework introduced by Stojnic and heavily relies on the use of Gordon's Gaussian min-max theorem (GMT), a comparison principle on Gaussian processes. As a result, corresponding characterizations for other ensembles of measurement matrices have not been developed. In this work, we analyze the corresponding performance of the ensemble of isotropically random orthogonal (i.r.o.) measurements. We consider the constrained version of the Generalized LASSO and derive a sharp characterization of its normalized squared error in the large-system limit. When compared to its Gaussian counterpart, our result analytically confirms the superiority in performance of the i.r.o \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:dTyEYWd-f8wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Using unknown occluders to recover hidden scenes",
            "Publication year": 2019,
            "Publication url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yedidia_Using_Unknown_Occluders_to_Recover_Hidden_Scenes_CVPR_2019_paper.html",
            "Abstract": "We consider the challenging problem of inferring a hidden moving scene from faint shadows cast on a diffuse surface. Recent work in passive non-line-of-sight (NLoS) imaging has shown that the presence of occluding objects in between the scene and the diffuse surface significantly improves the conditioning of the problem. However, that work assumes that the shape of the occluder is known a priori. In this paper, we relax this often impractical assumption, extending the range of applications for passive occluder-based NLoS imaging systems. We formulate the task of jointly recovering the unknown scene and unknown occluder as a blind deconvolution problem, for which we propose a simple but effective two-step algorithm. At the first step, the algorithm exploits motion in the scene in order to obtain an estimate of the occluder. In particular, it exploits the fact that motion in realistic scenes is typically sparse. The second step is more standard: using regularization, we deconvolve by the occluder estimate to solve for the hidden scene. We demonstrate the effectiveness of our method with simulations and experiments in a variety of settings.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:epqYDVWIO7EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improved bounds on Gaussian MAC and sparse regression via Gaussian inequalities",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8849764/",
            "Abstract": "We consider the Gaussian multiple-access channel with two critical departures from the classical asymptotics: a) number of users proportional to block-length and b) each user sends a fixed number of data bits. We provide improved bounds on the tradeoff between the user density and the energy-per-bit. Interestingly, in this information-theoretic problem we rely on Gordon's lemma from Gaussian process theory. From the engineering standpoint, we discover a surprising new effect: good coded-access schemes can achieve perfect multi-user interference cancellation at low user density.In addition, by a similar method we analyze the limits of false-discovery in binary sparse regression problem in the asymptotic regime of number of measurements going to infinity at fixed ratios with problem dimension, sparsity and noise level. Our rigorous bound matches the formal replica-method prediction for some range of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:vDijr-p_gm4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Decentralized Multi-Agent Linear Bandits with Safety Constraints",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2012.00314",
            "Abstract": "We study decentralized stochastic linear bandits, where a network of  agents acts cooperatively to efficiently solve a linear bandit-optimization problem over a -dimensional space. For this problem, we propose DLUCB: a fully decentralized algorithm that minimizes the cumulative regret over the entire network. At each round of the algorithm each agent chooses its actions following an upper confidence bound (UCB) strategy and agents share information with their immediate neighbors through a carefully designed consensus procedure that repeats over cycles. Our analysis adjusts the duration of these communication cycles ensuring near-optimal regret performance  at a communication rate of  per round. The structure of the network affects the regret performance via a small additive term - coined the regret of delay - that depends on the spectral gap of the underlying graph. Notably, our results apply to arbitrary network topologies without a requirement for a dedicated agent acting as a server. In consideration of situations with high communication cost, we propose RC-DLUCB: a modification of DLUCB with rare communication among agents. The new algorithm trades off regret performance for a significantly reduced total communication cost of  over all  rounds. Finally, we show that our ideas extend naturally to the emerging, albeit more challenging, setting of safe bandits. For the recently studied problem of linear bandits with unknown linear safety constraints, we propose the first safe decentralized algorithm. Our study contributes towards applying bandit techniques in safety-critical distributed systems that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:Dip1O2bNi0gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improved bounds on the epidemic threshold of exact SIS models on complex networks",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7798804/",
            "Abstract": "The SIS (susceptible-infected-susceptible) epidemic model on an arbitrary network, without making approximations, is a 2 n -state Markov chain with a unique absorbing state (the all-healthy state). This makes analysis of the SIS model and, in particular, determining the threshold of epidemic spread quite challenging. It has been shown that the exact marginal probabilities of infection can be upper bounded by an n-dimensional linear time-invariant system, a consequence of which is that the Markov chain is \u201cfast-mixing\u201d when the LTI system is stable, i.e. when equation (where \u03b2 is the infection rate per link, \u03b4 is the recovery rate, and \u03bb max (A) is the largest eigenvalue of the network's adjacency matrix). This well-known threshold has been recently shown not to be tight in several cases, such as in a star network. In this paper, we provide tighter upper bounds on the exact marginal probabilities of infection, by also taking \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:dQ2og3OwTAUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Asymptotically exact error analysis for the generalized equation-LASSO",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7282810/",
            "Abstract": "Given an unknown signal x 0  \u2208 R n  and linear noisy measurements y = Ax 0  + \u03c3v \u2208 R m , the generalized \u2113 2 2 -LASSO solves x\u0302 := arg min x  1/2\u2225y-Ax \u2225 2 2  + \u03c3\u03bbf(x). Here, f is a convex regularization function (e.g. \u2113 1 -norm, nuclearnorm) aiming to promote the structure of x 0  (e.g. sparse, lowrank), and, \u03bb \u2265 0 is the regularizer parameter. A related optimization problem, though not as popular or well-known, is often referred to as the generalized \u2113 2 -LASSO and takes the form x\u0302\u0302\u0302 := arg min x  \u2225y-Ax\u2225 2  +\u03bbf(x), and has been analyzed by Oymak, Thrampoulidis and Hassibi. Oymak et al. further made conjectures about the performance of the generalized \u2113 2 2 -LASSO. This paper establishes these conjectures rigorously. We measure performance with the normalized squared error NSE(\u03c3) := \u2225x-x 0 \u2225 2 2 /(m\u03c3 2 ). Assuming the entries of A are i.i.d. Gaussian N (0,1/m) and those of v are i.i.d. N(0,1), we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:eq2jaN3J8jMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Generalized Linear Bandits with Safety Constraints",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9054063/",
            "Abstract": "The classical multi-armed bandit is a class of sequential decision making problems where selecting actions incurs costs that are sampled independently from an unknown underlying distribution. Bandit algorithms have many applications in safety critical systems, where several constraints must be respected during the run of the algorithm in spite of uncertainty about problem parameters. This paper formulates a generalized linear stochastic multi-armed bandit problem with generalized linear safety constraints that depend on an unknown parameter vector. In this setting, we propose a Safe UCB-GLM algorithm for which we provide general and problem-dependent regret bounds.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:M7yex6snE4oC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Phase retrieval via linear programming: Fundamental limits and algorithmic improvements",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8262856/",
            "Abstract": "A recently proposed convex formulation of the phase retrieval problem estimates the unknown signal by solving a simple linear program. This new scheme, known as PhaseMax, is computationally efficient compared to standard convex relaxation methods based on lifting techniques. In this paper, we present an exact performance analysis of PhaseMax under Gaussian measurements in the large system limit. In contrast to previously known performance bounds in the literature, our results are asymptotically exact and they also reveal a sharp phase transition phenomenon. Furthermore, the geometrical insights gained from our analysis led us to a novel nonconvex formulation of the phase retrieval problem and an accompanying iterative algorithm based on successive linearization and maximization over a polytope. This new algorithm, which we call PhaseLamp, has provably superior recovery performance over the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:gsN89kCJA0AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sharp asymptotics and optimal performance for inference in binary models",
            "Publication year": 2020,
            "Publication url": "http://proceedings.mlr.press/v108/taheri20a.html",
            "Abstract": "We study convex empirical risk minimization for high-dimensional inference in binary models. Our first result sharply predicts the statistical performance of such estimators in the linear asymptotic regime under isotropic Gaussian features. Importantly, the predictions hold for a wide class of convex loss functions, which we exploit in order to prove a bound on the best achievable performance among them. Notably, we show that the proposed bound is tight for popular binary models (such as Signed, Logistic or Probit), by constructing appropriate loss functions that achieve it. More interestingly, for binary linear classification under the Logistic and Probit models, we prove that the performance of least-squares is no worse than 0.997 and 0.98 times the optimal one. Numerical simulations corroborate our theoretical findings and suggest they are accurate even for relatively small problem dimensions.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:4MWp96NkSFoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Regularized linear regression: A precise analysis of the estimation error",
            "Publication year": 2015,
            "Publication url": "http://proceedings.mlr.press/v40/Thrampoulidis15.html",
            "Abstract": "Non-smooth regularized convex optimization procedures have emerged as a powerful tool to recover structured signals (sparse, low-rank, etc.) from (possibly compressed) noisy linear measurements. We focus on the problem of linear regression and consider a general class of optimization methods that minimize a loss function measuring the misfit of the model to the observations with an added structured-inducing regularization term. Celebrated instances include the LASSO, Group-LASSO, Least-Absolute Deviations method, etc.. We develop a quite general framework for how to determine precise prediction performance guaranties (eg mean-square-error) of such methods for the case of Gaussian measurement ensemble. The machinery builds upon Gordon\u2019s Gaussian min-max theorem under additional convexity assumptions that arise in many practical applications. This theorem associates with a primary optimization (PO) problem a simplified auxiliary optimization (AO) problem from which we can tightly infer properties of the original (PO), such as the optimal cost, the norm of the optimal solution, etc. Our theory applies to general loss functions and regularization and provides guidelines on how to optimally tune the regularizer coefficient when certain structural properties (such as sparsity level, rank, etc.) are known.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:SdhP9T11ey4C",
            "Publisher": "PMLR"
        },
        {
            "Title": "Phase retrieval via polytope optimization: Geometry, phase transitions, and new algorithms",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1805.09555",
            "Abstract": "We study algorithms for solving quadratic systems of equations based on optimization methods over polytopes. Our work is inspired by a recently proposed convex formulation of the phase retrieval problem, which estimates the unknown signal by solving a simple linear program over a polytope constructed from the measurements. We present a sharp characterization of the high-dimensional geometry of the aforementioned polytope under Gaussian measurements. This characterization allows us to derive asymptotically exact performance guarantees for PhaseMax, which also reveal a phase transition phenomenon with respect to its sample complexity. Moreover, the geometric insights gained from our analysis lead to a new nonconvex formulation of the phase retrieval problem and an accompanying iterative algorithm, which we call PhaseLamp. We show that this new algorithm has superior recovery performance over the original PhaseMax method. Finally, as yet another variation on the theme of performing phase retrieval via polytope optimization, we propose a weighted version of PhaseLamp and demonstrate, through numerical simulations, that it outperforms several state-of-the-art algorithms under both generic Gaussian measurements as well as more realistic Fourier-type measurements that arise in phase retrieval applications.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:z_wVstp3MssC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Near-optimal sample complexity bounds for circulant binary embedding",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7953380/",
            "Abstract": "Binary embedding is the problem of mapping points from a high-dimensional space to a Hamming cube in lower dimension while preserving pairwise distances. An efficient way to accomplish this is to make use of fast embedding techniques involving Fourier transform e.g. circulant matrices. While binary embedding has been studied extensively, theoretical results on fast binary embedding are rather limited. In this work, we build upon the recent literature to obtain significantly better dependencies on the problem parameters. A set of N points in \u211d n  can be properly embedded into the Hamming cube {\u00b11} k  with \u03b4 distortion, by using k ~ \u03b4 -3  log N samples which is optimal in the number of points N and compares well with the optimal distortion dependency \u03b4 -2 . Our optimal embedding result applies in the regime log N \u2272 n 1/3 . Furthermore, if the looser condition log N \u2272 \u221an holds, we show that all but an arbitrarily \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:uJ-U7cs_P_0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The BOX-LASSO with application to GSSK modulation in massive MIMO systems",
            "Publication year": 2017,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/630831",
            "Abstract": "The export option will allow you to export the current search results of the entered query to a file. Different formats are available for download. To export the items, click on the button corresponding with the preferred download format.By default, clicking on the export buttons will result in a download of the allowed maximum amount of items. For anonymous users the allowed maximum amount is 50 search results.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:HbR8gkJAVGIC",
            "Publisher": "Institute of Electrical and Electronics Engineers (IEEE)"
        },
        {
            "Title": "Optimum training for mimo bpsk transmission",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8446040/",
            "Abstract": "In this paper, we derive an analytical expression for the bit error rate (BER) of binary phase shift keying (BPSK) symbols transmitted over a multiple-input multiple-output (MIMO) system under channel estimation errors. In this wireless communications system, the receiver uses the linear minimum mean squared error (LMMSE) estimator to estimate the channel matrix. The error in this estimation affects the following estimation that is used to recover the transmitted symbols. It is shown that the channel estimation error is Gaussian and hence the convex Gaussian min-max theorem (CGMT) can be applied to analyze the error of the signal estimation stage and finally obtain an expression for the BER. We use the BER expression to obtain the optimal pilot power allocation under a total transmit energy constraint. Numerical results show close matching between theory and simulations.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:9Nmd_mFXekcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimal combination of linear and spectral estimators for generalized linear models",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/article/10.1007/s10208-021-09531-x",
            "Abstract": "We study the problem of recovering an unknown signal\\({\\varvec {x}}\\) given measurements obtained from a generalized linear model with a Gaussian sensing matrix. Two popular solutions are based on a linear estimator\\(\\hat {\\varvec {x}}^\\mathrm {L}\\) and a spectral estimator\\(\\hat {\\varvec {x}}^\\mathrm {s}\\). The former is a data-dependent linear combination of the columns of the measurement matrix, and its analysis is quite simple. The latter is the principal eigenvector of a data-dependent matrix, and a recent line of work has studied its performance. In this paper, we show how to optimally combine\\(\\hat {\\varvec {x}}^\\mathrm {L}\\) and\\(\\hat {\\varvec {x}}^\\mathrm {s}\\). At the heart of our analysis is the exact characterization of the empirical joint distribution of\\(({\\varvec {x}},\\hat {\\varvec {x}}^\\mathrm {L},\\hat {\\varvec {x}}^\\mathrm {s})\\) in the high-dimensional limit. This allows us to compute the Bayes-optimal combination \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:WZBGuue-350C",
            "Publisher": "Springer US"
        },
        {
            "Title": "Safe Linear Thompson Sampling with Side Information",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9457159/",
            "Abstract": "The design and performance analysis of bandit algorithms in the presence of stage-wise safety or reliability constraints has recently garnered significant interest. In this work, we consider the linear stochastic bandit problem under additional unknown linear safety constraints that need to be satisfied at each round. For this problem, we present and analyze a new safe algorithm based on linear Thompson Sampling (TS). Our analysis shows that, with high probability, the algorithm chooses actions that are safe at each round and achieve cumulative regret of order   . Remarkably, this matches the regret bound provided by [1],[2] for the standard linear TS algorithm in the absence of safety constraints. Also, our analysis highlights how the inherently randomized nature of the TS selection rule suffices to properly expand the set of safe actions that the algorithm has access to at each round. In \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:oNZyr7d5Mn4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Safe Reinforcement Learning with Linear Function Approximation",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2106.06239",
            "Abstract": "Safety in reinforcement learning has become increasingly important in recent years. Yet, existing solutions either fail to strictly avoid choosing unsafe actions, which may lead to catastrophic results in safety-critical systems, or fail to provide regret guarantees for settings where safety constraints need to be learned. In this paper, we address both problems by first modeling safety as an unknown linear cost function of states and actions, which must always fall below a certain threshold. We then present algorithms, termed SLUCB-QVI and RSLUCB-QVI, for episodic Markov decision processes (MDPs) with linear function approximation. We show that SLUCB-QVI and RSLUCB-QVI, while with \\emph{no safety violation}, achieve a  regret, nearly matching that of state-of-the-art unsafe algorithms, where  is the duration of each episode,  is the dimension of the feature mapping,  is a constant characterizing the safety constraints, and  is the total number of action plays. We further present numerical simulations that corroborate our theoretical findings.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:0N-VGjzr574C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Benign overfitting in binary classification of gaussian mixtures",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9413946/",
            "Abstract": "Deep neural networks generalize well despite being exceedingly overparameterized, but understanding the statistical principles behind this so called benign-overfitting phenomenon is not yet well understood. Recently there has been remarkable progress towards understanding benign-overfitting in simpler models, such as linear regression and, even more recently, linear classification. This paper studies benign-overfitting for data generated from a popular binary Gaussian mixtures model (GMM) and classifiers trained by support-vector machines (SVM). Our approach has two steps. First, we leverage an idea introduced in [2] to relate the SVM solution to the least-squares (LS) solution. Second, we derive novel non-asymptotic bounds on the classification error of LS solution. Combining the two gives sufficient conditions on the overparameterization ratio and the signal-to-noise ratio that lead to benign overfitting. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:PoWvk5oyLR8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "SAFE LINEAR BANDITS",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9400288/",
            "Abstract": "Bandit algorithms have various applications in safety-critical systems, where it is important to respect the system's underlying constraints. The challenge is that such constraints are often unknown as they depend on the bandit's unknown parameters. In this talk, we formulate a linear stochastic multi-armed bandit problem with safety constraints that depend linearly on an unknown parameter vector. As such, the learner is unable to identify all safe actions and must act conservatively in ensuring that their actions satisfy the safety constraint at all rounds (at least with high probability). For these bandits, we propose new upper-confidence bound (UCB) and Thompson-sampling algorithms, which include necessary modifications to respect the safety constraints. For two settings -with and without bandit feedback information on the constraint- we prove regret bounds and discuss their optimality in relation to corresponding bounds \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:Ug5p-4gJ2f0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Exploiting occlusion in non-line-of-sight active imaging",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8345763/",
            "Abstract": "Active non-line-of-sight imaging systems are of growing interest for diverse applications. The most commonly proposed approaches to date rely on exploiting time-resolved measurements, i.e., measuring the time it takes for short-duration light pulses to transit the scene. This typically requires expensive, specialized, ultrafast lasers, and detectors that must be carefully calibrated. We develop an alternative approach that exploits the valuable role that natural occluders in a scene play in enabling accurate and practical image formation in such settings without such hardware complexity. In particular, we demonstrate that the presence of occluders in the hidden scene can obviate the need for collecting time-resolved measurements, and develop an accompanying analysis for such systems and their generalizations. Ultimately, the results suggest the potential to develop increasingly sophisticated future systems that are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:evX43VCCuoAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Analytic study of double descent in binary classification: The impact of loss",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9174344/",
            "Abstract": "Extensive empirical evidence reveals that, for a wide range of different learning methods and data sets, the risk curve exhibits a double-descent (DD) trend as a function of the model size. In our recent coauthored paper [Deng et al., '19], we proposed simple binary linear classification models and showed that the test error of gradient descent (GD) with logistic loss undergoes a DD. In this paper, we complement these results by extending them to GD with square loss. We show that the DD phenomenon persists, but we also identify several differences compared to logistic loss. This emphasizes that crucial features of DD curves (such as their transition threshold and global minima) depend both on the training data and on the learning algorithm. We further study the dependence of DD curves on the size of the training set. Similar to [Deng et al., '19] our results are analytic: we plot the DD curves by first deriving sharp \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:ML0RJ9NH7IQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The squared-error of generalized lasso: A precise analysis",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6736635/",
            "Abstract": "We consider the problem of estimating an unknown but structured signal x 0  from its noisy linear observations y = Ax 0  + z \u2208 \u211d m . To the structure of x 0  is associated a structure inducing convex function f(\u00b7). We assume that the entries of A are i.i.d. standard normal N(0, 1) and z ~ N(0, \u03c3 2 I m ). As a measure of performance of an estimate x* of x 0  we consider the \u201cNormalized Square Error\u201d (NSE) \u2225x* - x 0 \u2225 2 2 /\u03c3 2 . For sufficiently small \u03c3, we characterize the exact performance of two different versions of the well known LASSO algorithm. The first estimator is obtained by solving the problem argmin x  \u2225y - Ax\u2225 2  + \u03bbf(x). As a function of \u03bb, we identify three distinct regions of operation. Out of them, we argue that \u201cR ON \u201d is the most interesting one. When \u03bb \u2208 R ON , we show that the NSE is D f (x 0 , \u03bb)/m-D f (x 0 , \u03bb) for small \u03c3, where D f (x 0 , \u03bb) is the expected squared-distance of an i.i.d. standard normal vector to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:u5HHmVD_uO8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Asymptotic behavior of adversarial training in binary classification",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2010.13275",
            "Abstract": "It has been consistently reported that many machine learning models are susceptible to adversarial attacks i.e., small additive adversarial perturbations applied to data points can cause misclassification. Adversarial training using empirical risk minimization is considered to be the state-of-the-art method for defense against adversarial attacks. Despite being successful in practice, several problems in understanding generalization performance of adversarial training remain open. In this paper, we derive precise theoretical predictions for the performance of adversarial training in binary classification. We consider the high-dimensional regime where the dimension of data grows with the size of the training data-set at a constant ratio. Our results provide exact asymptotics for standard and adversarial errors of the estimators obtained by adversarial training with -norm bounded perturbations () for both discriminative binary models and generative Gaussian mixture models. Furthermore, we use these sharp predictions to uncover several intriguing observations on the role of various parameters including the over-parameterization ratio, the data model, and the attack budget on the adversarial and standard errors.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:i2xiXl-TujoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sharp global convergence guarantees for iterative nonconvex optimization: A Gaussian process perspective",
            "Publication year": 2021,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2021arXiv210909859A/abstract",
            "Abstract": "We consider a general class of regression models with normally distributed covariates, and the associated nonconvex problem of fitting these models from data. We develop a general recipe for analyzing the convergence of iterative algorithms for this task from a random initialization. In particular, provided each iteration can be written as the solution to a convex optimization problem satisfying some natural conditions, we leverage Gaussian comparison theorems to derive a deterministic sequence that provides sharp upper and lower bounds on the error of the algorithm with sample-splitting. Crucially, this deterministic sequence accurately captures both the convergence rate of the algorithm and the eventual error floor in the finite-sample regime, and is distinct from the commonly used\" population\" sequence that results from taking the infinite-sample limit. We apply our general framework to derive several concrete \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:hCrLmN-GePgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Recovering structured signals in high dimensions via non-smooth convex optimization: Precise performance analysis",
            "Publication year": 2016,
            "Publication url": "https://thesis.library.caltech.edu/9836/",
            "Abstract": "The typical scenario that arises in modern large-scale inference problems is one where the ambient dimension of the unknown signal is very large (e.g., high-resolution images, recommendation systems), yet its desired properties lie in some low-dimensional structure such as, sparsity or low-rankness. In the past couple of decades, non-smooth convex optimization methods have emerged as a powerful tool to extract those structures, since they are often computationally efficient, and also they offer enough flexibility while simultaneously being amenable to performance analysis. Especially, since the advent of Compressed Sensing (CS) there has been significant progress towards this direction. One of the key ideas is that  random  linear measurements offer an efficient way to acquire structured signals. When the measurement matrix has entries iid from a wide class of distributions (including Gaussians), a series of recent works have established a complete and transparent theory that  precisely  captures the performance in the  noiseless  setting. In the more practical scenario of  noisy  measurements the performance analysis task becomes significantly more challenging and corresponding  precise  and  unifying  results have hitherto remained scarce. The available class of optimization methods, often referred to as  regularized M-estimators, is now richer; additional factors (e.g., the noise distribution, the loss function, and the regularizer parameter) and several different measures of performance (e.g., squared-error, probability of support recovery) need to be taken into account.This thesis develops a novel analytical framework that overcomes these \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:JQOojiI6XY0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Analysis and optimization of aperture design in computational imaging",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8462521/",
            "Abstract": "There is growing interest in the use of coded aperture imaging systems for a variety of applications. Using an analysis framework based on mutual information, we examine the fundamental limits of such systems-and the associated optimum aperture coding-under simple but meaningful propagation and sensor models. Among other results, we show that when SNR is high and thermal noise dominates shot noise, spectrally-flat masks, which have 50% transmissivity, are optimal, but that when shot noise dominates thermal noise, randomly generated masks with lower transmissivity offer greater performance. We also provide comparisons to classical pinhole and lens-based cameras.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:j8SEvjWlNXcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The performance of box-relaxation decoding in massive MIMO with low-resolution ADCs",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8450800/",
            "Abstract": "We study massive multiple-input-multiple-output (MIMO) uplink systems with low-resolution analog-to-digital converters (ADCs) on each receiver antenna. For the decoding, we propose an efficient convex program, namely the boxrelaxation optimization (BRO), for which we further characterize its asymptotic bit-error rate (BER) under Gaussian channel and noise models. The derived formula is an explicit function of the ratio of receive to transmit antennas, of the SNR, and of the specifics of the quantization scheme. Hence, it can be used for tuning of these parameters such that the desired BER specifications are met. We further show the superiority of the BRO compared to standard linear detectors such as the zero-forcing (ZF) decoder. Numerical simulations corroborate our theoretical results.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:yB1At4FlUx8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Recovering structured signals in noise: Least-squares meets compressed sensing",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-16042-9_4",
            "Abstract": "The typical scenario that arises in most \u201cbig data\u201d problems is one where the ambient dimension of the signal is very large (e.g., high resolution images, gene expression data from a DNA microarray, social network data, etc.), yet is such that its desired properties lie in some low dimensional structure (sparsity, low-rankness, clusters, etc.). In the modern viewpoint, the goal is to come up with efficient algorithms to reveal these structures and for which, under suitable conditions, one can give theoretical guarantees. We specifically consider the problem of recovering such a structured signal (sparse, low-rank, block-sparse, etc.) from noisy compressed measurements. A general algorithm for such problems, commonly referred to as generalized LASSO, attempts to solve this problem by minimizing a least-squares cost with an added \u201cstructure-inducing\u201d regularization term (\u2113 1 norm, nuclear norm, mixed \u2113 2/\u2113 1 norm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:JoZmwDi-zQgC",
            "Publisher": "Birkh\u00e4user, Cham"
        },
        {
            "Title": "Provable benefits of overparameterization in model compression: From double descent to pruning neural networks",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2012.08749",
            "Abstract": "Deep networks are typically trained with many more parameters than the size of the training dataset. Recent empirical evidence indicates that the practice of overparameterization not only benefits training large models, but also assists - perhaps counterintuitively - building lightweight models. Specifically, it suggests that overparameterization benefits model pruning / sparsification. This paper sheds light on these empirical findings by theoretically characterizing the high-dimensional asymptotics of model pruning in the overparameterized regime. The theory presented addresses the following core question: \"should one train a small model from the beginning, or first train a large model and then prune?\". We analytically identify regimes in which, even if the location of the most informative features is known, we are better off fitting a large model and then pruning rather than simply training with the known informative features. This leads to a new double descent in the training of sparse models: growing the original model, while preserving the target sparsity, improves the test accuracy as one moves beyond the overparameterization threshold. Our analysis further reveals the benefit of retraining by relating it to feature correlations. We find that the above phenomena are already present in linear and random-features models. Our technical approach advances the toolset of high-dimensional analysis and precisely characterizes the asymptotic distribution of over-parameterized least-squares. The intuition gained by analytically studying simpler models is numerically verified on neural networks.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:xtoqd-5pKcoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exploring Weight Importance and Hessian Bias in Model Pruning",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2006.10903",
            "Abstract": "Model pruning is an essential procedure for building compact and computationally-efficient machine learning models. A key feature of a good pruning algorithm is that it accurately quantifies the relative importance of the model weights. While model pruning has a rich history, we still don't have a full grasp of the pruning mechanics even for relatively simple problems involving linear models or shallow neural nets. In this work, we provide a principled exploration of pruning by building on a natural notion of importance. For linear models, we show that this notion of importance is captured by covariance scaling which connects to the well-known Hessian-based pruning. We then derive asymptotic formulas that allow us to precisely compare the performance of different pruning methods. For neural networks, we demonstrate that the importance can be at odds with larger magnitudes and proper initialization is critical for magnitude-based pruning. Specifically, we identify settings in which weights become more important despite becoming smaller, which in turn leads to a catastrophic failure of magnitude-based pruning. Our results also elucidate that implicit regularization in the form of Hessian structure has a catalytic role in identifying the important weights, which dictate the pruning performance.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:g3aElNc5_aQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Precise error analysis of the lasso",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7178615/",
            "Abstract": "A classical problem that arises in numerous signal processing applications asks for the reconstruction of an unknown, k-sparse signal x 0  \u2208 \u211d n  from underdetermined, noisy, linear measurements y = Ax 0  + z \u2208 \u211d m . One standard approach is to solve the following convex program x\u0302 = arg min x  \u2225y - Ax\u2225 2 +\u03bb\u2225x\u2225 1 , which is known as the \u2113 2 -LASSO. We assume that the entries of the sensing matrix A and of the noise vector z are i.i.d Gaussian with variances 1/m and \u03c3 2 . In the large system limit when the problem dimensions grow to infinity, but in constant rates, we precisely characterize the limiting behavior of the normalized squared error \u2225x\u0302 - x 0 \u2225 2 2 /\u03c3 2 . Our numerical illustrations validate our theoretical predictions.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:k8Z6L05lTy4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimal placement of distributed energy storage in power networks",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7112614/",
            "Abstract": "We formulate the optimal placement, sizing and control of storage devices in a power network to minimize generation costs with the intent of load shifting. We assume deterministic demand, a linearized DC approximated power flow model and a fixed available storage budget. Our main result proves that when the generation costs are convex and nondecreasing, there always exists an optimal storage capacity allocation that places zero storage at generation-only buses that connect to the rest of the network via single links. This holds regardless of the demand profiles, generation capacities, line-flow limits and characteristics of the storage technologies. Through a counterexample, we illustrate that this result is not generally true for generation buses with multiple connections. For specific network topologies, we also characterize the dependence of the optimal generation cost on the available storage budget \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:kzcrU_BdoSEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The BOX-LASSO with application to GSSK modulation in massive MIMO systems",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8006695/",
            "Abstract": "The BOX-LASSO is a variant of the popular LASSO that includes an additional box-constraint. We propose its use as a decoder in modern Multiple Input Multiple Output (MIMO) communication systems with modulation methods such as the Generalized Space Shift Keying (GSSK) modulation, which produces constellation vectors that are inherently sparse and with bounded elements. In that direction, we prove novel explicit asymptotic characterizations of the squared-error and of the per-element error rate of the BOX-LASSO, under iid Gaussian measurements. In particular, the theoretical predictions can be used to quantify the improved performance of the BOX-LASSO, when compared to the previously used standard LASSO. We include simulation results that validate both these premises and our theoretical predictions.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:tzM49s52ZIMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Safe linear thompson sampling",
            "Publication year": 2019,
            "Publication url": "https://www.researchgate.net/profile/Ahmadreza-Moradipari/publication/337074687_Safe_Linear_Thompson_Sampling/links/5de58e134585159aa45ca95f/Safe-Linear-Thompson-Sampling.pdf",
            "Abstract": "The design and performance analysis of bandit algorithms in the presence of stage-wise safety or reliability constraints has recently garnered significant interest. In this work, we consider the linear stochastic bandit problem under additional linear safety constraints that need to be satisfied at each round. We provide a new safe algorithm based on linear Thompson Sampling (TS) for this problem and show a frequentist regret of order O (d3/2 log1/2 d\u00b7 T1/2 log3/2 T), which remarkably matches the results provided by [Abeille et al., 2017] for the standard linear TS algorithm in the absence of safety constraints. We compare the performance of our algorithm with a UCB-based safe algorithm and highlight how the inherently randomized nature of TS leads to a superior performance in expanding the set of safe actions the algorithm has access to at each round.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:AvfA0Oy_GE0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sharp guarantees for solving random equations with one-bit information",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8919905/",
            "Abstract": "We study the performance of a wide class of convex optimization-based estimators for recovering a signal from corrupted one-bit measurements in high-dimensions. Our general result predicts sharply the performance of such estimators in the linear asymptotic regime when the measurement vectors have entries iid Gaussian. This includes, as a special case, the previously studied least-squares estimator and various novel results for other popular estimators such as least-absolute deviations, hinge-loss and logistic-loss. Importantly, the sharp nature of our results allows for accurate comparisons between these different estimators. Numerical simulations corroborate our theoretical findings and suggest they are accurate even for relatively small problem dimensions.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:lmc2jWPfTJgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "UCB-based Algorithms for Multinomial Logistic Regression Bandits",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2103.11489",
            "Abstract": "Out of the rich family of generalized linear bandits, perhaps the most well studied ones are logisitc bandits that are used in problems with binary rewards: for instance, when the learner/agent tries to maximize the profit over a user that can select one of two possible outcomes (e.g., `click' vs `no-click'). Despite remarkable recent progress and improved algorithms for logistic bandits, existing works do not address practical situations where the number of outcomes that can be selected by the user is larger than two (e.g., `click', `show me later', `never show again', `no click'). In this paper, we study such an extension. We use multinomial logit (MNL) to model the probability of each one of  possible outcomes (+1 stands for the `not click' outcome): we assume that for a learner's action , the user selects one of  outcomes, say outcome , with a multinomial logit (MNL) probabilistic model with corresponding unknown parameter . Each outcome  is also associated with a revenue parameter  and the goal is to maximize the expected revenue. For this problem, we present MNL-UCB, an upper confidence bound (UCB)-based algorithm, that achieves regret  with small dependency on problem-dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. We present numerical simulations that corroborate our theoretical results.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:XoXfffV-tXoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Linear thompson sampling under unknown linear constraints",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9053865/",
            "Abstract": "We study how adding unknown linear safety constraints affects the performance of Thompson Sampling in the linear stochastic bandit problem. The additional constraints must be met at each round in spite of uncertainty about the environment requiring that the learner acts conservatively in choosing her actions. In this setting, we propose Safe-LTS, the first safe Thompson Sampling based algorithm, and we prove that it achieves no-regret learning. We obtain regrets that have the same dependence on the total number of rounds (modulo logarithmic factors) as Safe-UCB, a recently proposed safe algorithm that uses the upper confidence bound principle. Finally, we provide numerical simulations that demonstrate the efficacy of our algorithm.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:vbGhcppDl1QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Near-optimal coded apertures for imaging via Nazarov\u2019s theorem",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8682254/",
            "Abstract": "We characterize the fundamental limits of coded aperture imaging systems up to universal constants by drawing upon a theorem of Nazarov regarding Fourier transforms. Our work is performed under a simple propagation and sensor model that accounts for thermal and shot noise, scene correlation, and exposure time. Focusing on mean square error as a measure of linear reconstruction quality, we show that appropriate application of a theorem of Nazarov leads to essentially optimal coded apertures, up to a constant multiplicative factor in exposure time. Additionally, we develop a heuristically efficient algorithm to generate such patterns that explicitly takes into account scene correlations. This algorithm finds apertures that correspond to local optima of a certain potential on the hypercube, yet are guaranteed to be tight. Finally, for i.i.d. scenes, we show improvements upon prior work by using spectrally flat \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:uc_IGeMz5qoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Linear stochastic bandits under safety constraints",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1908.05814",
            "Abstract": "Bandit algorithms have various application in safety-critical systems, where it is important to respect the system constraints that rely on the bandit's unknown parameters at every round. In this paper, we formulate a linear stochastic multi-armed bandit problem with safety constraints that depend (linearly) on an unknown parameter vector. As such, the learner is unable to identify all safe actions and must act conservatively in ensuring that her actions satisfy the safety constraint at all rounds (at least with high probability). For these bandits, we propose a new UCB-based algorithm called Safe-LUCB, which includes necessary modifications to respect safety constraints. The algorithm has two phases. During the pure exploration phase the learner chooses her actions at random from a restricted set of safe actions with the goal of learning a good approximation of the entire unknown safe set. Once this goal is achieved, the algorithm begins a safe exploration-exploitation phase where the learner gradually expands their estimate of the set of safe actions while controlling the growth of regret. We provide a general regret bound for the algorithm, as well as a problem dependent bound that is connected to the location of the optimal action within the safe set. We then propose a modified heuristic that exploits our problem dependent analysis to improve the regret.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:URolC5Kub84C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Ber analysis of regularized least squares for bpsk recovery",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7952960/",
            "Abstract": "This paper investigates the problem of recovering an n-dimensional BPSK signal x 0  \u2208 {-1, 1} n  from m-dimensional measurement vector y = Ax+z, where A and z are assumed to be Gaussian with iid entries. We consider two variants of decoders based on the regularized least squares followed by hard-thresholding: the case where the convex relaxation is from {-1, 1} n  to \u211d n  and the box constrained case where the relaxation is to [-1, 1] n . For both cases, we derive an exact expression of the bit error probability when n and m grow simultaneously large at a fixed ratio. For the box constrained case, we show that there exists a critical value of the SNR, above which the optimal regularizer is zero. On the other side, the regularization can further improve the performance of the box relaxation at low to moderate SNR regimes. We also prove that the optimal regularizer in the bit error rate sense for the unboxed case is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:b1wdh0AR-JQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Phase Transitions for One-Vs-One and One-Vs-All Linear Separability in Multiclass Gaussian Mixtures",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9414099/",
            "Abstract": "We study a fundamental statistical question in multiclass classification: When are data linearly separable? Unlike binary classification, linear separability in multiclass settings can be defined in different ways. Here, we focus on the so called one-vs-one (OvO) and one-vs-all (OvA) linear separability. We consider data generated from a Gaussian mixture model (GMM) in a linear asymptotic high-dimensional regime. In this setting, we prove that both the OvO and OvA separability undergo a sharp phase-transition as a function of the overparameterization ratio. We present precise formulae characterizing the phase transitions as a function of the data geometry and the number of classes. Existing results on binary classification follow as special cases of our new formulae. Numerical simulations verify the validity of the asymptotic predictions in finite dimensions.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:edDO8Oi4QzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "BER analysis of the box relaxation for BPSK signal recovery",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7472383/",
            "Abstract": "We study the problem of recovering an n-dimensional BPSK signal from m linear noise-corrupted measurements using the box relaxation method which relaxes the discrete set {\u00b11}n to the convex set [-1,1]n to obtain a convex optimization algorithm followed by hard thresholding. When the noise and measurement matrix have iid standard normal entries, we obtain an exact expression for the bit-wise probability of error Pe in the limit of n and m growing and m/n fixed. At high SNR our result shows that the Pe of box relaxation is within 3dB of the matched filter bound (MFB) for square systems, and that it approaches the (MFB) as m grows large compared to n. Our results also indicate that as m, n \u2192 \u221e, for any fixed set of size k, the error events of the corresponding k bits in the box relaxation method are independent.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:kuK5TVdYjLIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Revealing hidden scenes by photon-efficient occlusion-based opportunistic active imaging",
            "Publication year": 2018,
            "Publication url": "https://www.osapublishing.org/abstract.cfm?uri=oe-26-8-9945",
            "Abstract": "The ability to see around corners, i.e., recover details of a hidden scene from its reflections in the surrounding environment, is of considerable interest in a wide range of applications. However, the diffuse nature of light reflected from typical surfaces leads to mixing of spatial information in the collected light, precluding useful scene reconstruction. Here, we employ a computational imaging technique that opportunistically exploits the presence of occluding objects, which obstruct probe-light propagation in the hidden scene, to undo the mixing and greatly improve scene recovery. Importantly, our technique obviates the need for the ultrafast time-of-flight measurements employed by most previous approaches to hidden-scene imaging. Moreover, it does so in a photon-efficient manner (i.e., it only requires a small number of photon detections) based on an accurate forward model and a computational algorithm that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:TIZ-Mc8IlK0C",
            "Publisher": "Optical Society of America"
        },
        {
            "Title": "The generalized lasso for sub-gaussian measurements with dithered quantization",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8962284/",
            "Abstract": "In the problem of structured signal recovery from high-dimensional linear observations, it is commonly assumed that full-precision measurements are available. Under this assumption, the recovery performance of the popular Generalized Lasso (G-Lasso) is by now well-established. In this paper, we extend these types of results to the practically relevant settings with quantized measurements. We study two extremes of the quantization schemes, namely, uniform and one-bit quantization; the former imposes no limit on the number of quantization bits, while the second only allows for one bit. In the presence of a uniform dithering signal and when measurement vectors are sub-gaussian, we show that the same algorithm (i.e., the G-Lasso) has favorable recovery guarantees for both uniform and one-bit quantization schemes. Our theoretical results, shed light on the appropriate choice of the range of values of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:XD-gHx7UXLsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Label-Imbalanced and Group-Sensitive Classification under Overparameterization",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2103.01550",
            "Abstract": "Label-imbalanced and group-sensitive classification seeks to appropriately modify standard training algorithms to optimize relevant metrics such as balanced error and/or equal opportunity. For label imbalances, recent works have proposed a logit-adjusted loss modification to standard empirical risk minimization. We show that this might be ineffective in general and, in particular so, in the overparameterized regime where training continues in the zero training-error regime. Specifically for binary linear classification of a separable dataset, we show that the modified loss converges to the max-margin SVM classifier despite the logit adjustment. Instead, we propose a more general vector-scaling loss that directly relates to the cost-sensitive SVM (CS-SVM), thus favoring larger margin to the minority class. Through an insightful sharp asymptotic analysis for a Gaussian-mixtures data model, we demonstrate the efficacy of CS-SVM in balancing the errors of the minority/majority classes. Our analysis also leads to a simple strategy for optimally tuning the involved margin-ratio parameter. Then, we show how our results extend naturally to binary classification with sensitive groups, thus treating the two common types of imbalances (label/group) in a unifying way. We corroborate our theoretical findings with numerical experiments on both synthetic and real-world datasets.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:q3CdL3IzO_QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The generalized lasso for sub-Gaussian observations with dithered quantization",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8636051/",
            "Abstract": "In the problem of structured signal recovery from high-dimensional linear observations, it is commonly assumed that full-precision measurements are available. Under this assumption, the recovery performance of the popular Generalized Lasso (G-Lasso) is by now well-established. In this paper, we extend these types of results to the practically relevant settings with quantized measurements. We study two extremes of the quantization schemes, namely, uniform and one-bit quantization; the former imposes no limit on the number of quantization bits, while the second only allows for one bit. In the presence of a uniform dithering signal and when measurement vectors are sub-gaussian, we show that the same algorithm (i.e., the G-Lasso) has favorable recovery guarantees for both uniform and one-bit quantization schemes. Our theoretical results, shed light on the appropriate choice of the range of values of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:kz9GbA2Ns4gC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Simple bounds for noisy linear inverse problems with exact side information",
            "Publication year": 2013,
            "Publication url": "https://arxiv.org/abs/1312.0641",
            "Abstract": "This paper considers the linear inverse problem where we wish to estimate a structured signal  from its corrupted observations. When the problem is ill-posed, it is natural to make use of a convex function  that exploits the structure of the signal. For example,  norm can be used for sparse signals. To carry out the estimation, we consider two well-known convex programs: 1) Second order cone program (SOCP), and, 2) Lasso. Assuming Gaussian measurements, we show that, if precise information about the value  or the -norm of the noise is available, one can do a particularly good job at estimation. In particular, the reconstruction error becomes proportional to the \"sparsity\" of the signal rather than the ambient dimension of the noise vector. We connect our results to existing works and provide a discussion on the relation of our results to the standard least-squares problem. Our error bounds are non-asymptotic and sharp, they apply to arbitrary convex functions and do not assume any distribution on the noise.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A model of double descent for high-dimensional binary linear classification",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1911.05822",
            "Abstract": "We consider a model for logistic regression where only a subset of features of size  is used for training a linear classifier over  training samples. The classifier is obtained by running gradient descent (GD) on logistic loss. For this model, we investigate the dependence of the classification error on the overparameterization ratio . First, building on known deterministic results on the implicit bias of GD, we uncover a phase-transition phenomenon for the case of Gaussian features: the classification error of GD is the same as that of the maximum-likelihood (ML) solution when , and that of the max-margin (SVM) solution when . Next, using the convex Gaussian min-max theorem (CGMT), we sharply characterize the performance of both the ML and the SVM solutions. Combining these results, we obtain curves that explicitly characterize the classification error for varying values of . The numerical results validate the theoretical predictions and unveil double-descent phenomena that complement similar recent findings in linear regression settings as well as empirical observations in more complex learning scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:BUYA1_V_uYcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Stage-wise Conservative Linear Bandits",
            "Publication year": 2020,
            "Publication url": "https://proceedings.neurips.cc/paper/2020/hash/804741413d7fe0e515b19a7ffc7b3027-Abstract.html",
            "Abstract": "We study stage-wise conservative linear stochastic bandits: an instance of bandit optimization, which accounts for (unknown) safety constraints that appear in applications such as online advertising and medical trials. At each stage, the learner must choose actions that not only maximize cumulative reward across the entire time horizon, but further satisfy a linear baseline constraint that takes the form of a lower bound on the instantaneous reward. For this problem, we present two novel algorithms, stage-wise conservative linear Thompson Sampling (SCLTS) and stage-wise conservative linear UCB (SCLUCB), that respect the baseline constraints and enjoy probabilistic regret bounds of order  and , respectively. Notably, the proposed algorithms can be adjusted with only minor modifications to tackle different problem variations, such as, constraints with bandit-feedback, or an unknown sequence of baseline rewards. We discuss these and other improvements over the state-of-the art. For instance, compared to existing solutions, we show that SCLTS plays the (non-optimal) baseline action at most  times (compared to ). Finally, we make connections to another studied form of safety-constraints that takes the form of an upper bound on the instantaneous reward. While this incurs additional complexity to the learning process as the optimal action is not guaranteed to belong to the safe-set at each round, we show that SCLUCB can properly adjust in this setting via a simple modification.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:S16KYo8Pm5AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Simple error bounds for regularized noisy linear inverse problems",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6875386/",
            "Abstract": "Consider estimating a structured signal x 0  from linear, underdetermined and noisy measurements y = Ax 0 +z, via solving a variant of the lasso algorithm: x\u0302 = arg min x {\u2225y-Ax\u22252+\u03bbf(x)}. Here, f is a convex function aiming to promote the structure of x 0 , say \u2113 1 -norm to promote sparsity or nuclear norm to promote low-rankness. We assume that the entries of A are independent and normally distributed and make no assumptions on the noise vector z, other than it being independent of A. Under this generic setup, we derive a general, non-asymptotic and rather tight upper bound on the \u2113 2 -norm of the estimation error \u2225x\u0302 - x 0 \u22252. Our bound is geometric in nature and obeys a simple formula; the roles of \u03bb, f and x 0  are all captured by a single summary parameter \u03b4(\u03bb\u2202f(x 0 )), termed the Gaussian squared distance to the scaled subdifferential. We connect our result to the literature and verify its validity through \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:VaXvl8Fpj5cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Theoretical insights into multiclass classification: A high-dimensional asymptotic view",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2011.07729",
            "Abstract": "Contemporary machine learning applications often involve classification tasks with many classes. Despite their extensive use, a precise understanding of the statistical properties and behavior of classification algorithms is still missing, especially in modern regimes where the number of classes is rather large. In this paper, we take a step in this direction by providing the first asymptotically precise analysis of linear multiclass classification. Our theoretical analysis allows us to precisely characterize how the test error varies over different training algorithms, data distributions, problem dimensions as well as number of classes, inter/intra class correlations and class priors. Specifically, our analysis reveals that the classification accuracy is highly distribution-dependent with different algorithms achieving optimal performance for different data distributions and/or training/features sizes. Unlike linear regression/binary classification, the test error in multiclass classification relies on intricate functions of the trained model (e.g., correlation between some of the trained weights) whose asymptotic behavior is difficult to characterize. This challenge is already present in simple classifiers, such as those minimizing a square loss. Our novel theoretical techniques allow us to overcome some of these challenges. The insights gained may pave the way for a precise understanding of other classification algorithms beyond those studied in this paper.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:SpbeaW3--B0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "AutoBalance: Optimized Loss Functions for Imbalanced Data",
            "Publication year": 2021,
            "Publication url": "https://openreview.net/forum?id=ebQXflQre5a",
            "Abstract": "Imbalanced datasets are commonplace in modern machine learning problems. The presence of under-represented classes or groups with sensitive attributes results in concerns about generalization and fairness. Such concerns are further exacerbated by the fact that large capacity deep nets can perfectly fit the training data and appear to achieve perfect accuracy and fairness during training, but perform poorly during test. To address these challenges, we propose AutoBalance, a bi-level optimization framework that automatically designs a training loss function to optimize a blend of accuracy and fairness-seeking objectives. Specifically, a lower-level problem trains the model weights, and an upper-level problem tunes the loss function by monitoring and optimizing the desired objective over the validation data. Our loss design enables personalized treatment for classes/groups by employing a parametric cross-entropy loss and individualized data augmentation schemes. We evaluate the benefits and performance of our approach for the application scenarios of imbalanced and group-sensitive classification. Extensive empirical evaluations demonstrate the benefits of AutoBalance over state-of-the-art approaches. Our experimental findings are complemented with theoretical insights on loss function design and the benefits of the train-validation split. All code is available open-source.Supplementary Material: zipCode Of Conduct: I certify that all co-authors of this work have read and commit to adhering to the NeurIPS Statement on Ethics, Fairness, Inclusivity, and Code of Conduct.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:5qfkUJPXOUwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Symbol error rate performance of box-relaxation decoders in massive mimo",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8352717/",
            "Abstract": "The maximum-likelihood (ML) decoder for symbol detection in large multiple-input multiple-output wireless communication systems is typically computationally prohibitive. In this paper, we study a popular and practical alternative, namely the box-relaxation optimization (BRO) decoder, which is a natural convex relaxation of the ML. For independent identically distributed real Gaussian channels with additive Gaussian noise, we obtain exact asymptotic expressions for the symbol error rate (SER) of the BRO. The formulas are particularly simple, they yield useful insights, and they allow accurate comparisons to the matched-filter bound (MFB) and to linear decoders, such as zero-forcing and linear minimum mean square error. For binary phase-shift keying signals, the SER performance of the BRO is within 3 dB of the MFB for square systems, and it approaches the MFB as the number of receive antennas grows large \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:35r97b3x0nAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sharp Guarantees and Optimal Performance for Inference in Binary and Gaussian-Mixture Models",
            "Publication year": 2021,
            "Publication url": "https://www.mdpi.com/980420",
            "Abstract": "We study convex empirical risk minimization for high-dimensional inference in binary linear classification under both discriminative binary linear models, as well as generative Gaussian-mixture models. Our first result sharply predicts the statistical performance of such estimators in the proportional asymptotic regime under isotropic Gaussian features. Importantly, the predictions hold for a wide class of convex loss functions, which we exploit to prove bounds on the best achievable performance. Notably, we show that the proposed bounds are tight for popular binary models (such as signed and logistic) and for the Gaussian-mixture model by constructing appropriate loss functions that achieve it. Our numerical simulations suggest that the theory is accurate even for relatively small problem dimensions and that it enjoys a certain universality property.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:foquWX3nUaYC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "The lasso with non-linear measurements is equivalent to one with linear measurements",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1506.02181",
            "Abstract": "Consider estimating an unknown, but structured, signal  from  measurement , where the 's are the rows of a known measurement matrix , and,  is a (potentially unknown) nonlinear and random link-function. Such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties. It could also arise by design, e.g., , corresponds to noisy 1-bit quantized measurements. Motivated by the classical work of Brillinger, and more recent work of Plan and Vershynin, we estimate  via solving the Generalized-LASSO for some regularization parameter  and some (typically non-smooth) convex structure-inducing regularizer function. While this approach seems to naively ignore the nonlinear function , both Brillinger (in the non-constrained case) and Plan and Vershynin have shown that, when the entries of  are iid standard normal, this is a good estimator of  up to a constant of proportionality , which only depends on . In this work, we considerably strengthen these results by obtaining explicit expressions for the squared error, for the \\emph{regularized} LASSO, that are asymptotically \\emph{precise} when  and  grow large. A main result is that the estimation performance of the Generalized LASSO with non-linear measurements is \\emph{asymptotically the same} as one whose measurements are linear , with  and , and,  standard normal. To the best of our knowledge, the derived expressions on the estimation performance are the first-known precise results in this context. One interesting consequence of our result \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:WqliGbK-hY8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Lifting high-dimensional non-linear models with Gaussian regressors",
            "Publication year": 2019,
            "Publication url": "http://proceedings.mlr.press/v89/thrampoulidis19a.html",
            "Abstract": "We study the problem of recovering a structured signal  from high-dimensional data  for some nonlinear (and potentially unknown) link function , when the regressors  are iid Gaussian. Brillinger (1982) showed that ordinary least-squares estimates  up to a constant of proportionality , which depends on . Recently, Plan & Vershynin (2015) extended this result to the high-dimensional setting deriving sharp error bounds for the generalized Lasso. Unfortunately, both least-squares and the Lasso fail to recover  when . For example, this includes all even link functions. We resolve this issue by proposing and analyzing an alternative convex recovery method. In a nutshell, our method treats such link functions as if they were linear in a lifted space of higher-dimension. Interestingly, our error analysis captures the effect of both the nonlinearity and the problem\u2019s geometry in a few simple summary parameters.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:EkHepimYqZsC",
            "Publisher": "PMLR"
        },
        {
            "Title": "Phaseless super-resolution in the continuous domain",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7952870/",
            "Abstract": "Phaseless super-resolution refers to the problem of super-resolving a signal from only its low-frequency Fourier magnitude measurements. In this paper, we consider the phaseless super-resolution problem of recovering a sum of sparse Dirac delta functions which can be located anywhere in the continuous time-domain. For such signals in the continuous domain, we propose a novel Semidefinite Programming (SDP) based signal recovery method to achieve the phaseless super-resolution. This work extends the recent work of Jaganathan et al. [1], which considered phaseless super-resolution for discrete signals on the grid.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:W5xh706n7nkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "General performance metrics for the lasso",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7606820/",
            "Abstract": "A recent line of work has established accurate predictions of the mean squared-error (MSE) performance of non-smooth convex optimization methods when used to recover structured signals (e.g. sparse, low-rank) from noisy linear (and possibly compressed) observations. Specifically, in a recent paper [15] we precisely characterized the MSE performance of a general class of regularized M-estimators using a framework that is based on Gaussian process methods. Here, we extend the framework to the analysis of a general class of Lipschitz performance metrics, which in addition to the standard MSE, includes the \u2113 1 -reconstruction error, the probability of successfully identifying whether an element belongs to the support of a sparse signal, the empirical distribution of the error, etc. For concreteness, we primarily focus on the problem of sparse recovery under \u2113 1 -regularized least-squares (aka LASSO). We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:Fu2w8maKXqMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Precise Error Analysis of Regularized  -Estimators in High Dimensions",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8365826/",
            "Abstract": "A popular approach for estimating an unknown signal x 0  \u2208 \u211d n  from noisy, linear measurements y = Ax 0  + z \u2208 \u211d m  is via solving a so called regularized M-estimator: x\u0302 := arg min x  \u00a3(y - Ax) + \u03bbf(x). Here, \u00a3 is a convex loss function, f is a convex (typically, non-smooth) regularizer, and \u03bb > 0 is a regularizer parameter. We analyze the squared error performance \u2225x\u0302-x 0 \u2225 2 2  of such estimators in the high-dimensional proportional regime where m, n \u2192 \u221e and m/n \u2192 \u03b4. The design matrix A is assumed to have entries iid Gaussian; only minimal and rather mild regularity conditions are imposed on the loss function, the regularizer, and on the noise and signal distributions. We show that the squared error converges in probability to a nontrivial limit that is given as the solution to a minimax convex-concave optimization problem on four scalar optimization variables. We identify a new summary parameter, termed the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "X54nJqcAAAAJ:hkOj_22Ku90C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2106.10865",
            "Abstract": "The growing literature on \"benign overfitting\" in overparameterized models has been mostly restricted to regression or binary classification settings; however, most success stories of modern machine learning have been recorded in multiclass settings. Motivated by this discrepancy, we study benign overfitting in multiclass linear classification. Specifically, we consider the following popular training algorithms on separable data: (i) empirical risk minimization (ERM) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) ERM with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classifier. First, we provide a simple sufficient condition under which all three algorithms lead to classifiers that interpolate the training data and have equal accuracy. When the data is generated from Gaussian mixtures or a multinomial logistic model, this condition holds under high enough effective overparameterization. Second, we derive novel error bounds on the accuracy of the MNI classifier, thereby showing that all three training algorithms lead to benign overfitting under sufficient overparameterization. Ultimately, our analysis shows that good generalization is possible for SVM solutions beyond the realm in which typical margin-based bounds apply.",
            "Abstract entirety": 1,
            "Author pub id": "X54nJqcAAAAJ:FPJr55Dyh1AC",
            "Publisher": "Unknown"
        }
    ]
}]