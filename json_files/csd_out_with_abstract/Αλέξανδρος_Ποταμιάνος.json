[{
    "name": "\u0391\u03bb\u03ad\u03be\u03b1\u03bd\u03b4\u03c1\u03bf\u03c2 \u03a0\u03bf\u03c4\u03b1\u03bc\u03b9\u03ac\u03bd\u03bf\u03c2",
    "romanize name": "Alexandros Potamianos",
    "School-Department": "\u0397\u03bb\u03b5\u03ba\u03c4\u03c1\u03bf\u03bb\u03cc\u03b3\u03c9\u03bd \u039c\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 \u039c\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ce\u03bd \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "ntua",
    "Rank": "\u0391\u03bd\u03b1\u03c0\u03bb\u03b7\u03c1\u03c9\u03c4\u03ae\u03c2 \u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 3760,
    "Scholar name": "Alexandros Potamianos",
    "Scholar id": "pBQViyUAAAAJ",
    "Affiliation": "National Technical University of Athens",
    "Citedby": 7264,
    "Interests": [
        "speech processing",
        "natural language processing",
        "signal processing",
        "dialogue"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=pBQViyUAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Unsupervised Stream Weight Computation in a Segmentaion Task: Application to Audio-Visual Speech Recognition",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4728440/",
            "Abstract": "We propose an efficient algorithm for unsupervised stream weight estimation in a segmentation task. Our method uses only the information carried by the test signal and the trained models. The work is based on results presented previously for the classification problem where it is indicated that the optimal stream weights are inversely proportional to the single stream misclassification error. We approximate this error relation by the intra- and inter-class distance ratio over the measured class distributions. This approach is then generalized to the segmentation problem by computing the distances among all the concerned classes. The proposed unsupervised estimation algorithm is evaluated on a an audio-visual speech recognition task. The obtained performances are comparable to the supervised minimum error training approach, up to a certain SNR level.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:Tiz5es2fbqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The Port-Dial/SpeDial Grammar Induction Platform for Spoken Dialogue Systems",
            "Publication year": 2014,
            "Publication url": "https://pure.unic.ac.cy/en/publications/the-port-dialspedial-grammar-induction-platform-for-spoken-dialog",
            "Abstract": "The Port- Dial/SpeDial Grammar Induction Platform for Spoken Dialogue Systems \u2014 UNIC | \nResearch Portal Skip to main navigation Skip to search Skip to main content UNIC | Research \nPortal Logo Home Profiles Research Units Research output Projects Search by expertise, \nname or affiliation The Port- Dial/SpeDial Grammar Induction Platform for Spoken Dialogue \nSystems Manolis Tsangaris, Vassiliki Kouloumenta, Katerina Louka, Spyros Georgiladakis, \nManolis Perakakis, Georgia Athanasopoulou, Ioannis Klasinas, Elias Iosif, Alexandros \nPotamianos Department of Digital Innovation Research output: Chapter in Book/Report/Conference \nproceeding \u203a Conference contribution \u203a peer-review Overview Original language English Title \nof host publication Proceedings of the IEEE International Workshop on Spoken Language \nTechnology (SLT) Publication status Published - 2014 Event IEEE International Workshop on - /\u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:Ri6SYOTghG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fantasy, curiosity and challenge as adaptation indicators in multimodal dialogue systems for preschoolers",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1640377.1640378",
            "Abstract": "In this paper, we investigate how fantasy, curiosity and challenge contribute to the user experience in multimodal dialogue computer games for preschool children. For this purpose, an on-line multimodal platform has been designed, implemented and used as a starting point to develop five task oriented games suitable for preschoolers, with varying levels of fantasy and curiosity elements, as well as, variable difficulty levels. Nine preschool children were asked to play these games in different configurations and choose the application setup that they enjoyed most. Results show that fantasy and curiosity are correlated with children's entertainment, while the level of difficulty seems to depend on each child's individual preferences and capabilities. In addition, a variety of objective metrics (task completion, interaction time, wrong answers), audio features and emotional state have been investigated as potential features \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:zA6iFVUQeVQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the effect of fundamental frequency on amplitude and frequency modulation patterns in speech resonances",
            "Publication year": 2010,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2010/i10_0649.pdf",
            "Abstract": "Amplitude modulation (AM) and frequency modulation (FM) in speech signals are believed to reflect various non-linear phenomena during the speech production process. In this paper, the amplitude and frequency modulation patterns are analyzed for the first three speech resonances in relation to the fundamental frequency (F0). The formant tracks are estimated, and the resonant signals are extracted and demodulated. The Amplitude Modulation Index (AMI) and Frequency Modulation Index (FMI) are computed, and examined in relation to the F0 value, as well as the relation between F0 and the first formant value (F1). Both AMI and FMI are significantly affected by pitch, with modulations being more frequently present in low F0 conditions. Evidence of non-linear interaction between the glottal source and the vocal tract is found in the dependence of the modulation patterns on the ratio of F1 over F0. AMI is amplified \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:KxtntwgDAa4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Detecting emotional state of a child in a conversational computer game",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0885230809000758",
            "Abstract": "The automatic recognition of user\u2019s communicative style within a spoken dialog system framework, including the affective aspects, has received increased attention in the past few years. For dialog systems, it is important to know not only what was said but also how something was communicated, so that the system can engage the user in a richer and more natural interaction. This paper addresses the problem of automatically detecting \u201cfrustration\u201d, \u201cpoliteness\u201d, and \u201cneutral\u201d attitudes from a child\u2019s speech communication cues, elicited in spontaneous dialog interactions with computer characters. Several information sources such as acoustic, lexical, and contextual features, as well as, their combinations are used for this purpose. The study is based on a Wizard-of-Oz dialog corpus of 103 children, 7\u201314 years of age, playing a voice activated computer game. Three-way classification experiments, as well as, pairwise \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:aqlVkmm33-oC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Unsupervised low-rank representations for speech emotion recognition",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2104.07072",
            "Abstract": "We examine the use of linear and non-linear dimensionality reduction algorithms for extracting low-rank feature representations for speech emotion recognition. Two feature sets are used, one based on low-level descriptors and their aggregations (IS10) and one modeling recurrence dynamics of speech (RQA), as well as their fusion. We report speech emotion recognition (SER) results for learned representations on two databases using different classification methods. Classification with low-dimensional representations yields performance improvement in a variety of settings. This indicates that dimensionality reduction is an effective way to combat the curse of dimensionality for SER. Visualization of features in two dimensions provides insight into discriminatory abilities of reduced feature sets.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:q3CdL3IzO_QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deep hierarchical fusion for machine intelligence applications",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US20200335092A1/en",
            "Abstract": "A method for processing multi-modal input includes receiving multiple signal inputs, each signal input having a corresponding input mode. Each signal input is processed in a series of mode-specific processing stages. Each successive mode-specific stage is associated with a successively longer scale of analysis of the signal input. A fused output is generated based on the output of a series of fused processing stages. Each successive fused processing stage is associated with a successively longer scale of analysis of the signal input. Multiple fused processing stages receive inputs from corresponding mode-specific processing stages, so that the fused output depends on the multiple of signal inputs.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:86PQX7AUzd4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A soft-clustering algorithm for automatic induction of semantic classes",
            "Publication year": 2007,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2007/i07_1609.pdf",
            "Abstract": "In this paper, we propose a soft-decision, unsupervised clustering algorithm that generates semantic classes automatically using the probability of class membership for each word, rather than deterministically assigning a word to a semantic class. Semantic classes are induced using an unsupervised, automatic procedure that uses a context-based similarity distance to measure semantic similarity between words. The proposed softdecision algorithm is compared with various \u201chard\u201d clustering algorithms, eg,[1], and it is shown to improve semantic class induction performance in terms of both precision and recall for a travel reservation corpus. It is also shown that additional performance improvement is achieved by combining (auto-induced) semantic with lexical information to derive the semantic similarity distance.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:blknAaTinKkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal processing and interaction: audio, video, text",
            "Publication year": 2008,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=eT4V2TBhE6UC&oi=fnd&pg=PA2&dq=info:dhsFzYDnbsAJ:scholar.google.com&ots=BZ1pS2Lc1f&sig=7Zs-6wdMOub0THCCkhtKsSsXNgY",
            "Abstract": "Multimodal Processing and Interaction: Audio, Video and Text presents high quality, state-of-the-art research ideas and results from theoretic, algorithmic and application viewpoints. This edited volume contains both state-of-the-art reviews and original contributions by leading experts in the scientific and technological field of multimedia. It grew out of a four-year collaboration among research groups participating in the European network of Excellence on Multimedia Understanding, Semantics, Computation and Learning (MUSCLE). Multimodal Processing and Interaction: Audio, Video and Text covers a broad spectrum of novel perspectives, analytic tools, algorithms, design practices and applications in multimedia science and engineering with emphasis on multimodal integration and modality fusion. This volume also contains contributions in the area of interaction with multimedia, especially multimodal interfaces for accessing multimedia content. Multimodal Processing and Interaction: Audio, Video and Text is designed for a professional audience composed of practitioners and researchers in industry and academia. This book is suitable for advanced-level students in computer science and engineering as well.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:epqYDVWIO7EC",
            "Publisher": "Springer Science & Business Media"
        },
        {
            "Title": "Neural Activation Semantic Models: Computational lexical semantic models of localized neural activations",
            "Publication year": 2018,
            "Publication url": "https://www.aclweb.org/anthology/C18-1243.pdf",
            "Abstract": "Neural activation models have been proposed in the literature that use a set of example words for which fMRI measurements are available in order to find a mapping between word semantics and localized neural activations. Successful mappings let us expand to the full lexicon of concrete nouns using the assumption that similarity of meaning implies similar neural activation patterns. In this paper, we propose a computational model that estimates semantic similarity in the neural activation space and investigates the relative performance of this model for various natural language processing tasks. Despite the simplicity of the proposed model and the very small number of example words used to bootstrap it, the neural activation semantic model performs surprisingly well compared to state-of-the-art word embeddings. Specifically, the neural activation semantic model performs better than the state-of-the-art for the task of semantic similarity estimation between very similar or very dissimilar words, while performing well on other tasks such as entailment and word categorization. These are strong indications that neural activation semantic models can not only shed some light into human cognition but also contribute to computation models for certain tasks.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:kz9GbA2Ns4gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tweester at SemEval-2016 Task 4: Sentiment analysis in Twitter using semantic-affective model adaptation",
            "Publication year": 2016,
            "Publication url": "https://www.aclweb.org/anthology/S16-1023.pdf",
            "Abstract": "We describe our submission to SemEval2016 Task 4: Sentiment Analysis in Twitter. The proposed system ranked first for the subtask B. Our system comprises of multiple independent models such as neural networks, semantic-affective models and topic modeling that are combined in a probabilistic way. The novelty of the system is the employment of a topic modeling approach in order to adapt the semantic-affective space for each tweet. In addition, significant enhancements were made in the main system dealing with the data preprocessing and feature extraction including the employment of word embeddings. Each model is used to predict a tweet\u2019s sentiment (positive, negative or neutral) and a late fusion scheme is adopted for the final decision.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ipzZ9siozwsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A comparison of four metrics for auto-inducing semantic classes",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1034626/",
            "Abstract": "A speech understanding system typically includes a natural language understanding module that defines concepts, i.e., groups of semantically related words. It is a challenge to build a set of concepts for a new domain for which prior knowledge and training data are limited. In our work, concepts are induced automatically from unannotated training data by grouping semantically similar words and phrases together into concept classes. Four context-dependent similarity metrics are proposed and their performance for auto-inducing concepts is evaluated. Two of these metrics are based on the Kullback-Leibler (KL) distance measure, a third is the Manhattan norm, and the fourth is the vector product (VP) similarity measure. The KL and VP metrics consistently underperform the other metrics on the four tasks investigated: movie information, a children's game, travel reservations, and Wall Street Journal news articles \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An affective evaluation tool using brain signals",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2451176.2451222",
            "Abstract": "We propose a new interface evaluation tool that incorporates affective metrics which are provided from the ElectroEncephaloGraphy (EEG) signals of the Emotiv EPOC neuro-headset device. The evaluation tool captures and analyzes information in real time from a multitude of sources such as EEG, affective metrics such as frustration, engagement and excitement and facial expression. The proposed tool has been used to gain detailed affective information of users interacting with a mobile multimodal (touch and speech) iPhone application, for which we investigated the effect of speech recognition errors and modality usage patterns.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:VOx2b1Wkg3QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Classification of cognitive load from speech using an i-vector framework.",
            "Publication year": 2014,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2014/i14_0751.pdf",
            "Abstract": "The goal in this work is to automatically classify speakers\u2019 level of cognitive load (low, medium, high) from a standard battery of reading tasks requiring varying levels of working memory. This is a challenging machine learning problem because of the inherent difficulty in defining/measuring cognitive load and due to intra-/inter-speaker differences in how their effects are manifested in behavioral cues. We experimented with a number of static and dynamic features extracted directly from the audio signal (prosodic, spectral, voice quality) and from automatic speech recognition hypotheses (lexical information, speaking rate). Our approach to classification addressed the wide variability and heterogeneity through speaker normalization and by adopting an i-vector framework that affords a systematic way to factorize the multiple sources of variability.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:uLbwQdceFCQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A saliency-based approach to audio event detection and summarization",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6334317/",
            "Abstract": "In this paper, we approach the problem of audio summarization by saliency computation of audio streams, exploring the potential of a modulation model for the detection of perceptually important audio events based on saliency models, along with various fusion schemes for their combination. The fusion schemes include linear, adaptive and nonlinear methods. A machine learning approach, where training of the features is performed, was also applied for the purpose of comparison with the proposed technique. For the evaluation of the algorithm we use audio data taken from movies and we show that nonlinear fusion schemes perform best. The results are reported on the MovSum database, using objective evaluations (against ground-truth denoting the perceptually important audio events). Analysis of the selected audio segments is also performed against a labeled database in respect to audio categories, while a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:tS2w5q8j5-wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Novel features for robust speech recognition",
            "Publication year": 2002,
            "Publication url": "https://asa.scitation.org/doi/abs/10.1121/1.4779131",
            "Abstract": "Recently there has been much research in the area of robust front\u2010ends and new features for automatic speech recongition (ASR). These efforts have had limited success for certain databases and recording conditions. In this work, we review some recent work on features for ASR: the articulatory front\u2010end of Li et al. (2000), nonlinear modulation features [Quatieri (2002); Dimitriadis and Maragos (2002)], chaotic features [Pitsikalis and Maragos (2002)], short\u2010time spectral moments [Paliwal et al. (2000)], etc. We extend the work of Potamianos and Maragos (2001) to show how some of these features relate to the standard front\u2010end of short\u2010time smooth spectral envelope. We also analyze some of these new features using classification and regression trees to show the relevance of the features for phone\u2010classification task.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:u9iWguZQMMsC",
            "Publisher": "Acoustical Society of America"
        },
        {
            "Title": "DARPA communicator evaluation: progress from 2000 to 2001.",
            "Publication year": 2002,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.8259&rep=rep1&type=pdf",
            "Abstract": "This paper describes the evaluation methodology and results of the DARPA Communicator spoken dialog system evaluation experiments in 2000 and 2001. Nine spoken dialog systems in the travel planning domain participated in the experiments resulting in a total corpus of 1904 dialogs. We describe and compare the experimental design of the 2000 and 2001 DARPA evaluations. We describe how we established a performance baseline in 2001 for complex tasks. We present our overall approach to data collection, the metrics collected, and the application of PARADISE to these data sets. We compare the results we achieved in 2000 for a number of core metrics with those for 2001. These results demonstrate large performance improvements from 2000 to 2001 and show that the Communicator program goal of conversational interaction for complex tasks has been achieved.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:5ugPr518TE4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Blind speech separation using parafac analysis and integer least squares",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1661215/",
            "Abstract": "We propose a new two-step frequency domain algorithm for blind speech separation (BSS) for unknown channel order. This new approach employs parallel factor analysis (PARAFAC) to separate the speech signals and a novel integer-least-squares-based method for matching the arbitrary permutations in the frequency domain. The proposed algorithm offers guaranteed convergence and good separation performance, measured both quantitatively and in subjective tests. Performance gains in signal-to-interference ratio of up to 10 db are achieved for certain source-sensor geometries",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Babyrobot-next generation social robots: Enhancing communication and collaboration development of TD and ASD children by developing and commercially exploiting the next generation of human-robot interaction technologies",
            "Publication year": 2016,
            "Publication url": "https://pure.unic.ac.cy/en/publications/babyrobot-next-generation-social-robots-enhancing-communication-a",
            "Abstract": "BabyRobot - Next Generation Social Robots: Enhancing Communication and Collaboration \nDevelopment of TD and ASD Children by Developing and Commercially Exploiting the Next \nGeneration of Human-Robot Interaction Technologies \u2014 UNIC | Research Portal Skip to main \nnavigation Skip to search Skip to main content UNIC | Research Portal Logo Home Profiles \nResearch Units Research output Projects Search by expertise, name or affiliation BabyRobot \n- Next Generation Social Robots: Enhancing Communication and Collaboration Development \nof TD and ASD Children by Developing and Commercially Exploiting the Next Generation of \nHuman-Robot Interaction Technologies Alexandros Potamianos, Costas Tzafestas, Elias Iosif, \nFranziska Kirstein, Petros Maragos, Kerstin Dauthenhahn, Joakim Gustafson, John Erland \nOstergaard, Stefan Kopp, Preben Wik, Oliver Pietquin, Samer Al Moubayed Department : -\u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:kh2fBNsKQNwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Valence, arousal and dominance estimation for english, german, greek, portuguese and spanish lexica using semantic models",
            "Publication year": 2015,
            "Publication url": "https://researchrepository.murdoch.edu.au/id/eprint/31005/",
            "Abstract": "We propose and evaluate the use of an affective-semantic model to expand the affective lexica of German, Greek, English, Spanish and Portuguese. Motivated by the assumption that semantic similarity implies affective similarity, we use word level semantic similarity scores as semantic features to estimate their corresponding affective scores. Various context-based semantic similarity metrics are investigated using contextual features that include both words and character n-grams. The model produces continuous affective ratings in three dimensions (valence, arousal and dominance) for all five languages, achieving consistent performance. We achieve classification accuracy (valence polarity task) between 85% and 91% for all five languages. For morphologically rich languages the proposed use of character n-grams is shown to improve performance.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:XD-gHx7UXLsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6527322/",
            "Abstract": "Multimodal streams of sensory information are naturally parsed and integrated by humans using signal-level feature extraction and higher level cognitive processes. Detection of attention-invoking audiovisual segments is formulated in this work on the basis of saliency models for the audio, visual, and textual information conveyed in a video stream. Aural or auditory saliency is assessed by cues that quantify multifrequency waveform modulations, extracted through nonlinear operators and energy tracking. Visual saliency is measured through a spatiotemporal attention model driven by intensity, color, and orientation. Textual or linguistic saliency is extracted from part-of-speech tagging on the subtitles information available with most movie distributions. The individual saliency streams, obtained from modality-depended cues, are integrated in a multimodal saliency curve, modeling the time-varying perceptual \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:5Ul4iDaHHb8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Auditory Teager energy cepstrum coefficients for robust speech recognition.",
            "Publication year": 2005,
            "Publication url": "http://cvsp.cs.ece.ntua.gr/publications/confr/DimitriadisMaragosPotamianos_AuditTeagEnergCepstrumRobustSpeechRecogn_Interspeech2005.pdf",
            "Abstract": "In this paper, a feature extraction algorithm for robust speech recognition is introduced. The feature extraction algorithm is motivated by the human auditory processing and the nonlinear Teager-Kaiser energy operator that estimates the true energy of the source of a resonance. The proposed features are labeled as Teager Energy Cepstrum Coefficients (TECCs). TECCs are computed by first filtering the speech signal through a dense non constant-Q Gammatone filterbank and then by estimating the \u201ctrue\u201d energy of the signal\u2019s source, ie, the short-time average of the output of the Teager-Kaiser energy operator. Error analysis and speech recognition experiments show that the TECCs and the mel frequency cepstrum coefficients (MFCCs) perform similarly for clean recording conditions; while the TECCs perform significantly better than the MFCCs for noisy recognition tasks. Specifically, relative word error rate improvement of 60% over the MFCC baseline is shown for the Aurora-3 database for the high-mismatch condition. Absolute error rate improvement ranging from 5% to 20% is shown for a phone recognition task in (various types of additive) noise.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:roLk4NBRz8UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Low-dimensional manifold distributional semantic models",
            "Publication year": 2014,
            "Publication url": "https://www.aclweb.org/anthology/C14-1069.pdf",
            "Abstract": "Motivated by evidence in psycholinguistics and cognition, we propose a hierarchical distributed semantic model (DSM) that consists of low-dimensional manifolds built on semantic neighborhoods. Each semantic neighborhood is sparsely encoded and mapped into a low-dimensional space. Global operations are decomposed into local operations in multiple sub-spaces; results from these local operations are fused to come up with semantic relatedness estimates. Manifold DSM are constructed starting from a pairwise word-level semantic similarity matrix. The proposed model is evaluated on semantic similarity estimation task significantly improving on the state-of-the-art.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:JQOojiI6XY0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deep actionable behavioral profiling and shaping",
            "Publication year": 2019,
            "Publication url": "https://patents.google.com/patent/US20190385597A1/en",
            "Abstract": "Behavioral profiling and shaping is used in a \u201cclosed-loop\u201d in that an interaction with at least one human is monitored and based on inferred characteristics of the interaction with that human (eg, their behavioral profile) the interaction is guided. In one exemplary embodiment, the interaction is between two humans, for example, a \u201ccustomer\u201d and an \u201cagent\u201d and the interaction is monitored and the agent is guided according to the inferred behavioral profile of the customer (or optionally of the agent themselves).",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:0N-VGjzr574C",
            "Publisher": "Unknown"
        },
        {
            "Title": "An embarrassingly simple approach for transfer learning from pretrained language models",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1902.10547",
            "Abstract": "A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:S16KYo8Pm5AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Semantic similarity computation for abstract and concrete nouns using network-based distributional semantic models",
            "Publication year": 2013,
            "Publication url": "https://repository.ubn.ru.nl/bitstream/handle/2066/119587/119587.pdf",
            "Abstract": "Motivated by cognitive lexical models, network-based distributional semantic models (DSMs) were proposed in [Iosif and Potamianos (2013)] and were shown to achieve state-of-the-art performance on semantic similarity tasks. Based on evidence for cognitive organization of concepts based on degree of concreteness, we investigate the performance and organization of network DSMs for abstract vs. concrete nouns. Results show a \u201cconcreteness effect\u201d for semantic similarity estimation. Network DSMs that implement the maximum sense similarity assumption perform best for concrete nouns, while attributional network DSMs perform best for abstract nouns. The performance of metrics is evaluated against human similarity ratings on an English and a Greek corpus.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:fQNAKQ3IYiAC",
            "Publisher": "Potsdam, Germany:[Sn]"
        },
        {
            "Title": "Statistical analysis of amplitude modulation in speech signals using an AM-FM model",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4960500/",
            "Abstract": "Several studies have been dedicated to the analysis and modeling of AM-FM modulations in speech and different algorithms have been proposed for the exploitation of modulations in speech applications. This paper details a statistical analysis of amplitude modulations using a multiband AM-FM analysis framework. The aim of this study is to analyze the phonetic- and speaker-dependency of modulations in the amplitude envelope of speech resonances. The analysis focuses on the dependence of such modulations on acoustic features such as, fundamental frequency, formant proximity, phone identity, as well as, speaker identity and contextual features. The results show that the amplitude modulation index of a speech resonance is mainly a function of the speaker-s average fundamental frequency, the phone identity, and the proximity between neighboring formant resonances. The results are especially relevant \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:bEWYMUwI8FkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Unsupervised semantic similarity computation between terms using web documents",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5291692/",
            "Abstract": "In this work, Web-based metrics that compute the semantic similarity between words or terms are presented and compared with the state of the art. Starting from the fundamental assumption that similarity of context implies similarity of meaning, relevant Web documents are downloaded via a Web search engine and the contextual information of words of interest is compared (context-based similarity metrics). The proposed algorithms work automatically, do not require any human-annotated knowledge resources, e.g., ontologies, and can be generalized and applied to different languages. Context-based metrics are evaluated both on the Charles-Miller data set and on a medical term data set. It is shown that context-based similarity metrics significantly outperform co-occurrence-based metrics, in terms of correlation with human judgment, for both tasks. In addition, the proposed unsupervised context-based similarity \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:YOwf2qJgpHMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Hierarchical bi-directional attention-based RNNs for supporting document classification on protein\u2013protein interactions affected by genetic mutations",
            "Publication year": 2018,
            "Publication url": "https://academic.oup.com/database/article-abstract/doi/10.1093/database/bay076/5077305",
            "Abstract": "In this paper, we describe a hierarchical bi-directional attention-based Re-current Neural Network (RNN) as a reusable sequence encoder architecture, which is used as sentence and document encoder for document classification. The sequence encoder is composed of two bi-directional RNN equipped with an attention mechanism that identifies and captures the most important elements, words or sentences, in a document followed by a dense layer for the classification task. Our approach utilizes the hierarchical nature of documents which are composed of sequences of sentences and sentences are composed of sequences of words. In our model, we use word embeddings to project the words to a low-dimensional vector space. We leverage word embeddings trained on PubMed for initializing the embedding layer of our network. We apply this model to biomedical literature specifically, on paper abstracts \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:M7yex6snE4oC",
            "Publisher": "Oxford Academic"
        },
        {
            "Title": "Kernel models for affective lexicon creation",
            "Publication year": 2011,
            "Publication url": "https://raw.githubusercontent.com/JDwangmo/sentimentAnalysis/master/references/Kernel-models-for-affective-lexicon-creation.pdf",
            "Abstract": "Emotion recognition algorithms for spoken dialogue applications typically employ lexical models that are trained on labeled in-domain data. In this paper, we propose a domainindependent approach to affective text modeling that is based on the creation of an affective lexicon. Starting from a small set of manually annotated seed words, continuous valence ratings for new words are estimated using semantic similarity scores and a kernel model. The parameters of the model are trained using least mean squares estimation. Word level scores are combined to produce sentence-level scores via simple linear and non-linear fusion. The proposed method is evaluated on the SemEval news headline polarity task and on the ChIMP politeness and frustration detection dialogue task, achieving state-of-theart results on both. For politeness detection, best results are obtained when the affective model is adapted using in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:2P1L_qKh6hAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Advanced front-end for robust speech recognition in extremely adverse environments.",
            "Publication year": 2007,
            "Publication url": "https://www.academia.edu/download/54313575/07_intespeech_HAFE.pdf",
            "Abstract": "In this paper, a unified approach to speech enhancement, feature extraction and feature normalization for speech recognition in adverse recording conditions is presented. The proposed frontend system consists of several different, independent, processing modules. Each of the algorithms contained in these modules has been independently applied to the problem of speech recognition in noise, significantly improving the recognition rates. In this work, these algorithms are merged in a single front-end and their combined performance is demonstrated. Specifically, the proposed advanced front-end extracts noise-invariant features via the following modules: Wiener filtering, voice-activity detection, robust feature extraction (nonlinear modulation or fractal features), parameter equalization and frame-dropping. The advanced front-end is applied to extremely adverse environments where most feature extraction schemes fail. We show that by combining speech enhancement, robust feature extraction and feature normalization up to a fivefold error rate reduction can be achieved for certain tasks.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:R3hNpaxXUhUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal system evaluation using modality efficiency and synergy metrics",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1452392.1452397",
            "Abstract": "In this paper, we propose two new objective metrics, relative modality efficiency and multimodal synergy, that can provide valuable information and identify usability problems during the evaluation of multimodal systems. Relative modality efficiency (when compared with modality usage) can identify suboptimal use of modalities due to poor interface design or information asymmetries. Multimodal synergy measures the added value from efficiently combining multiple input modalities, and can be used as a single measure of the quality of modality fusion and fission in a multimodal system. The proposed metrics are used to evaluate two multimodal systems that combine pen/speech and mouse/keyboard modalities respectively. The results provide much insight into multimodal interface usability issues, and demonstrate how multimodal systems should adapt to maximize modalities synergy resulting in efficient, natural \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Speech recognition for wireless applications",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/936802/",
            "Abstract": "Future wireless multimedia terminals will have a variety of applications that require speech recognition capabilities. We consider a robust distributed speech recognition system where representative parameters of the speech signal are extracted at the wireless terminal and transmitted to a centralized automatic speech recognition (ASR) server. We propose several unequal error protection schemes for the ASR bit stream and demonstrate the satisfactory performance of these schemes for typical wireless cellular channels. In addition, a \"soft-feature\" error concealment strategy is introduced at the ASR server that uses \"soft-outputs\" from the channel decoder. This soft-feature error concealment techniques reduces the ASR error rate by up to four times for certain channels. Also considered is a channel decoding technique with source information that improves ASR performance.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ns9cj8rnVeAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Adaptive language models for spoken dialogue systems",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5743648/",
            "Abstract": "In this paper, we investigate both generative and statistical approaches for language modeling in spoken dialogue systems. Semantic class-based finite state and n-gram grammars are used for improving coverage and modeling accuracy when little training data is available. We have implemented dialogue-state specific language model adaptation to reduce perplexity and improve the efficiency of grammars for spoken dialogue systems. A novel algorithm for combining state-independent n-gram and state-dependent finite state grammars using acoustic confidence scores is proposed. Using this combination strategy, a relative word error reduction of 12% is achieved for certain dialogue states within a travel reservation task. Finally, semantic class multigrams are proposed and briefly evaluated for language modeling in dialogue systems.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:MXK_kJrjxJIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Developmental acoustic study of American English diphthongs",
            "Publication year": 2014,
            "Publication url": "https://asa.scitation.org/doi/abs/10.1121/1.4894799",
            "Abstract": "Developmental trends of durational and spectral parameters of five American English diphthongs are investigated by age and gender. Specifically, diphthong durations, the fundamental frequency (F0), and the first three formant (F1, F2, F3) trajectories as well as formant transition rates are analyzed as a function of age, gender and diphthong type. In addition, the distance between diphthong onset and offset positions and those of nearby monophthongs in the formant space is computed and age-dependent trends are presented. Furthermore, a spectral transition mid-point is estimated for a given diphthong trajectory and normalized time durations from onsets to mid-points are analyzed as a function of age and diphthong type. Finally, diphthong classification results using formant-related parameters are reported. Results show the expected age-dependent reductions of diphthong duration, fundamental frequency \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:NJ774b8OgUMC",
            "Publisher": "Acoustical Society of America"
        },
        {
            "Title": "Affective evaluation of a mobile multimodal dialogue system using brain signals",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6424195/",
            "Abstract": "We propose the use of affective metrics such as excitement, frustration and engagement for the evaluation of multimodal dialogue systems. The affective metrics are elicited from the ElectroEncephaloGraphy (EEG) signals using the Emotiv EPOC neuroheadset device. The affective metrics are used in conjunction with traditional evaluation metrics (turn duration, input modality) to investigate the effect of speech recognition errors and modality usage patterns in a multimodal (touch and speech) dialogue form-filling application for the iPhone mobile device. Results show that: (1) engagement is higher for touch input, while excitement and frustration is higher for speech input, and (2) speech recognition errors and associated repairs correspond to specific dynamic patters of excitement and frustration. Use of such physiological channels and their elaborated interpretation is a challenging but also a potentially rewarding \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:8AbLer7MMksC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Affective lexicon creation for the Greek language",
            "Publication year": 2016,
            "Publication url": "https://researchrepository.murdoch.edu.au/id/eprint/31002/",
            "Abstract": "Starting from the English affective lexicon ANEW (Bradley and Lang, 1999a) we have created the first Greek affective lexicon. It contains human ratings for the three continuous affective dimensions of valence, arousal and dominance for 1034 words. The Greek affective lexicon is compared with affective lexica in English, Spanish and Portuguese. The lexicon is automatically expanded by selecting a small number of manually annotated words to bootstrap the process of estimating affective ratings of unknown words. We experimented with the parameters of the semantic-affective model in order to investigate their impact to its performance, which reaches 85% binary classification accuracy (positive vs. negative ratings). We share the Greek affective lexicon that consists of 1034 words and the automatically expanded Greek affective lexicon that contains 407K words.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:uc_IGeMz5qoC",
            "Publisher": "European Language Resources Association (ELRA)"
        },
        {
            "Title": "A semantic-affective compositional approach for the affective labelling of adjective-noun and noun-noun pairs",
            "Publication year": 2016,
            "Publication url": "https://www.aclweb.org/anthology/W16-0424.pdf",
            "Abstract": "Motivated by recent advances in the area of Compositional Distributional Semantic Models (CDSMs), we propose a compositional approach for estimating continuous affective ratings for adjective-noun (AN) and noun-noun (NN) pairs. The ratings are computed for the three basic dimensions of continuous affective spaces, namely, valence, arousal and dominance. We propose that similarly to the semantic modification that underlies CDSMs, affective modification may occur within the framework of affective spaces, especially when the constituent words of the linguistic structures under investigation form modifier-head pairs (eg, AN and NN). The affective content of the entire structure is determined from the interaction between the respective constituents, ie, the affect conveyed by the head is altered by the modifier. In addition, we investigate the fusion of the proposed model with the semantic-affective model proposed in (Malandrakis et al., 2013) applied both at word-and phrase-level. The automatically computed affective ratings were evaluated against human ratings in terms of correlation. The most accurate estimates are achieved via fusion and absolute performance improvement up to 5% and 4% is reported for NN and AN, respectively.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:zLWjf1WUPmwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Distributional semantic models for affective text analysis",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6578101/",
            "Abstract": "We present an affective text analysis model that can directly estimate and combine affective ratings of multi-word terms, with application to the problem of sentence polarity/semantic orientation detection. Starting from a hierarchical compositional method for generating sentence ratings, we expand the model by adding multi-word terms that can capture non-compositional semantics. The method operates similarly to a bigram language model, using bigram terms or backing off to unigrams based on a (degree of) compositionality criterion. The affective ratings for n-gram terms of different orders are estimated via a corpus-based method using distributional semantic similarity metrics between unseen words and a set of seed words. N-gram ratings are then combined into sentence ratings via simple algebraic formulas. The proposed framework produces state-of-the-art results for word-level tasks in English and German \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:Mojj43d5GZwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "DARPA communicator dialog travel planning systems: the june 2000 data collection.",
            "Publication year": 2001,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.2951&rep=rep1&type=pdf",
            "Abstract": "This paper describes results of an experiment with 9 different DARPA Communicator Systems who participated in the June 2000 data collection. All systems supported travel planning and utilized some form of mixed-initiative interaction. However they varied in several critical dimensions:(1) They targeted different back-end databases for travel information;(2) The used different modules for ASR, NLU, TTS and dialog management. We describe the experimental design, the approach to data collection, the metrics collected, and results comparing the systems.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:9yKSN-GCB0IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Stream weight computation for multi-stream classifiers",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1660030/",
            "Abstract": "In this paper, we provide theoretical results on the problem of optimal stream weight selection for the multi-stream classification problem. It is shown, that in the presence of estimation or modeling errors using stream weights can decrease the total classification error. The stream weights that minimize classification estimation error are shown to be inversely proportional to the single-stream pdf estimation error. It is also shown that under certain conditions, the optimal stream weights are inversely proportional to the single-stream classification error. We apply these results to the problem of audio-visual speech recognition and experimentally verify our claims. The applicability of the results to the problem of unsupervised stream weight estimation is also discussed",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:_kc_bZDykSQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "UDALM: Unsupervised Domain Adaptation through Language Modeling",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2104.07078",
            "Abstract": "In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our method is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding  accuracy, which is an  absolute improvement over the state-of-the-art.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:4fGpz3EwCPoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio salient event detection and summarization using audio and text modalities",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7362797/",
            "Abstract": "This paper investigates the problem of audio event detection and summarization, building on previous work [1,2] on the detection of perceptually important audio events based on saliency models. We take a synergistic approach to audio summarization where saliency computation of audio streams is assisted by using the text modality as well. Auditory saliency is assessed by auditory and perceptual cues such as Teager energy, loudness and roughness; all known to correlate with attention and human hearing. Text analysis incorporates part-of-speech tagging and affective modeling. A computational method for the automatic correction of the boundaries of the selected audio events is applied creating summaries that consist not only of salient but also meaningful and semantically coherent events. A non-parametric classification technique is employed and results are reported on the MovSum movie database using \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:fEOibwPWpKIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Detecting Politeness and frustration state of a child in a conversational computer game.",
            "Publication year": 2005,
            "Publication url": "https://sail.usc.edu/publications/files/YildirimInterSpeech2005.pdf",
            "Abstract": "In this study, we investigate politeness and frustration behavior of children during their spoken interaction with computer characters in a game. We focus on automatically detecting frustrated, polite and neutral attitudes from the child\u2019s speech (acoustic and language) communication cues and study their differences as a function of age and gender. The study is based on a Wizard-of-Oz dialog corpus of 103 children playing a voice activated computer game. Statistical analysis revealed that there was a significant gender effect on politeness with girls in this data exhibiting more explicit politeness markers. The analysis also showed that there is a positive correlation between frustration and the number of dialog turns reflecting the fact that longer time spent solving the puzzle of the game led to a more frustrated child. By combining acoustic and language cues for the task of automatic detection of politeness and frustration, we obtain average accuracy of 84.7% and 71.3%, respectively, by using age dependent models and 85% and 72%, respectively, for gender dependent models.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:Zph67rFs4hoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fusion of knowledge-based and data-driven approaches to grammar induction",
            "Publication year": 2014,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2014/i14_0288.pdf",
            "Abstract": "Using different sources of information for grammar induction results in grammars that vary in coverage and precision. Fusing such grammars with a strategy that exploits their strengths while minimizing their weaknesses is expected to produce grammars with superior performance. We focus on the fusion of grammars produced using a knowledge-based approach using lexicalized ontologies and a data-driven approach using semantic similarity clustering. We propose various algorithms for finding the mapping between the (non-terminal) rules generated by each grammar induction algorithm, followed by rule fusion. Three fusion approaches are investigated: early, mid and late fusion. Results show that late fusion provides the best relative F-measure performance improvement by 20%.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:hkOj_22Ku90C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Information seeking spoken dialogue systems\u2014part II: Multimodal dialogue",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4130374/",
            "Abstract": "For pt.1see ibid., vol. 9, p. 3 (2007). In this paper, the task and user interface modules of a multimodal dialogue system development platform are presented. The main goal of this work is to provide a simple, application-independent solution to the problem of multimodal dialogue design for information seeking applications. The proposed system architecture clearly separates the task and interface components of the system. A task manager is designed and implemented that consists of two main submodules: the electronic form module that handles the list of attributes that have to be instantiated by the user, and the agenda module that contains the sequence of user and system tasks. Both the electronic forms and the agenda can be dynamically updated by the user. Next a spoken dialogue module is designed that implements the speech interface for the task manager. The dialogue manager can handle complex error \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:ZeXyd9-uunAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Batch and adaptive PARAFAC-based blind separation of convolutive speech mixtures",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5233821/",
            "Abstract": "We present a frequency-domain technique based on PARAllel FACtor (PARAFAC) analysis that performs multichannel blind source separation (BSS) of convolutive speech mixtures. PARAFAC algorithms are combined with a dimensionality reduction step to significantly reduce computational complexity. The identifiability potential of PARAFAC is exploited to derive a BSS algorithm for the under-determined case (more speakers than microphones), combining PARAFAC analysis with time-varying Capon beamforming. Finally, a low-complexity adaptive version of the BSS algorithm is proposed that can track changes in the mixing environment. Extensive experiments with realistic and measured data corroborate our claims, including the under-determined case. Signal-to-interference ratio improvements of up to 6 dB are shown compared to state-of-the-art BSS algorithms, at an order of magnitude lower computational \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:JV2RwH3_ST0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Mixture of topic-based distributional semantic and affective models",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8334459/",
            "Abstract": "Typically, Distributional Semantic Models (DSMs) estimate semantic similarity between words using a single-model, where the multiple senses of polysemous words are conflated in a single representation. Similarly, in textual affective analysis tasks, ambiguous words are usually not treated differently when estimating word affective scores. In this work, a semantic mixture model is proposed enabling the combination of word similarity scores estimated across multiple topic-specific DSMs (TDSMs). Based on the assumption that semantic similarity implies affective similarity, we extend this model to perform sentence-level affect estimation. The proposed model outperforms the baseline approach achieving state-of-the-art results for semantic similarity estimation and sentence-level polarity detection.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:kuK5TVdYjLIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Emotiword: Affective lexicon creation with application to interaction and multimedia data",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-32436-9_3",
            "Abstract": "We present a fully automated algorithm for expanding an affective lexicon with new entries. Continuous valence ratings are estimated for unseen words using the underlying assumption that semantic similarity implies affective similarity. Starting from a set of manually annotated words, a linear affective model is trained using the least mean squares algorithm followed by feature selection. The proposed algorithm performs very well on reproducing the valence ratings of the Affective Norms for English Words (ANEW) and General Inquirer datasets. We then propose three simple linear and non-linear fusion schemes for investigating how lexical valence scores can be combined to produce sentence-level scores. These methods are tested on a sentence rating task of the SemEval 2007 corpus, on the ChIMP politeness and frustration detection dialogue task and on a movie subtitle polarity detection task.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:_xSYboBqXhAC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Using Oliver API for emotion-aware movie content characterization",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8877398/",
            "Abstract": "This paper demonstrates the utilization of Oliver  11 https://behavioralsignals.com/oliver/, the speech emotion recognition (SER) API created by Behavioral Signals, in the context of a movie content visualization application. Oliver API provides an emotion recognition as-a-service solution that can be accessed via a Web API. In this work, we demonstrate how one can send sound recordings from famous movies, retrieve respective emotional descriptors and use simple aggregations on these descriptors to visualize movie content. We have compiled a dataset of 60 movies, categorized over 8 directors. The classification examples included in this paper indicate the ability of simple emotion aggregations to discriminate between movie directors. In order for others to also experiment with the output of both the API's Emotional and Automatic Speech Recognition, the responses are provided as JSON files in this link: https \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:edDO8Oi4QzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Instantaneous frequency and bandwidth estimation using filterbank arrays",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6639229/",
            "Abstract": "Accurate estimation of the instantaneous frequency of speech resonances is a hard problem mainly due to phase discontinuities in the speech signal associated with excitation instants. We review a variety of approaches for enhanced frequency and bandwidth estimation in the time-domain and propose a new cognitively motivated approach using filterbank arrays. We show that by filtering speech resonances using filters of different center frequency, bandwidth and shape, the ambiguity in instantaneous frequency estimation associated with amplitude envelope minima and phase discontinuities can be significantly reduced. The novel estimators are shown to perform well on synthetic speech signals with frequency and bandwidth micro-modulations (i.e., modulations within a pitch period), as well as on real speech signals. Filterbank arrays, when applied to frequency and bandwidth modulation index estimation, are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:BrmTIyaxlBUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Data Augmentation Using GANs for Speech Emotion Recognition.",
            "Publication year": 2019,
            "Publication url": "https://slp-ntua.github.io/potam/preprints/conf/2019_INTERSPEECH_data_augmentation.pdf",
            "Abstract": "In this work, we address the problem of data imbalance for the task of Speech Emotion Recognition (SER). We investigate conditioned data augmentation using Generative Adversarial Networks (GANs), in order to generate samples for underrepresented emotions. We adapt and improve a conditional GAN architecture to generate synthetic spectrograms for the minority class. For comparison purposes, we implement a series of signal-based data augmentation methods. The proposed GAN-based approach is evaluated on two datasets, namely IEMOCAP and FEEL-25k, a large multi-domain dataset. Results demonstrate a 10% relative performance improvement in IEMOCAP and 5% in FEEL-25k, when augmenting the minority classes.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ruyezt5ZtCIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Soft feature decoding in a distributed automatic speech recognition system for use over wireless channels",
            "Publication year": 2004,
            "Publication url": "https://patents.google.com/patent/US6760699B1/en",
            "Abstract": "A method and apparatus for performing automatic speech recognition (ASR) in a distributed ASR system for use over a wireless channel takes advantage of probabilistic information concerning the likelihood that a given, portion of the data has been accurately decoded to a particular value. The probability of error in each feature in a transmitted feature set is employed to improve speech recognition performance under adverse channel conditions. Bit error probabilities for each of the bits which are used to encode a given ASR feature are used to compute the confidence level that the system may have in the decoded value of that feature. Features that have been corrupted with high probability are advantageously either not used or are weighted less in the acoustic distance computation performed by the speech recognizer. This novel approach to decoding of ASR features is referred to herein as \u201csoft feature decoding \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "SemSim: Resources for Normalized Semantic Similarity Computation Using Lexical Networks.",
            "Publication year": 2012,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.672.9859&rep=rep1&type=pdf",
            "Abstract": "We investigate the creation of corpora from web-harvested data following a scalable approach that has linear query complexity. Individual web queries are posed for a lexicon that includes thousands of nouns and the retrieved data are aggregated. A lexical network is constructed, in which the lexicon nouns are linked according to their context-based similarity. We introduce the notion of semantic neighborhoods, which are exploited for the computation of semantic similarity. Two types of normalization are proposed and evaluated on the semantic tasks of:(i) similarity judgement, and (ii) noun categorization and taxonomy creation. The created corpus along with a set of tools and noun similarities are made publicly available.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:cFHS6HbyZ2cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cognitively motivated distributional representations of meaning",
            "Publication year": 2016,
            "Publication url": "https://www.aclweb.org/anthology/L16-1195.pdf",
            "Abstract": "Although meaning is at the core of human cognition, state-of-the-art distributional semantic models (DSMs) are often agnostic to the findings in the area of semantic cognition. In this work, we present a novel type of DSMs motivated by the dual-processing cognitive perspective that is triggered by lexico-semantic activations in the short-term human memory. The proposed model is shown to perform better than state-of-the-art models for computing semantic similarity between words. The fusion of different types of DSMs is also investigated achieving results that are comparable or better than the state-of-the-art. The used corpora along with a set of tools, as well as large repositories of vectorial word representations are made publicly available for four languages (English, German, Italian, and Greek).",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:vDijr-p_gm4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Crossmodal network-based distributional semantic models",
            "Publication year": 2016,
            "Publication url": "https://www.aclweb.org/anthology/L16-1627.pdf",
            "Abstract": "Despite the recent success of distributional semantic models (DSMs) in various semantic tasks they remain disconnected with real-world perceptual cues since they typically rely on linguistic features. Text data constitute the dominant source of features for the majority of such models, although there is evidence from cognitive science that cues from other modalities contribute to the acquisition and representation of semantic knowledge. In this work, we propose the crossmodal extension of a two-tier text-based model, where semantic representations are encoded in the first layer, while the second layer is used for computing similarity between words. We exploit text-and image-derived features for performing computations at each layer, as well as various approaches for their crossmodal fusion. It is shown that the crossmodal model performs better (from 0.68 to 0.71 correlation coefficient) than the unimodal one for the task of similarity computation between words.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:EkHepimYqZsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Time-frequency distributions for automatic speech recognition",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/905994/",
            "Abstract": "The use of general time-frequency distributions as features for automatic speech recognition (ASR) is discussed in the context of hidden Markov classifiers. Short-time averages of quadratic operators, e.g., energy spectrum, generalized first spectral moments, and short-time averages of the instantaneous frequency, are compared to the standard front end features, and applied to ASR. Theoretical and experimental results indicate a close relationship among these feature sets.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:W7OEmFMy1HYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audiovisual attention modeling and salient event detection",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/978-0-387-76316-3_8.pdf",
            "Abstract": "In analyzing the visual and aural information of video streams the main issues that arise are: i) choosing appropriate features that capture important signal properties, ii) combining the information corresponding to the different modalities to allow for interaction and iii) defining efficient salient event detection schemes. In this chapter, the potential of using and integrating aural and visual features is explored, to create a model of audiovisual attention, with application to saliency-based summarization and automatic annotation of videos. The two modalities are processed independently with the saliency of each described by features that correspond to physical changes in the depicted scene. Their integration is performed by constructing temporal indexes of saliency that reveal dynamically evolving audiovisual events. Multimodal video analysis (ie, analysis of various information modalities) has gained in popularity with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "An investigation of vocal arousal dynamics in child-psychologist interactions using synchrony measures and a conversation-based model",
            "Publication year": 2014,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2014/i14_0218.pdf",
            "Abstract": "Researchers from various disciplines are concerned with the study of affective phenomena, especially arousal. Expressed affective modulations, which reflect both an individual\u2019s internal state and external factors, are central to the communicative process. Bone et al. developed a robust, unsupervised (rule-based) method which provides a scale-continuous, bounded arousal rating from the vocal signal. In this study, we investigate the joint-dynamics of child and psychologist vocal arousal in autism spectrum disorder (ASD) diagnostic interactions. Arousal synchrony is assessed with multiple methods. Results indicate that children with higher ASD severity tend to lead the arousal dynamics more, seemingly because the children aren\u2019t as responsive to the psychologist\u2019s affective modulations. A vocal arousal model is also proposed which incorporates social and conversational constructs. The model captures \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:Fu2w8maKXqMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multi-band long-term signal variability features for robust voice activity detection.",
            "Publication year": 2013,
            "Publication url": "https://www.academia.edu/download/49092246/Multi-band_long-term_signal_variability_20160924-21978-3ym5tt.pdf",
            "Abstract": "In this paper, we propose robust features for the problem of voice activity detection (VAD). In particular, we extend the long term signal variability (LTSV) feature to accommodate multiple spectral bands. The motivation of the multi-band approach stems from the non-uniform frequency scale of speech phonemes and noise characteristics. Our analysis shows that the multi-band approach offers advantages over the single band LTSV for voice activity detection. In terms of classification accuracy, we show 0.3%-61.2% relative improvement over the best accuracy of the baselines considered for 7 out 8 different noisy channels. Experimental results, and error analysis, are reported on the DARPA RATS corpora of noisy speech.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:_B80troHkn4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dialogue management in the Bell Labs communicator system",
            "Publication year": 2000,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/icslp_2000/i00_2603.pdf",
            "Abstract": "This paper describes a dialogue manager and its interaction with semantics and context tracking in a spoken dialogue system developed for general information retrieval and transaction applications. The dialogue system supports the following basic functionality: electronic form filling, database query, result navigation, attribute-value pair referencing, and value and reference resolution. General data structures and algorithms for representing and resolving ambiguity in a spoken dialogue system and a parsimonious parameterization for all application-dependent semantic and dialogue information are proposed. Dialogue management algorithms examine the semantics and dialogue state and adapt to the user\u2019s needs and task necessities. These algorithms are applied to a travel reservation application developed under the auspices of the DARPA Communicator project. The proposed algorithms are application \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:WF5omc3nYNoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Short-time instantaneous frequency and bandwidth features for speech recognition",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5373305/",
            "Abstract": "In this paper, we investigate the performance of modulation related features and normalized spectral moments for automatic speech recognition. We focus on the short-time averages of the amplitude weighted instantaneous frequencies and bandwidths, computed at each subband of a mel-spaced filterbank. Similar features have been proposed in previous studies, and have been successfully combined with MFCCs for speech and speaker recognition. Our goal is to investigate the stand-alone performance of these features. First, it is experimentally shown that the proposed features are only moderately correlated in the frequency domain, and, unlike MFCCs, they do not require a transformation to the cepstral domain. Next, the filterbank parameters (number of filters and filter overlap) are investigated for the proposed features and compared with those of MFCCs. Results show that frequency related features perform \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:HoB7MX3m0LUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An error-protected speech recognition system for wireless communications",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/994822/",
            "Abstract": "Future wireless multimedia terminals will have a variety of applications that require speech recognition capabilities. We consider a robust distributed speech recognition system where representative parameters of the speech signal are extracted at the wireless terminal and transmitted to a centralized automatic speech recognition (ASR) server. We propose two unequal error protection schemes for the ASR bit stream and demonstrate the satisfactory performance of these schemes for typical wireless cellular channels. In addition, a \"soft-feature\" error concealment strategy is introduced at the ASR server that uses \"soft-outputs\" from the channel decoder to compute the marginal distribution of only the reliable features during likelihood computation at the speech recognizer. This soft-feature error concealment technique reduces the ASR error rate by more than a factor of 2.5 for certain channels. Also considered is a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "End-to-end generative zero-shot learning via few-shot learning",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2102.04379",
            "Abstract": "Contemporary state-of-the-art approaches to Zero-Shot Learning (ZSL) train generative nets to synthesize examples conditioned on the provided metadata. Thereafter, classifiers are trained on these synthetic data in a supervised manner. In this work, we introduce Z2FSL, an end-to-end generative ZSL framework that uses such an approach as a backbone and feeds its synthesized output to a Few-Shot Learning (FSL) algorithm. The two modules are trained jointly. Z2FSL solves the ZSL problem with a FSL algorithm, reducing, in effect, ZSL to FSL. A wide class of algorithms can be integrated within our framework. Our experimental results show consistent improvement over several baselines. The proposed method, evaluated across standard benchmarks, shows state-of-the-art or competitive performance in ZSL and Generalized ZSL tasks.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:bz8QjSJIRt4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "COGNIMUSE: A multimodal video database annotated with saliency, events, semantics and emotion with application to summarization",
            "Publication year": 2017,
            "Publication url": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-017-0194-1",
            "Abstract": "Research related to computational modeling for machine-based understanding requires ground truth data for training, content analysis, and evaluation. In this paper, we present a multimodal video database, namely COGNIMUSE, annotated with sensory and semantic saliency, events, cross-media semantics, and emotion. The purpose of this database is manifold; it can be used for training and evaluation of event detection and summarization algorithms, for classification and recognition of audio-visual and cross-media events, as well as for emotion tracking. In order to enable comparisons with other computational models, we propose state-of-the-art algorithms, specifically a unified energy-based audio-visual framework and a method for text saliency computation, for the detection of perceptually salient events from videos. Additionally, a movie summarization system for the automatic production of summaries is presented. Two kinds of evaluation were performed, an objective based on the saliency annotation of the database and an extensive qualitative human evaluation of the automatically produced summaries, where we investigated what composes high-quality movie summaries, where both methods verified the appropriateness of the proposed methods. The annotation of the database and the code for the summarization system can be found at                    http://cognimuse.cs.ntua.gr/database                                    .",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:L7CI7m0gUJcC",
            "Publisher": "SpringerOpen"
        },
        {
            "Title": "Unsupervised combination of metrics for semantic class induction",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4123368/",
            "Abstract": "In this paper, unsupervised algorithms for combining semantic similarity metrics are proposed for the problem of automatic class induction. The automatic class induction algorithm is based on the work of Pargellis et al,. The semantic similarity metrics that are evaluated and combined are based on narrow- and wide-context vector- product similarity. The metrics are combined using linear weights that are computed 'on the fly' and are updated at each iteration of the class induction algorithm, forming a corpus-independent metric. Specifically, the weight of each metric is selected to be inversely proportional to the inter-class similarity of the classes induced by that metric and for the current iteration of the algorithm. The proposed algorithms are evaluated on two corpora: a semantically heterogeneous news domain (HR-Net) and an application-specific travel reservation corpus (ATIS). It is shown, that the (unsupervised \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:qUcmZB5y_30C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Adaptive categorical understanding for spoken dialogue systems",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1420367/",
            "Abstract": "In this paper, the speech understanding problem in the context of a spoken dialogue system is formalized in a maximum likelihood framework. Off-line adaptation of stochastic language models that interpolate dialogue state specific and general application-level language models is proposed. Word and dialogue-state n-grams are used for building categorical understanding and dialogue models, respectively. Acoustic confidence scores are incorporated in the understanding formulation. Problems due to data sparseness and out-of-vocabulary words are discussed. The performance of the speech recognition and understanding language models are evaluated with the \"Carmen Sandiego\" multimodal computer game corpus. Incorporating dialogue models reduces relative understanding error rate by 15%-25%, while acoustic confidence scores achieve a further 10% error reduction for this computer gaming application.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:0EnyYjriUFMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Using lexical, syntactic and semantic features for non-terminal grammar rule induction in spoken dialogue systems",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7078641/",
            "Abstract": "In this work, we propose an algorithm for the automatic induction of non-terminal grammar rules for Spoken Dialogue Systems (SDS). Initially, a grammar developer provides the system with a minimal set of rules that serve as seeding examples. Using these seed rules and (optionally) a seed corpus, in-domain data are harvested and filtered from the web. A challenging task is identifying relevant chunks (phrases) in the web-harvested corpus that are good candidates for enhancing the seed grammar. We propose and evaluate rule-based and statistical classification algorithms for this purpose that use lexical, syntactic and semantic features. Induced grammars are evaluated in terms of accuracy of the proposed rules for two spoken dialogue domains. Results show up to four times absolute precision improvement compared to the naive grammar induction approach using semantic phrase similarity.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:tKAzc9rXhukC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the effectiveness of PARAFAC-based estimation for blind speech separation",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4517569/",
            "Abstract": "This work establishes the effectiveness of parallel factor (PARAFAC) analysis in blind speech separation (BSS) problems. The BSS problem is formulated as a conjugate-symmetric PARAFAC model that is fitted optimally, using an efficient alternating least-squares algorithm that converges monotonically. The identifiability properties of the model are also presented, revealing the much broader identifiability potential of joint-diagonalization- based BSS methods. In order to focus on estimation performance, perfect resolution of the permutation ambiguity is assumed. Simulations under varying reverberation conditions and comparison with previous estimation methods that are widely used in BSS problems demonstrate significant performance gains. Signal-to- interference (SIR) ratio improvement of over 27 dB is achieved using PARAFAC. Average SIR gains of 2.5 and 6.3 dB are achieved compared to state-of-the-art \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:RYcK_YlVTxYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Ntua-slp at semeval-2018 task 1: Predicting affective content in tweets with deep attentive rnns and transfer learning",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1804.06658",
            "Abstract": "In this paper we present deep-learning models that submitted to the SemEval-2018 Task~1 competition: \"Affect in Tweets\". We participated in all subtasks for English tweets. We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Our model utilizes a set of word2vec word embeddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1st in Subtask E \"Multi-Label Emotion Classification\", 2nd in Subtask A \"Emotion Intensity Regression\" and achieved competitive results in other subtasks.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:tuHXwOkdijsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fusion of compositional network-based and lexical function distributional semantic models",
            "Publication year": 2015,
            "Publication url": "https://www.aclweb.org/anthology/W15-1105.pdf",
            "Abstract": "Distributional Semantic Models (DSMs) have been successful at modeling the meaning of individual words, with interest recently shifting to compositional structures, ie, phrases and sentences. Network-based DSMs represent and handle semantics via operators applied on word neighborhoods, ie, semantic graphs containing a target\u2019s most similar words. We extend network-based DSMs to address compositionality using an activation model (motivated by psycholinguistics) that operates on the fused neighborhoods of variable size activation. The proposed method is evaluated against and combined with the lexical function method proposed by (Baroni and Zamparelli, 2010). We show that, by fusing a network-based with a lexical function model, performance gains can be achieved.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:35r97b3x0nAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A review of ASR technologies for children's speech",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1640377.1640384",
            "Abstract": "In this paper, we review:(1) the acoustic and linguistic properties of children's speech for both read and spontaneous speech, and (2) the developments in automatic speech recognition for children with application to spoken dialogue and multimodal dialogue system design. First, the effect of developmental changes on the absolute values and variability of acoustic correlates is presented for read speech for children ages 6 and up. Then, verbal child-machine spontaneous interaction is reviewed and results from recent studies are presented. Age trends of acoustic, linguistic and interaction parameters are discussed, such as sentence duration, filled pauses, politeness and frustration markers, and modality usage. Some differences between child-machine and human-human interaction are pointed out. The implications for acoustic modeling, linguistic modeling and spoken dialogue system design for children are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:RGFaLdJalmkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Speech understanding for spoken dialogue systems: From corpus harvesting to grammar rule induction",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0885230816302613",
            "Abstract": "We investigate algorithms and tools for the semi-automatic authoring of grammars for spoken dialogue systems (SDS) proposing a framework that spans from corpora creation to grammar induction algorithms. A realistic human-in-the-loop approach is followed balancing automation and human intervention to optimize cost to performance ratio for grammar development. Web harvesting is the main approach investigated for eliciting spoken dialogue textual data, while crowdsourcing is also proposed as an alternative method. Several techniques are presented for constructing web queries and filtering the acquired corpora. We also investigate how the harvested corpora can be used for the automatic and semi-automatic (human-in-the-loop) induction of grammar rules. SDS grammar rules and induction algorithms are grouped into two types, namely, low- and high-level. Two families of algorithms are investigated for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:hMsQuOkrut0C",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Information seeking spoken dialogue systems\u2014Part I: Semantics and Pragmatics",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4130361/",
            "Abstract": "In this paper, the semantic and pragmatic modules of a spoken dialogue system development platform are presented and evaluated. The main goal of this research is to create spoken dialogue system modules that are portable across applications domains and interaction modalities. We propose a hierarchical semantic representation that encodes all information supplied by the user over multiple dialogue turns and can efficiently represent and be used to argue with ambiguous or conflicting information. Implicit in this semantic representation is a pragmatic module, consisting of context tracking, pragmatic analysis and pragmatic scoring submodules, which computes pragmatic confidence scores for all system beliefs. These pragmatic scores are obtained by combining semantic and pragmatic evidence from the various sub-modules (taking into account the modality of input) and are used to rank-order attribute-value \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:mB3voiENLucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "SEQ^ 3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1904.03651",
            "Abstract": "Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora. We present a sequence-to-sequence-to-sequence autoencoder (SEQ^3), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables. We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence. Constraining the length of the latent word sequences forces the model to distill important information from the input. A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable. Continuous relaxations enable us to sample from categorical distributions, allowing gradient-based optimization, unlike alternatives that rely on reinforcement learning. The proposed model does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:Dip1O2bNi0gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A study in efficiency and modality usage in multimodal form filling systems",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4566088/",
            "Abstract": "The usage patterns of speech and visual input modes are investigated as a function of relative input mode efficiency for both desktop and personal digital assistant (PDA) working environments. For this purpose the form-filling part of a multimodal dialogue system is implemented and evaluated; three multimodal modes of interaction are implemented: ldquoClick-to-Talk,rdquo ldquoOpen-Mike,rdquo and ldquoModality-Selection.rdquo ldquoModality-Selectionrdquo implements an adaptive interface where the system selects the most efficient input mode at each turn, effectively alternating between a ldquoClick-to-Talkrdquo and ldquoOpen-Mikerdquo interaction style as proposed in ldquoModality tracking in the multimodal Bell Labs Communicator,rdquo in Proceedings of the Automatic Speech Recognition and Understanding Workshop, by A. Potamianos, , 2003. The multimodal systems are evaluated and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:iH-uZ7U-co4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Toward the automatic extraction of policy networks using web links and documents",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6269877/",
            "Abstract": "Policy networks are widely used by political scientists and economists to explain various financial and social phenomena, such as the development of partnerships between political entities or institutions from different levels of governance. The analysis of policy networks demands a series of arduous and time-consuming manual steps including interviews and questionnaires. In this paper, we estimate the strength of relations between actors in policy networks using features extracted from data harvested from the web. Features include webpage counts, outlinks, and lexical information extracted from web documents or web snippets. The proposed approach is automatic and does not require any external knowledge source, other than the specification of the word forms that correspond to the political actors. The features are evaluated both in isolation and jointly for both positive and negative (antagonistic) actor \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:vRqMK49ujn8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robust recognition of children's speech",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1255448/",
            "Abstract": "Developmental changes in speech production introduce age-dependent spectral and temporal variability in the speech signal produced by children. Such variabilities pose challenges for robust automatic recognition of children's speech. Through an analysis of age-related acoustic characteristics of children's speech in the context of automatic speech recognition (ASR), effects such as frequency scaling of spectral envelope parameters are demonstrated. Recognition experiments using acoustic models trained from adult speech and tested against speech from children of various ages clearly show performance degradation with decreasing age. On average, the word error rates are two to five times worse for children speech than for adult speech. Various techniques for improving ASR performance on children's speech are reported. A speaker normalization algorithm that combines frequency warping and model \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:IjCSPb-OGe4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "DeepPurple: lexical, string and affective feature fusion for sentence-level semantic similarity estimation",
            "Publication year": 2013,
            "Publication url": "https://www.aclweb.org/anthology/S13-1014.pdf",
            "Abstract": "This paper describes our submission for the* SEM shared task of Semantic Textual Similarity. We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:D_sINldO8mEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Soft-feature decoding for speech recognition over wireless channels",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/940819/",
            "Abstract": "A distributed automatic speech recognition (ASR) system is considered where features of the speech signal are extracted at the wireless terminal and transmitted to a centralized ASR server. An unequal error protection scheme is used for the quantized ASR feature stream. At the receiver, coherent demodulation is performed and the probability of error for each bit is computed using the max-log MAP algorithm. A 'soft-feature' decoding strategy is introduced at the ASR server that uses the marginal distribution of only the reliable features during likelihood computation. Alternatively, the confidence of each feature is computed from the bit error probabilities and each feature in the probability computation is weighted as a function of the feature confidence. The performance of the proposed soft-feature algorithms is evaluated over typical cellular wireless channels and it is shown to reduce ASR error rate by over 50% for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:_FxGoFyzp5QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multiple time resolution analysis of speech signal using mce training with application to speech recognition",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4960455/",
            "Abstract": "In this paper, we propose two methods of multiple time-resolution analysis of speech and their application to automatic speech recognition (ASR). Constant frame-rate multi-scale analysis is proposed based on a box of multi-scale features. Then a variable rate analysis is proposed based on the selection of the optimal temporal resolution on the fly by a properly trained non-linear classifier unit. The classifier's parameters are trained using the discriminative method of minimum classification error (MCE) training. We use the recently proposed conditional random fields (CRF) phonetic recognition system that effectively combines highly correlated features. Results are reported on a frame-wise classification task and also on TIMIT phone recognition task. Results show that (i) CRFs can effectively combine multi-scale features and (ii) MCE trained variable rate CRFs are competitive with the ldquoboxrdquo combination \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:g5m5HwL7SMYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Engagement detection for children with autism spectrum disorder",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7953119/",
            "Abstract": "Children with Autism Spectrum Disorder (ASD) face several difficulties in social communication. Hence, analyzing social interaction can provide insight on their social and cognitive skills. In this paper, we investigate the degree of engagement of children in interactions with their parents. Features derived from both participants including acoustic, linguistic and dialogue act features are explored. The effect of visual cues is also investigated. We experimented on the task of engagement detection using video-recorded sessions consisting of interactions of typically developing (TD) and ASD children. Results show that engagement is easier to predict for TD children than for ASD children, and that the parent's actions/movements are better predictors of the child's degree of engagement.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:vbGhcppDl1QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The spedial datasets: datasets for spoken dialogue systems analytics",
            "Publication year": 2016,
            "Publication url": "https://www.aclweb.org/anthology/L16-1016.pdf",
            "Abstract": "The SpeDial consortium is sharing two datasets that were used during the SpeDial project. By sharing them with the community we are providing a resource to reduce the duration of cycle of development of new Spoken Dialogue Systems (SDSs). The datasets include audios and several manual annotations, ie, miscommunication, anger, satisfaction, repetition, gender and task success. The datasets were created with data from real users and cover two different languages: English and Greek. Detectors for miscommunication, anger and gender were trained for both systems. The detectors were particularly accurate in tasks where humans have high annotator agreement such as miscommunication and gender. As expected due to the subjectivity of the task, the anger detector had a less satisfactory performance. Nevertheless, we proved that the automatic detection of situations that can lead to problems in SDSs is possible and can be a promising direction to reduce the duration of SDS\u2019s development cycle.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:0KyAp5RtaNEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Introduction to the special issue on speech and language processing of children's speech for child-machine interaction applications",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1145/1998384.1998385",
            "Abstract": "The rapid advancement of speech recognition and spoken dialogue technologies has enabled the use of voice in numerous interactive applications today. Although children represent an important user segment for speech processing technologies, the majority of the research effort so far has focused on adult users. This is due to a variety of reasons including lack of appropriate speech data from children across the developmental trajectory, challenges associated with conducting experiments with children, as well as fundamental misconceptions about children-computer interaction. From the very early experiments at AT&T Bell Labs, it was made clear that children\u2019s speech posed a challenge to speech recognizers designed for adult voices. The acoustic characteristics of children\u2019s speech vary widely with age, resulting in spectral and temporal patterns that differ from those of adults. In addition, variability in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:SP6oXDckpogC",
            "Publisher": "ACM"
        },
        {
            "Title": "Modulation features for speech recognition",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5743733/",
            "Abstract": "Automatic speech recognition (ASR) systems can benefit from including into their acoustic processing part new features that account for various nonlinear and time-varying phenomena during speech production. In this paper, we develop robust methods to extract novel acoustic features from speech signals of the modulation type based on time-varying models for speech analysis. Further, we integrate the new speech features with the standard linear ones (mel-frequency cesptrum) to develop a augmented set of acoustic features and demonstrate its efficacy by showing significant improvements in HMM-based word recognition over the TIMIT database.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:QIV2ME_5wuYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio-based distributional semantic models for music auto-tagging and similarity measurement",
            "Publication year": 2016,
            "Publication url": "https://arxiv.org/abs/1612.08391",
            "Abstract": "The recent development of Audio-based Distributional Semantic Models (ADSMs) enables the computation of audio and lexical vector representations in a joint acoustic-semantic space. In this work, these joint representations are applied to the problem of automatic tag generation. The predicted tags together with their corresponding acoustic representation are exploited for the construction of acoustic-semantic clip embeddings. The proposed algorithms are evaluated on the task of similarity measurement between music clips. Acoustic-semantic models are shown to outperform the state-of-the-art for this task and produce high quality tags for audio/music clips.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:Z5m8FVwuT1cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Human-computer interfaces to multimedia content a review",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/978-0-387-76316-3_2.pdf",
            "Abstract": "Human Computer Interaction (HCI) is the study of interaction between users and computer systems. HCI is a multi-disciplinary subject, combining topics such as: psychology and cognitive science that studies user\u2019s perceptual, cognitive, and problem solving skills, ergonomics (ie, the study of the physical capabilities of the user), design, as well as computer science, and engineering. HCI is concerned among others with theories of interaction, development of new interfaces and interaction techniques, eg for mobile computing, methodologies for designing interfaces, implementation of software toolkits, design of hardware devices, and techniques for evaluating and comparing interfaces. As the number, diversity, and complexity of interactive applications increases users need to continuously learn, adapt, and cope with new interfaces. As stated in [3]:\u201ca long term goal of HCI is to design systems that minimize the barrier \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:pqnbT2bcN3wC",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "SAIL: A hybrid approach to sentiment analysis",
            "Publication year": 2013,
            "Publication url": "https://www.aclweb.org/anthology/S13-2072.pdf",
            "Abstract": "This paper describes our submission for SemEval2013 Task 2: Sentiment Analysis in Twitter. For the limited data condition we use a lexicon-based model. The model uses an affective lexicon automatically generated from a very large corpus of raw web data. Statistics are calculated over the word and bigram affective ratings and used as features of a Naive Bayes tree model. For the unconstrained data scenario we combine the lexicon-based model with a classifier built on maximum entropy language models and trained on a large external dataset. The two models are fused at the posterior level to produce a final output. The approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:mvPsJ3kp5DgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Ntua-slp at semeval-2018 task 3: Tracking ironic tweets using ensembles of word and character level attentive rnns",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1804.06659",
            "Abstract": "In this paper we present two deep-learning systems that competed at SemEval-2018 Task 3 \"Irony detection in English tweets\". We design and ensemble two independent models, based on recurrent neural networks (Bi-LSTM), which operate at the word and character level, in order to capture both the semantic and syntactic information in tweets. Our models are augmented with a self-attention mechanism, in order to identify the most informative words. The embedding layer of our word-level model is initialized with word2vec word embeddings, pretrained on a collection of 550 million English tweets. We did not utilize any handcrafted features, lexicons or external datasets as prior information and our models are trained end-to-end using back propagation on constrained data. Furthermore, we provide visualizations of tweets with annotations for the salient tokens of the attention layer that can help to interpret the inner workings of the proposed models. We ranked 2nd out of 42 teams in Subtask A and 2nd out of 31 teams in Subtask B. However, post-task-completion enhancements of our models achieve state-of-the-art results ranking 1st for both subtasks.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:MLfJN-KU85MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Combining statistical similarity measures for automatic induction of semantic classes",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1566510/",
            "Abstract": "In this paper, an unsupervised semantic class induction algorithm is proposed that is based on the principle that similarity of context implies similarity of meaning. Two semantic similarity metrics that are variations of the vector product distance are used in order to measure the semantic distance between words and to automatically generate semantic classes. The first metric computes \"wide-context\" similarity between words using a \"bag-of-words\" model, while the second metric computes \"narrow-context\" similarity using a bigram language model. A hybrid metric that is defined as the linear combination of the wide and narrow-context metrics is also proposed and evaluated. To cluster words into semantic classes an iterative clustering algorithm is used. The semantic metrics are evaluated on two corpora: a semantically heterogeneous Web news domain (HR-Net) and an application-specific travel reservation corpus \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:4JMBOYKVnBMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Quality evaluation of computational models for movie summarization",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7148146/",
            "Abstract": "In this paper we present a movie summarization system and we investigate what composes high quality movie summaries in terms of user experience evaluation. We propose state-of-the-art audio, visual and text techniques for the detection of perceptually salient events from movies. The evaluation of such computational models is usually based on the comparison of the similarity between the system-detected events and some ground-truth data. For this reason, we have developed the MovSum movie database, which includes sensory and semantic saliency annotation as well as cross-media relations, for objective evaluations. The automatically produced movie summaries were qualitatively evaluated, in an extensive human evaluation, in terms of informativeness and enjoyability accomplishing very high ratings up to 80% and 90%, respectively, which verifies the appropriateness of the proposed methods.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:-_dYPAW6P2MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Spoken dialogue grammar induction from crowdsourced data",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6854193/",
            "Abstract": "We design and evaluate various crowdsourcing tasks for eliciting spoken dialogue data. Task design is based on an array of parameters that quantify the basic characteristics of the elicitation questions, e.g., how open-ended is a question. The crowdsourced data are used for and evaluated on the unsupervised induction of semantic classes for speech understanding grammars. We show that grammar induction performance is significantly affected by the crowdsourcing task parameters, e.g., paraphrasing tasks prime high lexical entrain-ment and result in poor corpus/grammar quality. The task parameters along with perplexity filters are used for corpus selection achieving grammar induction performance that is comparable to that of using in-domain spoken dialogue data.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:PR6Y55bgFSsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Blending speech and visual input in multimodal dialogue systems",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4123382/",
            "Abstract": "In this paper the efficiency and usage patterns of input modes in multimodal dialogue systems is investigated for desktop and personal digital assistant (PDA) working environments. For this purpose a form-filling travel reservation system is designed and implemented that efficiently combines the speech and visual modalities; three multimodal modes of interaction are implemented, namely: \"click-to-talk\", \"open-mike\" and \"modality-selection\". The three multimodal systems are evaluated and compared with the \"GUI-only\" and \"speech-only\" unimodal systems. User interface evaluation includes both objective and subjective metrics and shows that all three multimodal systems outperform the unimodal systems on the PDA environment. For the desktop environment the multimodal systems score better than the \"speech-only\" system but worse than the \"GUI-only\" system. In all evaluation experiments, the synergy between \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:GnPB-g6toBAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Pattern search multidimensional scaling",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1806.00416",
            "Abstract": "We present a novel view of nonlinear manifold learning using derivative-free optimization techniques. Specifically, we propose an extension of the classical multi-dimensional scaling (MDS) method, where instead of performing gradient descent, we sample and evaluate possible \"moves\" in a sphere of fixed radius for each point in the embedded space. A fixed-point convergence guarantee can be shown by formulating the proposed algorithm as an instance of General Pattern Search (GPS) framework. Evaluation on both clean and noisy synthetic datasets shows that pattern search MDS can accurately infer the intrinsic geometry of manifolds embedded in high-dimensional spaces. Additionally, experiments on real data, even under noisy conditions, demonstrate that the proposed pattern search MDS yields state-of-the-art results.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:LI9QrySNdTsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised HMM adaptation based on speech-silence discrimination",
            "Publication year": 2000,
            "Publication url": "https://patents.google.com/patent/US6076057A/en",
            "Abstract": "An unsupervised, discriminative, sentence level, HMM adaptation based on speech-silence classification is presented. Silence and speech regions are determined either using a speech end-pointer or the segmentation obtained from the recognizer in a first pass. The discriminative training procedure using a GPD or any other discriminative training algorithm, employed in conjunction with the HMM-based recognizer, is then used to increase the discrimination between silence and speech.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "SAIL: Sentiment Analysis using Semantic Similarity and Contrast Features.",
            "Publication year": 2014,
            "Publication url": "https://sail.usc.edu/~malandra/files/papers/semeval2014.pdf",
            "Abstract": "This paper describes our submission to SemEval2014 Task 9: Sentiment Analysis in Twitter. Our model is primarily a lexicon based one, augmented by some preprocessing, including detection of Multi-Word Expressions, negation propagation and hashtag expansion and by the use of pairwise semantic similarity at the tweet level. Feature extraction is repeated for sub-strings and contrasting sub-string features are used to better capture complex phenomena like sarcasm. The resulting supervised system, using a Naive Bayes model, achieved high performance in classifying entire tweets, ranking 7th on the main set and 2nd when applied to sarcastic tweets.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:WqliGbK-hY8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Attention-based conditioning methods for external knowledge integration",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1906.03674",
            "Abstract": "In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:foquWX3nUaYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Video event detection and summarization using audio, visual and text saliency",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4960393/",
            "Abstract": "Detection of perceptually important video events is formulated here on the basis of saliency models for the audio, visual and textual information conveyed in a video stream. Audio saliency is assessed by cues that quantify multifrequency waveform modulations, extracted through nonlinear operators and energy tracking. Visual saliency is measured through a spatiotemporal attention model driven by intensity, color and motion. Text saliency is extracted from part-of-speech tagging on the subtitles information available with most movie distributions. The various modality curves are integrated in a single attention curve, where the presence of an event may be signified in one or multiple domains. This multimodal saliency curve is the basis of a bottom-up video summarization algorithm, that refines results from unimodal or audiovisual-based skimming. The algorithm performs favorably for video summarization in terms of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:M3ejUd6NZC8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The effect of input mode on inactivity and interaction times of multimodal systems",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1322192.1322212",
            "Abstract": "In this paper, the efficiency and usage patterns of input modes in multimodal dialogue systems is investigated for both desktop and personal digital assistant (PDA) working environments. For this purpose a form-filling travel reservation application is evaluated that combines the speech and visual modalities; three multimodal modes of interaction are implemented, namely:\" Click-To-Talk\",\" Open-Mike\" and\" Modality-Selection\". The three multimodal systems are evaluated and compared with the\" GUI-Only\" and\" Speech-Only\" unimodal systems. Mode and duration statistics are computed for each system, for each turn and for each attribute in the form. Turn time is decomposed in interaction and inactivity time and the statistics for each input modeare computed. Results show that multimodal and adaptive interfaces are superior in terms of interaction time, but not always in terms of inactivity time. Also users tend to use \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:TFP_iSt0sucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "EmpBot: A T5-based Empathetic Chatbot focusing on Sentiments",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2111.00310",
            "Abstract": "In this paper, we introduce EmpBot: an end-to-end empathetic chatbot. Empathetic conversational agents should not only understand what is being discussed, but also acknowledge the implied feelings of the conversation partner and respond appropriately. To this end, we propose a method based on a transformer pretrained language model (T5). Specifically, during finetuning we propose to use three objectives: response language modeling, sentiment understanding, and empathy forcing. The first objective is crucial for generating relevant and coherent responses, while the next ones are significant for acknowledging the sentimental state of the conversational partner and for favoring empathetic responses. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. The inclusion of the sentiment understanding and empathy forcing auxiliary losses favor empathetic responses, as human evaluation results indicate, comparing with the current state-of-the-art.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:sNmaIFBj_lkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Affective evaluation of multimodal dialogue games for preschoolers using physiological signals.",
            "Publication year": 2013,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2013/i13_2415.pdf",
            "Abstract": "In this pilot study, we investigate the differences in the electroencephalography (EEG) signal patterns of children and adults while interacting with a multimodal dialogue computer game. The gaming application is designed for preschoolers, implements five popular learning tasks and has variable levels of difficulty. In this pilot, to simplify the data collection process for young children, we use the NeuroSky MindSet device which is a single forehead dry sensor device. The raw signals and the estimated attention, meditation and arousal signals are analyzed during the interaction and compared for adult and children user populations. Results show consistent variations as a function of modality used (speech vs mouse input), difficulty level and task success. The physiological signal pattern within an interaction turn is also estimated and analyzed. Overall, children and adults demonstrated very similar physiological signal \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:SdhP9T11ey4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "DeepPurple: Estimating sentence semantic similarity using n-gram regression models and web snippets",
            "Publication year": 2012,
            "Publication url": "https://www.aclweb.org/anthology/S12-1082.pdf",
            "Abstract": "We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, and 3) sentence length. Lexical semantic similarity is computed via co-occurrence counts on a corpus harvested from the web using a modified mutual information metric. State-of-the-art results are obtained for semantic similarity computation at the word level, however, the fusion of this information at the sentence level provides only moderate improvement on Task 6 of SemEval\u201912. Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:p2g8aNsByqUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dialogue act semantic representation and classification using recurrent neural networks",
            "Publication year": 2017,
            "Publication url": "https://scholar.google.com/scholar?cluster=16643689825442242887&hl=en&oi=scholarr",
            "Abstract": "In this work, we present a model that incorporates Dialogue Act (DA) semantics in the framework of Recurrent Neural Networks (RNNs) for DA classification. Specifically, we propose a novel scheme for automatically encoding DA semantics via the extraction of salient keywords that are representative of the DA tags. The proposed model is applied to the Switchboard corpus and achieves 1.7%(absolute) improvement in classification accuracy with respect to the baseline model. We demonstrate that the addition of discourse-level features enhances the DA classification as well as makes the algorithm more robust: the proposed model does not require the preprocessing of dialogue transcriptions.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ILKRHgRFtOwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "System and method for measuring domain independence of semantic classes",
            "Publication year": 2003,
            "Publication url": "https://patents.google.com/patent/US20030233232A1/en",
            "Abstract": "A system for, and method of, measuring a degree of independence of semantic classes in separate domains. In one embodiment, the system includes:(1) a cross-domain distance calculator that estimates a similarity between n-gram contexts for the semantic classes in each of the separate domains to determine domain-dependent relative entropies associated with the semantic classes and (2) a distance summer, associated with the cross-domain distance calculator, that adds the domain-dependent distances over a domain vocabulary to yield the degree of independence of the semantic classes.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:FPJr55Dyh1AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised stream-weights computation in classification and recognition tasks",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4782028/",
            "Abstract": "In this paper, we provide theoretical results on the problem of optimal stream weight selection for the two stream classification problem. It is shown that in the presence of estimation or modeling errors using stream weights can decrease the total classification error. Specifically, we show that stream weights should be selected to be proportional to the feature stream reliability and informativeness. Next, we turn our attention to the problem of unsupervised stream weights computation in real tasks. Based on the theoretical results we propose to use models and ldquoanti-modelsrdquo (class-specific background models) to estimate stream weights. A nonlinear function of the ratio of the inter- to intra-class distance is proposed for stream weight estimation. The resulting unsupervised stream weight estimation algorithm is evaluated on both artificial data and on the problem of audiovisual speech classification. Finally, the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:NaGl4SEjCO4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Linguistic analysis of spontaneous children speech",
            "Publication year": 2008,
            "Publication url": "https://www.isca-speech.org/archive_v0/wocci_2008/papers/woc8_04.pdf",
            "Abstract": "In this paper, we investigate the duration, lexical and linguistic properties of children\u2019s spontaneous speech for children ages 8 to 14 interacting with animated characters in a computer game. Age and gender trends are studied for parameters such as phone and sentence duration, speaking rate, fluency (mispronounciations and hesitations), vocabulary size and linguistic variability measured via bigram language model perplexity. The analysis shows significant differences between read-and spontaneous children speech in terms of absolute values of acoustic and linguistic parameters, as well as, linguistic variability. In addition, spontaneous data present clear gender-specific trends, eg, increased \u201clanguage exploration\u201d by girls in the 12-14 age group. The applicability of these results for acoustic and linguistic modeling and spoken dialogue systems interface design is discussed.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:70eg2SAEIzsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the effects of filterbank design and energy computation on robust speech recognition",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5638124/",
            "Abstract": "In this paper, we examine how energy computation and filterbank design contribute to the overall front-end robustness, especially when the investigated features are applied to noisy speech signals, in mismatched training-testing conditions. In prior work (\u201cAuditory Teager energy cepstrum coefficients for robust speech recognition,\u201d D. Dimitriadis, P. Maragos, and A. Potamianos, in Proc. Eurospeech'05, Sep. 2005), a novel feature set called \u201cTeager energy cepstrum coefficients\u201d (TECCs) has been proposed, employing a dense, smooth filterbank and alternative energy computation schemes. TECCs were shown to be more robust to noise and exhibit improved performance compared to the widely used Mel frequency cepstral coefficients (MFCCs). In this paper, we attempt to interpret these results using a combined theoretical and experimental analysis framework. Specifically, we investigate in detail the connection \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:lSLTfruPkqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Modulation and chaotic acoustic features for speech recognition",
            "Publication year": 2002,
            "Publication url": "https://www.academia.edu/download/39503379/2002_dimitriadis_pitsikalis_maragos_potamianos.pdf",
            "Abstract": "Automatic speech recognition systems can benefit from including into their acoustic processing part new features that account for various nonlinear and time-varying phenomena during speech production. In this paper, we develop robust methods for extracting novel acoustic features from speech signals based on nonlinear and time-varying models of speech. These new modulation-and chaotic-type features are integrated with the standard linear ones (melfrequency cesptrum) to develop a generalized hybrid set of acoustic features. The efficacy by showing significant improvements in HMM-based phoneme recognition over the TIMIT database.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:BqipwSGYUEgC",
            "Publisher": "Anaheim, CA; Calgary: ACTA Press, 1998-"
        },
        {
            "Title": "Hybrid natural language generation for spoken dialogue systems",
            "Publication year": 2001,
            "Publication url": "https://www.microsoft.com/en-us/research/publication/hybrid-natural-language-generation-for-spoken-dialogue-systems/",
            "Abstract": "The natural language generation component of most dialogue systems is based on templates. Template-based generators are hard to maintain and reuse, and the sentences they produce lack the variability and robustness needed by conversational systems. In this paper, we propose a flexible and domainindependent natural language generator for spoken dialogue systems which combines fixed surface expressions with freely generated text. The generation algorithm follows a hybrid approach, combining finite state machine (FSM) grammars and corpus-based language models. In this approach, the FSM grammar (a reversible parser grammar) is constrained by a word and concept\u00a2-gram that takes terminals and non-terminal cooccurrences into account. The\u00a2-gram grammar helps prevent inappropriate derivations, therefore improving the quality of the generated texts. Furthermore, the proposed algorithm achieves faster than real-time performance because of the limited number of derivations.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio-Based Distributional Representations of Meaning Using a Fusion of Feature Encodings.",
            "Publication year": 2016,
            "Publication url": "https://www.researchgate.net/profile/Giannis-Karamanolakis/publication/307889586_Audio-Based_Distributional_Representations_of_Meaning_Using_a_Fusion_of_Feature_Encodings/links/584f291608aecb6bd8d02780/Audio-Based-Distributional-Representations-of-Meaning-Using-a-Fusion-of-Feature-Encodings.pdf",
            "Abstract": "Recently a \u201cBag-of-Audio-Words\u201d approach was proposed [1] for the combination of lexical features with audio clips in a multimodal semantic representation, ie, an Audio Distributional Semantic Model (ADSM). An important step towards the creation of ADSMs is the estimation of the semantic distance between clips in the acoustic space, which is especially challenging given the diversity of audio collections. In this work, we investigate the use of different feature encodings in order to address this challenge following a two-step approach. First, an audio clip is categorized with respect to three classes, namely, music, speech and other. Next, the feature encodings are fused according to the posterior probabilities estimated in the previous step. Using a collection of audio clips annotated with tags we derive a mapping between words and audio clips. Based on this mapping and the proposed audio semantic distance, we construct an ADSM model in order to compute the distance between words (lexical semantic similarity task). The proposed model is shown to significantly outperform (23.6% relative improvement in correlation coefficient) the state-of-the-art results reported in the literature.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:URolC5Kub84C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Open challenges in modelling, analysis and synthesis of human behaviour in human\u2013human and human\u2013machine interactions",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/article/10.1007/s12559-015-9326-z",
            "Abstract": "Modelling, analysis and synthesis of behaviour are the subject of major efforts in computing science, especially when it comes to technologies that make sense of human\u2013human and human\u2013machine interactions. This article outlines some of the most important issues that still need to be addressed to ensure substantial progress in the field, namely (1) development and adoption of virtuous data collection and sharing practices, (2) shift in the focus of interest from individuals to dyads and groups, (3) endowment of artificial agents with internal representations of users and context, (4) modelling of cognitive and semantic processes underlying social behaviour and (5) identification of application domains and strategies for moving from laboratory to the real-world products.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ZfRJV9d4-WMC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Integrating recurrence dynamics for speech emotion recognition",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1811.04133",
            "Abstract": "We investigate the performance of features that can capture nonlinear recurrence dynamics embedded in the speech signal for the task of Speech Emotion Recognition (SER). Reconstruction of the phase space of each speech frame and the computation of its respective Recurrence Plot (RP) reveals complex structures which can be measured by performing Recurrence Quantification Analysis (RQA). These measures are aggregated by using statistical functionals over segment and utterance periods. We report SER results for the proposed feature set on three databases using different classification methods. When fusing the proposed features with traditional feature sets, we show an improvement in unweighted accuracy of up to 5.7% and 10.7% on Speaker-Dependent (SD) and Speaker-Independent (SI) SER tasks, respectively, over the baseline. Following a segment-based approach we demonstrate state-of-the-art performance on IEMOCAP using a Bidirectional Recurrent Neural Network.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:k8Z6L05lTy4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Statistical recursive finite state machine parsing for speech understanding.",
            "Publication year": 2000,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.590.8603&rep=rep1&type=pdf",
            "Abstract": "In this paper, a statistical framework for semantic parsing is described. The statistical model uses two information sources to disambiguate between rules: rule weights that capture vertical relationships in the parse tree, and concept\u00a2-grams that capture horizontal relationships. Rule design consists of simple local mapping rules that non-experts can write, and the rules are implemented as weighted finite state transducers. A general parser for context free grammars is implemented using a finite state machine library. Semantic decoding is implemented by recursively composing the rule transducer with the word-graph automaton produced from the speech recognizer. Detailed metrics for evaluating semantic parse accuracy are proposed. The parser is evaluated on the ATIS travel task with resulting precision and recall rates of over 95%. The proposed finite state transducer formulation allows the incorporation of rules and probabilities in a unified framework and the straightforward combination of acoustic, language, and understanding models.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:Se3iqnhoufwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards incorporating language morphology into statistical machine translation systems",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1566533/",
            "Abstract": "In this paper, a novel algorithm for incorporating morphological knowledge into statistical machine translation (SMT) systems is proposed. First, word stems are acquired automatically for the source and target languages using an unsupervised morphological acquisition algorithm. Then a word-stem based SMT system is built and combined with a phrase-based word level SMT system using a general statistical framework. The combined lexical and morphological SMT system is implemented using late integration and lattice re-scoring. The system is then evaluated on the Europarl corpus, using automatic evaluation methods for various training corpus sizes. It is shown, that both the BLEU and NIST scores of the lexical-morphological system improve by about 14% over the baseline English to Greek translation system when using a 1M word training corpus",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ldfaerwXgEUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Web data harvesting for speech understanding grammar induction.",
            "Publication year": 2013,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2013/i13_2733.pdf",
            "Abstract": "The development of a speech understanding grammar for spoken dialogue systems can be greatly accelerated by using an in-domain corpus. The development of such a corpus, however, is a slow and expensive process. This paper proposes unsupervised, language-agnostic methods for finding relevant corpora in the web and mining the most informative parts. We show that by utilizing perplexity we are able to increase the in-domainess (precision) of the mined corpora, while by utilizing pragmatic constraints and search engine rank we can increase the generalizability (recall). We show that automatic grammar induction algorithms achieve superior performance on the automatically mined corpora compared to in-domain manually collected corpora for a travel application.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:9vf0nzSNQJEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Demonstration of assembly work using augmented reality",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1282280.1282301",
            "Abstract": "In this paper, we describe the demonstration system in which a wooden 3D puzzle is assembled using an augmented reality system. The 3D puzzle emulated a simplified assembly task in a factory. We use the 3D puzzle as a means to study how to implement an augmented assembly system to a real setting in a factory and explore the advantages and disadvantages of augmented reality techniques. With this demonstration system we also test, demonstrate and evaluate different input modalities for augmented assembly setup. Furthermore, the demonstration system helps us better interact with industry and get invaluable feedback.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:L8Ckcad2t8MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deep Hierarchical Fusion with Application in Sentiment Analysis.",
            "Publication year": 2019,
            "Publication url": "https://www.researchgate.net/profile/Efthymios-Georgiou/publication/335829412_Deep_Hierarchical_Fusion_with_Application_in_Sentiment_Analysis/links/5e754fb9299bf1892cfbd1ba/Deep-Hierarchical-Fusion-with-Application-in-Sentiment-Analysis.pdf",
            "Abstract": "Recognizing the emotional tone in spoken language is a challenging research problem that requires modeling not only the acoustic and textual modalities separately but also their crossinteractions. In this work, we introduce a hierarchical fusion scheme for sentiment analysis of spoken sentences. Two bidirectional Long-Short-Term-Memory networks (BiLSTM), followed by multiple fully connected layers, are trained in order to extract feature representations for each of the textual and audio modalities. The representations of the unimodal encoders are both fused at each layer and propagated forward, thus achieving fusion at the word, sentence and high/sentiment levels. The proposed approach of deep hierarchical fusion achieves stateof-the-art results for sentiment analysis tasks. Through an ablation study, we show that the proposed fusion method achieves greater performance gains over the unimodal baseline compared to other fusion approaches in the literature.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:XoXfffV-tXoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Feeling is Understanding: From Affective to Semantic Spaces",
            "Publication year": 2015,
            "Publication url": "https://www.aclweb.org/anthology/W15-0121.pdf",
            "Abstract": "Motivated by theories of language development we investigate the contribution of affect to lexical semantics in the context of distributional semantic models (DSMs). The relationship between semantic and affective spaces is computationally modeled for the task of semantic similarity computation between words. It is shown that affective spaces contain salient information for lexical semantic tasks. We further investigate specific semantic relationships where affective information plays a prominent role. The relations between semantic similarity and opposition are studied in the framework of a binary classification problem applied for the discrimination of synonyms and antonyms. For the case of antonyms, the use of affective features results in 33% relative improvement in classification accuracy compared to the use of semantic features.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:_Re3VWB3Y0AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An unstructured distributional semantic model for morphologically rich languages: the Dutch case study",
            "Publication year": 2014,
            "Publication url": "https://research.tue.nl/en/publications/an-unstructured-distributional-semantic-model-for-morphologically",
            "Abstract": "An unstructured distributional semantic model for morphologically rich languages: the Dutch \ncase study \u2014 Eindhoven University of Technology research portal Skip to main navigation Skip \nto search Skip to main content Eindhoven University of Technology research portal Logo Help \n& FAQ English Nederlands Home Researchers Research Output Organisational units Activities \nProjects Prizes Press / Media Facilities / Equipment Datasets Courses Research areas \nStudent theses Search by expertise, name or affiliation An unstructured distributional semantic \nmodel for morphologically rich languages: the Dutch case study Kalliopi Zervanou, Elias Iosif, \nAlexandros Potamianos Research output: Contribution to conference \u203a Poster \u203a Academic 1 \nDownloads (Pure) Overview Original language English Publication status Published - 2014 \nExternally published Yes Event 24th Meeting of Computational Linguistics in The (CLIN \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:WZBGuue-350C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards speaker and environmental robustness in ASR: the hiwire project",
            "Publication year": 2006,
            "Publication url": "https://hal.archives-ouvertes.fr/hal-00110502/",
            "Abstract": "In this paper, we present algorithms for dealing with variability and mismatch in speech recognition due to environmental conditions and non-native speaker populations. The proposed algorithms cover a broad spectrum of ideas including robust feature extraction, feature compensation and speech enhancement. Specifically the following algorithms are presented and evaluated: beamforming for multi-microphone speech recognition, robust modulation and fractal features, Teager energy cepstrum coefficients, parametric feature equalization, speech enhancement, and acoustic modeling for non-native speech recognition. Also the problem of feature fusion and voice activity detection are discussed. Evaluation results on the AURORA databases under the auspices of the HIWIRE project show that significant gains can be achieved under adverse or mismatched conditions using these algorithms. Relative error rate reduction of up to 50% was shown for multi-microphone speech recognition, robust feature combination and speech enhancement. 30-40% reduction was shown for parametric feature equalization and non-native acoustic models.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ZHo1McVdvXMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised stream weight estimation using anti-models",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4218113/",
            "Abstract": "In this paper, a novel solution to the problem of unsupervised stream weight estimation for multi-stream classification tasks is proposed. Our work is based on theoretical results in A. Potamianos et al. (2006) for the two-class problem were the optimal stream weights are shown to be inversely proportional to the single stream misclassification error. These two-class results are applied to the multi-class problem by using models and \"anti-models\" (class-specific background models) thus posing the multi-class problem as multiple two-class problems. A nonlinear function of the ratio of the inter- to intra-class distance is proposed as an estimate for single stream classification error and used for stream weight estimation. The proposed unsupervised stream weight estimation algorithm is evaluated on both artificial data and on the problem of audio-visual speech recognition. It is shown that the proposed algorithm achieves \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:u_35RYKgDlwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Affective language model adaptation via corpus selection",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6854521/",
            "Abstract": "Motivated by methods used in language modeling and grammar induction, we propose the use of pragmatic constraints and perplexity as criteria to filter the unlabeled data used to generate the semantic similarity model. We investigate unsupervised adaptation algorithms of the semantic-affective models proposed in [1, 2]. Affective ratings at the utterance level are generated based on an emotional lexicon, which in turn is created using a semantic (similarity) model estimated over raw, unlabeled text. The proposed adaptation method creates task-dependent semantic similarity models and task-dependent word/term affective ratings. The proposed adaptation algorithms are tested on anger/distress detection of transcribed speech data and sentiment analysis in tweets showing significant relative classification error reduction of up to 10%.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:LjlpjdlvIbIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Structural attention neural networks for improved sentiment analysis",
            "Publication year": 2017,
            "Publication url": "https://arxiv.org/abs/1701.01811",
            "Abstract": "We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree. To our knowledge, the proposed models achieve state of the art performance on the Stanford Sentiment Treebank dataset.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ML0RJ9NH7IQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Modality tracking in the multimodal Bell Labs Communicator",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1318427/",
            "Abstract": "We describe our efforts in designing and building the multimodal Bell Labs Communicator system. Several innovations are necessary for generalizing the speech-only user interface and the semantic/pragmatic algorithms to handle both speech and visual modalities, including changes to the parser, semantic and pragmatic modules to integrate better the user input from different modalities within and across dialogue turns, as well as extending the concept of e-forms to include visual modality. The paper also describes the design of a natural, consistent and efficient multimodal user interface. We introduce the concept of \"dominant modality\": the system adaptively tracks the most probable interaction mode and suggests that modality to the user. Modality tracking provides an elegant solution to the \"turn-taking\" issue in multimodal spoken dialogue systems.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:j3f4tGmQtD8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Transition features for CRF-based speech recognition and boundary detection",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5373287/",
            "Abstract": "In this paper, we investigate a variety of spectral and time domain features for explicitly modeling phonetic transitions in speech recognition. Specifically, spectral and energy distance metrics, as well as, time derivatives of phonological descriptors and MFCCs are employed. The features are integrated in an extended Conditional Random Fields statistical modeling framework that supports general-purpose transition models. For evaluation purposes, we measure both phonetic recognition task accuracy and precision/recall of boundary detection. Results show that when transition features are used in a CRF-based recognition framework, recognition performance improves significantly due to the reduction of phone deletions. The boundary detection performance also improves mainly for transitions among silence, stop, and fricative phonetic classes.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:uWQEDVKXjbEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Region-based vocal tract length normalization for ASR",
            "Publication year": 2008,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2008/i08_1365.pdf",
            "Abstract": "In this paper, we propose a Region-based multi-parametric Vocal Tract Length Normalization (R-VTLN) algorithm for the problem of automatic speech recognition (ASR). The proposed algorithm extends the well-established mono-parametric utterance-based VTLN algorithm of Lee and Rose [1] by dividing the speech frames of a test utterance into regions and by warping independently the features corresponding to each region using a maximum likelihood criterion. We propose two algorithms for classifying frames into regions: an unsupervised clustering algorithm and an unsupervised algorithm assigning frames to regions based on phonetic-class labels obtained from a first recognition pass. We also investigate the ability of various mono-parametric and multi-parametric warping functions to reduce the spectral distance between two speakers, as a function of phone. R-VTLN is shown to significantly outperform \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:isC4tDSrTZIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A comparison of the squared energy and Teager-Kaiser operators for short-term energy estimation in additive noise",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4804767/",
            "Abstract": "Time-frequency distributions that evaluate the signal's energy content both in the time and frequency domains are indispensable signal processing tools, especially, for nonstationary signals. Various short-time energy computation schemes are used in practice, including the mean squared amplitude and Teager-Kaiser energy approaches. Herein, we focus primarily on the short- and medium-term properties of these two energy estimation schemes, as well as, on their performance in the presence of additive noise. To facilitate this analysis and generalize the approach, we use a harmonic noise model to approximate the noise component. The error analysis is conducted both in the continuous- and discrete-time domains, deriving similar conclusions. The estimation errors are measured in terms of normalized deviations from the expected signal energy and are shown to greatly depend on both the signals' spectral \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:M3NEmzRMIkIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "M 3: MultiModal Masking Applied to Sentiment Analysis}}",
            "Publication year": 2021,
            "Publication url": "https://scholar.google.com/scholar?cluster=13005806274926321234&hl=en&oi=scholarr",
            "Abstract": "A common issue when training multimodal architectures is that not all modalities contribute equally to the model\u2019s prediction and the network tends to over-rely on the strongest modality. In this work, we present M3, a training procedure based on modality masking for deep multimodal architectures. During network training, we randomly select one modality and mask its features, forcing the model to make its prediction in the absence of this modality. This structured regularization allows the network to better exploit complementary information in input modalities. We implement M3 as a generic layer that can be integrated with any multimodal architecture. Our experiments show that M3 outperforms other masking schemes and improves performance for our strong baseline. We evaluate M3 for multimodal sentiment analysis on CMU-MOSEI, achieving results comparable to the state-of-the-art.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:umqufdRvDiIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tweester at SemEval-2017 Task 4: Fusion of Semantic-Affective and pairwise classification models for sentiment analysis in Twitter",
            "Publication year": 2017,
            "Publication url": "https://www.aclweb.org/anthology/S17-2112.pdf",
            "Abstract": "In this paper, we describe our submission to SemEval2017 Task 4: Sentiment Analysis in Twitter. Specifically the proposed system participated both to tweet polarity classification (two-, three-and five class) and tweet quantification (two and five-class) tasks.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:EYYDruWGBe4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spoken dialogue evaluation for the bell labs communicator system",
            "Publication year": 2002,
            "Publication url": "https://www.researchgate.net/profile/Sungbok-Lee/publication/228531916_Spoken_dialogue_evaluation_for_the_Bell_Labs_communicator_system/links/09e4150d1fdf10b585000000/Spoken-dialogue-evaluation-for-the-Bell-Labs-communicator-system.pdf",
            "Abstract": "Task-oriented dialogues collected by the Bell Labs Communicator system are analyzed based on objective dialogue metrics and user survey data on system usability. Analyses are presented that confirm and extend some of the previous results shown for other systems. An \u201cerror correction metric\u201d is also introduced, and its effectiveness as an objective metric for system performance evaluation is investigated. Results indicate that both user satisfaction and task completion are better correlated with the error correction metric than with the recognition accuracy at the word or sentence level. Multiple regression analysis reveals that the most effective predictors of user satisfaction (among objective dialogue metrics tested in the study) are the number of user words in dialogue and the recognition accuracy at the \u201cconcept\u201d level. These two metrics explain 27.2% of variation in user satisfaction. When perceived task completion judgment (a subjective measure that is the single best predictor of user satisfaction) is included in the regression, 45.6% of variation is explained. Adding more variables, such as dialogue duration, is found to have marginal or negative effects. Overall, recognition accuracy at the concept level (as opposed to plain word or sentence accuracy) is shown to be an important metric for dialogue evaluation. In this study, the dialogue system is also evaluated using subjective metrics at each dialogue turn to identify \u201chot-spots\u201d in the dialogue. We also measure inter-labeler agreement with Communicator callers. Results indicate that it is difficult for independent human raters to reproduce the Likert ratings given initially by the Communicator \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:maZDTaKrznsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Associative and Semantic Features Extracted From Web-Harvested Corpora.",
            "Publication year": 2012,
            "Publication url": "http://www.lrec-conf.org/proceedings/lrec2012/pdf/536_Paper.pdf",
            "Abstract": "We address the problem of automatic classification of associative and semantic relations between words, and particularly those that hold between nouns. Lexical relations such as synonymy, hypernymy/hyponymy, constitute the fundamental types of semantic relations. Associative relations are harder to define, since they include a long list of diverse relations, eg,\u201cCause-Effect\u201d,\u201cInstrument-Agency\u201d. Motivated by findings from the literature of psycholinguistics and corpus linguistics, we propose features that take advantage of general linguistic properties. For evaluation we merged three datasets assembled and validated by cognitive scientists. A proposed priming coefficient that measures the degree of asymmetry in the order of appearance of the words in text achieves the best classification results, followed by context-based similarity metrics. The web-based features achieve classification accuracy that exceeds 85%.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Continuous models of affect from text using n-grams",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6639324/",
            "Abstract": "We propose a method of affective text analysis and modeling that is capable of generating continuous valence ratings at the sentence level starting from word and multi-word term valence ratings. Motivated from the language modeling literature, a back-off algorithm is employed to efficiently fuse the valence of single-word and multi-word terms. Specifically, a term detection criterion is used to select the appropriate n-gram terms, starting with bigrams and potentially backing off to unigrams. Term affective ratings are generated by a lexicon expansion method, using semantic similarity estimates computed on a large web corpus. The proposed framework provides state-of-the art results in the sentence level SemEval'07 task of news headline polarity detection, reaching an accuracy of 75%.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:q3oQSFYPqjQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Spectral moment features augmented by low order cepstral coefficients for robust ASR",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5437270/",
            "Abstract": "We propose a novel Automatic Speech Recognition (ASR) front-end, that consists of the first central Spectral Moment time-frequency distribution Augmented by low order Cepstral coefficients (SMAC). We prove that the first central spectral moment is proportional to the spectral derivative with respect to the filter's central frequency. Consequently, the spectral moment is an estimate of the frequency domain derivative of the speech spectrum. However information related to the entire speech spectrum, such as the energy and the spectral tilt, is not adequately modeled. We propose adding this information with few cepstral coefficients. Furthermore, we use a mel-spaced Gabor filterbank with 70% frequency overlap in order to overcome the sensitivity to pitch harmonics. The novel SMAC front-end was evaluated for the speech recognition task for a variety of recording conditions. The experimental results have shown that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:dfsIfKJdRG4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Ambiguity representation and resolution in spoken dialogue systems.",
            "Publication year": 2001,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.488.7829&rep=rep1&type=pdf",
            "Abstract": "Spoken natural language often contains ambiguities that must be addressed by a spoken dialogue system. In this work, we present the internal semantic representation and resolution strategy of a dialogue system designed to understand ambiguous input. These mechanisms are domain independent; task-specific knowledge is represented in parameterizable data structures. Speech input is processed through the speech recognizer, parser, interpreter, context tracker, pragmatic analyzer and pragmatic scorer. The context tracker combines dialogue context and parser output to yield raw attribute-value (AV) pairs from which candidate values are derived. The pragmatic analyzer adjusts the confidence associated with each AV candidate based on system intent, eg, implicit confirmation and user input. Pragmatic confidence scores are introduced to measure the dialogue managers confidence for each AV: MYCIN-like scoring is used to merge multiple information sources. Pragmatic analysis and scoring is combined with explicit error correction capabilities to achieve efficient ambiguity resolution. The proposed strategies greatly improve dialogue interaction, eliminating about half of the errors in dialogues from a travel reservation task.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:kNdYIx-mwKoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Root Cause Analysis of Miscommunication Hotspots in Spoken Dialogue Systems.",
            "Publication year": 2016,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2016/pdfs/1273.PDF",
            "Abstract": "A major challenge in Spoken Dialogue Systems (SDS) is the detection of problematic communication (hotspots), as well as the classification of these hotspots into different types (root cause analysis). In this work, we focus on two classes of root cause, namely, erroneous speech recognition vs. other (eg, dialogue strategy). Specifically, we propose an automatic algorithm for detecting hotspots and classifying root causes in two subsequent steps. Regarding hotspot detection, various lexico-semantic features are used for capturing repetition patterns along with affective features. Lexico-semantic and repetition features are also employed for root cause analysis. Both algorithms are evaluated with respect to the Let\u2019s Go dataset (bus information system). In terms of classification unweighted average recall, performance of 80% and 70% is achieved for hotspot detection and root cause analysis, respectively.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:AvfA0Oy_GE0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Similarity computation using semantic networks created from web-harvested data",
            "Publication year": 2015,
            "Publication url": "https://www.cambridge.org/core/journals/natural-language-engineering/article/similarity-computation-using-semantic-networks-created-from-webharvested-data/62E3431D60D1296CAF2907B6C8689E6B",
            "Abstract": "We investigate language-agnostic algorithms for the construction of unsupervised distributional semantic models using web-harvested corpora. Specifically, a corpus is created from web document snippets, and the relevant semantic similarity statistics are encoded in a semantic network. We propose the notion of semantic neighborhoods that are defined using co-occurrence or context similarity features. Three neighborhood-based similarity metrics are proposed, motivated by the hypotheses of attributional and maximum sense similarity. The proposed metrics are evaluated against human similarity ratings achieving state-of-the-art results.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:XiVPGOgt02cC",
            "Publisher": "Cambridge University Press"
        },
        {
            "Title": "Cross-topic distributional semantic representations via unsupervised mappings",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1904.05674",
            "Abstract": "In traditional Distributional Semantic Models (DSMs) the multiple senses of a polysemous word are conflated into a single vector space representation. In this work, we propose a DSM that learns multiple distributional representations of a word based on different topics. First, a separate DSM is trained for each topic and then each of the topic-based DSMs is aligned to a common vector space. Our unsupervised mapping approach is motivated by the hypothesis that words preserving their relative distances in different topic semantic sub-spaces constitute robust \\textit{semantic anchors} that define the mappings between them. Aligned cross-topic representations achieve state-of-the-art results for the task of contextual word similarity. Furthermore, evaluation on NLP downstream tasks shows that multiple topic-based embeddings outperform single-prototype models.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:PoWvk5oyLR8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "NTUA-SLP at IEST 2018: Ensemble of neural transfer methods for implicit emotion classification",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1809.00717",
            "Abstract": "In this paper we present our approach to tackle the Implicit Emotion Shared Task (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from which a certain word has been removed, we are asked to predict the emotion of the missing word. In this work, we experiment with neural Transfer Learning (TL) methods. Our models are based on LSTM networks, augmented with a self-attention mechanism. We use the weights of various pretrained models, for initializing specific layers of our networks. We leverage a big collection of unlabeled Twitter messages, for pretraining word2vec word embeddings and a set of diverse language models. Moreover, we utilize a sentiment analysis dataset for pretraining a model, which encodes emotion related information. The submitted model consists of an ensemble of the aforementioned TL models. Our team ranked 3rd out of 30 participants, achieving an F1 score of 0.703.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:oNZyr7d5Mn4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Investigations in articulatory synthesis",
            "Publication year": 2007,
            "Publication url": "http://cvsp.cs.ntua.gr/publications/confr/2007_KTMP_InvestigationsArticulatorySynthesis_ICPhS.pdf",
            "Abstract": "Modern articulatory speech synthesizers simulate the human speech production system in an increasingly accurate manner. In this direction, we relax the simplifying assumption of zero mean flow velocity during speech production and we investigate potential effects. Further, we introduce a reduced parameter set for our 3D articulatory model which simplifies its control and does not allow humanly infeasible articulations. Vowel-Fricative-Vowel synthesis experiments using our twofold augmented synthesizer are reported.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:NhqRSupF_l8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "The HIWIRE database, a noisy and non-native English speech corpus for cockpit communication",
            "Publication year": 2007,
            "Publication url": "https://www.academia.edu/download/44474025/HIWIRE_db_description_paper.pdf",
            "Abstract": "The HIWIRE database was collected to investigate humanmachine spoken dialogue communication in the cockpit. The two main adverse conditions encountered in aeronautical applications have been simulated in this database, namely, cockpit noise and non-native speech. The database collection process, application domain (command and control in the cockpit) and speaker population are described in detail. In addition, baseline speech recognition experiments are presented for the robust speech recognition task and the non-native tasks. Model adaptation results are also included in the paper, to serve as a baseline for further research in this area.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:dhFuZR0502QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Segment-based speech emotion recognition using recurrent neural networks",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8273599/",
            "Abstract": "Recently, Recurrent Neural Networks (RNNs) have produced state-of-the-art results for Speech Emotion Recognition (SER). The choice of the appropriate time-scale for Low Level Descriptors (LLDs) (local features) and statistical functionals (global features) is key for a high performing SER system. In this paper, we investigate both local and global features and evaluate the performance at various time-scales (frame, phoneme, word or utterance). We show that for RNN models, extracting statistical functionals over speech segments that roughly correspond to the duration of a couple of words produces optimal accuracy. We report state-of-the-art SER performance on the IEMOCAP corpus at a significantly lower model and computational complexity.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:KUbvn5osdkgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Combining Statistical Similarity Metrics for Automatic Induction of Semantic Classes",
            "Publication year": 2005,
            "Publication url": "https://pure.unic.ac.cy/en/publications/combining-statistical-similarity-metrics-for-automatic-induction-",
            "Abstract": "Combining Statistical Similarity Metrics for Automatic Induction of Semantic Classes \u2014 UNIC | \nResearch Portal Skip to main navigation Skip to search Skip to main content UNIC | Research \nPortal Logo Home Profiles Research Units Research output Projects Search by expertise, name \nor affiliation Combining Statistical Similarity Metrics for Automatic Induction of Semantic Classes \nApostolos Pangos, Elias Iosif, Alexandros Potamianos, Eric Fosler-Lussier Department of Digital \nInnovation Research output: Chapter in Book/Report/Conference proceeding \u203a Conference \ncontribution \u203a peer-review Overview Original language English Title of host publication \nProceedings of the 2005 IEEE International Workshop on Automatic Speech Recognition \nand Understanding (ASRU) Publication status Published - 2005 Event IEEE International \nWorkshop on Automatic Speech Recognition and Understanding - San Juan, Puerto Rico /\u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:YohjEiUPhakC",
            "Publisher": "Unknown"
        },
        {
            "Title": "SemEval-2014 Task 2: Grammar Induction for Spoken Dialogue Systems",
            "Publication year": 2014,
            "Publication url": "https://www.aclweb.org/anthology/S14-2002.pdf",
            "Abstract": "In this paper we present the SemEval-2014 Task 2 on spoken dialogue grammar induction. The task is to classify a lexical fragment to the appropriate semantic category (grammar rule) in order to construct a grammar for spoken dialogue systems. We describe four subtasks covering two languages, English and Greek, and three speech application domains, travel reservation, tourism and finance. The classification results are compared against the groundtruth. Weighted and unweighted precision, recall and fmeasure are reported. Three sites participated in the task with five systems, employing a variety of features and in some cases using external resources for training. The submissions manage to significantly beat the baseline, achieving a f-measure of 0.69 in comparison to 0.56 for the baseline, averaged across all subtasks.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:UHK10RUVsp4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Speaker adaptation for audio-visual speech recognition",
            "Publication year": 2000,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.2384",
            "Abstract": "In this paper, speaker adaptation is investigated for audiovisual automatic speech recognition# ASR# using the multistream hidden Markov model# HMM#. First, audio-only and visual-only HMM parameters are adapted bycombining maximum a posteriori and maximum likelihood linear regression adaptation. Subsequently, the audio-visual HMM stream exponents are adapted to better capture the reliabilityofeach modality for the speci# c speaker, by means of discriminative training. Various visual feature sets are compared, and features based on linear discriminant analysis are demonstrated to result in superior multispeaker and speaker-adapted recognition performance. In addition, visual feature mean normalization is shown to signi# cantly improve visual-only and audio-visual ASR performance. Adaptation experiments on a 49-subject database are reported. On average, a 28# relativeword error reduction is achieved by adapting the multi-speaker audiovisual HMM to each subject in the database. 1. INTRODU...",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:p__nRnzSRKYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal prediction of affective dimensions and depression in human-computer interactions",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2661806.2661810",
            "Abstract": "Depression is one of the most common mood disorders. Technology has the potential to assist in screening and treating people with depression by robustly modeling and tracking the complex behavioral cues associated with the disorder (eg, speech, language, facial expressions, head movement, body language). Similarly, robust affect recognition is another challenge which stands to benefit from modeling such cues. The Audio/Visual Emotion Challenge (AVEC) aims toward understanding the two phenomena and modeling their correlation with observable cues across several modalities. In this paper, we use multimodal signal processing methodologies to address the two problems using data from human-computer interactions. We develop separate systems for predicting depression levels and affective dimensions, experimenting with several methods for combining the multimodal information. The proposed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:kzcrU_BdoSEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cognitive multimodal processing: from signal to behavior",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2666253.2666264",
            "Abstract": "Affective computing, social and behavioral signal processing are emerging research disciplines that attempt to automatically label the emotional, social and cognitive state of humans using features extracted from audio-visual streams. I argue that this monumental task cannot succeed unless the particularities of the human cognitive processing are incorporated into our models, especially given that often the quantities we are called to model are either biased cognitive abstractions of the real world or altogether fictional creations of our cognition. A variety of cognitive processes that make computational modeling especially challenging are outlined herein, notably: 1)(joint) attention and saliency, 2) common ground, conceptual semantic spaces and representation learning, 3) fusion across time, modalities and cognitive representation layers, and 4) dual-system processing (system one vs. system two) and cognitive \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:_Ybze24A_UAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Developmental aspects of American English diphthong trajectories in the formant space",
            "Publication year": 2013,
            "Publication url": "https://asa.scitation.org/doi/abs/10.1121/1.4798783",
            "Abstract": "Formant trajectories of five American English diphthongs embedded in the target words BAIT (/ei/), BITE (/ai/), POUT (/au/), BOAT (/Ou/), BOYS (/oi/) are investigated in the F1-F2 space as a function of age and gender. Age range considered is from 5 to 18 years. In this report, the focus is given to the differences in position between the start/end points of diphthongs and nine monophthons. Averaged formant data across subjects in each age group are examined for the purpose. Two findings are worth mentioning. Firstly, across all age groups, the start and end positions of diphthongs hardly match with the monophthongs that are typically used to transcribe the diphthongs across all age groups (c.f., Holbrook and Fairbanks, 1962). For instance, the start position of /ei/ is very close to /I/ than to /e/, and the end points of /ei, ai, oi/ are significantly different with respect to each other. Secondly, in addition to the areas of vowel \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:eJXPG6dFmWUC",
            "Publisher": "Acoustical Society of America"
        },
        {
            "Title": "Affective Conditioning on Hierarchical Networks applied to Depression Detection from Transcribed Clinical Interviews",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2006.08336",
            "Abstract": "In this work we propose a machine learning model for depression detection from transcribed clinical interviews. Depression is a mental disorder that impacts not only the subject's mood but also the use of language. To this end we use a Hierarchical Attention Network to classify interviews of depressed subjects. We augment the attention layer of our model with a conditioning mechanism on linguistic features, extracted from affective lexica. Our analysis shows that individuals diagnosed with depression use affective language to a greater extent than not-depressed. Our experiments show that external affective information improves the performance of the proposed architecture in the General Psychotherapy Corpus and the DAIC-WoZ 2017 depression datasets, achieving state-of-the-art 71.6 and 68.6 F1 scores respectively.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ClCfbGk0d_YC",
            "Publisher": "Unknown"
        },
        {
            "Title": "AGORA: A GUI approach to multimodal user interfaces",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/abs/10.5555/1289189.1289221",
            "Abstract": "Multi-modal user interfaces bring the best of the two worlds to the user; the navigational aspect of graphical user interfaces and the declarative aspect of voice interfaces. The user can move around forms providing a concrete visual representation, while speech input allows natural form filling and form navigation. Unlike the unimodal speech interfaces, the visual interface efficiently reflects the state of the dialog as well as the speech recognition and understanding results. Agora uses a hierarchical form flow motivated from the traditional GUI design philosophy and is loosely based on VoiceXML. Agora also brings up several design issues including: ambiguity resolution, modality synchronization, focus representation, preferred modality for input and output.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:J_g5lzvAfSwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Robust AM-FM features for speech recognition",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1495427/",
            "Abstract": "In this letter, a nonlinear AM-FM speech model is used to extract robust features for speech recognition. The proposed features measure the amount of amplitude and frequency modulation that exists in speech resonances and attempt to model aspects of the speech acoustic information that the commonly used linear source-filter model fails to capture. The robustness and discriminability of the AM-FM features is investigated in combination with mel cepstrum coefficients (MFCCs). It is shown that these hybrid features perform well in the presence of noise, both in terms of phoneme-discrimination (J-measure) and in terms of speech recognition performance in several different tasks. Average relative error rate reduction up to 11% for clean and 46% for mismatched noisy conditions is achieved when AM-FM features are combined with MFCCs.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:UeHWp8X0CEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Towards adapting fantasy, curiosity and challenge in multimodal dialogue systems for preschoolers",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1647314.1647324",
            "Abstract": "We investigate how fantasy, curiosity and challenge contribute to the user experience in multimodal dialogue computer games for preschool children. For this purpose, an on-line multimodal platform has been designed, implemented and used as a starting point to develop web-based speech-enabled applications for children. Five task oriented games suitable for preschoolers have been implemented with varying levels of fantasy and curiosity elements, as well as, variable difficulty levels. Nine preschool children, ages 4-6, were asked to play these games in three sessions; in each session only one of the fantasy, curiosity or challenge factor was evaluated. Both objective and subjective criteria were used to evaluate the factors and applications. Results show that fantasy and curiosity are correlated with children's entertainment, while the level of difficulty seems to depend on each child's individual preferences and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:hMod-77fHWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Affective classification of generic audio clips using regression models.",
            "Publication year": 2013,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2013/i13_2832.pdf",
            "Abstract": "We investigate acoustic modeling, feature extraction and feature selection for the problem of affective content recognition of generic, non-speech, non-music sounds. We annotate and analyze a database of generic sounds containing a subset of the BBC sound effects library. We use regression models, longterm features and wrapper-based feature selection to model affect in the continuous 3-D (arousal, valence, dominance) emotional space. The frame-level features for modeling are extracted from each audio clip and combined with functionals to estimate long term temporal patterns over the duration of the clip. Experimental results show that the regression models provide similar categorical performance as the more popular Gaussian Mixture Models. They are also capable of predicting accurate affective ratings on continuous scales, achieving 62-67% 3-class accuracy and 0.69-0.75 correlation with human \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:eflP2zaiRacC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning of semantic relations between ontology concepts using statistical techniques",
            "Publication year": 2008,
            "Publication url": "https://www.researchgate.net/profile/Vangelis-Karkaletsis/publication/228850107_Learning_of_Semantic_Relations_between_Ontology_Concepts_using_Statistical_Techniques/links/02bfe50d1e33224649000000/Learning-of-Semantic-Relations-between-Ontology-Concepts-using-Statistical-Techniques.pdf",
            "Abstract": "Acquiring domain knowledge for constructing ontologies is a resource demanding and time consuming task. Thus, the automatic or semi-automatic construction, enrichment and adaptation of ontologies, the so-called ontology learning task is highly desired. Although an emerging field, a significant amount of research has been performed in ontology learning, leading to a large number of proposed approaches and practical systems. This paper presents our approach on automated learning of ontologies from texts which are semantically annotated with instances of ontologies\u2019 concepts. Statistical techniques are applied to metadata extracted from the annotated texts, to discover semantic relations among the annotated concepts as well as to find cardinality restrictions for these concepts and their relations. The proposed method was applied to corpora from two different domains, athletics and biomedical, and was evaluated against the existing manually created ontologies for these domains.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:fPk4N6BV_jEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "BabyExp: Constructing a Huge Multimodal Resource to Acquire Commonsense Knowledge Like Children Do.",
            "Publication year": 2010,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.8689&rep=rep1&type=pdf",
            "Abstract": "The BabyExp project is collecting very dense audio and video recordings of the first 3 years of life of a baby. The corpus constructed in this way will be transcribed with automated techniques and made available to the research community. Moreover, techniques to extract commonsense conceptual knowledge incrementally from these multimodal data are also being explored within the project. The current paper describes BabyExp in general, and presents pilot studies on the feasability of the automated audio and video transcriptions.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:3s1wT3WcHBgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Up from limited dialog systems!",
            "Publication year": 2012,
            "Publication url": "https://www.aclweb.org/anthology/W12-1801.pdf",
            "Abstract": "In the last two decades, information-seeking spoken dialog systems (SDS) have moved from research prototypes to real-life commercial applications. Still, dialog systems are limited by the scale, complexity of the task and coverage of knowledge required by problemsolving machines or mobile personal assistants. Future spoken interaction are required to be multilingual, understand and act on large scale knowledge bases in all its forms (from structured to unstructured). The Web research community have striven to build large scale and open multilingual resources (eg Wikipedia) and knowledge bases (eg Yago). We argue that a) it is crucial to leverage this massive amount of Web lightly structured knowledge and b) the scale issue can be addressed collaboratively and design open standards to make tools and resources available to the whole speech and language community.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:UxriW0iASnsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spoken Dialogue Evaluation for the Bell Labs Communicator System",
            "Publication year": 2003,
            "Publication url": "https://scholar.google.com/scholar?cluster=7040478171869879560&hl=en&oi=scholarr",
            "Abstract": "Task-oriented dialogues collected by the Bell Labs Communicator system are analyzed based on objective dialogue metrics and user survey data on system usability. Analyses are presented that confirm and extend some of the previous results shown for other systems. An error correction metric\u201d is also introduced, and its effectiveness as an objective metric for system performance evaluation is investigated. Results indicate that both user satisfaction and task completion are better correlated with the error correction metric than with the recognition accuracy at the word or sentence level. Multiple regression analysis reveals that the most effective predictors of user satisfaction (among objective dialogue metrics tested in the study) are the number of user words in dialogue and the recognition accuracy at the\" concept\" level. These two metrics explain 27. 2% of variation in user satisfaction. When perceived task completion judgment (a subjective measure that is the single best predictor of user satisfaction) is included in the regression, 45.6% of variation is explained. Adding more variables, such as dialogue duration, is found to have marginal or negative effects. Overall, recognition accuracy at the concept level (as opposed to plain word or sentence accuracy) is shown to be an important metric for dialogue evaluation. In this study, the dialogue system is also evaluated using subjective metrics at each dialogue turn to identify \u201chot-spots\u201d in the dialogue. We also measure inter-labeler agreement with Communicator callers. Results indicate that it is difficult for independent human raters to reproduce the Likert ratings given initially by the Communicator \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:rmuvC79q63oC",
            "Publisher": "Gulf Professional Publishing"
        },
        {
            "Title": "Lexical and affective models in early acquisition of semantics.",
            "Publication year": 2017,
            "Publication url": "https://www.isca-speech.org/archive_v0/WOCCI_2017/pdfs/WOCCI_2017_paper_5.pdf",
            "Abstract": "Motivated by theories of early language development in children we investigate the contribution of affective features to early acquisition of lexical semantics. For the task of semantic similarity between words, semantic and affective spaces are modeled using network-based distributed semantic models. We propose a method for constructing semantic activations from a combination of lexical and affective relations and show that affective information plays a prominent role in our lexical development model.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:tYavs44e6CUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Movie summarization based on audiovisual saliency detection",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4712308/",
            "Abstract": "Based on perceptual and computational attention modeling studies, we formulate measures of saliency for an audiovisual stream. Audio saliency is captured by signal modulations and related multi-frequency band features, extracted through nonlinear operators and energy tracking. Visual saliency is measured by means of a spatiotemporal attention model driven by various feature cues (intensity, color, motion). Audio and video curves are integrated in a single attention curve, where events may be enhanced, suppressed or vanished. The presence of salient events is signified on this audiovisual curve by geometrical features such as local extrema, sharp transition points and level sets. An audiovisual saliency-based movie summarization algorithm is proposed and evaluated. The algorithm is shown to perform very well in terms of summary informativeness and enjoyability for movie clips of various genres.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ULOm3_A8WrAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Speech Emotion Recognition Using Affective Saliency.",
            "Publication year": 2016,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2016/pdfs/1311.PDF",
            "Abstract": "We investigate an affective saliency approach for speech emotion recognition of spoken dialogue utterances that estimates the amount of emotional information over time. The proposed saliency approach uses a regression model that combines features extracted from the acoustic signal and the posteriors of a segment-level classifier to obtain frame or segment-level ratings. The affective saliency model is trained using a minimum classification error (MCE) criterion that learns the weights by optimizing an objective loss function related to the classification error rate of the emotion recognition system. Affective saliency scores are then used to weight the contribution of frame-level posteriors and/or features to the speech emotion classification decision. The algorithm is evaluated for the task of anger detection on four call-center datasets for two languages, Greek and English, with good results.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:uWiczbcajpAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Ntua-slp at semeval-2018 task 2: Predicting emojis using rnns with context-aware attention",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1804.06657",
            "Abstract": "In this paper we present a deep-learning model that competed at SemEval-2018 Task 2 \"Multilingual Emoji Prediction\". We participated in subtask A, in which we are called to predict the most likely associated emoji in English tweets. The proposed architecture relies on a Long Short-Term Memory network, augmented with an attention mechanism, that conditions the weight of each word, on a \"context vector\" which is taken as the aggregation of a tweet's meaning. Moreover, we initialize the embedding layer of our model, with word2vec word embeddings, pretrained on a dataset of 550 million English tweets. Finally, our model does not rely on hand-crafted features or lexicons and is trained end-to-end with back-propagation. We ranked 2nd out of 48 teams.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:b1wdh0AR-JQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Metrics for measuring domain independence of semantic classes.",
            "Publication year": 2001,
            "Publication url": "http://www.cs.cmu.edu/~./dod/papers/PAGE447.pdf",
            "Abstract": "The design of dialogue systems for a new domain requires semantic classes (concepts) to be identified and defined. This process could be made easier by importing relevant concepts from previously studied domains to the new one. We propose two methodologies, based on comparison of semantic classes across domains, for determining which concepts are domain-independent, and which are specific to the new task. The concept-comparison technique uses a context-dependent Kullback-Leibler distance measurement to compare all pairwise combinations of semantic classes, one from each domain. The concept-projection method uses a similar metric to project a single semantic class from one domain into the lexical environment of another. Initial results show that both methods are good indicators of the degree of domain independence for a wide range of concepts, manually generated for three different tasks: Carmen (children\u2019s game), Movie (information retrieval) and Travel (flight reservations).",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:mVmsd5A6BfQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Predicting audio-visual salient events based on visual, audio and text modalities for movie summarization",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7351630/",
            "Abstract": "In this paper, we present a new and improved synergistic approach to the problem of audio-visual salient event detection and movie summarization based on visual, audio and text modalities. Spatio-temporal visual saliency is estimated through a perceptually inspired frontend based on 3D (space, time) Gabor filters and frame-wise features are extracted from the saliency volumes. For the auditory salient event detection we extract features based on Teager-Kaiser Energy Operator, while text analysis incorporates part-of-speech tagging and affective modeling of single words on the movie subtitles. For the evaluation of the proposed system, we employ an elementary and non-parametric classification technique like KNN. Detection results are reported on the MovSum database, using objective evaluations against ground-truth denoting the perceptually salient events, and human evaluations of the movie summaries \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:j8SEvjWlNXcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A codec for speech recognition in a wireless system",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/874767/",
            "Abstract": "We consider a distributed speech recognition system where representative parameters of the speech signal are extracted at the wireless terminal and transmitted to a centralized automatic speech recognition (ASR) server. Several error protection schemes are proposed for the ASR feature stream. In addition, a \"soft-feature\" error concealment strategy is introduced at the ASR server that uses the marginal distribution of only the reliable features during likelihood computation. The performance of the error protection and concealment schemes is evaluated over typical cellular wireless channels and it is shown to reduce ASR error rate up to 65% for certain channels.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:RHpTSmoSYBkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Auto-induced semantic classes",
            "Publication year": 2004,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167639304000408",
            "Abstract": "Advanced computer dialogue agents contain a natural language understanding component that requires knowledge of semantic classes and concepts. These are frequently generated manually for new tasks. We avoid this time-consuming procedure by using a two-step unsupervised clustering process. First, a semantic generalizer automatically induces semantic classes using training data from well-studied applications (domains) for which large transcribed corpora of human\u2013human dialogues are available. Candidate word pairs are grouped into similar semantic groups according to the similarity of their lexical bigram contexts. We show that the proposed algorithms for automatically inducing semantic classes perform very well for typical spoken dialogue applications. We exceed 90% precision for the first 100 cluster assignments for narrowly defined tasks such as a movie information task. For a heterogeneous \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:4DMP91E08xMC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Word Semantic Similarity for Morphologically Rich Languages.",
            "Publication year": 2014,
            "Publication url": "https://www.academia.edu/download/45874384/Word_Semantic_Similarity_for_Morphologic20160523-32293-17z95le.pdf",
            "Abstract": "In this work, we investigate the role of morphology in the performance of semantic similarity for morphologically rich languages, such as German and Greek. The challenge in processing languages with richer morphology than English, lies in reducing estimation error while addressing the semantic distortion that a reduction of the vocabulary size entails. For this purpose, we propose a methodology for selective stemming, based on a semantic distortion metric. This was tested for the task of similarity estimation between words using two types of corpus-based similarity metrics: co-occurrence-based and context-based. The performance of morphologically rich languages is boosted by stemming with the context-based metric, unlike English, where the best results are obtained by the co-occurrence-based metric. Overall, the key finding is that the estimation error reduction is different, if a word is used as a feature, where errors are averaged typically among many different features, than when it is used as a target word.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:ZuybSZzF8UAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving speech recognition for children using acoustic adaptation and pronunciation modeling.",
            "Publication year": 2014,
            "Publication url": "https://slp-ntua.github.io/potam/preprints/conf/2014_WOCCI_children_ASR.pdf",
            "Abstract": "Developing a robust Automatic Speech Recognition (ASR) system for children is a challenging task because of increased variability in acoustic and linguistic correlates as function of young age. The acoustic variability is mainly due to the developmental changes associated with vocal tract growth. On the linguistic side, the variability is associated with limited knowledge of vocabulary, pronunciations and other linguistic constructs. This paper presents a preliminary study towards better acoustic modeling, pronunciation modeling and front-end processing for children\u2019s speech. Results are presented as a function of age. Speaker adaptation significantly reduces mismatch and variability improving recognition results across age groups. In addition, introduction of pronunciation modeling shows promising performance improvements.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:W5xh706n7nkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A review of the acoustic and linguistic properties of children's speech",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4412809/",
            "Abstract": "In this paper, we review the acoustic and linguistic properties of children's speech for both read and spontaneous speech. First, the effect of developmental changes on the absolute values and variability of acoustic correlates is presented for read speech for children ages 6 and up. Then, verbal child-machine spontaneous interaction is reviewed and results from recent studies are presented. Age trends of acoustic, linguistic and interaction parameters are discussed, such as sentence duration, filled pauses, politeness and frustration markers, and modality usage. Some differences between child-machine and human-human interaction are pointed out. The implications for acoustic modeling, linguistic modeling and spoken dialogue systems design for children are discussed.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:hC7cP41nSMkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Speech data augmentation",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US20200335086A1/en",
            "Abstract": "Data augmentation is used for speech emotion recognition tasks where certain emotional labels, eg, sadness, are significantly underrepresented in a training dataset. This is typical for data collected in real-life applications. We propose conditioned data augmentation using Generative Adversarial Networks (GANs), in order to generate samples for underrepresented emotions. We propose a conditional GAN architecture to generate synthetic spectrograms for the minority class. For comparison purposes, we implement a series of signal-based data augmentation methods. Results on the speech emotion recognition task show that the proposed data augmentation method significantly improves classification performance as compared to traditional speech data augmentation methods.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:_axFR9aDTf0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "tucSage: Grammar Rule Induction for Spoken Dialogue Systems via Probabilistic Candidate Selection",
            "Publication year": 2014,
            "Publication url": "https://www.aclweb.org/anthology/S14-2119.pdf",
            "Abstract": "We describe the grammar induction system for Spoken Dialogue Systems (SDS) submitted to SemEval\u201914: Task 2. A statistical model is trained with a rich feature set and used for the selection of candidate rule fragments. Posterior probabilities produced by the fragment selection model are fused with estimates of phraselevel similarity based on lexical and contextual information. Domain and language portability are among the advantages of the proposed system that was experimentally validated for three thematically different domains in two languages.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:dQ2og3OwTAUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A supervised approach to movie emotion tracking",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5946961/",
            "Abstract": "In this paper, we present experiments on continuous time, continuous scale affective movie content recognition (emotion tracking). A major obstacle for emotion research has been the lack of appropriately annotated databases, limiting the potential for supervised algorithms. To that end we develop and present a database of movie affect, an notated in continuous time, on a continuous valence-arousal scale. Supervised learning methods are proposed to model the continuous affective response using hidden Markov Models (independent) in each dimension. These models classify each video frame into one of seven discrete categories (in each dimension); the discrete-valued curves are then converted to continuous values via spline interpolation. A variety of audio-visual features are investigated and an optimal feature set is selected. The potential of the method is experimentally verified on twelve 30-minute movie \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:yD5IFk8b50cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Creating conversational interfaces for children",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/985544/",
            "Abstract": "Creating conversational interfaces for children is challenging in several respects. These include acoustic modeling for automatic speech recognition (ASR), language and dialog modeling, and multimodal-multimedia user interface design. First, issues in ASR of children's speech are introduced by an analysis of developmental changes in the spectral and temporal characteristics of the speech signal using data obtained from 456 children, ages five to 18 years. Acoustic modeling adaptation and vocal tract normalization algorithms that yielded state-of-the-art ASR performance on children's speech are described. Second, an experiment designed to better understand how children interact with machines using spoken language is described. Realistic conversational multimedia interaction data were obtained from 160 children who played a voice-activated computer game in a Wizard of Oz (WoZ) scenario. Results of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "pBQViyUAAAAJ:qjMakFHDy7sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Hierarchical bidirectional attention-based RNN in BioCreative VI precision medicine track, document triage task",
            "Publication year": 2017,
            "Publication url": "https://slp-ntua.github.io/potam/preprints/conf/2017_BIOCREATIVE_aris.pdf",
            "Abstract": "In this paper, we describe our submission to the\u201d Document Triage Task\u201d, of the BioCreative VI Precision Medicine Track, in which we ranked first among ten teams. The submitted system is a Hierarchical Bidirectional Attention-Based Recurrent Neural Network (RNN). Our approach utilizes the hierarchical nature of documents, which are composed of sequences of sentences, where sentences are composed of sequences of words. We propose a reusable sequence encoder architecture, which is used as sentence and document encoder. The sequence encoder, is composed of a bidirectional RNN, equipped with an attention mechanism, which identifies and captures the most important elements (words or sentences) in a sequence. Furthermore, we argue that the title of the paper itself, usually contains important information, compared to the other sentences of the abstract. For this reason, we propose a shortcut connection, which integrates the title\u2019s vector representation, directly to the final feature representation of the document. We leverage word embeddings, trained on PubMed, for initializing the embedding layer of our network. Moreover, our system does not rely on handcrafted features. Furthermore, we train our system end-to-end using back-propagation, with stochastic gradient descent. We make the source code available to the research community 1.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:lmc2jWPfTJgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised semantic similarity computation using web search engines",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4427120/",
            "Abstract": "In this paper, we propose two novel web-based metrics for semantic similarity computation between words. Both metrics use a web search engine in order to exploit the retrieved information for the words of interest. The first metric considers only the page counts returned by a search engine, based on the work of [1]. The second downloads a number of the top ranked documents and applies \"widecontext\" and \"narrow-context\" metrics. The proposed metrics work automatically, without consulting any human annotated knowledge resource. The metrics are compared with WordNet-based methods. The metrics' performance is evaluated in terms of correlation with respect to the pairs of the commonly used Charles - Miller dataset. The proposed \"wide-context\" metric achieves 71% correlation, which is the highest score achieved among the fully unsupervised metrics in the literature up to date.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:hFOr9nPyWt4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Cross-domain classification using generalized domain acts.",
            "Publication year": 2000,
            "Publication url": "https://www.isca-speech.org/archive_v0/icslp_2000/i00_3502.html",
            "Abstract": "Cross-domain classification for speech understanding is an interesting research problem because of the need for portable solutions in the design for spoken dialogue systems. In this paper, a two-tier classifier is proposed for speech understanding. The first tier consists of domain independent dialogue acts while the second tier consists of application actions that are domain specific. A maximum likelihood and a minimum classification error formulation are proposed for the first tier of the classifier, ie, for dialogue act classification. The performance of the classifier is investigated for three application domains. Cross-domain classification error is two to four times higher than in-domain classification error. A 10-15% reduction in cross-domain classification error rate is achieved by adding generic domain independent training data for each dialogue act and by mapping words to semantic concepts.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:7T2F9Uy0os0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "DARPA communicator: cross-system results for the 2001 evaluation.",
            "Publication year": 2002,
            "Publication url": "http://www.cs.cmu.edu/~air/papers/commeval01-7.pdf",
            "Abstract": "This paper describes the evaluation methodology and results of the 2001 DARPA Communicator evaluation. The experiment spanned 6 months of 2001 and involved eight DARPA Communicator systems in the travel planning domain. It resulted in a corpus of 1242 dialogs which include many more dialogues for complex tasks than the 2000 evaluation. We describe the experimental design, the approach to data collection, and the results. We compare the results by the type of travel plan and by system. The results demonstrate some large differences across sites and show that the complex trips are clearly more difficult.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:wbdj-CoPYUoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal user interface for augmented assembly",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4412822/",
            "Abstract": "In this paper, a multimodal system for augmented reality aided assembly work is designed and implemented. The multimodal interface allows for speech and gestural input. The system emulates a simplified assembly task in a factory. A 3D puzzle is used to study how to implement the augmented assembly system to a real setting in a factory. The system is used as a demonstrator and as a test-bed to evaluate different input modalities for augmented assembly setups. Preliminary system evaluation results are presented, the user experience is discussed, and some directions for future work are given.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:rO6llkc54NcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sensory-Aware Multimodal Fusion for Word Semantic Similarity Estimation",
            "Publication year": 2017,
            "Publication url": "https://www.eurasip.org/Proceedings/Eusipco/Eusipco2017/wpapers/ML4.pdf",
            "Abstract": "Traditional semantic models are disembodied from the human perception and action. In this work, we attempt to address this problem by grounding semantic representations of words to the acoustic and visual modalities. Specifically we estimate multimodal word representations via the fusion of auditory and visual modalities with the text modality. We employ middle and late fusion of representations with modality weights assigned to each of the unimodal representations. We also propose a fusion method that assigns different weights to each word, based on how relevant that word is for the audio and visual modalities. The proposed methods are evaluated for the task of semantic similarity computation between words. To our knowledge, this is the first work that combines text, audio and visual features for the computation of multimodal semantic word representations. Multimodal models outperform the unimodal models, indicating the importance of multimodal fusion and perceptual grounding.",
            "Abstract entirety": 1,
            "Author pub id": "pBQViyUAAAAJ:g3aElNc5_aQC",
            "Publisher": "Unknown"
        }
    ]
}]