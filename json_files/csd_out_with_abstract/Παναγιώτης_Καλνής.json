[{
    "name": "\u03a0\u03b1\u03bd\u03b1\u03b3\u03b9\u03ce\u03c4\u03b7\u03c2 \u039a\u03b1\u03bb\u03bd\u03ae\u03c2",
    "romanize name": "Panagiotis Kalnis",
    "School-Department": "\u0395\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03c2 \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "King Abdullah University of Science & Technology",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 1946,
    "Scholar name": "Panos Kalnis",
    "Scholar id": "-NdSrrYAAAAJ",
    "Affiliation": "Professor of Computer Science, King Abdullah University of Science and Technology (KAUST)",
    "Citedby": 10647,
    "Interests": [
        "Big Data",
        "Cloud computing",
        "Parallel Systems",
        "Graphs",
        "Privacy"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=-NdSrrYAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "GRACE: A compressed communication framework for distributed machine learning",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9546514/",
            "Abstract": "Powerful computer clusters are used nowadays to train complex deep neural networks (DNN) on large datasets. Distributed training increasingly becomes communication bound. For this reason, many lossy compression techniques have been proposed to reduce the volume of transferred data. Unfortunately, it is difficult to argue about the behavior of compression methods, because existing work relies on inconsistent evaluation testbeds and largely ignores the performance impact of practical system configurations. In this paper, we present a comprehensive survey of the most influential compressed communication methods for DNN training, together with an intuitive classification (i.e., quantization, sparsification, hybrid and low-rank). Next, we propose GRACE, a unified framework and API that allows for consistent and easy implementation of compressed communication on popular machine learning toolkits. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:0Kh4an1R61UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Top-k OLAP Queries Using Augmented Spatial Access Methods.",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=1690547555670225088&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:JV2RwH3_ST0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Local and global recoding methods for anonymizing set-valued data",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-010-0192-8",
            "Abstract": "In this paper, we study the problem of protecting privacy in the publication of set-valued data. Consider a collection of supermarket transactions that contains detailed information about items bought together by individuals. Even after removing all personal characteristics of the buyer, which can serve as links to his identity, the publication of such data is still subject to privacy attacks from adversaries who have partial knowledge about the set. Unlike most previous works, we do not distinguish data as sensitive and non-sensitive, but we consider them both as potential quasi-identifiers and potential sensitive data, depending on the knowledge of the adversary. We define a new version of the k-anonymity guarantee, the k  m -anonymity, to limit the effects of the data dimensionality, and we propose efficient algorithms to transform the database. Our anonymization model relies on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:dhFuZR0502QC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Repeatability & workability evaluation of SIGMOD 2009",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1815933.1815944",
            "Abstract": "SIGMOD 2008 was the first database conference that offered to test submitters' programs against their data to verify the repeatability of the experiments published [1]. Given the positive feedback concerning the SIGMOD 2008 repeatability initiative, SIGMOD 2009 modified and expanded the initiative with a workability assessment.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:4DMP91E08xMC",
            "Publisher": "ACM"
        },
        {
            "Title": "Distributed Spatial Databases.",
            "Publication year": 2009,
            "Publication url": "https://scholar.google.com/scholar?cluster=3475575683477945896&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:blknAaTinKkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Matrix algebra framework for portable, scalable and efficient query engines for RDF graphs",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3302424.3303962",
            "Abstract": "Existing query engines for RDF graphs follow one of two design paradigms: relational or graph-based. We explore sparse matrix algebra as a third paradigm and propose MAGiQ: a framework for implementing SPARQL query engines that are portable on various hardware architectures, scalable over thousands of compute nodes, and efficient for very large RDF datasets. MAGiQ represents the RDF graph as a sparse matrix and defines a domain-specific language of algebraic operations. SPARQL queries are translated into matrix algebra programs that are oblivious to the underlying computing infrastructure. Existing matrix algebra libraries, optimized for each particular architecture, are called to execute the program and handle the performance issues. We present three case studies of matrix algebra back-end libraries: SuiteSparse, MATLAB, and CombBLAS; we demonstrate how MAGiQ can effortlessly be ported \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:DwWRdx-KAo4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Daiet: a system for data aggregation inside the network",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3127479.3132018",
            "Abstract": "Many data center applications nowadays rely on distributed computation models like MapReduce and Bulk Synchronous Parallel (BSP) for data-intensive computation at scale [4]. These models scale by leveraging the partition/aggregate pattern where data and computations are distributed across many worker servers, each performing part of the computation. A communication phase is needed each time workers need to synchronize the computation and, at last, to produce the final output. In these applications, the network communication costs can be one of the dominant scalability bottlenecks especially in case of multi-stage or iterative computations [1].",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:h9G0ZmjYpDoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the discrepancy between the theoretical analysis and practical implementations of compressed communication for distributed deep learning",
            "Publication year": 2020,
            "Publication url": "https://ojs.aaai.org/index.php/AAAI/article/view/5793",
            "Abstract": "Compressed communication, in the form of sparsification or quantization of stochastic gradients, is employed to reduce communication costs in distributed data-parallel training of deep neural networks. However, there exists a discrepancy between theory and practice: while theoretical analysis of most existing compression methods assumes compression is applied to the gradients of the entire model, many practical implementations operate individually on the gradients of each layer of the model.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:wgKq3sYidysC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Preserving anonymity in location based services",
            "Publication year": 2006,
            "Publication url": "https://137.132.84.186/handle/1900.100/2215",
            "Abstract": "The increasing trend of embedding positioning capabilities (e.g., GPS) in mobile devices facilitates the widespread use of Location Based Services. For such applications to succeed, the privacy and confidentiality issues are of paramount importance. Existing techniques, like encryption, safeguard the communication channel from eavesdroppers. Nevertheless, the queries themselves may disclose the physical location, identity and habits of the user.   In this paper, we present a framework for preserving the anonymity of users issuing spatial queries to Location Based Services. We propose transformations based on the well-established K-anonymity technique to compute exact answers for Range and Nearest Neighbor queries, without revealing sensitive information about the user. Our methods optimize the entire process of anonymizing the requests and processing the transformed spatial queries. Extensive experimental studies suggest that our methods are applicable to real-life scenarios with numerous mobile users.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:yKzB5RS27GgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Semantics in the Deep: Semantic Analytics for Big Data",
            "Publication year": 2019,
            "Publication url": "https://www.mdpi.com/2306-5729/4/2/63/htm",
            "Abstract": "One cannot help but classify the continuous birth and demise of Artificial Intelligence (AI) trends into the everlasting theme of the battle between connectionist and symbolic AI. During the past decade, the Semantic Web has brought a flavor of intelligence into everyday web browsing and has remained a hot research topic for years. The advent of big data has posed challenges in its scalability; at the same time, it has offered fertile ground for machine learning and computational intelligence techniques to flourish and to optimize, in previously intractable fields. But a remnant of the disbelief that meant such methods were shunned in the past, when compared to the sound foundations of logic, still remains, despite their aweing achievements. As a result, two fundamental questions naturally arise:",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:J6OZcwVsj5AC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Multi-query optimization for on-line analytical processing",
            "Publication year": 2003,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0306437902000261",
            "Abstract": "Multi-dimensional expressions (MDX) provide an interface for asking several related OLAP queries simultaneously. An interesting problem is how to optimize the execution of an MDX query, given that most data warehouses maintain a set of redundant materialized views to accelerate OLAP operations. A number of greedy and approximation algorithms have been proposed for different versions of the problem. In this paper we evaluate experimentally their performance, concluding that they do not scale well for realistic workloads. Motivated by this fact, we develop two novel greedy algorithms. Our algorithms construct the execution plan in a top\u2013down manner by identifying in each step the most beneficial view, instead of finding the most promising query. We show by extensive experimentation that our methods outperform the existing ones in most cases.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:roLk4NBRz8UC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Automatic tuning of bag-of-tasks applications",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7113338/",
            "Abstract": "This paper presents APlug, a framework for automatic tuning of large scale applications of many independent tasks. APlug suggests the best decomposition of the original computation into smaller tasks and the best number of CPUs to use, in order to meet user-specific constraints. We show that the problem is not trivial because there is large variability in the execution time of tasks, and it is possible for a task to occupy a CPU by performing useless computations. APlug collects a sample of task execution times and builds a model, which is then used by a discrete event simulator to calculate the optimal parameters. We provide a C++ API and a stand-alone implementation of APlug, and we integrate it with three typical applications from computational chemistry, bioinformatics, and data mining. A scenario for optimizing resources utilization is used to demonstrate our framework. We run experiments on 16,384 CPUs on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:dhpJJ7xvgBgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Incremental frequent subgraph mining on large evolving graphs",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8014497/",
            "Abstract": "Frequent subgraph mining is a core graph operation used in many domains, such as graph data management and knowledge exploration, bioinformatics, and security. Most existing techniques target static graphs. However, modern applications, such as social networks, utilize large evolving graphs. Mining these graphs using existing techniques is infeasible, due to the high computational cost. In this paper, we propose IncGM+, a fast incremental approach for continuous frequent subgraph mining on a single large evolving graph. We adapt the notion of \u201cfringe\u201d to the graph context, that is the set of subgraphs on the border between frequent and infrequent subgraphs. IncGM+ maintains fringe subgraphs and exploits them to prune the search space. To boost the efficiency, we propose an efficient index structure to maintain selected embeddings with minimal memory overhead. These embeddings are utilized to avoid \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:sYWwZaPVD1oC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Real datasets for file-sharing peer-to-peer systems",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11408079_19",
            "Abstract": "The fundamental drawback of unstructured peer-to-peer (P2P) networks is the flooding-based query processing protocol that seriously limits their scalability. As a result, a significant amount of research work has focused on designing efficient search protocols that reduce the overall communication cost. What is lacking, however, is the availability of real data, regarding the exact content of users\u2019 libraries and the queries that these users ask. Using trace-driven simulations will clearly generate more meaningful results and further illustrate the efficiency of a generic query processing protocol under a real-life scenario.Motivated by this fact, we developed a Gnutella-style probe and collected detailed data over a period of two months. They involve around 4,500 users and contain the exact files shared by each user, together with any available metadata (e.g., artist for songs) and information about the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:hqOjcs7Dif8C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Introduction to spatio-temporal data driven urban computing",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s10619-020-07300-3",
            "Abstract": "This special issue of Distributed and Parallel Databases journal covers recent advances in spatio-temporal data analytics in the context of urban computing. It contains 9 articles that present solid research studies and innovative ideas in the area of spatio-temporal data analytics for urban computing applications. All of the 9 papers went through at least two rounds of rigorous reviews by the guest editors and invited reviewers.Location-based recommender systems are becoming increasingly important in the community of urban computing. The paper, by Hao Zhou et al.,\u201cHybrid route recommendation with taxi and shared bicycles,\u201d develops a two-phase data-driven recommendation framework that integrates prediction and recommendation phases for providing reliable route recommendation results. Another paper, by Hao Zhang et al.,\u201cOn accurate POI recommendation via transfer learning,\u201d proposes a transfer \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:CLhUwle04lcC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Collective travel planning in spatial networks",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7360162/",
            "Abstract": "Travel planning and recommendation are important aspects of transportation. We propose and investigate a novel Collective Travel Planning (CTP) query that finds the lowest-cost route connecting multiple sources and a destination, via at most    meeting points. When multiple travelers target the same destination (e.g., a stadium or a theater), they may want to assemble at meeting points and then go together to the destination by public transport to reduce their global travel cost (e.g., energy, money, or greenhouse-gas emissions). This type of functionality holds the potential to bring significant benefits to society and the environment, such as reducing energy consumption and greenhouse-gas emissions, enabling smarter and greener transportation, and reducing traffic congestions. The CTP query is Max SNP-hard. To compute the query efficiently, we develop two algorithms, including an exact algorithm and an approximation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:bbjcffOLshcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Scheduling Broadcasts in a Network of Timelines",
            "Publication year": 2016,
            "Publication url": "https://arxiv.org/abs/1610.06052",
            "Abstract": "Broadcasts and timelines are the primary mechanism of information exchange in online social platforms today. Services like Facebook, Twitter and Instagram have enabled ordinary people to reach large audiences spanning cultures and countries, while their massive popularity has created increasingly competitive marketplaces of attention. Timing broadcasts to capture the attention of such geographically diverse audiences has sparked interest from many startups and social marketing gurus. However, formal study is lacking on both the timing and frequency problems. We study for the first time the broadcast scheduling problem of specifying the timing and frequency of publishing content to maximise the attention received. We validate and quantify three interacting behavioural phenomena to parametrise social platform users: information overload, bursty circadian rhythms and monotony aversion, which is defined here for the first time. We formalise a timeline information exchange process based on these phenomena, and formulate an objective function that quantifies the expected collective attention. We finally present experiments on real data from Twitter, where we discover a counter-intuitive scheduling strategy that outperforms popular heuristics while producing fewer posts.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:ZYsTHYU9jrMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Static and dynamic view selection in distributed data warehouse systems",
            "Publication year": 2002,
            "Publication url": "https://search.proquest.com/openview/e5e21108a66c21076592c4a0556d273d/1?pq-origsite=gscholar&cbl=18750&diss=y",
            "Abstract": "View materialization is commonly used to accelerate On-Line Analytical Processing (OLAP) operations. Views may be materialized at any of the three layers of a decision support system:(i) at the server side (ie the data warehouse),(ii) at a mid-tier, between the server and the client and,(iii) at the client side. Typically, static view selection is used at the server side, while dynamic approaches are employed for the other two cases.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:Zbx7W2Xs4QsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Errata for\" Lightning Fast and Space Efficient Inequality Joins\"(PVLDB 8 (13): 2074--2085)",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/3099622.3099629",
            "Abstract": "This is in response to recent feedback from some readers, which requires some clarifications regarding our IEJoin algorithm published in [1]. The feedback revolves around four points: (1) a typo in our illustrating example of the join process; (2) a naming error for the index used by our algorithm to improve the bit array scan; (3) the sort order used in our algorithms; and (4) a missing explanation on how duplicates are handled by our self join algorithm.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:LlXTz_FrCmAC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Efficient and accurate nearest neighbor and closest pair search in high-dimensional space",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1806907.1806912",
            "Abstract": "Nearest Neighbor (NN) search in high-dimensional space is an important problem in many applications. From the database perspective, a good solution needs to have two properties: (i) it can be easily incorporated in a relational database, and (ii) its query cost should increase sublinearly with the dataset size, regardless of the data and query distributions. Locality-Sensitive Hashing (LSH) is a well-known methodology fulfilling both requirements, but its current implementations either incur expensive space and query cost, or abandon its theoretical guarantee on the quality of query results.Motivated by this, we improve LSH by proposing an access method called the Locality-Sensitive B-tree (LSB-tree) to enable fast, accurate, high-dimensional NN search in relational databases. The combination of several LSB-trees forms a LSB-forest that has strong quality guarantees, but improves dramatically the efficiency of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:aqlVkmm33-oC",
            "Publisher": "ACM"
        },
        {
            "Title": "Privacy-preserving publication of user locations in the proximity of sensitive sites",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-69497-7_9",
            "Abstract": "Location-based services, such as on-line maps, obtain the exact location of numerous mobile users. This information can be published for research or commercial purposes. However, privacy may be compromised if a user is in the proximity of a sensitive site (e.g., hospital). To preserve privacy, existing methods employ the K-anonymity paradigm to hide each affected user in a group that contains at least K\u2009\u2212\u20091 other users. Nevertheless, current solutions have the following drawbacks: (i) they may fail to achieve anonymity, (ii) they may cause excessive distortion of location data and (iii) they incur high computational cost.In this paper, we define formally the attack model and discuss the conditions that guarantee privacy. Then, we propose two algorithms which employ 2-D to 1-D transformations to anonymize the locations of users in the proximity of sensitive sites. The first algorithm, called MK \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:_kc_bZDykSQC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Method and apparatus for scheduling broadcasts in social networks",
            "Publication year": 2018,
            "Publication url": "https://patents.google.com/patent/US20180007151A1/en",
            "Abstract": "A method, apparatus, and computer readable medium are provided for maximizing consumption of broadcasts by a producer. An example method includes receiving selection of a total number of time slots to use for scheduling broadcasts, and receiving information regarding the producer's followers. The example method further 5 includes identifying, by a processor and based on the received information, discount factors associated with the producer's followers, and calculating, by the processor and based on the received information, a predicted number of competitor broadcasts during each time slot of the total number of time slots. Finally, the example method includes determining, by the processor and based on the discount factors and the predicted 10 number of competitor broadcasts during each time slot, a number of broadcasts for the producer to transmit in each time slot of the total number of time slots.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:oYwriLWYh5YC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Parallel motif extraction from very long sequences",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2505515.2505575",
            "Abstract": "Motifs are frequent patterns used to identify biological functionality in genomic sequences, periodicity in time series, or user trends in web logs. In contrast to a lot of existing work that focuses on collections of many short sequences, modern applications require mining of motifs in one very long sequence (ie, in the order of several gigabytes). For this case, there exist statistical approaches that are fast but inaccurate; or combinatorial methods that are sound and complete. Unfortunately, existing combinatorial methods are serial and very slow. Consequently, they are limited to very short sequences (ie, a few megabytes), small alphabets (typically 4 symbols for DNA sequences), and restricted types of motifs.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:YFjsv_pBGBYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A framework for efficient data anonymization under privacy and accuracy constraints",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1538909.1538911",
            "Abstract": "Recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy-preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual's record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. However, existing approaches suffer from at least one of the following drawbacks: (i) l-diversification is solved by techniques developed for the simpler k-anonymization problem, causing unnecessary information loss. (ii) The anonymization process is inefficient in terms of computational and I/O cost. (iii) Previous research focused exclusively on the privacy-constrained problem and ignored the equally important accuracy-constrained (or dual) anonymization problem.In this article, we propose a framework for efficient anonymization of microdata that addresses \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:MXK_kJrjxJIC",
            "Publisher": "ACM"
        },
        {
            "Title": "Personalized trajectory matching in spatial networks",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-013-0331-0",
            "Abstract": "With the increasing availability of moving-object tracking data, trajectory search and matching is increasingly important. We propose and investigate a novel problem called personalized trajectory matching (PTM). In contrast to conventional trajectory similarity search by spatial distance only, PTM takes into account the significance of each sample point in a query trajectory. A PTM query takes a trajectory with user-specified weights for each sample point in the trajectory as its argument. It returns the trajectory in an argument data set with the highest similarity to the query trajectory. We believe that this type of query may bring significant benefits to users in many popular applications such as route planning, carpooling, friend recommendation, traffic analysis, urban computing, and location-based services in general. PTM query processing faces two challenges: how to prune the search space during the query \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:BqipwSGYUEgC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Preventing location-based identity inference in anonymous spatial queries",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4358948/",
            "Abstract": "The increasing trend of embedding positioning capabilities (for example, GPS) in mobile devices facilitates the widespread use of location-based services. For such applications to succeed, privacy and confidentiality are essential. Existing privacy-enhancing techniques rely on encryption to safeguard communication channels, and on pseudonyms to protect user identities. Nevertheless, the query contents may disclose the physical location of the user. In this paper, we present a framework for preventing location-based identity inference of users who issue spatial queries to location-based services. We propose transformations based on the well-established K-anonymity concept to compute exact answers for range and nearest neighbor search, without revealing the query source. Our methods optimize the entire process of anonymizing the requests and processing the transformed spatial queries. Extensive \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:QAOzB4mb83kC",
            "Publisher": "IEEE"
        },
        {
            "Title": "DeepReduce: A Sparse-tensor Communication Framework for Distributed Deep Learning",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2102.03112",
            "Abstract": "Sparse tensors appear frequently in distributed deep learning, either as a direct artifact of the deep neural network's gradients, or as a result of an explicit sparsification process. Existing communication primitives are agnostic to the peculiarities of deep learning; consequently, they impose unnecessary communication overhead. This paper introduces DeepReduce, a versatile framework for the compressed communication of sparse tensors, tailored for distributed deep learning. DeepReduce decomposes sparse tensors in two sets, values and indices, and allows both independent and combined compression of these sets. We support a variety of common compressors, such as Deflate for values, or run-length encoding for indices. We also propose two novel compression schemes that achieve superior results: curve fitting-based for values and bloom filter-based for indices. DeepReduce is orthogonal to existing gradient sparsifiers and can be applied in conjunction with them, transparently to the end-user, to significantly lower the communication overhead. As proof of concept, we implement our approach on Tensorflow and PyTorch. Our experiments with large real models demonstrate that DeepReduce transmits fewer data and imposes lower computational overhead than existing methods, without affecting the training accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:gnsKu8c89wgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "SABRE: a Sensitive Attribute Bucketization and REdistribution framework for t-closeness",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-010-0191-9",
            "Abstract": "Today, the publication of microdata poses a privacy threat: anonymous personal records can be re-identified using third data sources. Past research has tried to develop a concept of privacy guarantee that an anonymized data set should satisfy before publication, culminating in the notion of t-closeness. To satisfy t-closeness, the records in a data set need to be grouped into Equivalence Classes (ECs), such that each EC contains records of indistinguishable quasi-identifier values, and its local distribution of sensitive attribute (SA) values conforms to the global table distribution of SA values. However, despite this progress, previous research has not offered an anonymization algorithm tailored for t-closeness. In this paper, we cover this gap with SABRE, a SA Bucketization and REdistribution framework for t-closeness. SABRE first greedily partitions a table into buckets of similar SA values and then \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Privacy preservation in the publication of sparse multidimensional data",
            "Publication year": 2010,
            "Publication url": "https://scholar.google.com/scholar?cluster=7575829217516111475&hl=en&oi=scholarr",
            "Abstract": "Nowadays, most human activities leave some trace in a digital registry. Whether one buys something in a supermarket or gets admitted in the hospital, an entry to her credit card history or to her medical record will be made. Data collected this way can be used to improve the quality of services, eg, companies can adjust faster to the needs of the consumers, or even contribute to science advancement; doctors can study the medical histories of thousands or even millions of patients. On the other hand, such data collections pose a significant threat to the privacy of the individuals that are associated with them. Even if some information that directly identifies a person (eg, the name or the social security number) is removed from a database record, the individual that is associated to the record can be identified by linking the remaining information (eg, age, zip code of residence) to external knowledge (eg, a public voters table).",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:oXKBmVzQOggC",
            "Publisher": "CRC press"
        },
        {
            "Title": "Introduction to Spatio-temporal data management and analytics for Smart City research",
            "Publication year": 2020,
            "Publication url": "https://vbn.aau.dk/en/publications/introduction-to-spatio-temporal-data-management-and-analytics-for",
            "Abstract": "This special issue of the GeoInformatica journal covers recent advances in spatio-temporal data management and analytics in the context of smart city and urban computing. It contains 11 articles that present solid research studies and innovative ideas in the area of spatio-temporal data management for smart city research. All of the 11 papers went through several rounds of rigorous reviews by the guest editors and invited reviewers.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:3WNXLiBY60kC",
            "Publisher": "Springer"
        },
        {
            "Title": "Evaluation of top-k olap queries using aggregate r\u2013trees",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11535331_14",
            "Abstract": "A top-k OLAP query groups measures with respect to some abstraction level of interesting dimensions and selects the k groups with the highest aggregate value. An example of such a query is \u201cfind the 10 combinations of product-type and month with the largest sum of sales\u201d. Such queries may also be applied in a spatial database context, where objects are augmented with some measures that must be aggregated according to a spatial division. For instance, consider a map of objects (e.g., restaurants), where each object carries some non-spatial measure (e.g., the number of customers served during the last month). Given a partitioning of the space into regions (e.g., by a regular grid), the goal is to find the regions with the highest number of served customers. A straightforward method to evaluate a top-k OLAP query is to compute the aggregate value for each group and then select the groups with the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:kNdYIx-mwKoC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Ad-hoc distributed spatial joins on mobile devices",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1639266/",
            "Abstract": "PDAs, cellular phones and other mobile devices are now capable of supporting complex data manipulation operations. Here, we focus on ad-hoc spatial joins of datasets residing in multiple non-cooperative servers. Assuming that there is no mediator available, the spatial joins must be evaluated on the mobile device. Contrary to common applications that consider the cost at the server side, our main issue is the minimization of the transferred data, while meeting the resource constraints of the device. We show that existing methods, based on partitioning and pruning, are inadequate in many realistic situations. Then, we present novel algorithms that estimate the data distribution before deciding the physical operator independently for each partition. Our experiments with a prototype implementation on a WiFi-enabled PDA, suggest that the proposed methods outperform the competitors in terms of efficiency and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:IWHjjKOFINEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Query optimizations over decentralized RDF graphs",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7929955/",
            "Abstract": "Applications in life sciences, decentralized social networks, Internet of Things, and statistical linked dataspaces integrate data from multiple decentralized RDF graphs via SPARQL queries. Several approaches have been proposed to optimize query processing over a small number of heterogeneous data sources by utilizing schema information. In the case of schema similarity and interlinks among sources, these approaches cause unnecessary data retrieval and communication, leading to poor scalability and response time. This paper addresses these limitations and presents Lusail, a system for scalable and efficient SPARQL query processing over decentralized graphs. Lusail achieves scalability and low query response time through various optimizations at compile and run times. At compile time, we use a novel locality-aware query decomposition technique that maximizes the number of query triple patterns \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:MqTxh1vmwXEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimization of spatial joins on mobile devices",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-45072-6_14",
            "Abstract": "Mobile devices like PDAs are capable of retrieving information from various types of services. In many cases, the user requests cannot directly be processed by the service providers, if their hosts have limited query capabilities or the query combines data from various sources, which do not collaborate with each other. In this paper, we present a framework for optimizing spatial join queries that belong to this class. We presume that the connection and queries are ad-hoc, there is no mediator available and the services are non-collaborative. We also assume that the services are not willing to share their statistics or indexes with the client. We retrieve statistics dynamically in order to generate a low-cost execution plan, while considering the storage and computational power limitations of the PDA. Since acquiring the statistics causes overhead, we describe an adaptive algorithm that optimizes the overall process \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:0EnyYjriUFMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Accelerating SPARQL queries by exploiting hash-based locality and adaptive partitioning",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s00778-016-0420-y.pdf",
            "Abstract": "State-of-the-art distributed RDF systems partition data across multiple computer nodes (workers). Some systems perform cheap hash partitioning, which may result in expensive query evaluation. Others try to minimize inter-node communication, which requires an expensive data preprocessing phase, leading to a high startup cost. Apriori knowledge of the query workload has also been used to create partitions, which, however, are static and do not adapt to workload changes. In this paper, we propose AdPart, a distributed RDF system, which addresses the shortcomings of previous work. First, AdPart applies lightweight partitioning on the initial data, which distributes triples by hashing on their subjects; this renders its startup overhead low. At the same time, the locality-aware query optimizer of AdPart takes full advantage of the partitioning to (1) support the fully parallel processing of join patterns on subjects \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:Yw6v6SrDvuUC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Distributed Privacy Preserving Data Collection using Cryptographic Techniques",
            "Publication year": 2009,
            "Publication url": "https://www.researchgate.net/profile/Panos-Kalnis/publication/229005407_Distributed_Privacy_Preserving_Data_Collection_using_Cryptographic_Techniques/links/0912f508aa7b4c8349000000/Distributed-Privacy-Preserving-Data-Collection-using-Cryptographic-Techniques.pdf",
            "Abstract": "We study the distributed k-anonymous data collection problem: a data collector (eg, a medical research institute) wishes to collect data (eg, medical records) from a group of respondents (eg, patients). Each respondent owns a multiattributed record which contains both non-sensitive (eg, quasiidentifiers) and sensitive information (eg, a particular disease), and submits it to the data collector. Assuming T is the table formed by all the respondent data records, we say that the data collection process is k-anonymous if it allows the data collector to obtain a k-anonymized version of T without revealing the original records to any adversary. In contrast to most k-anonymization approaches which trust the data collector, our work assumes that the adversary can be any third party, including the data collector and the other responders.We propose a distributed data collection protocol that outputs a k-anonymized table by generalization of quasi-identifier attributes. The protocol employs cryptographic techniques such as homomorphic encryption, private information retrieval and secure multiparty computation to ensure the privacy goal in the process of data collection. Meanwhile, the protocol is designed to leak limited but non-critical information (mainly statistical information about the non-sensitive attributes of the data respondents) to achieve practicability and efficiency. Experiments show that the utility of the k-anonymized table derived by our protocol is in par with the utility achieved by traditional k-anonymization techniques that trust the data collector.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:qbqt7gslDFUC",
            "Publisher": "Stanford University, Technical Report"
        },
        {
            "Title": "Karect: accurate correction of substitution, insertion and deletion errors for next-generation sequencing data",
            "Publication year": 2015,
            "Publication url": "https://academic.oup.com/bioinformatics/article-abstract/31/21/3421/195621",
            "Abstract": " Motivation: Next-generation sequencing generates large amounts of data affected by errors in the form of substitutions, insertions or deletions of bases. Error correction based on the high-coverage information, typically improves de novo assembly. Most existing tools can correct substitution errors only; some support insertions and deletions, but accuracy in many cases is low. Results: We present Karect, a novel error correction technique based on multiple alignment. Our approach supports substitution, insertion and deletion errors. It can handle non-uniform coverage as well as moderately covered areas of the sequenced genome. Experiments with data from Illumina, 454 FLX and Ion Torrent sequencing machines demonstrate that Karect is more accurate than previous methods, both in terms of correcting individual-bases errors (up to 10% increase in accuracy gain) and post de novo \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:wlzmIqt2EaEC",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "Indexing Spatio-Temporal Data Ware-houses",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=9966801067863908836&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:eIKNFFVQvJAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "AUC-MF: point of interest recommendation with AUC maximization",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8731461/",
            "Abstract": "The task of point of interest (POI) recommendation aims to recommend unvisited places to users based on their check-in history. A major challenge in POI recommendation is data sparsity, because a user typically visits only a very small number of POIs among all available POIs. In this paper, we propose AUC-MF to address the POI recommendation problem by maximizing Area Under the ROC curve (AUC). AUC has been widely used for measuring classification performance with imbalanced data distributions. To optimize AUC, we transform the recommendation task to a classification problem, where the visited locations are positive examples and the unvisited are negative ones. We define a new lambda for AUC to utilize the LambdaMF model, which combines the lambda-based method and matrix factorization model in collaborative filtering. Experiments on two datasets show that the proposed AUC-MF \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:qhW0HyKmSusC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Anonymity in unstructured data",
            "Publication year": 2008,
            "Publication url": "https://www.academia.edu/download/43882580/TR-2008-04.pdf",
            "Abstract": "In this paper we study the problem of protecting privacy in the publication of set-valued data. Consider a collection of transactional data that contains detailed information about items bought together by individuals. Even after removing all personal characteristics of the buyer, which can serve as links to his identity, the publication of such data is still subject to privacy attacks from adversaries who have partial knowledge about the set. Unlike most previous works, we do not distinguish data as sensitive and non-sensitive, but we consider them both as potential quasi-identifiers and potential sensitive data, depending on the point of view of the adversary. We define a new version of the k-anonymity guarantee, the km-anonymity, to limit the effects of the data dimensionality and we propose efficient algorithms to transform the database. Our anonymization model relies on generalization instead of suppression, which is the most common practice in related works on such data. We develop an algorithm which finds the optimal solution, however, at a high cost which makes it inapplicable for large, realistic problems. Then, we propose two greedy heuristics, which scale much better and in most of the cases find a solution close to the optimal. The proposed algorithms are experimentally evaluated using real datasets.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:ULOm3_A8WrAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Parallel trajectory similarity joins in spatial networks",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-018-0502-0",
            "Abstract": "The matching of similar pairs of objects, called similarity join, is fundamental functionality in data management. We consider two cases of trajectory similarity joins (TS-Joins), including a threshold-based join (Tb-TS-Join) and a top-k TS-Join (k-TS-Join), where the objects are trajectories of vehicles moving in road networks. Given two sets of trajectories and a threshold , the Tb-TS-Join returns all pairs of trajectories from the two sets with similarity above . In contrast, the k-TS-Join does not take a threshold as a parameter, and it returns the top-k most similar trajectory pairs from the two sets. The TS-Joins target diverse applications such as trajectory near-duplicate detection, data cleaning, ridesharing recommendation, and traffic congestion prediction. With these applications in mind, we provide purposeful definitions of similarity. To enable efficient processing of the TS-Joins on large sets of trajectories, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:rJyh6hJnyfgC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Top-k term publish/subscribe for geo-textual data streams",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-020-00607-8",
            "Abstract": "Massive amounts of data that contain spatial, textual, and temporal information are being generated at a rapid pace. With streams of such data, which includes check-ins and geo-tagged tweets, available, users may be interested in being kept up-to-date on which terms are popular in the streams in a particular region of space. To enable this functionality, we aim at efficiently processing two types of general top-k term subscriptions over streams of spatio-temporal documents: region-based top-k spatial-temporal term (RST) subscriptions and similarity-based top-k spatio-temporal term (SST) subscriptions. RST subscriptions continuously maintain the top-k most popular trending terms within a user-defined region. SST subscriptions free users from defining a region and maintain top-k locally popular terms based on a ranking function that combines term frequency, term recency, and term proximity. To solve the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:Kv9jytqXTosC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Parallel algorithm for incremental betweenness centrality on large graphs",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8070346/",
            "Abstract": "Betweenness centrality quantifies the importance of nodes in a graph in many applications, including network analysis, community detection and identification of influential users. Typically, graphs in such applications evolve over time. Thus, the computation of betweenness centrality should be performed incrementally. This is challenging because updating even a single edge may trigger the computation of all-pairs shortest paths in the entire graph. Existing approaches cannot scale to large graphs: they either require excessive memory (i.e., quadratic to the size of the input graph) or perform unnecessary computations rendering them prohibitively slow. We propose iCENTRAL; a novel incremental algorithm for computing betweenness centrality in evolving graphs. We decompose the graph into biconnected components and prove that processing can be localized within the affected components. iCENTRAL is the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:ON9TkA72AHEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Supplementary Material for: DRABAL: novel method to mine large high-throughput screening assays using Bayesian active learning",
            "Publication year": 2016,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/624144",
            "Abstract": "Background Mining high-throughput screening (HTS) assays is key for enhancing decisions in the area of drug repositioning and drug discovery. However, many challenges are encountered in the process of developing suitable and accurate methods for extracting useful information from these assays. Virtual screening and a wide variety of databases, methods and solutions proposed to-date, did not completely overcome these challenges. This study is based on a multi-label classification (MLC) technique for modeling correlations between several HTS assays, meaning that a single prediction represents a subset of assigned correlated labels instead of one label. Thus, the devised method provides an increased probability for more accurate predictions of compounds that were not tested in particular assays. Results Here we present DRABAL, a novel MLC solution that incorporates structure learning of a Bayesian network as a step to model dependency between the HTS assays. In this study, DRABAL was used to process more than 1.4 million interactions of over 400,000 compounds and analyze the existing relationships between five large HTS assays from the PubChem BioAssay Database. Compared to different MLC methods, DRABAL significantly improves the F1Score by about 22%, on average. We further illustrated usefulness and utility of DRABAL through screening FDA approved drugs and reported ones that have a high probability to interact with several targets, thus enabling drug-multi-target repositioning. Specifically DRABAL suggests the Thiabendazole drug as a common activator of the NCP1 and Rab-9A proteins, both of which \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:_n8fIOMweQoC",
            "Publisher": "figshare"
        },
        {
            "Title": "StarDB: a large-scale DBMS for strings",
            "Publication year": 2015,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/578861",
            "Abstract": "Strings and applications using them are proliferating in science and business. Currently, strings are stored in file systems and processed using ad-hoc procedural code. Existing techniques are not flexible and cannot efficiently handle complex queries or large datasets. In this paper, we demonstrate StarDB, a distributed database system for analytics on strings. StarDB hides data and system complexities and allows users to focus on analytics. It uses a comprehensive set of parallel string operations and provides a declarative query language to solve complex queries. StarDB automatically tunes itself and runs with over 90% efficiency on supercomputers, public clouds, clusters, and workstations. We test StarDB using real datasets that are 2 orders of magnitude larger than the datasets reported by previous works.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:mgoTDWlsYNUC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Approximate and exact hybrid algorithms for private nearest-neighbor queries with database protection",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/article/10.1007/s10707-010-0121-4",
            "Abstract": "Mobile devices with global positioning capabilities allow users to retrieve points of interest (POI) in their proximity. To protect user privacy, it is important not to disclose exact user coordinates to un-trusted entities that provide location-based services. Currently, there are two main approaches to protect the location privacy of users: (i) hiding locations inside cloaking regions (CRs) and (ii) encrypting location data using private information retrieval (PIR) protocols. Previous work focused on finding good trade-offs between privacy and performance of user protection techniques, but disregarded the important issue of protecting the POI dataset D. For instance, location cloaking requires large-sized CRs, leading to excessive disclosure of POIs (O(|D|) in the worst case). PIR, on the other hand, reduces this bound to , but at the expense of high processing and communication overhead. We propose hybrid \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:HDshCWvjkbEC",
            "Publisher": "Springer US"
        },
        {
            "Title": "To 4,000 compute nodes and beyond: Network-aware vertex placement in large-scale graph processing systems",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2534169.2491726",
            "Abstract": "The explosive growth of \"big data\" is giving rise to a new breed of large scale graph systems, such as Pregel. This poster describes our ongoing work in characterizing and minimizing the communication cost of Bulk Synchronous Parallel (BSP) graph mining systems, like Pregel, when scaling to 4,096 compute nodes. Existing implementations generally assume a fixed communication cost. This is sufficient in small deployments as the BSP programming model (i.e., overlapping computation and communication) masks small variations in the underlying network. In large scale deployments, such variations can dominate the overall runtime characteristics. In this poster, we first quantify the impact of network communication on the total compute time of a Pregel system. We then propose an efficient vertex placement strategy that subsamples highly connected vertices and applies the Reverse Cuthill-McKee (RCM \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:RJujIP1NYNUC",
            "Publisher": "ACM"
        },
        {
            "Title": "Discovery of path nearby clusters in spatial networks",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6990621/",
            "Abstract": "The discovery of regions of interest in large cities is an important challenge. We propose and investigate a novel query called the path nearby cluster (PNC) query that finds regions of potential interest (e.g., sightseeing places and commercial districts) with respect to a user-specified travel route. Given a set of spatial objects O (e.g., POIs, geo-tagged photos, or geo-tagged tweets) and a query route q, if a cluster c has high spatial-object density and is spatially close to q, it is returned by the query (a cluster is a circular region defined by a center and a radius). This query aims to bring important benefits to users in popular applications such as trip planning and location recommendation. Efficient computation of the PNC query faces two challenges: how to prune the search space during query processing, and how to identify clusters with high density effectively. To address these challenges, a novel collective search \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:dMpQl7XwOw4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "In-network computation is a dumb idea whose time has come",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3152434.3152461",
            "Abstract": "Programmable data plane hardware creates new opportunities for infusing intelligence into the network. This raises a fundamental question: what kinds of computation should be delegated to the network?",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:sfsSB7lKuh0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Anonymous publication of sensitive transactional data",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5487522/",
            "Abstract": "Existing research on privacy-preserving data publishing focuses on relational data: in this context, the objective is to enforce privacy-preserving paradigms, such as k-anonymity and \u2113-diversity, while minimizing the information loss incurred in the anonymizing process (i.e., maximize data utility). Existing techniques work well for fixed-schema data, with low dimensionality. Nevertheless, certain applications require privacy-preserving publishing of transactional data (or basket data), which involve hundreds or even thousands of dimensions, rendering existing methods unusable. We propose two categories of novel anonymization methods for sparse high-dimensional data. The first category is based on approximate nearest-neighbor (NN) search in high-dimensional spaces, which is efficiently performed through locality-sensitive hashing (LSH). In the second category, we propose two data transformations that capture \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:L8Ckcad2t8MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Privacy Preservation in the Publication of Sparse Multidimensional Data",
            "Publication year": 2010,
            "Publication url": "https://www.taylorfrancis.com/chapters/privacy-preservation-publication-sparse-multidimensional-data-manolis-terrovitis-nikos-mamoulis-panos-kalnis/e/10.1201/b10373-9",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:j6V8Syvup0UC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "Outsourcing search services on private spatial data",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4812485/",
            "Abstract": "Social networking and content sharing service providers, e.g., Facebook and Google Maps, enable their users to upload and share a variety of user-generated content, including location data such as points of interest. Users wish to share location data through an (untrusted) service provider such that trusted friends can perform spatial queries on the data. We solve the problem by transforming the location data before uploading them. We contribute spatial transformations that re-distribute locations in space and a transformation that employs cryptographic techniques. The data owner selects transformation keys and shares them with the trusted friends. Without the keys, it is infeasible for an attacker to reconstruct the exact original data points from the transformed points. These transformations achieve different tradeoffs between query efficiency and data security. In addition, we describe an attack model for studying the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "PRIVE: anonymous location-based queries in distributed mobile systems",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1242572.1242623",
            "Abstract": "Nowadays, mobile users with global positioning devices canaccess Location Based Services (LBS) and query about pointsof interest in their proximity. For such applications to succeed, privacy and confidentiality are essential. Encryptionalone is not adequate; although it safeguards the systemagainst eavesdroppers, the queries themselves may disclosethe location and identity of the user. Recently, there havebeen proposed centralized architectures based on K-anonymity, which utilize an intermediate anonymizer between themobile users and the LBS. However, the anonymizer mustbe updated continuously with the current locations of allusers. Moreover, the complete knowledge of the entire systemposes a security threat, if the anonymizer is compromised. In this paper we address two issues:(i) We show thatexisting approaches may fail to provide spatial anonymityfor some distributions of user locations and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:9yKSN-GCB0IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Advances in Spatial and Temporal Databases: 9th International Symposium, SSTD 2005, Angra dos Reis, Brazil, August 22-24, 2005",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=7532014816168410669&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:4uoR24qA-WYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ITISS: an efficient framework for querying big temporal data",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s10707-019-00362-1",
            "Abstract": "In the real word, temporal data can be found in many applications, and it is rapidly increasing nowadays. It is urgently important and challenging to manage and operate big temporal data efficiently and effectively, due to the large volume of big temporal data and the real-time response requirement. Processing big temporal data using a distributed system is a desired choice, since a single-machine based system usually has the limited computing ability. Nevertheless, existing distributed systems or methods either are disk-based solutions, or cannot support native queries, which may not well meet the demands of low latency and high throughput. To attack these issues, this article suggests a new approach to handle big temporal data. Our approach is an In-memory based Two-level Index Solution in Spark, dubbed as ITISS. The proposed framework of our solution is easily understood and implemented, but without \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:xlVdBZVQT58C",
            "Publisher": "Springer US"
        },
        {
            "Title": "Rethinking gradient sparsification as total error minimization",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2108.00951",
            "Abstract": "Gradient compression is a widely-established remedy to tackle the communication bottleneck in distributed training of large deep neural networks (DNNs). Under the error-feedback framework, Top- sparsification, sometimes with  as little as  of the gradient size, enables training to the same model quality as the uncompressed case for a similar iteration count. From the optimization perspective, we find that Top- is the communication-optimal sparsifier given a per-iteration  element budget. We argue that to further the benefits of gradient sparsification, especially for DNNs, a different perspective is necessary -- one that moves from per-iteration optimality to consider optimality for the entire training. We identify that the total error -- the sum of the compression errors for all iterations -- encapsulates sparsification throughout training. Then, we propose a communication complexity model that minimizes the total error under a communication budget for the entire training. We find that the hard-threshold sparsifier, a variant of the Top- sparsifier with  determined by a constant hard-threshold, is the optimal sparsifier for this model. Motivated by this, we provide convex and non-convex convergence analyses for the hard-threshold sparsifier with error-feedback. Unlike with Top- sparsifier, we show that hard-threshold has the same asymptotic convergence and linear speedup property as SGD in the convex case and has no impact on the data-heterogeneity in the non-convex case. Our diverse experiments on various DNNs and a logistic regression model demonstrated that the hard-threshold sparsifier is more communication-efficient than Top-.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:7XUxBq3GufIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A demonstration of Lusail: Querying linked data at scale",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3035918.3058731",
            "Abstract": "There has been a proliferation of datasets available as interlinked RDF data accessible through SPARQL endpoints. This has led to the emergence of various applications in life science, distributed social networks, and Internet of Things that need to integrate data from multiple endpoints.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:10A3hb2jUxYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Pivoted Subgraph Isomorphism: The Optimist, the Pessimist and the Realist.",
            "Publication year": 2019,
            "Publication url": "https://scholar.google.com/scholar?cluster=9303775392086304163&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:oE_QS-WwsdAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Private queries in location based services: anonymizers are not necessary",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1376616.1376631",
            "Abstract": "Mobile devices equipped with positioning capabilities (eg, GPS) can ask location-dependent queries to Location Based Services (LBS). To protect privacy, the user location must not be disclosed. Existing solutions utilize a trusted anonymizer between the users and the LBS. This approach has several drawbacks:(i) All users must trust the third party anonymizer, which is a single point of attack.(ii) A large number of cooperating, trustworthy users is needed.(iii) Privacy is guaranteed only for a single snapshot of user locations; users are not protected against correlation attacks (eg, history of user movement).",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:d1gkVwhDpl0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Parallel trajectory-to-location join",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8409287/",
            "Abstract": "The matching between trajectories and locations, called Trajectory-to-Location join (TL-Join), is fundamental functionality in spatiotemporal data management. Given a set of trajectories, a set of locations, and a threshold 8, the TL-Join finds all (trajectory, location) pairs from the two sets with spatiotemporal correlation above 8. This join targets diverse applications, including location recommendation, event tracking, and trajectory activity analyses. We address three challenges in relation to the TL-Join: how to define the spatiotemporal correlation between trajectories and locations, how to prune the search space effectively when computing the join, and how to perform the computation in parallel. Specifically, we define new metrics to measure the spatiotemporal correlation between trajectories and locations. We develop a novel parallel collaborative (PCol) search method based on a divide-and-conquer strategy. For \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:aNch6Af-aFkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A general framework for searching in distributed data repositories",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1213117/",
            "Abstract": "This paper proposes a general framework for searching large distributed repositories. Examples of such repositories include sites with music/video content, distributed digital libraries, distributed caching systems, etc. The framework is based on the concept of neighborhood; each client keeps a list of the most beneficial sites according to past experience, which are visited first when the client searches for some particular content. Exploration methods continuously update the neighborhoods in order to follow changes in access patterns. Depending on the application, several variations of search and exploration processes are proposed. Experimental evaluation demonstrates the benefits of the framework in different scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On discovering moving clusters in spatio-temporal data",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11535331_21",
            "Abstract": "A moving cluster is defined by a set of objects that move close to each other for a long time interval. Real-life examples are a group of migrating animals, a convoy of cars moving in a city, etc. We study the discovery of moving clusters in a database of object trajectories. The difference of this problem compared to clustering trajectories and mining movement patterns is that the identity of a moving cluster remains unchanged while its location and content may change over time. For example, while a group of animals are migrating, some animals may leave the group or new animals may enter it. We provide a formal definition for moving clusters and describe three algorithms for their automatic discovery: (i) a straight-forward method based on the definition, (ii) a more efficient method which avoids redundant checks and (iii) an approximate algorithm which trades accuracy for speed by borrowing ideas from the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Tracking moving objects in anonymized trajectories",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-85654-2_19",
            "Abstract": "Multiple target tracking (MTT) is a well-studied technique in the field of radar technology, which associates anonymized measurements with the appropriate object trajectories. This technique, however, suffers from combinatorial explosion, since each new measurement may potentially be associated with any of the existing tracks. Consequently, the complexity of existing MTT algorithms grows exponentially with the number of objects, rendering them inapplicable to large databases. In this paper, we investigate the feasibility of applying the MTT framework in the context of large trajectory databases. Given a history of object movements, where the corresponding object ids have been removed, our goal is to track the trajectory of every object in the database in successive timestamps. Our main contribution lies in the transition from an exponential solution to a polynomial one. We introduce a novel method that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:4TOpqqG69KYC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Progress and challenges in bioinformatics approaches for enhancer identification",
            "Publication year": 2016,
            "Publication url": "https://academic.oup.com/bib/article-abstract/17/6/967/2606433",
            "Abstract": "Enhancers are cis-acting DNA elements that play critical roles in distal regulation of gene expression. Identifying enhancers is an important step for understanding distinct gene expression programs that may reflect normal and pathogenic cellular conditions. Experimental identification of enhancers is constrained by the set of conditions used in the experiment. This requires multiple experiments to identify enhancers, as they can be active under specific cellular conditions but not in different cell types/tissues or cellular states. This has opened prospects for computational prediction methods that can be used for high-throughput identification of putative enhancers to complement experimental approaches. Potential functions and properties of predicted enhancers have been catalogued and summarized in several enhancer-oriented databases. Because the current methods for the computational prediction of enhancers \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:ZqLB7dQ1iF4C",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "Parallel semantic trajectory similarity join",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9101683/",
            "Abstract": "Matching similar pairs of trajectories, called trajectory similarity join, is a fundamental functionality in spatial data management. We consider the problem of semantic trajectory similarity join (STS-Join). Each semantic trajectory is a sequence of Points-of-interest (POIs) with both location and text information. Thus, given two sets of semantic trajectories and a threshold \u03b8, the STS-Join returns all pairs of semantic trajectories from the two sets with spatio-textual similarity no less than \u03b8. This join targets applications such as term-based trajectory near-duplicate detection, geo-text data cleaning, personalized ridesharing recommendation, keyword-aware route planning, and travel itinerary recommendation.With these applications in mind, we provide a purposeful definition of spatio-textual similarity. To enable efficient STS-Join processing on large sets of semantic trajectories, we develop trajectory pair filtering techniques \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:WHBERAHVdrEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Efficient OLAP operations in spatial data warehouses",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-47724-1_23",
            "Abstract": "Spatial databases store information about the position of individual objects in space. In many applications however, such as traffic supervision or mobile communications, only summarized data, like the number of cars in an area or phones serviced by a cell, is required. Although this information can be obtained from transactional spatial databases, its computation is expensive, rendering online processing inapplicable. Driven by the non-spatial paradigm, spatial data warehouses can be constructed to accelerate spatial OLAP operations. In this paper we consider the star-schema and we focus on the spatial dimensions. Unlike the non-spatial case, the groupings and the hierarchies can be numerous and unknown at design time, therefore the well-known materialization techniques are not directly applicable. In order to address this problem, we construct an ad-hoc grouping hierarchy based on the spatial \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:u5HHmVD_uO8C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Hi-Jack: a novel computational framework for pathway-based inference of host\u2013pathogen interactions",
            "Publication year": 2015,
            "Publication url": "https://academic.oup.com/bioinformatics/article-abstract/31/14/2332/254936",
            "Abstract": " Motivation: Pathogens infect their host and hijack the host machinery to produce more progeny pathogens. Obligate intracellular pathogens, in particular, require resources of the host to replicate. Therefore, infections by these pathogens lead to alterations in the metabolism of the host, shifting in favor of pathogen protein production. Some computational identification of mechanisms of host\u2013pathogen interactions have been proposed, but it seems the problem has yet to be approached from the metabolite-hijacking angle. Results: We propose a novel computational framework, Hi-Jack, for inferring pathway-based interactions between a host and a pathogen that relies on the idea of metabolite hijacking. Hi-Jack searches metabolic network data from hosts and pathogens, and identifies candidate reactions where hijacking occurs. A novel scoring function ranks candidate hijacked reactions and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:qSd0DAb9jMoC",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "Lightning fast and space efficient inequality joins",
            "Publication year": 2015,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/593180",
            "Abstract": "Inequality joins, which join relational tables on inequality conditions, are used in various applications. While there have been a wide range of optimization methods for joins in database systems, from algorithms such as sort-merge join and band join, to various indices such as B+-tree, R*-tree and Bitmap, inequality joins have received little attention and queries containing such joins are usually very slow. In this paper, we introduce fast inequality join algorithms. We put columns to be joined in sorted arrays and we use permutation arrays to encode positions of tuples in one sorted array w.r.t. the other sorted array. In contrast to sort-merge join, we use space efficient bit-arrays that enable optimizations, such as Bloom filter indices, for fast computation of the join results. We have implemented a centralized version of these algorithms on top of PostgreSQL, and a distributed version on top of Spark SQL. We have compared against well known optimization techniques for inequality joins and show that our solution is more scalable and several orders of magnitude faster.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:8JTMrWI6FdcC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "A benchmark for betweenness centrality approximation algorithms on large graphs",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3085504.3085510",
            "Abstract": "Betweenness centrality quantifies the importance of graph nodes in a variety of applications including social, biological and communication networks. Its computation is very costly for large graphs; therefore, many approximate methods have been proposed. Given the lack of a golden standard, the accuracy of most approximate methods is evaluated on tiny graphs and is not guaranteed to be representative of realistic datasets that are orders of magnitude larger. In this paper, we develop BeBeCA, a benchmark for betweenness centrality approximation methods on large graphs. Specifically:(i) We generate a golden standard by deploying a parallel implementation of Brandes algorithm using 96,000 CPU cores on a supercomputer to compute exact betweenness centrality values for several large graphs with up to 126M edges.(ii) We propose an evaluation methodology to assess various aspects of approximation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:UKB2i6g6GNMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A hybrid technique for private location-based queries with database protection",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-02982-0_9",
            "Abstract": "Mobile devices with global positioning capabilities allow users to retrieve points of interest (POI) in their proximity. To protect user privacy, it is important not to disclose exact user coordinates to un-trusted entities that provide location-based services. Currently, there are two main approaches to protect the location privacy of users: (i) hiding locations inside cloaking regions (CRs) and (ii) encrypting location data using private information retrieval (PIR) protocols. Previous work focused on finding good trade-offs between privacy and performance of user protection techniques, but disregarded the important issue of protecting the POI dataset D. For instance, location cloaking requires large-sized CRs, leading to excessive disclosure of POIs (O(|D|) in the worst case). PIR, on the other hand, reduces this bound to , but at the expense of high processing and communication overhead.We propose \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "RACE: A scalable and elastic parallel system for discovering repeats in very long sequences",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/2536206.2536214",
            "Abstract": "A wide range of applications, including bioinformatics, time series, and log analysis, depend on the identification of repetitions in very long sequences. The problem of finding maximal pairs subsumes most important types of repetition-finding tasks. Existing solutions require both the input sequence and its index (typically an order of magnitude larger than the input) to fit in memory. Moreover, they are serial algorithms with long execution time. Therefore, they are limited to small datasets, despite the fact that modern applications demand orders of magnitude longer sequences.In this paper we present RACE, a parallel system for finding maximal pairs in very long sequences. RACE supports parallel execution on stand-alone multicore systems, in addition to scaling to thousands of nodes on clusters or supercomputers. RACE does not require the input or the index to fit in memory; therefore, it supports very long \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:GnPB-g6toBAC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "ACME: A scalable parallel system for extracting frequent patterns from a very long sequence",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-014-0370-1",
            "Abstract": "Modern applications, including bioinformatics, time series, and web log analysis, require the extraction of frequent patterns, called motifs, from one very long (i.e., several gigabytes) sequence. Existing approaches are either heuristics that are error-prone, or exact (also called combinatorial) methods that are extremely slow, therefore, applicable only to very small sequences (i.e., in the order of megabytes). This paper presents ACME, a combinatorial approach that scales to gigabyte-long sequences and is the first to support supermaximal motifs. ACME is a versatile parallel system that can be deployed on desktop multi-core systems, or on thousands of CPUs in the cloud. However, merely using more compute nodes does not guarantee efficiency, because of the related overheads. To this end, ACME introduces an automatic tuning mechanism that suggests the appropriate number of CPUs to utilize, in order \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:KqW5X_olkfQC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Discriminative identification of transcriptional responses of promoters and enhancers after stimulus",
            "Publication year": 2017,
            "Publication url": "https://academic.oup.com/nar/article-abstract/45/4/e25/2333916",
            "Abstract": "Promoters and enhancers regulate the initiation of gene expression and maintenance of expression levels in spatial and temporal manner. Recent findings stemming from the Cap Analysis of Gene Expression (CAGE) demonstrate that promoters and enhancers, based on their expression profiles after stimulus, belong to different transcription response subclasses. One of the most promising biological features that might explain the difference in transcriptional response between subclasses is the local chromatin environment. We introduce a novel computational framework, PEDAL, for distinguishing effectively transcriptional profiles of promoters and enhancers using solely histone modification marks, chromatin accessibility and binding sites of transcription factors and co-activators. A case study on data from MCF-7 cell-line reveals that PEDAL can identify successfully the transcription response subclasses of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:EsrhoZGmrkoC",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "Supplementary Material for: DASPfind: new efficient method to predict drug\u2013target interactions",
            "Publication year": 2016,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/624146",
            "Abstract": "Background Identification of novel drug\u2013target interactions (DTIs) is important for drug discovery. Experimental determination of such DTIs is costly and time consuming, hence it necessitates the development of efficient computational methods for the accurate prediction of potential DTIs. To-date, many computational methods have been proposed for this purpose, but they suffer the drawback of a high rate of false positive predictions. Results Here, we developed a novel computational DTI prediction method, DASPfind. DASPfind uses simple paths of particular lengths inferred from a graph that describes DTIs, similarities between drugs, and similarities between the protein targets of drugs. We show that on average, over the four gold standard DTI datasets, DASPfind significantly outperforms other existing methods when the single top-ranked predictions are considered, resulting in 46.17 % of these predictions being correct, and it achieves 49.22 % correct single top ranked predictions when the set of all DTIs for a single drug is tested. Furthermore, we demonstrate that our method is best suited for predicting DTIs in cases of drugs with no known targets or with few known targets. We also show the practical use of DASPfind by generating novel predictions for the Ion Channel dataset and validating them manually. Conclusions DASPfind is a computational method for finding reliable new interactions between drugs and proteins. We show over six different DTI datasets that DASPfind outperforms other state-of-the-art methods when the single top-ranked predictions are considered, or when a drug with no known targets or with few known targets is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:aCwMkEyfDy8C",
            "Publisher": "figshare"
        },
        {
            "Title": "Scaling distributed machine learning with in-network aggregation",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1903.06701",
            "Abstract": "Training machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide an efficient solution that speeds up training by up to 5.5 for a number of real-world benchmark models.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:anDooRL1HQEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Providing k-anonymity in location based services",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1882471.1882473",
            "Abstract": "The offering of anonymity in relational databases has attracted a great deal of attention in the database community during the last decade [4]. Among the different solution approaches that have been proposed to tackle this problem, K-anonymity has received increased attention and has been extensively studied in various forms. New forms of data that come into existence, like location data capturing user movement, pave the way for the offering of cutting edge services such as the prevailing Location Based Services (LBSs). Given that these services assume an in-depth knowledge of the mobile users' whereabouts it is certain that the assumed knowledge may breach the privacy of the users. Thus, concrete approaches are necessary to preserve the anonymity of the mobile users when requesting LBSs. In this work, we survey recent advancements for the offering of K-anonymity in LBSs. Most of the approaches that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "ACM"
        },
        {
            "Title": "ERA: efficient serial and parallel suffix tree construction for very long strings",
            "Publication year": 2011,
            "Publication url": "https://arxiv.org/abs/1109.6884",
            "Abstract": "The suffix tree is a data structure for indexing strings. It is used in a variety of applications such as bioinformatics, time series analysis, clustering, text editing and data compression. However, when the string and the resulting suffix tree are too large to fit into the main memory, most existing construction algorithms become very inefficient. This paper presents a disk-based suffix tree construction method, called Elastic Range (ERa), which works efficiently with very long strings that are much larger than the available memory. ERa partitions the tree construction process horizontally and vertically and minimizes I/Os by dynamically adjusting the horizontal partitions independently for each vertical partition, based on the evolving shape of the tree and the available memory. Where appropriate, ERa also groups vertical partitions together to amortize the I/O cost. We developed a serial version; a parallel version for shared-memory and shared-disk multi-core systems; and a parallel version for shared-nothing architectures. ERa indexes the entire human genome in 19 minutes on an ordinary desktop computer. For comparison, the fastest existing method needs 15 minutes using 1024 CPUs on an IBM BlueGene supercomputer.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:-f6ydRqryjwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Trajectory similarity join in spatial networks",
            "Publication year": 2017,
            "Publication url": "https://vbn.aau.dk/ws/files/271273908/p1178_shang.pdf",
            "Abstract": "The matching of similar pairs of objects, called similarity join, is fundamental functionality in data management. We consider the case of trajectory similarity join (TS-Join), where the objects are trajectories of vehicles moving in road networks. Thus, given two sets of trajectories and a threshold \u03b8, the TS-Join returns all pairs of trajectories from the two sets with similarity above \u03b8. This join targets applications such as trajectory near-duplicate detection, data cleaning, ridesharing recommendation, and traffic congestion prediction.With these applications in mind, we provide a purposeful definition of similarity. To enable efficient TS-Join processing on large sets of trajectories, we develop search space pruning techniques and take into account the parallel processing capabilities of modern processors. Specifically, we present a two-phase divideand-conquer algorithm. For each trajectory, the algorithm first finds similar trajectories. Then it merges the results to achieve a final result. The algorithm exploits an upper bound on the spatiotemporal similarity and a heuristic scheduling strategy for search space pruning. The algorithm\u2019s per-trajectory searches are independent of each other and can be performed in parallel, and the merging has constant cost. An empirical study with real data offers insight in the performance of the algorithm and demonstrates that is capable of outperforming a well-designed baseline algorithm by an order of magnitude.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:JIEWM9yDoCIC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Fast data anonymization with low information loss",
            "Publication year": 2007,
            "Publication url": "http://www.vldb.org/conf/2007/papers/research/p758-ghinita.pdf",
            "Abstract": "Recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual\u2019s record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. However, existing approaches suffer from at least one of the following drawbacks:(i) The information loss metrics are counter-intuitive and fail to capture data inaccuracies inflicted for the sake of privacy.(ii) l-diversity is solved by techniques developed for the simpler k-anonymity problem, which introduces unnecessary inaccuracies.(iii) The anonymization process is inefficient in terms of computation and I/O cost.In this paper we propose a framework for efficient privacy preservation that addresses these deficiencies. First, we focus on one-dimensional (ie, single attribute) quasiidentifiers, and study the properties of optimal solutions for k-anonymity and l-diversity, based on meaningful information loss metrics. Guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. Finally, we generalize our solutions to multi-dimensional quasi-identifiers using space-mapping techniques. Extensive experimental evaluation shows that our techniques clearly outperform the state-of-the-art, in terms of execution time and information loss.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:UeHWp8X0CEIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Processing ad-hoc joins on mobile devices",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-30075-5_59",
            "Abstract": "Mobile devices are capable of retrieving and processing data from remote databases. In a wireless data transmission environment, users are typically charged by the size of transferred data, rather than the amount of time they stay connected. We propose algorithms that join information from non-collaborative remote databases on mobile devices. Our methods minimize the data transferred during the join process, by also considering the limitations of mobile devices. Experimental results show that our approach can perform join processing on mobile devices effectively.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:Zph67rFs4hoC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "DeepReduce: A Sparse-tensor Communication Framework for Federated Deep Learning",
            "Publication year": 2021,
            "Publication url": "https://openreview.net/forum?id=OAy508Q3T8",
            "Abstract": "Sparse tensors appear frequently in federated deep learning, either as a direct artifact of the deep neural network\u2019s gradients, or as a result of an explicit sparsification process. Existing communication primitives are agnostic to the peculiarities of deep learning; consequently, they impose unnecessary communication overhead. This paper introduces DeepReduce, a versatile framework for the compressed communication of sparse tensors, tailored to federated deep learning. DeepReduce decomposes sparse tensors into two sets, values and indices, and allows both independent and combined compression of these sets. We support a variety of common compressors, such as Deflate for values, or run-length encoding for indices. We also propose two novel compression schemes that achieve superior results: curve fitting-based for values, and bloom filter-based for indices. DeepReduce is orthogonal to existing gradient sparsifiers and can be applied in conjunction with them, transparently to the end-user, to significantly lower the communication overhead. As proof of concept, we implement our approach on TensorFlow and PyTorch. Our experiments with large real models demonstrate that DeepReduce transmits 320% less data than existing sparsifiers, without affecting accuracy. Code is available at https://github. com/hangxu0304/DeepReduce.Supplementary Material: pdfCode Of Conduct: I certify that all co-authors of this work have read and commit to adhering to the NeurIPS Statement on Ethics, Fairness, Inclusivity, and Code of Conduct.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:ibZ2AwG9z6wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An adaptive peer-to-peer network for distributed caching of olap results",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/564691.564695",
            "Abstract": "Peer-to-Peer (P2P) systems are becoming increasingly popular as they enable users to exchange digital information by participating in complex networks. Such systems are inexpensive, easy to use, highly scalable and do not require central administration. Despite their advantages, however, limited work has been done on employing database systems on top of P2P networks. Here we propose the PeerOLAP architecture for supporting On-Line Analytical Processing queries. A large number low-end clients, each containing a cache with the most useful results, are connected through an arbitrary P2P network. If a query cannot be answered locally (ie by using the cache contents of the computer where it is issued), it is propagated through the network until a peer that has cached the answer is found. An answer may also be constructed by partial results from many peers. Thus PeerOLAP acts as a large distributed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:qjMakFHDy7sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Quality and efficiency in high dimensional nearest neighbor search",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1559845.1559905",
            "Abstract": "Nearest neighbor (NN) search in high dimensional space is an important problem in many applications. Ideally, a practical solution (i) should be implementable in a relational database, and (ii) its query cost should grow sub-linearly with the dataset size, regardless of the data and query distributions. Despite the bulk of NN literature, no solution fulfills both requirements, except locality sensitive hashing (LSH). The existing LSH implementations are either rigorous or adhoc. Rigorous-LSH ensures good quality of query results, but requires expensive space and query cost. Although adhoc-LSH is more efficient, it abandons quality control, ie, the neighbor it outputs can be arbitrarily bad. As a result, currently no method is able to ensure both quality and efficiency simultaneously in practice.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:eQOLeE2rZwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Similarity evaluation on tree-structured data",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1066157.1066243",
            "Abstract": "Tree-structured data are becoming ubiquitous nowadays and manipulating them based on similarity is essential for many applications. The generally accepted similarity measure for trees is the edit distance. Although similarity search has been extensively studied, searching for similar trees is still an open problem due to the high complexity of computing the tree edit distance. In this paper, we propose to transform tree-structured data into an approximate numerical multidimensional vector which encodes the original structure information. We prove that the L 1 distance of the corresponding vectors, whose computational complexity is O (| T 1|+| T 2|), forms a lower bound for the edit distance between trees. Based on the theoretical analysis, we describe a novel algorithm which embeds the proposed distance into a filter-and-refine framework to process similarity search on tree-structured data. The experimental results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Adaptive partitioning for very large RDF data",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1505.02728",
            "Abstract": "Distributed RDF systems partition data across multiple computer nodes (workers). Some systems perform cheap hash partitioning, which may result in expensive query evaluation, while others apply heuristics aiming at minimizing inter-node communication during query evaluation. This requires an expensive data preprocessing phase, leading to high startup costs for very large RDF knowledge bases. Apriori knowledge of the query workload has also been used to create partitions, which however are static and do not adapt to workload changes; hence, inter-node communication cannot be consistently avoided for queries that are not favored by the initial data partitioning. In this paper, we propose AdHash, a distributed RDF system, which addresses the shortcomings of previous work. First, AdHash applies lightweight partitioning on the initial data, that distributes triples by hashing on their subjects; this renders its startup overhead low. At the same time, the locality-aware query optimizer of AdHash takes full advantage of the partitioning to (i)support the fully parallel processing of join patterns on subjects and (ii) minimize data communication for general queries by applying hash distribution of intermediate results instead of broadcasting, wherever possible. Second, AdHash monitors the data access patterns and dynamically redistributes and replicates the instances of the most frequent ones among workers. As a result, the communication cost for future queries is drastically reduced or even eliminated. To control replication, AdHash implements an eviction policy for the redistributed patterns. Our experiments with synthetic and real data verify that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:gkldIfsazJcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Privacy-preserving anonymization of set-valued data",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/1453856.1453874",
            "Abstract": "In this paper we study the problem of protecting privacy in the publication of set-valued data. Consider a collection of transactional data that contains detailed information about items bought together by individuals. Even after removing all personal characteristics of the buyer, which can serve as links to his identity, the publication of such data is still subject to privacy attacks from adversaries who have partial knowledge about the set. Unlike most previous works, we do not distinguish data as sensitive and non-sensitive, but we consider them both as potential quasi-identifiers and potential sensitive data, depending on the point of view of the adversary. We define a new version of the k-anonymity guarantee, the km-anonymity, to limit the effects of the data dimensionality and we propose efficient algorithms to transform the database. Our anonymization model relies on generalization instead of suppression, which is the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "sands-lab/layer-wise-aaai20: Code repository for AAAI'20 paper: On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning",
            "Publication year": 2019,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/667393",
            "Abstract": "Code repository for AAAI'20 paper: On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:71d7Y1FijdoC",
            "Publisher": "Github"
        },
        {
            "Title": "POEMS: Peer-based overload management",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-85481-4_27",
            "Abstract": "The Internet has become increasingly important to many emerging application such as Blog, Wikis, podcasts, and others Web-based communities and social-networking services, i.e. Web 2.0. Behind the scenes, added functionalities depend on the ability of users to work with the data stored on servers, i.e. DBMSs. However, the unpredictability and fluctuations of requests could result in overload, which can substantially degrade the quality of service. It is a challenging task to provide quality of service with inexpensive and scalable infrastructure. In this paper, we look at a new architectural design dimension, POEMS, that is online transformable between a single-node server and peer-based service network architectures. POEMS operates as a conventional DBMS under normal load conditions and transforms to peer-to-peer operation mode for processing under heavy load. In contrast to traditional distributed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:mB3voiENLucC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Location-aware top-k term publish/subscribe",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8509294/",
            "Abstract": "Massive amount of data that contain spatial, textual, and temporal information are being generated at a high scale. These spatio-temporal documents cover a wide range of topics in local area. Users are interested in receiving local popular terms from spatio-temporal documents published with a specified region. We consider the Top-k Spatial-Temporal Term (ST2) Subscription. Given an ST2 subscription, we continuously maintain up-to-date top-k most popular terms over a stream of spatio-temporal documents. The ST2 subscription takes into account both frequency and recency of a term generated from spatio-temporal document streams in evaluating its popularity. We propose an efficient solution to process a large number of ST2 subscriptions over a stream of spatio-temporal documents. The performance of processing ST2 subscriptions is studied in extensive experiments based on two real spatio-temporal \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:lSLsV1MU4ZUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Outsourcing of private spatial data for search services",
            "Publication year": 2009,
            "Publication url": "https://scholar.google.com/scholar?cluster=16589205884109292868&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:JTtNqH-x4gYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Proxy-server architectures for olap",
            "Publication year": 2001,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/376284.375712",
            "Abstract": "Data warehouses have been successfully employed for assisting decision making by offering a global view of the enterprise data and providing mechanisms for On-Line Analytical processing. Traditionally, data warehouses are utilized within the limits of an enterprise or organization. The growth of Internet and WWW however, has created new opportunities for data sharing among ad-hoc, geographically spanned and possibly mobile users. Since it is impractical for each enterprise to set up a worldwide infrastructure, currently such applications are handled by the central warehouse. This often yields poor performance, due to overloading of the central server and low transfer rate of the network.In this paper we propose an architecture for OLAP cache servers (OCS). An OCS is the equivalent of a proxy-server for web documents, but it is designed to accommodate data from warehouses and support OLAP operations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:ufrVoPGSRksC",
            "Publisher": "ACM"
        },
        {
            "Title": "DASPfind: new efficient method to predict drug\u2013target interactions",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/article/10.1186/s13321-016-0128-4",
            "Abstract": "Identification of novel drug\u2013target interactions (DTIs) is important for drug discovery. Experimental determination of such DTIs is costly and time consuming, hence it necessitates the development of efficient computational methods for the accurate prediction of potential DTIs. To-date, many computational methods have been proposed for this purpose, but they suffer the drawback of a high rate of false positive predictions. Here, we developed a novel computational DTI prediction method, DASPfind. DASPfind uses simple paths of particular lengths inferred from a graph that describes DTIs, similarities between drugs, and similarities between the protein targets of drugs. We show that on average, over the four gold standard DTI datasets, DASPfind significantly outperforms other existing methods when the single top-ranked predictions are considered, resulting in 46.17 % of these predictions being correct, and it \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:tFzHCjejgA0C",
            "Publisher": "BioMed Central"
        },
        {
            "Title": "Combining vertex-centric graph processing with sparql for large-scale rdf data analytics",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7959641/",
            "Abstract": "Modern applications require sophisticated analytics on RDF graphs that combine structural queries with generic graph computations. Existing systems support either declarative SPARQL queries, or generic graph processing, but not both. We bridge the gap by introducing Spartex, a versatile framework for complex RDF analytics. Spartex extends SPARQL to combine seamlessly generic graph algorithms (e.g., PageRank, Shortest Paths, etc.) with SPARQL queries. Spartex builds on existing vertex-centric graph processing frameworks, such as Graphlab or Pregel. It implements a generic SPARQL operator as a vertex-centric program that interprets SPARQL queries and executes them efficiently using a built-in optimizer. In addition, any graph algorithm implemented in the underlying vertex-centric framework, can be executed in Spartex. We present various scenarios where our framework simplifies significantly the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:Pqt4MY__2vwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "User oriented trajectory search for trip recommendation",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2247596.2247616",
            "Abstract": "Trajectory sharing and searching have received significant attentions in recent years. In this paper, we propose and investigate a novel problem called User Oriented Trajectory Search (UOTS) for trip recommendation. In contrast to conventional trajectory search by locations (spatial domain only), we consider both spatial and textual domains in the new UOTS query. Given a trajectory data set, the query input contains a set of intended places given by the traveler and a set of textual attributes describing the traveler's preference. If a trajectory is connecting/close to the specified query locations, and the textual attributes of the trajectory are similar to the traveler'e preference, it will be recommended to the traveler for reference. This type of queries can bring significant benefits to travelers in many popular applications such as trip planning and recommendation.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:r0BpntZqJG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Distributed privacy preserving data collection",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-20149-3_9",
            "Abstract": "We study the distributed privacy preserving data collection problem: an untrusted data collector (e.g., a medical research institute) wishes to collect data (e.g., medical records) from a group of respondents (e.g., patients). Each respondent owns a multi-attributed record which contains both non-sensitive (e.g., quasi-identifiers) and sensitive information (e.g., a particular disease), and submits it to the data collector. Assuming T is the table formed by all the respondent data records, we say that the data collection process is privacy preserving if it allows the data collector to obtain a k-anonymized or l-diversified version of T without revealing the original records to the adversary.We propose a distributed data collection protocol that outputs an anonymized table by generalization of quasi-identifier attributes. The protocol employs cryptographic techniques such as homomorphic encryption, private \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:hFOr9nPyWt4C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Localized signature table: Fast similarity search on transaction data",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1031171.1031237",
            "Abstract": "Recently, techniques for supporting efficient similarity search over huge transaction datasets have emerged as an important research area. Several indexing schemes have been proposed towards this direction. Typically, these schemes provide a tradeoff between searching efficiency and indexing overhead in terms of space.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:mVmsd5A6BfQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Huffman coding based encoding techniques for fast distributed deep learning",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3426745.3431334",
            "Abstract": "Distributed stochastic algorithms, equipped with gradient compression techniques, such as codebook quantization, are becoming increasingly popular and considered state-of-the-art in training large deep neural network (DNN) models. However, communicating the quantized gradients in a network requires efficient encoding techniques. For this, practitioners generally use Elias encoding-based techniques without considering their computational overhead or data-volume. In this paper, based on Huffman coding, we propose several lossless encoding techniques that exploit different characteristics of the quantized gradients during distributed DNN training. Then, we show their effectiveness on 5 different DNN models across three different data-sets, and compare them with classic state-of-the-art Elias-based encoding techniques. Our results show that the proposed Huffman-based encoders (ie, RLH, SH, and SHS \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:Dmoar05iI2YC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Evaluating SPARQL queries on massive RDF datasets",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/2824032.2824083",
            "Abstract": "Distributed RDF systems partition data across multiple computer nodes. Partitioning is typically based on heuristics that minimize inter-node communication and it is performed in an initial, data pre-processing phase. Therefore, the resulting partitions are static and do not adapt to changes in the query workload; as a result, existing systems are unable to consistently avoid communication for queries that are not favored by the initial data partitioning. Furthermore, for very large RDF knowledge bases, the partitioning phase becomes prohibitively expensive, leading to high startup costs.In this paper, we propose AdHash, a distributed RDF system which addresses the shortcomings of previous work. First, AdHash initially applies lightweight hash partitioning, which drastically minimizes the startup cost, while favoring the parallel processing of join patterns on subjects, without any data communication. Using a locality \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:JdL-Xu2nR38C",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Querying and Mining Strings Made Easy",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-69179-4_1",
            "Abstract": "With the advent of large string datasets in several scientific and business applications, there is a growing need to perform ad-hoc analysis on strings. Currently, strings are stored, managed, and queried using procedural codes. This limits users to certain operations supported by existing procedural applications and requires manual query planning with limited tuning opportunities. This paper presents StarQL, a generic and declarative query language for strings. StarQL is based on a native string data model that allows StarQL to support a large variety of string operations and provide semantic-based query optimization. String analytic queries are too intricate to be solved on one machine. Therefore, we propose a scalable and efficient data structure that allows StarQL implementations to handle large sets of strings and utilize large computing infrastructures. Our evaluation shows that StarQL is able to express \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:r56sNq9gaawC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Generalized multidimensional data mapping and query processing",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1093382.1093383",
            "Abstract": "Multidimensional data points can be mapped to one-dimensional space to exploit single dimensional indexing structures such as the B+-tree. In this article we present a Generalized structure for data Mapping and query Processing (GiMP), which supports extensible mapping methods and query processing. GiMP can be easily customized to behave like many competent indexing mechanisms for multi-dimensional indexing, such as the UB-Tree, the Pyramid technique, the iMinMax, and the iDistance. Besides being an extendible indexing structure, GiMP also serves as a framework to study the characteristics of the mapping and hence the efficiency of the indexing scheme. Specifically, we introduce a metric called mapping redundancy to characterize the efficiency of a mapping method in terms of disk page accesses and analyze its behavior for point, range and kNN queries. We also address the fundamental \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:LkGwnXOMwfcC",
            "Publisher": "ACM"
        },
        {
            "Title": "PHD-Store: an adaptive SPARQL engine with dynamic partitioning for distributed RDF repositories",
            "Publication year": 2014,
            "Publication url": "https://arxiv.org/abs/1405.4979",
            "Abstract": "Many repositories utilize the versatile RDF model to publish data. Repositories are typically distributed and geographically remote, but data are interconnected (e.g., the Semantic Web) and queried globally by a language such as SPARQL. Due to the network cost and the nature of the queries, the execution time can be prohibitively high. Current solutions attempt to minimize the network cost by redistributing all data in a preprocessing phase, but here are two drawbacks: (i) redistribution is based on heuristics that may not benefit many of the future queries; and (ii) the preprocessing phase is very expensive even for moderate size datasets. In this paper we propose PHD-Store, a SPARQL engine for distributed RDF repositories. Our system does not assume any particular initial data placement and does not require prepartitioning; hence, it minimizes the startup cost. Initially, PHD-Store answers queries using a potentially slow distributed semi-join algorithm, but adapts dynamically to the query load by incrementally redistributing frequently accessed data. Redistribution is done in a way that future queries can benefit from fast hash-based parallel execution. Our experiments with synthetic and real data verify that PHD-Store scales to very large datasets; many repositories; converges to comparable or better quality of partitioning than existing methods; and executes large query loads 1 to 2 orders of magnitude faster than our competitors.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:ovGv7akYl-cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "DEEP: a general computational framework for predicting enhancers",
            "Publication year": 2015,
            "Publication url": "https://academic.oup.com/nar/article-abstract/43/1/e6/2902605",
            "Abstract": "Transcription regulation in multicellular eukaryotes is orchestrated by a number of DNA functional elements located at gene regulatory regions. Some regulatory regions (e.g. enhancers) are located far away from the gene they affect. Identification of distal regulatory elements is a challenge for the bioinformatics research. Although existing methodologies increased the number of computationally predicted enhancers, performance inconsistency of computational models across different cell-lines, class imbalance within the learning sets and ad hoc rules for selecting enhancer candidates for supervised learning, are some key questions that require further examination. In this study we developed DEEP, a novel ensemble prediction framework. DEEP integrates three components with diverse characteristics that streamline the analysis of enhancer's properties in a great variety of cellular conditions. In our method \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:RgznTc0nqo4C",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "GCN-MF: disease-gene association identification by graph convolutional networks and matrix factorization",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3292500.3330912",
            "Abstract": "Discovering disease-gene association is a fundamental and critical biomedical task, which assists biologists and physicians to discover pathogenic mechanism of syndromes. With various clinical biomarkers measuring the similarities among genes and disease phenotypes, network-based semi-supervised learning (NSSL) has been commonly utilized by these studies to address this class-imbalanced large-scale data issue. However, most existing NSSL approaches are based on linear models and suffer from two major limitations: 1) They implicitly consider a local-structure representation for each candidate; 2) They are unable to capture nonlinear associations between diseases and genes. In this paper, we propose a new framework for disease-gene association task by combining Graph Convolutional Network (GCN) and matrix factorization, named GCN-MF. With the help of GCN, we could capture non-linear \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:8O4vDxvErlEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "View selection using randomized search",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0169023X02000459",
            "Abstract": "An important issue in data warehouse development is the selection of a set of views to materialize in order to accelerate On-line analytical processing queries, given certain space and maintenance time constraints. Existing methods provide good results but their high execution cost limits their applicability for large problems. In this paper, we explore the application of randomized, local search algorithms to the view selection problem. The efficiency of the proposed techniques is evaluated using synthetic datasets, which cover a wide range of data and query distributions. The results show that randomized search methods provide near-optimal solutions in limited time, being robust to data and query skew. Furthermore, they can be easily adapted for various versions of the problem, including the simultaneous existence of size and time constraints, and view selection in dynamic environments. The proposed heuristics \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:W7OEmFMy1HYC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "A survey and experimental comparison of distributed SPARQL engines for very large RDF data",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/3151106.3151109",
            "Abstract": "Distributed SPARQL engines promise to support very large RDF datasets by utilizing shared-nothing computer clusters. Some are based on distributed frameworks such as MapReduce; others implement proprietary distributed processing; and some rely on expensive preprocessing for data partitioning. These systems exhibit a variety of trade-offs that are not well-understood, due to the lack of any comprehensive quantitative and qualitative evaluation. In this paper, we present a survey of 22 state-of-the-art systems that cover the entire spectrum of distributed RDF data processing and categorize them by several characteristics. Then, we select 12 representative systems and perform extensive experimental evaluation with respect to preprocessing cost, query performance, scalability and workload adaptability, using a variety of synthetic and real large datasets with up to 4.3 billion triples. Our results provide valuable \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:tSVUDDkujAIC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Protein-protein interactions decoys datasets for machine learning algorithm development",
            "Publication year": 2021,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/666961",
            "Abstract": "This is the most complete and diverse protein docking decoys set derived from the Benchmark5, Scorers_set. We used three different rigid-body docking programs to generate the decoys for the Bechmark5. We analyzed all docking decoys with more than 150 different scoring functions from different sources (CCharppi, FreeSASA, CIPS, CONSRANK). We provide a balanced and unbalanced version of the data. This balanced data is intended for the training and test of machine learning algorithms. the unbalanced data is provided to simulated the real-world scenario. We also provide a set of rigid-body docking decoys from Interactome3D that spans 1391 interactions. We obtained the labels for this set using a weakly-supervised approach we called hAIkal. We used this data to augment the train data and improve machine learning classifiers.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:rDsFeusoTZkC",
            "Publisher": "KAUST Research Repository"
        },
        {
            "Title": "Outsourced similarity search on metric data assets",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5620912/",
            "Abstract": "This paper considers a cloud computing setting in which similarity querying of metric data is outsourced to a service provider. The data is to be revealed only to trusted users, not to the service provider or anyone else. Users query the server for the most similar data objects to a query example. Outsourcing offers the data owner scalability and a low-initial investment. The need for privacy may be due to the data being sensitive (e.g., in medicine), valuable (e.g., in astronomy), or otherwise confidential. Given this setting, the paper presents techniques that transform the data prior to supplying it to the service provider for similarity queries on the transformed data. Our techniques provide interesting trade-offs between query cost and accuracy. They are then further extended to offer an intuitive privacy guarantee. Empirical studies with real data demonstrate that the techniques are capable of offering privacy while enabling \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:RHpTSmoSYBkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "MobiHide: A Mobilea Peer-to-Peer System for Anonymous Location-Based Queries",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-73540-3_13",
            "Abstract": "Modern mobile phones and PDAs are equipped with positioning capabilities (e.g., GPS). Users can access public location-based services (e.g., Google Maps) and ask spatial queries. Although communication is encrypted, privacy and confidentiality remain major concerns, since the queries may disclose the location and identity of the user. Commonly, spatial  -anonymity is employed to hide the query initiator among a group of  users. However, existing work either fails to guarantee privacy, or exhibits unacceptably long response time.In this paper we propose MobiHide, a Peer-to-Peer system for anonymous location-based queries, which addresses these problems. MobiHide employs the Hilbert space-filling curve to map the 2-D locations of mobile users to 1-D space. The transformed locations are indexed by a Chord-based distributed hash table, which is formed by the mobile devices. The resulting \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Spatial anonymity",
            "Publication year": 2009,
            "Publication url": "https://docs.lib.purdue.edu/ccpubs/152/",
            "Abstract": "Let U be a user who is asking via a mobile device (eg, phone, PDA) a query relevant to his current location, such as \u2018\u2018find the nearest betting office.\u2019\u2019This query can be answered by a Location Based Service (LBS) in a public web server (eg, Google Maps, apQuest), which is not trustworthy. Since the query may be sensitive, U uses encryption and a pseudonym, in order to protect his privacy. However, the query still contains the exact location, which may reveal the identity of U. For example, if Uasks the query within his residence, an attacker may use public information (eg, white pages) to associate the location with U. Spatial k-Anonymity (SKA) solves this problem by ensuring that an attacker cannot identify U as the querying user with probability larger than 1\u2215 k, where k is a user-defined anonymity requirement. To achieve this, a centralized or distributed anonymization service replaces the exact location of U with an area (called Anonymizing Spatial Region or ASR). The ASR encloses U and at least k-1 additional users. The LBS receives the ASR and retrieves the query results for any point inside the ASR. Those results are forwarded to the anonymization service, which removes the false hits and returns the actual answer to U.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:qUcmZB5y_30C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Active caching of on-line-analytical-processing queries in www proxies",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/952088/",
            "Abstract": "The Internet is offering more than just regular Web pages to the users. Decision makers can now issue analytical, as opposed to transactional, queries that involve massive data (such as, aggregations of millions of rows in a relational database) in order to identify useful trends and patterns. Such queries are referred to as On-Line-Analytical-Processing (OLAP) queries. Typically, pages carrying query results do not exhibit temporal locality and, therefore, are not considered for caching at WWW proxies. In OLAP processing, this becomes a major hurdle as the cost of such queries is much higher than traditional transactional queries. This paper proposes a systematic technique to reduce the response time for OLAP queries originating from geographically distributed private LANs and issued through the Web towards the central data warehouse (DW) of an enterprise. An active caching scheme is proposed that enables \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Location diversity: Enhanced privacy protection in location based services",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-01721-6_5",
            "Abstract": "Location-based Services are emerging as popular applications in pervasive computing. Spatial k-anonymity is used in Location-based Services to protect privacy, by hiding the association of a specific query with a specific user. Unfortunately, this approach fails in many practical cases such as: (i) personalized services, where the user identity is required, or (ii) applications involving groups of users (e.g., employees of the same company); in this case, associating a query to any member of the group, violates privacy.In this paper, we introduce the concept of Location Diversity, which solves the above-mentioned problems. Location Diversity improves Spatial k-anonymity by ensuring that each query can be associated with at least \u2113 different semantic locations (e.g., school, shop, hospital, etc). We present an attack model that maps each observed query to a linear equation involving semantic locations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:3fE2CSJIrl8C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Improved suffix blocking for record linkage and entity resolution",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0169023X16300714",
            "Abstract": "Record linkage is the problem that identifies the different records that represent the same real-world object. Entity resolution is the problem that ensures that a real-world object is represented by a single record. The incremental versions of record linkage and entity resolution address the respective problems after the insertion of a new record in the dataset. Record linkage, entity resolution and their incremental versions are of paramount importance and arise in several contexts such as data warehouses, heterogeneous databases and data analysis. Blocking techniques are usually utilized to address these problems in order to avoid comparing all record pairs. Suffix blocking is one of the most efficient and accurate blocking techniques. In this paper, we consider the non-incremental variation of record linkage and present a method that is more than five times faster and achieves similar accuracy to the current state-of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:svGagg1hbZMC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Grami: Frequent subgraph and pattern mining in a single large graph",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/2732286.2732289",
            "Abstract": "Mining frequent subgraphs is an important operation on graphs; it is defined as finding all subgraphs that appear frequently in a database according to a given frequency threshold. Most existing work assumes a database of many small graphs, but modern applications, such as social networks, citation graphs, or protein-protein interactions in bioinformatics, are modeled as a single large graph. In this paper we present GraMi, a novel framework for frequent subgraph mining in a single large graph. GraMi undertakes a novel approach that only finds the minimal set of instances to satisfy the frequency threshold and avoids the costly enumeration of all instances required by previous approaches. We accompany our approach with a heuristic and optimizations that significantly improve performance. Additionally, we present an extension of GraMi that mines frequent patterns. Compared to subgraphs, patterns offer a more \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:ns9cj8rnVeAC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Editor\u2019s Note: Special Issue on Big Data Management and Intelligent Analytics",
            "Publication year": 2019,
            "Publication url": "https://search.proquest.com/openview/4d9adfa19e3861e7909104348c1680de/1?pq-origsite=gscholar&cbl=2034525",
            "Abstract": "Hao Cheng, Yuanyuan Zhang, Tianshu Song, Rui Liu, and Yi Xu https://doi. org/10.1007/s11280-018-0563-4 & A novel temporal and topic-aware recommender model by Dandan Song, Zhifan Li, Mingming Jiang, Lifei Qin, and Lejian Liao https://doi. org/10.1007/s11280-018-0595-9 & IQGA: A route selection method based on quantum genetic algorithm-toward urban traffic management under big data environment by Yuefei Tian, Wenbin Hu, Bo Du,",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:kQqwFFzsCTwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the anonymization of sparse high-dimensional data",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4497480/",
            "Abstract": "Existing research on privacy-preserving data publishing focuses on relational data: in this context, the objective is to enforce privacy-preserving paradigms, such as k- anonymity and lscr-diversity, while minimizing the information loss incurred in the anonymizing process (i.e. maximize data utility). However, existing techniques adopt an indexing- or clustering- based approach, and work well for fixed-schema data, with low dimensionality. Nevertheless, certain applications require privacy-preserving publishing of transaction data (or basket data), which involves hundreds or even thousands of dimensions, rendering existing methods unusable. We propose a novel anonymization method for sparse high-dimensional data. We employ a particular representation that captures the correlation in the underlying data, and facilitates the formation of anonymized groups with low information loss. We propose an efficient \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:Y0pCki6q_DkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Olap Results, Distributed Caching.",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=6401172541403151796&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:NMxIlDl6LWMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Efficient processing of distributed Iceberg semi-joins",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-30075-5_61",
            "Abstract": "The Iceberg SemiJoin (ISJ) of two datasets  and  returns the tuples in  which join with at least k tuples of . The ISJ operator is essential in many practical applications including OLAP, Data Mining and Information Retrieval. In this paper we consider the distributed evaluation of Iceberg SemiJoins, where  and  reside on remote servers. We developed an efficient algorithm which employs Bloom filters. The novelty of our approach is that we interleave the evaluation of the Iceberg set in server  with the pruning of unmatched tuples in server . Therefore, we are able to (i) eliminate unnecessary tuples early, and (ii) extract accurate Bloom filters from the intermediate hash tables which are constructed during the generation of the Iceberg set. Compared to conventional two-phase approaches, our experiments demonstrate that our method transmits up to 80% less data through the network, while \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:ZeXyd9-uunAC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Enabling search services on outsourced private spatial data",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-009-0169-7",
            "Abstract": "Cloud computing services enable organizations and individuals to outsource the management of their data to a service provider in order to save on hardware investments and reduce maintenance costs. Only authorized users are allowed to access the data. Nobody else, including the service provider, should be able to view the data. For instance, a real-estate company that owns a large database of properties wants to allow its paying customers to query for houses according to location. On the other hand, the untrusted service provider should not be able to learn the property locations and, e.g., selling the information to a competitor. To tackle the problem, we propose to transform the location datasets before uploading them to the service provider. The paper develops a spatial transformation that re-distributes the locations in space, and it also proposes a cryptographic-based transformation. The data owner \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:M3ejUd6NZC8C",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Fast and scalable inequality joins",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s00778-016-0441-6.pdf",
            "Abstract": "Inequality joins, which is to join relations with inequality conditions, are used in various applications. Optimizing joins has been the subject of intensive research ranging from efficient join algorithms such as sort-merge join, to the use of efficient indices such as -tree, -tree and Bitmap. However, inequality joins have received little attention and queries containing such joins are notably very slow. In this paper, we introduce fast inequality join algorithms based on sorted arrays and space-efficient bit-arrays. We further introduce a simple method to estimate the selectivity of inequality joins which is then used to optimize multiple predicate queries and multi-way joins. Moreover, we study an incremental inequality join algorithm to handle scenarios where data keeps changing. We have implemented a centralized version of these algorithms on top of PostgreSQL, a distributed version on top of Spark SQL, and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:1Aeql8wG3wEC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Scalemine: Scalable parallel frequent subgraph mining in a single large graph",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7877139/",
            "Abstract": "Frequent Subgraph Mining is an essential operation for graph analytics and knowledge extraction. Due to its high computational cost, parallel solutions are necessary. Existing approaches either suffer from load imbalance, or high communication and synchronization overheads. In this paper we propose ScaleMine; a novel parallel frequent subgraph mining system for a single large graph. ScaleMine introduces a novel two-phase approach. The first phase is approximate; it quickly identifies subgraphs that are frequent with high probability, while collecting various statistics. The second phase computes the exact solution by employing the results of the approximation to achieve good load balance; prune the search space; generate efficient execution plans; and guide intra-task parallelism. Our experiments show that ScaleMine scales to 8,192 cores on a Cray XC40 (12\u00d7 more than competitors); supports graphs with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:uWy0R8PweswC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Delineating social network data anonymization via random edge perturbation",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2396761.2396823",
            "Abstract": "Social network data analysis raises concerns about the privacy of related entities or individuals. To address this issue, organizations can publish data after simply replacing the identities of individuals with pseudonyms, leaving the overall structure of the social network unchanged. However, it has been shown that attacks based on structural identification (eg, a walk-based attack) enable an adversary to re-identify selected individuals in an anonymized network. In this paper we explore the capacity of techniques based on random edge perturbation to thwart such attacks. We theoretically establish that any kind of structural identification attack can effectively be prevented using random edge perturbation and show that, surprisingly, important properties of the whole network, as well as of subgraphs thereof, can be accurately calculated and hence data analysis tasks performed on the perturbed data, given that the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:isC4tDSrTZIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deep understanding of big geospatial data for self-driving cars",
            "Publication year": 2020,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/664604",
            "Abstract": "Self-driving cars are capable of sensing environment and moving with little or no human input. Effective control of self-driving cars based on big geospatial data is one of the promising future directions of intelligent transportation. Specifically, big geospatial data understanding is helpful in acquiring travel behavior, vehicle mobility, traffic flow, nearby environment, and traffic-aware navigation. This special issue contains 10 research articles that present solid and novel research studies in the area of geospatial data analytics for self-driving applications, and 1survey article that investigates existing studies related to self-driving cars. All of the 11 papers went through at least two rounds of rigorous reviews by the guest editors and invited reviewers.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:p-HGrieyzrAC",
            "Publisher": "Elsevier BV"
        },
        {
            "Title": "Cluster-based subscription matching for geo-textual data streams",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8731608/",
            "Abstract": "Geo-textual data that contain spatial, textual, and temporal information are being generated at a very high rate. These geo-textual data cover a wide range of topics. Users may be interested in receiving local popular topics from geo-textual messages. We study the cluster-based subscription matching (CSM) problem. Given a stream of geo-textual messages, we maintain up-to-date clustering results based on a threshold-based online clustering algorithm. Based on the clustering result, we feed subscribers with their preferred geo-textual message clusters according to their specified keywords and location. Moreover, we summarize each cluster by selecting a set of representative messages. The CSM problem considers spatial proximity, textual relevance, and message freshness during the clustering, cluster feeding, and summarization processes. To solve the CSM problem, we propose a novel solution to cluster \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:qYOp8iumCsAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Mizan: a system for dynamic load balancing in large-scale graph processing",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2465351.2465369",
            "Abstract": "Pregel [23] was recently introduced as a scalable graph mining system that can provide significant performance improvements over traditional MapReduce implementations. Existing implementations focus primarily on graph partitioning as a preprocessing step to balance computation across compute nodes. In this paper, we examine the runtime characteristics of a Pregel system. We show that graph partitioning alone is insufficient for minimizing end-to-end computation. Especially where data is very large or the runtime behavior of the algorithm is unknown, an adaptive approach is needed. To this end, we introduce Mizan, a Pregel system that achieves efficient load balancing to better adapt to changes in computing needs. Unlike known implementations of Pregel, Mizan does not assume any a priori knowledge of the structure of the graph or behavior of the algorithm. Instead, it monitors the runtime characteristics \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:M3NEmzRMIkIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spartex: A vertex-centric framework for RDF data analytics",
            "Publication year": 2015,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/581344",
            "Abstract": "A growing number of applications require combining SPARQL queries with generic graph search on RDF data. However, the lack of procedural capabilities in SPARQL makes it inappropriate for graph analytics. Moreover, RDF engines focus on SPARQL query evaluation whereas graph management frameworks perform only generic graph computations. In this work, we bridge the gap by introducing SPARTex, an RDF analytics framework based on the vertex-centric computation model. In SPARTex, user-defined vertex centric programs can be invoked from SPARQL as stored procedures. SPARTex allows the execution of a pipeline of graph algorithms without the need for multiple reads/writes of input data and intermediate results. We use a cost-based optimizer for minimizing the communication cost. SPARTex evaluates queries that combine SPARQL and generic graph computations orders of magnitude faster than existing RDF engines. We demonstrate a real system prototype of SPARTex running on a local cluster using real and synthetic datasets. SPARTex has a real-time graphical user interface that allows the participants to write regular SPARQL queries, use our proposed SPARQL extension to declaratively invoke graph algorithms or combine/pipeline both SPARQL querying and generic graph analytics.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:Mx5hWS9ctUkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A reciprocal framework for spatial k-anonymity",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0306437909000994",
            "Abstract": "Spatial K-anonymity (SKA) exploits the concept of K-anonymity in order to protect the identity of users from location-based attacks. The main idea of SKA is to replace the exact location of a user U with an anonymizing spatial region (ASR) that contains at least K\u22121 other users, so that an attacker can pinpoint U with probability at most 1/K. Simply generating an ASR that includes K users does not guarantee SKA. Previous work defined the reciprocity property as a sufficient condition for SKA. However, the only existing reciprocal method, Hilbert Cloak, relies on a specialized data structure. In contrast, we propose a general framework for implementing reciprocal algorithms using any existing spatial index on the user locations. We discuss ASR construction methods with different tradeoffs on effectiveness (i.e., ASR size) and efficiency (i.e., construction cost). Then, we present case studies of applying our framework on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:YOwf2qJgpHMC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "DWFS: a wrapper feature selection tool based on a parallel genetic algorithm",
            "Publication year": 2015,
            "Publication url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0117988",
            "Abstract": "Many scientific problems can be formulated as classification tasks. Data that harbor relevant information are usually described by a large number of features. Frequently, many of these features are irrelevant for the class prediction. The efficient implementation of classification models requires identification of suitable combinations of features. The smaller number of features reduces the problem\u2019s dimensionality and may result in higher classification performance. We developed DWFS, a web-based tool that allows for efficient selection of features for a variety of problems. DWFS follows the wrapper paradigm and applies a search strategy based on Genetic Algorithms (GAs). A parallel GA implementation examines and evaluates simultaneously large number of candidate collections of features. DWFS also integrates various filtering methods that may be applied as a pre-processing step in the feature selection process. Furthermore, weights and parameters in the fitness function of GA can be adjusted according to the application requirements. Experiments using heterogeneous datasets from different biomedical applications demonstrate that DWFS is fast and leads to a significant reduction of the number of features without sacrificing performance as compared to several widely used existing methods. DWFS can be accessed online at www.cbrc.kaust.edu.sa/dwfs.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:6pF0wJmtdfAC",
            "Publisher": "Public Library of Science"
        },
        {
            "Title": "Authenticated join processing in outsourced databases",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1559845.1559849",
            "Abstract": "Database outsourcing requires that a query server constructs a proof of result correctness, which can be verified by the client using the data owner's signature. Previous authentication techniques deal with range queries on a single relation using an authenticated data structure (ADS). On the other hand, authenticated join processing is inherently more complex than ranges since only the base relations (but not their combination) are signed by the owner. In this paper, we present three novel join algorithms depending on the ADS availability:(i) Authenticated Indexed Sort Merge Join (AISM), which utilizes a single ADS on the join attribute,(ii) Authenticated Index Merge Join (AIM) that requires an ADS (on the join attribute) for both relations, and (iii) Authenticated Sort Merge Join (ASM), which does not rely on any ADS. We experimentally demonstrate that the proposed methods outperform two benchmark algorithms \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:Se3iqnhoufwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Searching trajectories by regions of interest",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7883890/",
            "Abstract": "With the increasing availability of moving-object tracking data, trajectory search is increasingly important. We propose and investigate a novel query type named trajectory search by regions of interest (TSR query). Given an argument set of trajectories, a TSR query takes a set of regions of interest as a parameter and returns the trajectory in the argument set with the highest spatial-density correlation to the query regions. This type of query is useful in many popular applications such as trip planning and recommendation, and location based services in general. TSR query processing faces three challenges: how to model the spatial-density correlation between query regions and data trajectories, how to effectively prune the search space, and how to effectively schedule multiple so-called query sources. To tackle these challenges, a series of new metrics are defined to model spatial-density correlations. An efficient \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:JqN3CTdJtl0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Answering similarity queries in peer-to-peer networks",
            "Publication year": 2006,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0306437904000948",
            "Abstract": "A variety of peer-to-peer (P2P) systems for sharing digital information are currently available and most of them perform searching by exact key matching. In this paper we focus on similarity searching and describe FuzzyPeer, a generic broadcast-based P2P system which supports a wide range of fuzzy queries. As a case study we present an image retrieval application implemented on top of FuzzyPeer. Users provide sample images whose sets of features are propagated through the peers. The answer consists of the top-k most similar images within the query horizon. In our system the participation of peers is ad hoc and dynamic, their functionality is symmetric and there is no centralized index.In order to avoid flooding the network with messages, we develop a technique that takes advantage of the fuzzy nature of the queries. Specifically, some queries are \u201cfrozen\u201d inside the network, and are satisfied by the streaming \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:YsMSGLbcyi4C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Optimization algorithms for simultaneous multidimensional queries in OLAP environments",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-44801-2_26",
            "Abstract": "Multi-Dimensional Expressions (MDX) provide an interface for asking several related OLAP queries simultaneously. An interesting problem is how to optimize the execution of an MDX query, given that most data warehouses maintain a set of redundant materialized views to accelerate OLAP operations. A number of greedy and approximation algorithms have been proposed for different versions of the problem. In this paper we evaluate experimentally their performance using the APB and TPC-H benchmarks, concluding that they do not scale well for realistic workloads. Motivated by this fact, we developed two novel greedy algorithms. Our algorithms construct the execution plan in a top-down manner by identifying in each step the most beneficial view, instead of finding the most promising query. We show by extensive experimentation that our methods outperform the existing ones in most cases",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Dcmp: A distributed cycle minimization protocol for peer-to-peer networks",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4359425/",
            "Abstract": "Broadcast-based peer-to-peer (P2P) networks, including flat (for example, Gnutella) and two-layer superpeer implementations (for example, Kazaa), are extremely popular nowadays due to their simplicity, ease of deployment, and versatility. The unstructured network topology, however, contains many cyclic paths, which introduce numerous duplicate messages in the system. Although such messages can be identified and ignored, they still consume a large proportion of the bandwidth and other resources, causing bottlenecks in the entire network. In this paper, we describe the distributed cycle minimization protocol (DCMP), a dynamic fully decentralized protocol that significantly reduces the duplicate messages by eliminating unnecessary cycles. As queries are transmitted through the peers, DCMP identifies the problematic paths and attempts to break the cycles while maintaining the connectivity of the network. In \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:KlAtU1dfN6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A demonstration of MAGiQ: matrix algebra approach for solving RDF graph queries",
            "Publication year": 2018,
            "Publication url": "https://repository.kaust.edu.sa/handle/10754/631495",
            "Abstract": "Existing RDF engines follow one of two design paradigms: relational or graph-based. Such engines are typically designed for specific hardware architectures, mainly CPUs, and are not easily portable to new architectures. Porting an existing engine to a different architecture (e.g., many-core architectures) entails almost redesign from scratch. We explore sparse matrix algebra as a third paradigm for designing a portable, scalable, and efficient RDF engine. We demonstrate MAGiQ; a matrix algebra approach for evaluating complex SPARQL queries over large RDF datasets. MAGiQ represents an RDF graph as a sparse matrix, and translates SPARQL queries to matrix algebra programs. MAGiQ takes advantage of the existing rich software infrastructure for processing sparse matrices, optimized for many architectures (e.g., CPUs, GPUs, distributed), effortlessly. This demo motivates the adoption of matrix algebra in RDF graph processing by showing MAGiQ's performance with different matrix algebra backend engines. MAGiQ, using a GPU, is orders of magnitude faster in solving complex queries on a billion edge graph than state-of-the-art RDF systems.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:lPDSu1ZU3VAC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "DRABAL: novel method to mine large high-throughput screening assays using Bayesian active learning",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/article/10.1186/s13321-016-0177-8",
            "Abstract": "Mining high-throughput screening (HTS) assays is key for enhancing decisions in the area of drug repositioning and drug discovery. However, many challenges are encountered in the process of developing suitable and accurate methods for extracting useful information from these assays. Virtual screening and a wide variety of databases, methods and solutions proposed to-date, did not completely overcome these challenges. This study is based on a multi-label classification (MLC) technique for modeling correlations between several HTS assays, meaning that a single prediction represents a subset of assigned correlated labels instead of one label. Thus, the devised method provides an increased probability for more accurate predictions of compounds that were not tested in particular assays. Here we present DRABAL, a novel MLC solution that incorporates structure learning of a Bayesian network as a step to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:PklR0melJeUC",
            "Publisher": "BioMed Central"
        },
        {
            "Title": "Lusail: a system for querying linked data at scale",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3164135.3164144",
            "Abstract": "The RDF data model allows publishing interlinked RDF datasets, where each dataset is independently maintained and is queryable via a SPARQL endpoint. Many applications would benefit from querying the resulting large, decentralized, geo-distributed graph through a federated SPARQL query processor. A crucial factor for good performance in federated query processing is pushing as much computation as possible to the local endpoints. Surprisingly, existing federated SPARQL engines are not effective at this task since they rely only on schema information. Consequently, they cause unnecessary data retrieval and communication, leading to poor scalability and response time. This paper addresses these limitations and presents Lusail, a scalable and efficient federated SPARQL system for querying large RDF graphs that are geo-distributed on different endpoints. Lusail uses a novel query rewriting algorithm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:09LM3QYkMKUC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Comparing memory-efficient genome assemblers on stand-alone and cloud infrastructures",
            "Publication year": 2013,
            "Publication url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0075505",
            "Abstract": "A fundamental problem in bioinformatics is genome assembly. Next-generation sequencing (NGS) technologies produce large volumes of fragmented genome reads, which require large amounts of memory to assemble the complete genome efficiently. With recent improvements in DNA sequencing technologies, it is expected that the memory footprint required for the assembly process will increase dramatically and will emerge as a limiting factor in processing widely available NGS-generated reads. In this report, we compare current memory-efficient techniques for genome assembly with respect to quality, memory consumption and execution time. Our experiments prove that it is possible to generate draft assemblies of reasonable quality on conventional multi-purpose computers with very limited available memory by choosing suitable assembly methods. Our study reveals the minimum memory requirements for different assembly programs even when data volume exceeds memory capacity by orders of magnitude. By combining existing methodologies, we propose two general assembly strategies that can improve short-read assembly approaches and result in reduction of the memory footprint. Finally, we discuss the possibility of utilizing cloud infrastructures for genome assembly and we comment on some findings regarding suitable computational resources for assembly.",
            "Abstract entirety": 1,
            "Author pub id": "-NdSrrYAAAAJ:O3NaXMp0MMsC",
            "Publisher": "Public Library of Science"
        },
        {
            "Title": "Mining chemical activity status from high-throughput screening assays",
            "Publication year": 2015,
            "Publication url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144426",
            "Abstract": "High-throughput screening (HTS) experiments provide a valuable resource that reports biological activity of numerous chemical compounds relative to their molecular targets. Building computational models that accurately predict such activity status (active vs. inactive) in specific assays is a challenging task given the large volume of data and frequently small proportion of active compounds relative to the inactive ones. We developed a method, DRAMOTE, to predict activity status of chemical compounds in HTP activity assays. For a class of HTP assays, our method achieves considerably better results than the current state-of-the-art-solutions. We achieved this by modification of a minority oversampling technique. To demonstrate that DRAMOTE is performing better than the other methods, we performed a comprehensive comparison analysis with several other methods and evaluated them on data from 11 PubChem assays through 1,350 experiments that involved approximately 500,000 interactions between chemicals and their target proteins. As an example of potential use, we applied DRAMOTE to develop robust models for predicting FDA approved drugs that have high probability to interact with the thyroid stimulating hormone receptor (TSHR) in humans. Our findings are further partially and indirectly supported by 3D docking results and literature information. The results based on approximately 500,000 interactions suggest that DRAMOTE has performed the best and that it can be used for developing robust virtual screening models. The datasets and implementation of all solutions are available as a MATLAB toolbox online at www.cbrc.kaust \u2026",
            "Abstract entirety": 0,
            "Author pub id": "-NdSrrYAAAAJ:SgM-ki2adj0C",
            "Publisher": "Public Library of Science"
        }
    ]
}]