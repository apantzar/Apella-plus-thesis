[{
    "name": "\u0393\u03b5\u03c1\u03ac\u03c3\u03b9\u03bc\u03bf\u03c2 \u03a0\u03bf\u03c4\u03b1\u03bc\u03b9\u03ac\u03bd\u03bf\u03c2",
    "romanize name": "Gerasimos Potamianos",
    "School-Department": "\u0397\u03bb\u03b5\u03ba\u03c4\u03c1\u03bf\u03bb\u03cc\u03b3\u03c9\u03bd \u039c\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 \u039c\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ce\u03bd \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "uth",
    "Rank": "\u0391\u03bd\u03b1\u03c0\u03bb\u03b7\u03c1\u03c9\u03c4\u03ae\u03c2 \u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 8779,
    "Scholar name": "Gerasimos Potamianos",
    "Scholar id": "DjcaM1MAAAAJ",
    "Affiliation": "University of Thessaly, Volos, Greece",
    "Citedby": 6208,
    "Scholar url": "https://scholar.google.com/citations?user=DjcaM1MAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Joint face and head tracking inside multi-camera smart rooms",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/article/10.1007/s11760-007-0018-3",
            "Abstract": "The paper introduces a novel detection and tracking system that provides both frame-view and world-coordinate human location information, based on video from multiple synchronized and calibrated cameras with overlapping fields of view. The system is developed and evaluated for the specific scenario of a seminar lecturer presenting in front of an audience inside a \u201csmart room\u201d, its aim being to track the lecturer\u2019s head centroid in the three-dimensional (3D) space and also yield two-dimensional (2D) face information in the available camera views. The proposed approach is primarily based on a statistical appearance model of human faces by means of well-known AdaBoost-like face detectors, extended to address the head pose variation observed in the smart room scenario of interest. The appearance module is complemented by two novel components and assisted by a simple tracking drift detection \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Large-vocabulary audio-visual speech recognition: A summary of the Johns Hopkins Summer 2000 Workshop",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/962801/",
            "Abstract": "We report a summary of the Johns Hopkins Summer 2000 Workshop on audio-visual automatic speech recognition (ASR) in the large-vocabulary, continuous speech domain. Two problems of audio-visual ASR were mainly addressed: visual feature extraction and audio-visual information fusion. First, image transform and model-based visual features were considered, obtained by means of the discrete cosine transform (DCT) and active appearance models, respectively. The former were demonstrated to yield superior automatic speech reading. Subsequently, a number of feature fusion and decision fusion techniques for combining the DCT visual features with traditional acoustic ones were implemented and compared. Hierarchical discriminant feature fusion and asynchronous decision fusion by means of the multi-stream hidden Markov model consistently improved ASR for both clean and noisy speech \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:Y0pCki6q_DkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The IBM RT07 evaluation systems for speaker diarization on lecture meetings",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-68585-2_46",
            "Abstract": "We present the IBM systems for the Rich Transcription 2007 (RT07) speaker diarization evaluation task on lecture meeting data. We first overview our baseline system that was developed last year, as part of our speech-to-text system for the RT06s evaluation. We then present a number of simple schemes considered this year in our effort to improve speaker diarization performance, namely: (i) A better speech activity detection (SAD) system, a necessary pre-processing step to speaker diarization; (ii) Use of word information from a speaker-independent speech recognizer; (iii) Modifications to speaker cluster merging criteria and the underlying segment model; and (iv) Use of speaker models based on Gaussian mixture models, and their iterative refinement by frame-level re-labeling and smoothing of decision likelihoods. We report development experiments on the RT06s evaluation test set that demonstrate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Audio-visual codebook dependent cepstral normalization",
            "Publication year": 2010,
            "Publication url": "https://patents.google.com/patent/US7664637B2/en",
            "Abstract": "An arrangement for yielding enhanced audio features towards the provision of enhanced audio-visual features for speech recognition. Input is provided in the form of noisy audio-visual features and noisy audio features related to the noisy audio-visual features.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:7T2F9Uy0os0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio-visual data collection system",
            "Publication year": 2002,
            "Publication url": "https://patents.google.com/patent/US20020120643A1/en",
            "Abstract": "Methods and apparatus for obtaining visual data in connection with speech recognition. An image capture device captures visible images, a text-supplying device supplies text, and a substantially fully frontal image of a human face is captured during the reading of text from the text-supplying device.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:xtRiw3GOFMkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Patch-based analysis of visual speech from multiple views",
            "Publication year": 2008,
            "Publication url": "https://eprints.qut.edu.au/15247/",
            "Abstract": "Obtaining a robust feature representation of visual speech is of crucial importance in the design of audio-visual automatic speech recognition systems. In the literature, when visual appearance based features are employed for this purpose, they are typically extracted using a \"holistic\" approach. Namely, a transformation of the pixel values of the entire region-of-interest (ROI) is obtained, with the ROI covering the speaker's mouth and often surrounding facial area. In this paper, we instead consider a \"patch\" based visual feature extraction approach, within the appearance based framework. In particular, we conduct a novel analysis to determine which areas (patches) of the mouth ROI are the most informative for visual speech. Furthermore, we extend this analysis beyond the traditional frontal views, by investigating profile views as well. Not surprisingly, and for both frontal and profile views, we conclude that the central mouth patches are the most informative, but less so than the holistic features of the entire ROI. Nevertheless, fusion of holistic and the best patch based features further improves visual speech recognition performance, compared to either feature set alone. Finally, we discuss scenarios where the patch based approach may be preferable to holistic features.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:-_dYPAW6P2MC",
            "Publisher": "AVISA"
        },
        {
            "Title": "MobiLipNet: Resource-Efficient Deep Learning Based Lipreading.",
            "Publication year": 2019,
            "Publication url": "https://www.researchgate.net/profile/Alexandros-Koumparoulis/publication/335829172_MobiLipNet_Resource-Efficient_Deep_Learning_Based_Lipreading/links/5ea8ca8c92851cb26762e092/MobiLipNet-Resource-Efficient-Deep-Learning-Based-Lipreading.pdf",
            "Abstract": "Recent works in visual speech recognition utilize deep learning advances to improve accuracy. Focus however has been primarily on recognition performance, while ignoring the computational burden of deep architectures. In this paper we address these issues concurrently, aiming at both high computational efficiency and recognition accuracy in lipreading. For this purpose, we investigate the MobileNet convolutional neural network architectures, recently proposed for image classification. In addition, we extend the 2D convolutions of MobileNets to 3D ones, in order to better model the spatio-temporal nature of the lipreading problem. We investigate two architectures in this extension, introducing the temporal dimension as part of either the depthwise or the pointwise MobileNet convolutions. To further boost computational efficiency, we also consider using pointwise convolutions alone, as well as networks operating on half the mouth region. We evaluate the proposed architectures on speaker-independent visual-only continuous speech recognition on the popular TCD-TIMIT corpus. Our best system outperforms a baseline CNN by 4.27% absolute in word error rate and over 12 times in computational efficiency, whereas, compared to a state-of-the-art ResNet, it is 37 times more efficient at a minor 0.07% absolute error rate degradation.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:Z5m8FVwuT1cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Detecting audio-visual synchrony using deep neural networks",
            "Publication year": 2015,
            "Publication url": "https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_0548.pdf",
            "Abstract": "In this paper, we address the problem of automatically detecting whether the audio and visual speech modalities in frontal pose videos are synchronous or not. This is of interest in a wide range of applications, for example spoof detection in biometrics, lip-syncing, speaker detection and diarization in multi-subject videos, and video data quality assurance. In our adopted approach, we investigate the use of deep neural networks (DNNs) for this purpose. The proposed synchrony DNNs operate directly on audio and visual features over relatively wide contexts, or, alternatively, on appropriate hidden (bottleneck) or output layers of DNNs trained for single-modal or audio-visual automatic speech recognition. In all cases, the synchrony DNN classes consist of the \u201cin-sync\u201d and a number of \u201cout-of-sync\u201d targets, the latter considered at multiples of\u00b130 msec steps of overall asynchrony between the two modalities. We apply \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:PELIpwtuRlgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio-visual ASR from multiple views inside smart rooms",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4042060/",
            "Abstract": "Visual information from a speaker's mouth region is known to improve automatic speech recognition robustness. However, the vast majority of audio-visual automatic speech recognition (AVASR) studies assume frontal images of the speaker's face, which is not always the case in realistic human-computer interaction (HCI) scenarios. One such case of interest is HCI inside smart rooms, equipped with pan-tilt-zoom (PTZ) cameras that closely track the subject's head. Since however these cameras are fixed in space, they cannot necessarily obtain frontal views of the speaker. Clearly, AVASR from non-frontal views is required, as well as fusion of multiple camera views, if available. In this paper, we report our very preliminary work on this subject. In particular, we concentrate on two topics: first, the design of an AVASR system that operates on profile face views and its comparison with a traditional frontal-view AVASR \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:blknAaTinKkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Experiments in acoustic source localization using sparse arrays in adverse indoors environments",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6952878/",
            "Abstract": "In this paper we experiment with 2-D source localization in smart homes under adverse conditions using sparse distributed microphone arrays. We propose some improvements to deal with problems due to high reverberation, noise and use of a limited number of microphones. These consist of a pre-filtering stage for dereverberation and an iterative procedure that aims to increase accuracy. Experiments carried out in relatively large databases with both simulated and real recordings of sources in various positions indicate that the proposed method exhibits a better performance compared to others under challenging conditions while also being computationally efficient. It is demonstrated that although reverberation degrades localization performance, this degradation can be compensated by identifying the reliable microphone pairs and disposing of the outliers.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:olpn-zPbct0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio visual speech recognition",
            "Publication year": 2000,
            "Publication url": "https://infoscience.epfl.ch/record/82633/files/rr00-35.pdf",
            "Abstract": "We have made significant progress in automatic speech recognition ASR for well-defined applications like dictation and medium vocabulary transaction processing tasks in relatively controlled environments. However, for ASR to approach human levels of performance and for speech to become a truly pervasive user interface, we need novel, nontraditional approaches that have the potential of yielding dramatic ASR improvements. Visual speech is one such source for making large improvements in high noise environments with the potential of channel and task independence. It is not effected by the acoustic environment and noise, and it possibly contains the greatest amount of complementary information to the acoustic signal. In this workshop, our goal was to advance the state-of-the-art in ASR by demonstrating the use of visual information in addition to the traditional audio for large vocabulary continuous speech recognition LVCSR. Starting with an appropriate audio-visual database, collected and provided by IBM, we demonstrated for the first time that LVCSR performance can be improved by the use of visual information in the clean audio case. Specifically, by conducting audio lattice rescoring experiments, we showed a 7 relative word error rate WER reduction in that condition. Furthermore, for the harder problem of speech contaminated by speech babble\" noise at 10 dB SNR, we demonstrated that recognition performance can be improved by 27 in relative WER reduction, compared to an equivalent audio-only recognizer matched to the noise environment. We believe that this paves the way to seriously address the challenge of speech \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:u-x6o8ySG0sC",
            "Publisher": "IDIAP"
        },
        {
            "Title": "A real-time prototype for small-vocabulary audio-visual ASR",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1221655/",
            "Abstract": "We present a prototype for the automatic recognition of audio-visual speech, developed to augment the IBM ViaVoice/spl trade/ speech recognition system. Frontal face, full frame video is captured through a USB 2.0 interface by means of an inexpensive PC camera, and processed to obtain appearance-based visual features. Subsequently, these are combined with audio features, synchronously extracted from the acoustic signal, using a simple discriminant feature fusion technique. On the average, the required computations utilize approximately 67% of a Pentium/spl trade/ 4, 1.8 GHz processor, leaving the remaining resources available to hidden Markov model based speech recognition. Real-time performance is there- fore achieved for small-vocabulary tasks, such as connected-digit recognition. In the paper, we discuss the prototype architecture based on the ViaVoice engine, the basic algorithms employed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:qUcmZB5y_30C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio-visual speech synchronization detection using a bimodal linear prediction model",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5204303/",
            "Abstract": "In this work, we study the problem of detecting audio-visual (AV) synchronization in video segments containing a speaker in frontal head pose. The problem holds important applications in biometrics, for example spoofing detection, and it constitutes an important step in AV segmentation necessary for deriving AV fingerprints in multimodal speaker recognition. To attack the problem, we propose a time-evolution model for AV features and derive an analytical approach to capture the notion of synchronization between them. We report results on an appropriate AV database, using two types of visual features extracted from the speaker's facial area: geometric ones and features based on the discrete cosine image transform. Our results demonstrate that the proposed approach provides substantially better AV synchrony detection over a baseline method that employs mutual information, with the geometric visual features \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:HDshCWvjkbEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Computers in the human interaction loop",
            "Publication year": 2009,
            "Publication url": "https://scholar.google.com/scholar?cluster=17414465922651870713&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:EkHepimYqZsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Method for 3D Tracking Using Multiple Cameras",
            "Publication year": 2006,
            "Publication url": "http://web.cse.ohio-state.edu/tech-report/2006/TR79.pdf",
            "Abstract": "We present a computer vision system to robustly track an object in 3D by combining evidence from multiple calibrated cameras. Its novelty lies in the proposed unified approach to 3D kernel based tracking, that amounts to fusing the appearance features from all available camera sensors, as opposed to tracking the object appearance in the individual 2D views and fusing the results. The elegance of the method resides in its inherent ability to handle problems encountered by various 2D trackers, including scale selection, occlusion, view-dependence, and correspondence across different views. We apply the method on the CHIL project database for tracking the presenter\u2019s head during lectures inside smart rooms equipped with four calibrated cameras. As compared to traditional 2D based mean shift tracking approaches, the proposed algorithm results in 35% relative reduction in overall 3D tracking error and a 70% reduction in the number of tracker re-initializations.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:SP6oXDckpogC",
            "Publisher": "Unknown"
        },
        {
            "Title": "System and method for microphone activation using visual speech cues",
            "Publication year": 2004,
            "Publication url": "https://patents.google.com/patent/US6754373B1/en",
            "Abstract": "A system for activating a microphone based on visual speech cues, in accordance with the invention, includes a feature tracker coupled to an image acquisition device. The feature tracker tracks features in an image of a user. A region of interest extractor is coupled to the feature tracker. The region of interest extractor extracts a region of interest from the image of the user. A visual speech activity detector is coupled to the region of interest extractor and measures changes in the region of interest to determine if a visual speech cue has been generated by the user. A microphone is turned on by the visual speech activity detector when a visual speech cue has been determined by the visual speech activity detector. Methods for activating a microphone based on visual speech cues are also included.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:mB3voiENLucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Speech activity detection fusing acoustic phonetic and energy features",
            "Publication year": 2005,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2005/i05_0241.pdf",
            "Abstract": "With the wider deployment of automatic speech recognition (ASR) systems, the importance of robust speech activity detection has been elevated both as a means of reducing bandwidth in client/server ASR and for overall system stability from barge-in through the recognition process. In this paper we investigate a novel technique for speech activity detection, that we have found to be effective in handling non-stationary noise events without negatively impacting the recognition process. This technique is based on combining acoustic phonetic likelihood based features with energy features extracted from the signal waveform. Reported results on two speech activity detection tasks demonstrate that the proposed method outperforms techniques which rely solely on acoustic or energy features.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:maZDTaKrznsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Recent advances in the automatic recognition of audiovisual speech",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1230212/",
            "Abstract": "Visual speech information from the speaker's mouth region has been successfully shown to improve noise robustness of automatic speech recognizers, thus promising to extend their usability in the human computer interface. In this paper, we review the main components of audiovisual automatic speech recognition (ASR) and present novel contributions in two main areas: first, the visual front-end design, based on a cascade of linear image transforms of an appropriate video region of interest, and subsequently, audiovisual speech integration. On the latter topic, we discuss new work on feature and decision fusion combination, the modeling of audiovisual speech asynchrony, and incorporating modality reliability estimates to the bimodal recognition process. We also briefly touch upon the issue of audiovisual adaptation. We apply our algorithms to three multisubject bimodal databases, ranging from small- to large \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:u5HHmVD_uO8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Deliverable D4. 1: Robustness of Distant-Speech Recognition and Speaker Identification\u2013Development of Baseline System",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=2687161805209458240&hl=en&oi=scholarr",
            "Abstract": "This document reports on baseline development of DIRHA subsystems corresponding to the tasks of workpackage 4 of the project. Particular emphasis is given on the development and evaluation of distant automatic speech recognition baseline components in the four languages of interest in DIRHA and of the baseline distant speaker identification and verification.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:BrmTIyaxlBUC",
            "Publisher": "DIRHA Consortium, Tech. Rep"
        },
        {
            "Title": "Bilingual corpus for AVASR using multiple sensors and depth information",
            "Publication year": 2011,
            "Publication url": "https://www.academia.edu/download/30675170/48.pdf",
            "Abstract": "In this paper we present the Bilingual Audio-Visual Corpus with Depth information (BAVCD). The database contains utterances of connected digits, spoken by 15 subjects in English and 6 subjects in Greek, and collected employing multiple audio-visual sensors. Among them, of particular interest is the use of the Microsoft Kinect device, which is able to capture facial depth images using the structured light technique in addition to the traditional RGB video. The database allows conducting research on multiple aspects of small-vocabulary audio-visual automatic speech recognition, such as the use of visual depth information for speechreading, fusion of multiple video and audio streams, and language dependencies of the task. Preliminary results on the corpus are also presented.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:ZHo1McVdvXMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Fully Convolutional Sequence Learning Approach for Cued Speech Recognition from Videos",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9287365/",
            "Abstract": "Cued Speech constitutes a sign-based communication variant for the speech and hearing impaired, which involves visual information from lip movements combined with hand positional and gestural cues. In this paper, we consider its automatic recognition in videos, introducing a deep sequence learning approach that consists of two separately trained components: an image learner based on convolutional neural networks (CNNs) and a fully convolutional encoder-decoder. Specifically, handshape and lip visual features extracted from a 3D-CNN feature learner, as well as hand position embeddings obtained by a 2D-CNN, are concatenated and fed to a time-depth separable (TDS) block structure, followed by a multi-step attention-based convolutional decoder for phoneme prediction. To our knowledge, this is the first work where recognition of cued speech is addressed using a common modeling approach based \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:KUbvn5osdkgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Resource-Adaptive Deep Learning for Visual Speech Recognition.",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=3286098729164146918&hl=en&oi=scholarr",
            "Abstract": "We focus on the problem of efficient architectures for lipreading that allow trading-off computational resources for visual speech recognition accuracy. In particular, we make two contributions: First, we introduce MobiLipNetV3, an efficient and accurate lipreading model, based on our earlier work on MobiLipNetV2 and incorporating recent advances in convolutional neural network architectures. Second, we propose a novel recognition paradigm, called MultiRate Ensemble (MRE), that combines a \u201clean\u201d and a \u201cfull\u201d MobiLipNetV3 in the lipreading pipeline, with the latter applied at a lower frame rate. This architecture yields a family of systems offering multiple accuracy vs. efficiency operating points depending on the frame-rate decimation of the \u201cfull\u201d model, thus allowing adaptation to the available device resources. We evaluate our approach on the TCD-TIMIT corpus, popular in speaker-independent lipreading of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:tYavs44e6CUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio and visual modality combination in speech processing applications",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3015783.3015797",
            "Abstract": "Chances are that most of us have experienced difficulty in listening to our interlocutor during face-to-face conversation while in highly noisy environments, such as next to heavy traffic or over the background of high-intensity speech babble or loud music. In such occasions, we may have found ourselves looking at the speaker's lower face, while our interlocutor articulates speech, in order to help us enhance speech intelligibility. In fact, what we resort to in such circumstances is known as lipreading or speechreading, namely the recognition of the so-called\" visual speech modality\" and its combination (fusion) with the available noisy audio data.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:1yQoGdGgb4wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Far-field audio-visual scene perception of multi-party human-robot interaction for children and adults",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8462425/",
            "Abstract": "Human-robot interaction (HRI) is a research area of growing interest with a multitude of applications for both children and adult user groups, as, for example, in edutainment and social robotics. Crucial, however, to its wider adoption remains the robust perception of HRI scenes in natural, untethered, and multi-party interaction scenarios, across user groups. Towards this goal, we investigate three focal HRI perception modules operating on data from multiple audio-visual sensors that observe the HRI scene from the far-field, thus bypassing limitations and platform-dependency of contemporary robotic sensing. In particular, the developed modules fuse intra- and/or inter-modality data streams to perform: (i) audio-visual speaker localization; (ii) distant speech recognition; and (iii) visual recognition of hand-gestures. Emphasis is also placed on ensuring high speech and gesture recognition rates for both children and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:kzcrU_BdoSEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Emotion Understanding in Videos Through Body, Context, and Visual-Semantic Embedding Loss",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-66415-2_52",
            "Abstract": "We present our winning submission to the First International Workshop on Bodily Expressed Emotion Understanding (BEEU) challenge. Based on recent literature on the effect of context/environment on emotion, as well as visual representations with semantic meaning using word embeddings, we extend the framework of Temporal Segment Network to accommodate these. Our method is verified on the validation set of the Body Language Dataset (BoLD) and achieves 0.26235 Emotion Recognition Score on the test set, surpassing the previous best result of 0.2530.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:kuK5TVdYjLIC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Detection, diarization, and transcription of far-field lecture speech",
            "Publication year": 2007,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2007/i07_2161.pdf",
            "Abstract": "Speech processing of lectures recorded inside smart rooms has recently attracted much interest. In particular, the topic has been central to the Rich Transcription (RT) Meeting Recognition Evaluation campaign series, sponsored by NIST, with emphasis placed on benchmarking speech activity detection (SAD), speaker diarization (SPKR), speech-to-text (STT), and speakerattributed STT (SASTT) technologies. In this paper, we present the IBM systems developed to address these tasks in preparation for the RT 2007 evaluation, focusing on the far-field condition of lecture data collected as part of European project CHIL. For their development, the systems are benchmarked on a subset of the RT Spring 2006 (RT06s) evaluation test set, where they yield significant improvements for all SAD, SPKR, and STT tasks over RT06s results; for example, a 16% relative reduction in word error rate is reported in STT, attributed to a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:J_g5lzvAfSwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automated decision making using time-varying stream reliability prediction",
            "Publication year": 2007,
            "Publication url": "https://patents.google.com/patent/US7228279B2/en",
            "Abstract": "Automated decision making techniques are provided. For example, a technique for generating a decision associated with an individual or an entity includes the following steps. First, two or more data streams associated with the individual or the entity are captured. Then, at least one time-varying measure is computed in accordance with the two or more data streams. Lastly, a decision is computed based on the at least one time-varying measure. One form of the time-varying measure may include a measure of the coverage of a model associated with previously-obtained training data by at least a portion of the captured data. Another form of the time-varying measure may include a measure of the stability of at least a portion of the captured data. While either measure may be employed alone to compute a decision, preferably both the coverage and stability measures are employed. The technique may be used to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:3s1wT3WcHBgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Special issue on joint audio-visual speech processing",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=5827520607317210142&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:5Ul4iDaHHb8C",
            "Publisher": "HINDAWI PUBLISHING CORPORATION"
        },
        {
            "Title": "A cascade visual front end for speaker independent automatic speechreading",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/article/10.1023/A:1011352422845",
            "Abstract": "We propose a three-stage pixel-based visual front end for automatic speechreading (lipreading) that results in significantly improved recognition performance of spoken words or phonemes. The proposed algorithm is a cascade of three transforms applied on a three-dimensional video region-of-interest that contains the speaker's mouth area. The first stage is a typical image compression transform that achieves a high-energy, reduced-dimensionality representation of the video data. The second stage is a linear discriminant analysis-based data projection, which is applied on a concatenation of a small amount of consecutive image transformed video data. The third stage is a data rotation by means of a maximum likelihood linear transform that optimizes the likelihood of the observed data under the assumption of their class-conditional multivariate normal distribution with diagonal covariance. We applied the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:ULOm3_A8WrAC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "A cascade image transform for speaker independent automatic speechreading",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/871552/",
            "Abstract": "We propose a three-stage pixel based visual front end for automatic speechreading (lipreading) that results in improved recognition performance of spoken words or phonemes. The proposed algorithm is a cascade of three transforms applied to a three-dimensional video region of interest that contains the speaker's mouth area. The first stage is a typical image compression transform that achieves a high \"energy\", reduced-dimensionality representation of the video data. The second stage is a linear discriminant analysis based data projection, which is applied to a concatenation of a small number of consecutive image transformed video data. The third stage is a data rotation by means of a maximum likelihood linear transform. Such a transform optimizes the likelihood of the observed data under the assumption of their class conditional Gaussian distribution with diagonal covariance. We apply the algorithm to visual \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:YsMSGLbcyi4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improving audio-visual speech recognition with an infrared headset",
            "Publication year": 2003,
            "Publication url": "https://www.isca-speech.org/archive_open/avsp03/av03_175.html",
            "Abstract": "Visual speech is known to improve accuracy and noise robustness of automatic speech recognizers. However, almost all audio-visual ASR systems require tracking frontal facial features for visual information extraction, a computationally intensive and error-prone process. In this paper, we consider a specially designed infrared headset to capture audio-visual data, that consistently focuses on the speaker\u2019s mouth region, thus eliminating the need for face tracking. We conduct small-vocabulary recognition experiments on such data, benchmarking their ASR performance against traditional frontal, fullface videos, collected both at an ideal studio-like environment and at a more challenging office domain. By using the infrared headset, we report a dramatic improvement in visual-only ASR that amounts to a relative 30% and 54% word error rate reduction, compared to the studio and office data, respectively. Furthermore \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:R3hNpaxXUhUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multi-room speech activity detection using a distributed microphone network in domestic environments",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7362588/",
            "Abstract": "Domestic environments are particularly challenging for distant speech recognition: reverberation, background noise and interfering sources, as well as the propagation of acoustic events across adjacent rooms, critically degrade the performance of standard speech processing algorithms. In this application scenario, a crucial task is the detection and localization of speech events generated by users within the various rooms. A specific challenge of multi-room environments is the inter-room interference that negatively affects speech activity detectors. In this paper, we present and compare different solutions for the multi-room speech activity detection task. The combination of a model-based room-independent speech activity detection module with a room-dependent inside/outside classification stage, based on specific features, provides satisfactory performance. The proposed methods are evaluated on a multi-room \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:JoZmwDi-zQgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automatic speech activity detection, source localization, and speech recognition on the CHIL seminar corpus",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1521563/",
            "Abstract": "To realize the long-term goal of ubiquitous computing, technological advances in multi-channel acoustic analysis are needed in order to solve several basic problems, including speaker localization and tracking, speech activity detection (SAD) and distant-talking automatic speech recognition (ASR). The European Commission integrated project CHIL, \"computers in the human interaction loop\", aims to make significant advances in these three technologies. In this work, we report the results of our initial automatic source localization, speech activity detection, and speech recognition experiments on the CHIL seminar corpus, which is comprised of spontaneous speech collected by both near-and far-field microphones. In addition to the audio sensors, the seminars were also recorded by calibrated video cameras. This simultaneous audio-visual data capture enables the realistic evaluation of component technologies as \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:mVmsd5A6BfQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Person tracking in smart rooms using dynamic programming and adaptive subspace learning",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4037036/",
            "Abstract": "We present a robust vision system for single person tracking inside a smart room using multiple synchronized, calibrated, stationary cameras. The system consists of two main components, namely initialization and tracking, assisted by an additional component that detects tracking drift. The main novelty lies in the adaptive tracking mechanism that is based on subspace learning of the tracked person appearance in selected two-dimensional camera views. The sub-space is learned on the fly, during tracking, but in contrast to the traditional literature approach, an additional \"forgetting\" mechanism is introduced, as a means to reduce drifting. The proposed algorithm replaces mean-shift tracking, previously employed in our work. By combining the proposed technique with a robust initialization component that is based on face detection and spatio-temporal dynamic programming, the resulting vision system significantly \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:GnPB-g6toBAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Joint estimation of DOA and speech based on EM beamforming",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5496144/",
            "Abstract": "In this paper, we propose a multi-microphone joint optimal estimation of the direction of arrival (DOA) and the source speech signal through newly introduced EM beamforming. This produces a posterior PDF for the DOA, based only on the reliable speech spectrum. By maximizing over the posterior PDF of the DOA, we achieve maximum a posteriori DOA estimation. After convergence, the estimated source spectrum through weighted sum in the Bayesian sense is a maximum likelihood estimate (MLE). This is a sufficient statistic for minimum mean square error (MMSE) optimal estimation using a subsequent single channel MMSE filter.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:M05iB0D1s5AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio-visual speech recognition",
            "Publication year": 2005,
            "Publication url": "https://philpapers.org/rec/POTASR",
            "Abstract": "G. Potamianos & J. Luettin, Audio-visual speech recognition - PhilPapers Sign in | Create \nan account PhilPapers PhilPeople PhilArchive PhilEvents PhilJobs PhilPapers home \nSyntax Advanced Search Syntax Advanced Search Syntax Advanced Search Audio-visual \nspeech recognition G. Potamianos & J. Luettin In Alex Barber (ed.), Encyclopedia of \nLanguage and Linguistics. Elsevier (2005) Abstract This article has no associated abstract. (fix \nit) Keywords No keywords specified (fix it) Categories Aspects of Consciousness in \nPhilosophy of Mind Philosophy of Consciousness in Philosophy of Mind (categorize this \npaper) Buy the book Find it on Amazon.com Options Edit this record Mark as duplicate \nExport citation Find it on Scholar Request removal from index Revision history Download \noptions PhilArchive copy Upload a copy of this paper Check publisher's policy Papers \ncurrently archived: 59,666 External links This . \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:1qzjygNMrQYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Room-localized speech activity detection in multi-microphone smart homes",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/article/10.1186/s13636-019-0158-8",
            "Abstract": "Voice-enabled interaction systems in domestic environments have attracted significant interest recently, being the focus of smart home research projects and commercial voice assistant home devices. Within the multi-module pipelines of such systems, speech activity detection (SAD) constitutes a crucial component, providing input to their activation and speech recognition subsystems. In typical multi-room domestic environments, SAD may also convey spatial intelligence to the interaction, in addition to its traditional temporal segmentation output, by assigning speech activity at the room level. Such room-localized SAD can, for example, disambiguate user command referents, allow localized system feedback, and enable parallel voice interaction sessions by multiple subjects in different rooms. In this paper, we investigate a room-localized SAD system for smart homes equipped with multiple microphones distributed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:AvfA0Oy_GE0C",
            "Publisher": "SpringerOpen"
        },
        {
            "Title": "Far-field multimodal speech processing and conversational interaction in smart spaces",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4538701/",
            "Abstract": "Robust speech processing constitutes a crucial component in the development of usable and natural conversational interfaces. In this paper we are particularly interested in human-computer interaction taking place in \"smart\" spaces - equipped with a number of far- field, unobtrusive microphones and camera sensors. Their availability allows multi-sensory and multi-modal processing, thus improving robustness of speech-based perception technologies in a number of scenarios of interest, for example lectures and meetings held inside smart conference rooms, or interaction with domotic devices in smart homes. In this paper, we overview recent work at IBM Research in developing state-of-the-art speech technology in smart spaces. In particular we discuss acoustic scene analysis, speech activity detection, speaker diarization, and speech recognition, emphasizing multi-sensory or multi-modal processing. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:BqipwSGYUEgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automatic speech recognition",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-1-84882-054-8_6",
            "Abstract": " Automatic speech recognition (ASR) is a critical component for CHIL services. For example, it provides the input to higher-level technologies, such as summarization and question answering, as discussed in Chapter 8. In the spirit of ubiquitous computing, the goal of ASR in CHIL is to achieve a high performance using far-field sensors (networks of microphone arrays and distributed far-field microphones). However, close-talking microphones are also of interest, as they are used to benchmark ASR system development by providing a best-case acoustic channel scenario to compare against.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:_xSYboBqXhAC",
            "Publisher": "Springer, London"
        },
        {
            "Title": "Audio-visual speech recognition using an infrared headset",
            "Publication year": 2004,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167639304001116",
            "Abstract": "It is well known that frontal video of the speaker\u2019s mouth region contains significant speech information that, when combined with the acoustic signal, can improve accuracy and noise robustness of automatic speech recognition (ASR) systems. However, extraction of such visual speech information from full-face videos is computationally expensive, as it requires tracking faces and facial features. In addition, robust face detection remains challenging in practical human\u2013computer interaction (HCI), where the subject\u2019s posture and environment (lighting, background) are hard to control, and thus successfully compensate for. In this paper, in order to bypass these hindrances to practical bimodal ASR, we consider the use of a specially designed, wearable audio-visual headset, a feasible solution in certain HCI scenarios. Such a headset can consistently focus on the speaker\u2019s mouth region, thus eliminating altogether the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "ATHENA: A Greek multi-sensory database for home automation control",
            "Publication year": 2014,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2014/i14_1608.pdf",
            "Abstract": "In this paper we present a Greek speech database with real multi-modal data in a smart home two-room environment. In total, 20 speakers were recorded in 240 one-minute long sessions. The recordings include utterances of activation keywords and commands for home automation control, but also phonetically rich sentences and conversational speech. Audio, speaker movements and gestures were captured by 20 condenser microphones installed on the walls and ceiling, 6 MEMS microphones, 2 close-talk microphones and one Kinect camera. The new publicly available database exhibits adverse noise conditions because of background noises and acoustic events performed during the recordings to better approximate a realistic everyday home scenario. Thus, it is suitable for experimentation on voice activity and event detection, source localization, speech enhancement and far-field speech recognition. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:HE397vMXCloC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Object assembly guidance in child-robot interaction using RGB-D based 3d tracking",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8594187/",
            "Abstract": "This work examines how and to what benefit an autonomous humanoid robot can supervise a child in an object assembly task. In order to understand the child's actions, a novel 3D object tracking algorithm for RGB-D data is employed. The tracker consists of two stages: the first performs a tracking-by-detection scheme on the color stream, to locate the objects on the image plane, while the second uses a particle filter that operates on the depth data stream to refine the first stage output and infer the objects' rotations. Given the six degrees-of-freedom of the assembly part poses, the system is able to recognize which connections have been completed at any given time. This information is then used to select an appropriate verbal or gestural response for the robot. Experimental results show that (a) the tracking algorithm is accurate, fast and robust to severe occlusions and fast movements, (b) the proposed method of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:2KloaMYe4IUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The SL-ReDu Environment for Self-monitoring and Objective Learner Assessment in Greek Sign Language",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-78095-1_7",
            "Abstract": "Here we discuss the design and implementation features of a platform aiming to provide two distinct modules for self-monitoring and objective assessment of learners of the Greek Sign Language (GSL) as L2. The platform is designed according to user needs of both learners and instructors. It incorporates the educational content of the A0 and A1 levels of CEFR. The platform provides a user-friendly environment that guarantees improvement of learner\u2019s skills, objectivity in learner assessment and enhanced SL knowledge grading. Active learner language production is assessed via an innovative SL recognition engine, while standard multimedia-based drills assess learners\u2019 comprehension skills.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:k8Z6L05lTy4C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Audio-visual automatic speech recognition and related bimodal speech technologies: A review of the state-of-the-art and open problems",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5373530/",
            "Abstract": "Summary form only given. The presentation will provide an overview of the main research achievements and the state-of-the-art in the area of audiovisual speech processing, mainly focusing in the area of audio-visual automatic speech recognition. The topic has been of interest in the speech research community due to the potential of increased robustness to acoustic noise that the visual modality holds. Nevertheless, significant challenges remain that have hindered practical applications of the technology most notably difficulties with visual speech information extraction and audio-visual fusion algorithms that remain robust to the audio-visual environment variability inherent in practical, unconstrained interaction scenarios and audio-visual data sources, for example multiparty interaction in smart spaces, broadcast news, etc. These challenges are also shared across a number of interesting audio-visual speech \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:Tiz5es2fbqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "SL-ReDu: Greek sign language recognition for educational applications. Project description and early results",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3389189.3398006",
            "Abstract": "We present SL-ReDu, a recently commenced innovative project that aims to exploit deep-learning progress to advance the state-of-the-art in video-based automatic recognition of Greek Sign Language (GSL), while focusing on the use-case of GSL education as a second language. We first briefly overview the project goals, focal areas, and timeline. We then present our initial deep learning-based approach for GSL recognition that employs efficient visual tracking of the signer hands, convolutional neural networks for feature extraction, and attention-based encoder-decoder sequence modeling for sign prediction. Finally, we report experimental results for small-vocabulary, isolated GSL recognition on the single-signer\" Polytropon\" corpus. To our knowledge, this work constitutes the first application of deep-learning techniques to GSL.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:ILKRHgRFtOwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio-visual speech recognition in challenging environments.",
            "Publication year": 2003,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.5485&rep=rep1&type=pdf",
            "Abstract": "Visual speech information is known to improve accuracy and noise robustness of automatic speech recognizers. However, todate, all audio-visual ASR work has concentrated on \u201cvisually clean\u201d data with limited variation in the speaker\u2019s frontal pose, lighting, and background. In this paper, we investigate audiovisual ASR in two practical environments that present significant challenges to robust visual processing:(a) Typical offices, where data are recorded by means of a portable PC equipped with an inexpensive web camera, and (b) automobiles, with data collected at three approximate speeds. The performance of all components of a state-of-the-art audio-visual ASR system is reported on these two sets and benchmarked against \u201cvisually clean\u201d data recorded in a studio-like environment. Not surprisingly, both audio-and visual-only ASR degrade, more than doubling their respective word error rates. Nevertheless, visual speech remains beneficial to ASR.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:Se3iqnhoufwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deep affordance-grounded sensorimotor object recognition",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2017/html/Thermos_Deep_Affordance-Grounded_Sensorimotor_CVPR_2017_paper.html",
            "Abstract": "It is well-established by cognitive neuroscience that human perception of objects constitutes a complex process, where object appearance information is combined with evidence about the so-called object\" affordances\", namely the types of actions that humans typically perform when interacting with them. This fact has recently motivated the\" sensorimotor\" approach to the challenging task of automatic object recognition, where both information sources are fused to improve robustness. In this work, the aforementioned paradigm is adopted, surpassing current limitations of sensorimotor object recognition research. Specifically, the deep learning paradigm is introduced to the problem for the first time, developing a number of novel neuro-biologically and neuro-physiologically inspired architectures that utilize state-of-the-art neural networks for fusing the available information sources in multiple ways. The proposed methods are evaluated using a large RGB-D corpus, which is specifically collected for the task of sensorimotor object recognition and is made publicly available. Experimental results demonstrate the utility of affordance information to object recognition, achieving an up to 29% relative error reduction by its inclusion.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:ZuybSZzF8UAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dynamic stream weight modeling for audio-visual speech recognition",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4218258/",
            "Abstract": "To generate optimal multi-stream audio-visual speech recognition performance, appropriate dynamic weighting of each modality is desired. In this paper, we propose to estimate such weights based on a combination of acoustic signal space observations and single-modality audio and visual speech model likelihoods. Two modeling approaches are investigated for such weight estimation: one based on a sigmoid fitting function, the other employing Gaussian mixture models. Reported experiments demonstrate that the later approach outperforms sigmoid based modeling, and is dramatically superior to the static weighting scheme.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:RHpTSmoSYBkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Deliverable D3. 2 Multi-microphone front-end",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=9463004703869182328&hl=en&oi=scholarr",
            "Abstract": "This document reports on multi-microphone processing techniques addressing source localization, acoustic echo cancellation, source enhancement, and acoustic event detection and classification. It provides baseline results obtained using the realistic simulated data of the first",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:eflP2zaiRacC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio visual speech recognition in noisy visual environments",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2141622.2141646",
            "Abstract": "Speech recognition is a natural means of interaction for a human with a smart assistive environment. In order for this interaction to be effective, such a system should attain a high recognition rate even under adverse conditions. Audio-visual speech recognition (AVSR) can be of help in such environments, especially under the presence of audio noise. However the impact of visual noise to its performance has not been studied sufficiently in the literature. In this paper, we examine the effects of visual noise to AVSR, reporting experiments on the relatively simple task of connected digit recognition, under moderate acoustic noise and a variety of types of visual noise. The latter can be caused by either faulty sensors or video signal transmission problems that can be found in smart assistive environments. Our AVSR system exhibits higher accuracy in comparison to an audio-only recognizer and robust performance in most \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Emotion Understanding in Videos Through Body, Context, and Visual-Semantic Embedding Loss",
            "Publication year": 2020,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2020arXiv201016396P/abstract",
            "Abstract": "We present our winning submission to the First International Workshop on Bodily Expressed Emotion Understanding (BEEU) challenge. Based on recent literature on the effect of context/environment on emotion, as well as visual representations with semantic meaning using word embeddings, we extend the framework of Temporal Segment Network to accommodate these. Our method is verified on the validation set of the Body Language Dataset (BoLD) and achieves 0.26235 Emotion Recognition Score on the test set, surpassing the previous best result of 0.2530.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:gsN89kCJA0AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Lipreading using profile versus frontal views",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4064511/",
            "Abstract": "Visual information from a speaker's mouth region is known to improve automatic speech recognition robustness. However, the vast majority of audio-visual automatic speech recognition (AVASR) studies assume frontal images of the speaker's face. In contrast, this paper investigates extracting visual speech information from the speaker's profile view, and, to our knowledge, constitutes the first real attempt to attack this problem. As with any AVASR system, the overall recognition performance depends heavily on the visual front end. This is especially the case with profile-view data, as the facial features are heavily compacted compared to the frontal scenario. In this paper, we particularly describe our visual front end approach, and report experiments on a multi-subject, small-vocabulary, bimodal, multi-sensory database that contains synchronously captured audio with frontal and profile face video. Our experiments \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:hqOjcs7Dif8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Noisy audio feature enhancement using audio-visual speech data",
            "Publication year": 2002,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.2650&rep=rep1&type=pdf",
            "Abstract": "We investigate improving automatic speech recognition (ASR) in noisy conditions by enhancing noisy audio features using visual speech captured from the speaker\u2019s face. The enhancement is achieved by applying a linear filter to the concatenated vector of noisy audio and visual features, obtained by mean square error estimation of the clean audio features in a training stage. The performance of the enhanced audio features is evaluated on two ASR tasks: A connected digits task and speaker-independent, largevocabulary, continuous speech recognition. In both cases and at sufficiently low signal-to-noise ratios (SNRs), ASR trained on the enhanced audio features significantly outperforms ASR trained on the noisy audio, achieving for example a 46% relative reduction in word error rate on the digits task at-3.5 dB SNR. However, the method fails to capture the full visual modality benefit to ASR, as demonstrated by its comparison to discriminant audio-visual feature fusion introduced in previous work.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:u9iWguZQMMsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Asynchrony modeling for audio-visual speech recognition",
            "Publication year": 2002,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.7.5071&rep=rep1&type=pdf",
            "Abstract": "We investigate the use of multi-stream HMMs in the automatic recognition of audio-visual speech. Multi-stream HMMs allow the modeling of asynchrony between the audio and visual state sequences at a variety of levels (phone, syllable, word, etc.) and are equivalent to product, or composite, HMMs. In this paper, we consider such models synchronized at the phone boundary level, allowing various degrees of audio and visual state-sequence asynchrony. Furthermore, we investigate joint training of all product HMM parameters, instead of just composing the model from separately trained audio-and visual-only HMMs. We report experiments on a multi-subject connected digit recognition task, as well as on a more complex, speaker-independent large-vocabulary dictation task. Our results demonstrate that in both cases, joint multistream HMM training is superior to separate training of singlestream HMMs. In addition, we observe that allowing state-sequence asynchrony between the HMM audio and visual components improves connected digit recognition significantly, however it degrades performance on the dictation task. The resulting multi-stream models dramatically improve speech recognition robustness to noise, by successfully exploiting the visual modality speech information: For example, at 11 dB SNR, they reduce connected digit word error rate from the audio-only 2.3% to 0.77% audio-visual, and, for the largevocabulary task, from 28.3% to 19.5%. Compared to the audioonly performance at 10 dB SNR, the use of multi-stream HMMs achieves an effective SNR gain of up to 9 dB and 7 dB respectively, for the two recognition tasks considered.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:WF5omc3nYNoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Large-vocabulary audio-visual speech recognition by machines and humans",
            "Publication year": 2001,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.81&rep=rep1&type=pdf",
            "Abstract": "We compare automatic recognition with human perception of audio-visual speech, in the large-vocabulary, continuous speech recognition (LVCSR) domain. Specifically, we study the benefit of the visual modality for both machines and humans, when combined with audio degraded by speech-babble noise at various signal-to-noise ratios (SNRs). We first consider an automatic speechreading system with a pixel based visual front end that uses feature fusion for bimodal integration, and we compare its performance with an audio-only LVCSR system. We then describe results of human speech perception experiments, where subjects are asked to transcribe audio-only and audiovisual utterances at various SNRs. For both machines and humans, we observe approximately a 6 dB effective SNR gain compared to the audio-only performance at 10 dB, however such gains significantly diverge at other SNRs. Furthermore, automatic audio-visual recognition outperforms human audioonly speech perception at low SNRs.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:UebtZRa9Y70C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Stream confidence estimation for audio-visual speech recognition.",
            "Publication year": 2000,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.6253&rep=rep1&type=pdf",
            "Abstract": "We investigate the use of single modality confidence measures as a means of estimating adaptive, local weights for improved audio-visual automatic speech recognition. We limit our work to the toy problem of audio-visual phonetic classification by means of a two-stream Gaussian mixture model (GMM), where each stream models the class conditional audio-or visual-only observation probability, raised to an appropriate exponent. We consider such stream exponents as two-dimensional piecewise constant functions of the audio and visual stream local confidences, and we estimate them by minimizing the misclassification error on a held-out data set. Three stream confidence measures are investigated, namely the stream entropy, the n-best likelihood ratio average, and an n-best stream likelihood dispersion measure. The later results in superior audio-visual phonetic classification, as indicated by our experiments on a 260-subject, 40-hour long, large vocabulary, continuous speech audio-visual dataset. By using local, dispersion-based stream exponents, we achieve an additional 20% phone classification accuracy improvement over the improvement that global stream exponents add to clean audio-only phonetic classification. The performance of the algorithm however still falls significantly short of an \u201coracle\u201d(cheating) confidence estimation scheme.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:W7OEmFMy1HYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improved face finding in visually challenging environments",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1521612/",
            "Abstract": "Finding faces in visually challenging environments is crucial to many applications, such as audio-visual automatic speech recognition, video indexing, person recognition, and video surveillance. In this study, we investigate several algorithms to improve face detection accuracy in visually challenging environments using the IBM appearance based face detection system. The algorithms considered are trainable skintone pre-screening, Hamming windowing of the face images, DCT coefficient selection, and the AdaBoost technique. When these methods are combined, an up to 68% relative reduction in face detection error is observed on visually challenging datasets.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:pyW8ca7W8N0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multichannel speech enhancement using MEMS microphones",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7178467/",
            "Abstract": "In this work, we investigate the efficacy of Micro Electro-Mechanical System (MEMS) microphones, a newly developed technology of very compact sensors, for multichannel speech enhancement. Experiments are conducted on real speech data collected using a MEMS microphone array. First, the effectiveness of the array geometry for noise suppression is explored, using a new corpus containing speech recorded in diffuse and localized noise fields with a MEMS microphone array configured in linear and hexagonal array geometries. Our results indicate superior performance of the hexagonal geometry. Then, MEMS microphones are compared to Electret Condenser Microphones (ECMs), using the ATHENA database, which contains speech recorded in realistic smart home noise conditions with hexagonal-type arrays of both microphone types. MEMS microphones exhibit performance similar to ECMs. Good \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:tkaPQYYpVKoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audiovisual automatic speech recognition: Progress and challenges",
            "Publication year": 2008,
            "Publication url": "https://asa.scitation.org/doi/abs/10.1121/1.2936018",
            "Abstract": "The paper overviews recent progress and challenges in a number of audiovisual speech processing technologies with main emphasis on the problem of automatic speech recognition. It is well known that visual channel information can improve automatic speech processing for human\u2010computer interaction. To automatically process and incorporate such information into automatic systems, a number of steps are required that are surprisingly similar accross speech technologies. Crucial above all is the issue of feature representation of visual speech and its robust extraction. In addition, appropriate integration of the audio and visual representations is required, in order to ensure improved performance of the bimodal systems over audio\u2010only baselines. These topics are discussed in detail in the talk, with main emphasis on their application to the speech recognition problem in the challenging environments of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:D03iK_w7-QYC",
            "Publisher": "Acoustical Society of America"
        },
        {
            "Title": "Exploiting lower face symmetry in appearance-based automatic speechreading.",
            "Publication year": 2005,
            "Publication url": "http://isca-speech.org/archive_open/archive_papers/avsp05/av05_079.pdf",
            "Abstract": "Appearance-based visual speech feature extraction is being widely used in the automatic speechreading and audio-visual speech recognition literature. In its most common application, the discrete cosine transform (DCT) is utilized to compress the image of the speaker\u2019s mouth region-of-interest (ROI), and the highest energy spatial frequency components are retained as visual features. Good generalization performance of the resulting system however requires robust ROI extraction and its consistent normalization, designed to compensate for speaker headpose and other data variations. In general, one expects that the ROI-if correctly normalized-will be nearly laterally symmetric, due to the approximate symmetry of human faces. We thus argue that forcing lateral ROI symmetry can be beneficial to automatic speechreading, providing a mechanism to compensate for small face and mouth tracking errors, which \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:dhFuZR0502QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A comparison of multicamera person-tracking algorithms",
            "Publication year": 2006,
            "Publication url": "http://andrewsenior.com/papers/SeniorVS06.pdf",
            "Abstract": "In this paper, we present a comparison of four novel algorithms that have been applied to the tracking of people in an indoor scenario. Tracking is carried out in 3D or 2D (ground plane) to provide position information for a variety of surveillance, HCI or meeting-support services. The algorithms, based on background subtraction, face detection, particle filter feature-matching and edge alignment of a cylindrical model are described and comparative results presented using independent test data produced and ground-truthed for the EU CHIL project.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:lSLTfruPkqcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Computers in the human interaction loop",
            "Publication year": 2010,
            "Publication url": "https://scholar.google.com/scholar?cluster=6505549392666818195&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:WA5NYHcadZ8C",
            "Publisher": "Handbook on Ambient Intelligence and Smart Environments (AISE) 1071 1071"
        },
        {
            "Title": "Multi-view fusion for action recognition in child-robot interaction",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8451146/",
            "Abstract": "Answering the challenge of leveraging computer vision methods in order to enhance Human Robot Interaction (HRI) experience, this work explores methods that can expand the capabilities of an action recognition system in such tasks. A multi-view action recognition system is proposed for integration in HRI scenarios with special users, such as children, in which there is limited data for training and many state-of-the-art techniques face difficulties. Different feature extraction approaches, encoding methods and fusion techniques are combined and tested in order to create an efficient system that recognizes children pantomime actions. This effort culminates in the integration of a robotic platform and is evaluated under an alluring Children Robot Interaction scenario.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:ZfRJV9d4-WMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Mutual information based visual feature selection for lipreading",
            "Publication year": 2004,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2004/i04_2037.pdf",
            "Abstract": "Image transforms, such as the discrete cosine, are widely used to extract visual features from the speaker\u2019s mouth region to be used in automatic speechreading and audio-visual speech recognition. Typically, the spatial frequency components with the highest energy in the transform space are retained for recognition. This paper proposes an alternative technique for selecting such features, by utilizing the mutual information criterion instead. Mutual information between each individual spatial frequency component and the speech classes of interest is employed as a measure of its appropriateness for speech classification. The highest mutual information components are then selected as visual speech features. Extensions to this scheme by using joint mutual information between candidate feature pairs and classes are also considered. The algorithm is tested on visual-only speech recognition of connected-digit \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal Fusion and Sequence Learning for Cued Speech Recognition from Videos",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-78095-1_21",
            "Abstract": "Cued Speech (CS) constitutes a non-vocal mode of communication that relies on lip movements in conjunction with hand positional and gestural cues, in order to disambiguate phonetic information and make it accessible to the speech and hearing impaired. In this study, we address the automatic recognition of CS from videos, employing deep learning techniques and extending our earlier work on this topic as follows: First, for visual feature extraction, in addition to hand positioning embeddings and convolutional neural network-based appearance features of the mouth region and signing hand, we consider structural information of the hand and mouth articulators. Specifically, we utilize the OpenPose framework to extract 2D lip keypoints and hand skeletal coordinates of the signer, and we also infer 3D hand skeletal coordinates from the latter exploiting own earlier work on 2D-to-3D hand-pose regression. Second \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:kz9GbA2Ns4gC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "A hardware-software framework for high-reliability people fall detection",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4716690/",
            "Abstract": "This paper presents a hardware and software framework for reliable fall detection in the home environment, with particular focus on the protection and assistance to the elderly. The integrated prototype includes three different sensors: a 3D time-of-flight range camera, a wearable MEMS accelerometer and a microphone. These devices are connected with custom interface circuits to a central PC that collects and processes the information with a multi-threading approach. For each of the three sensors, an optimized algorithm for fall-detection has been developed and benchmarked on a collected mulitimodal database. This work is expected to lead to a multi-sensory approach employing appropriate fusion techniques aiming to improve system efficiency and reliability.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:JV2RwH3_ST0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio-visual selection process for the synthesis of photo-realistic talking-head animations",
            "Publication year": 2003,
            "Publication url": "https://patents.google.com/patent/US6654018B1/en",
            "Abstract": "A system and method for generating photo-realistic talking-head animation from a text input utilizes an audio-visual unit selection process. The lip-synchronization is obtained by optimally selecting and concatenating variable-length video units of the mouth area. The unit selection process utilizes the acoustic data to determine the target costs for the candidate images and utilizes the visual data to determine the concatenation costs. The image database is prepared in a hierarchical fashion, including high-level features (such as a full 3D modeling of the head, geometric size and position of elements) and pixel-based, low-level features (such as a PCA-based metric for labeling the various feature bitmaps).",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:4TOpqqG69KYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A hybrid approach to hand detection and type classification in upper-body videos",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8611755/",
            "Abstract": "Detection of hands in videos and their classification into left and right types are crucial in various human-computer interaction and data mining systems. A variety of effective deep learning methods have been proposed for this task, such as region-based convolutional neural networks (R-CNNs), however the large number of their proposal windows per frame deem them computationally intensive. For this purpose we propose a hybrid approach that is based on substituting the \u201cselective search\u201d R-CNN module by an image processing pipeline assuming visibility of the facial region, as for example in signing and cued speech videos. Our system comprises two main phases: preprocessing and classification. In the preprocessing stage we incorporate facial information, obtained by an AdaBoost face detector, into a skin-tone based segmentation scheme that drives Kalman filtering based hand tracking, generating very \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:35r97b3x0nAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Visual speech recognition across multiple views",
            "Publication year": 2009,
            "Publication url": "https://www.igi-global.com/chapter/visual-speech-recognition/31072",
            "Abstract": "It is well known that visual speech information extracted from video of the speaker\u2019s mouth region can improve performance of automatic speech recognizers, especially their robustness to acoustic degradation. However, the vast majority of research in this area has focused on the use of frontal videos of the speaker\u2019s face, a clearly restrictive assumption that limits the applicability of audio-visual automatic speech recognition (AVASR) technology in realistic human-computer interaction. In this chapter, the authors advance beyond the single-camera, frontal-view AVASR paradigm, investigating various important aspects of the visual speech recognition problem across multiple camera views of the speaker, expanding on their recent work. The authors base their study on an audio-visual database that contains synchronous frontal and profile views of multiple speakers, uttering connected digit strings. They first develop \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:35N4QoGY0k4C",
            "Publisher": "IGI Global"
        },
        {
            "Title": "Exploring ROI size in deep learning based lipreading.",
            "Publication year": 2017,
            "Publication url": "https://avsp2017.loria.fr/wp-content/uploads/2017/07/AVSP2017_paper_24.pdf",
            "Abstract": "Automatic speechreading systems have increasingly exploited deep learning advances, resulting in dramatic gains over traditional methods. State-of-the-art systems typically employ convolutional neural networks (CNNs), operating on a video region-of-interest (ROI) that contains the speaker\u2019s mouth. However, little or no attention has been paid to the effects of ROI physical coverage and resolution on the resulting recognition performance within the deep learning framework. In this paper, we investigate such choices for a visual-only speech recognition system based on CNNs and long short-term memory models that we present in detail. Further, we employ a separate CNN to perform face detection and facial landmark localization, driving ROI extraction. We conduct experiments on a multi-speaker corpus of connected digits utterances, recorded in ideal visual conditions. Our results show that ROI design affects automatic speechreading performance significantly: the best visual-only word error rate (5.07%) corresponds to a ROI that contains a large part of the lower face, in addition to just the mouth, and at a relatively high resolution. Noticeably, the result represents a 27% relative error reduction compared to employing the entire lower face as the ROI.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:UHK10RUVsp4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio-visual speech synchrony detection by a family of bimodal linear prediction models",
            "Publication year": 2011,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=aqXD3iBuQewC&oi=fnd&pg=PA31&dq=info:aNkepYERtQcJ:scholar.google.com&ots=eIAiBp75Vm&sig=8yNEhcpNL3rCc1QVdg-GGGNW2K8",
            "Abstract": "Detecting whether the video of a speaking person in frontal head pose corresponds to the accompanying audio track is of interest in numerous multimodal biometrics-related applications. In many practical occasions, the audio and visual modalities may not be in sync; for example, we may observe static faces in images, the camera may be focusing on a nonspeaker, or a subject may be speaking in a foreign language with audio being translated to another language. Spoofing attacks in audiovisual biometric systems also often involve audio and visual data streams that are not in sync. Audiovisual (AV) synchrony indicates consistency between the audio and visual streams and thus the reliability for the segments to belong to the same individual. Such segments could then serve as building blocks for generating bimodal fingerprints of the different individuals present in the AV data, which can be important for security, authentication, and biometric purposes. AV segmentation can also be important for speaker turn detection, as well as automatic indexing and retrieval of different occurrences of a speaker.The problem of AV synchrony detection has already been considered in the literature. We refer to Bredin and Chollet (2007) for a comprehensive review on this topic, where the authors present a detailed discussion on different aspects of AV synchrony detection, including feature processing, dimensionality reduction, and correspondence detection measures. In that paper, AV synchrony detection is applied to the problem of identity verification, but the authors also mention additional applications in sound source localization, AV sequence indexing, film \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:cFHS6HbyZ2cC",
            "Publisher": "Ch"
        },
        {
            "Title": "The Athena-RC system for speech activity detection and speaker localization in the DIRHA smart home",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6843273/",
            "Abstract": "We present our system for speech activity detection and speaker localization inside a smart home with multiple rooms equipped with microphone arrays of known geometry and placement. The smart home is developed as part of the DIRHA European funded project, providing both simulated and real data for system development and evaluation, under extremely challenging conditions of noise, reverberation, and speech overlap. Our proposed approach performs speech activity detection first, by employing multi-microphone decision fusion on traditional statistical models and acoustic features, within a Viterbi decoding framework, further assisted by signal energy- and model log-likelihood threshold-based heuristics. Then it performs speaker localization using traditional time-difference of arrival estimation between properly selected microphone pairs, further assisted by a dereverberation component. The system \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:5ugPr518TE4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Deep sensorimotor learning for RGB-D object recognition",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1077314219301432",
            "Abstract": "Research findings in cognitive neuroscience establish that humans, early on, develop their understanding of real-world objects by observing others interact with them or by performing active exploration and physical interactions with them. This fact has motivated the so-called \u201csensorimotor\u201d learning approach, where the object appearance information (sensory) is combined with the object affordances (motor), i.e. the types of actions a human can perform with the object. In this work, the aforementioned paradigm is adopted, and a neuro-biologically inspired two-stream model for RGB-D object recognition is investigated. Both streams are realized as state-of-the-art deep neural networks that process and fuse appearance and affordance information in multiple ways. In particular, three model variants are developed to efficiently encode the spatio-temporal nature of the hand\u2013object interaction, while an attention \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:4MWp96NkSFoC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Fusing body posture with facial expressions for joint recognition of affect in child\u2013robot interaction",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8769871/",
            "Abstract": "In this letter, we address the problem of multi-cue affect recognition in challenging scenarios such as child\u2013robot interaction. Toward this goal we propose a method for automatic recognition of affect that leverages body expressions alongside facial ones, as opposed to traditional methods that typically focus only on the latter. Our deep-learning based method uses hierarchical multi-label annotations and multi-stage losses, can be trained both jointly and separately, and offers us computational models for both individual modalities, as well as for the whole body emotion. We evaluate our method on a challenging child\u2013robot interaction database of emotional expressions collected by us, as well as on the GEneva multimodal emotion portrayal public database of acted emotions by adults, and show that the proposed method achieves significantly better results than facial-only expression baselines.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:evX43VCCuoAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio-visual speech recognition incorporating facial depth information captured by the Kinect",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6334244/",
            "Abstract": "We investigate the use of facial depth data of a speaking subject, captured by the Kinect device, as an additional speech-informative modality to incorporate to a traditional audiovisual automatic speech recognizer. We present our feature extraction algorithm for both visual and accompanying depth modalities, based on a discrete cosine transform of the mouth region-of-interest data, further transformed by a two-stage linear discriminant analysis projection to incorporate speech dynamics and improve classification. For automatic speech recognition utilizing the three available data streams (audio, visual, and depth), we consider both the feature and decision fusion paradigms, the latter via a state-synchronous tri-stream hidden Markov model. We report multi-speaker recognition results on a small-vocabulary task employing our recently collected bilingual audio-visual corpus with depth information, demonstrating \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:WbkHhVStYXYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robust multi-view multi-camera face detection inside smart rooms using spatio-temporal dynamic programming",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1613054/",
            "Abstract": "Robust face detection presents a difficult problem in real interaction scenarios, that, in order to achieve, most often requires employing additional sources of information. In this paper, we consider two such sources: temporal information, available in the form of video sequences, and spatial information, available from multiple calibrated cameras with synchronous, overlapping fields of view of the 3D scene of interest. These two sources are exploited jointly, using a novel dynamic programming approach, for a lecture scenario inside appropriately equipped smart rooms, aiming at robust face detection of the lecturer within the available 2D camera views. Experimental results, reported on the CHIL project database, demonstrate that the proposed approach outperforms purely frame-based face detection",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:ZeXyd9-uunAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An extended pose-invariant lipreading system",
            "Publication year": 2007,
            "Publication url": "https://eprints.qut.edu.au/12843/",
            "Abstract": "In recent work, we have concentrated on the problem of lipreading from non-frontal views (poses). In particular, we have focused on the use of profile views, and proposed two approaches for lipreading on basis of visual features extracted from such views: (a) Direct statistical modeling of the features, namely use of view-dependent statistical models; and (b) Normalization of such features by their projection onto the ``space'' of frontal-view visual features, which allows employing one set of statistical models for all available views. The latter approach has been considered for two only poses (frontal and profile views), and for visual features of a specific dimensionality. In this paper, we further extend this work, by investigating its applicability to the case where data from three views are available (frontal, left- and right-profile). In addition, we examine the effect of visual feature dimensionality on the pose-normalization approach. Our experiments demonstrate that results generalize well to three views, but also that feature dimensionality is crucial to the effectiveness of the approach. In particular, feature dimensionality larger than 30 is detrimental to multi-pose visual speech recognition performance.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:iH-uZ7U-co4C",
            "Publisher": "Tilburg University"
        },
        {
            "Title": "Maximum entropy and MCE based HMM stream weight estimation for audio-visual ASR",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5743873/",
            "Abstract": "In this paper, we propose a new fast and flexible algorithm based on the maximum entropy (MAXENT) criterion to estimate stream weights in a state-synchronous multi-stream HMM. The technique is compared to the minimum classification error (MCE) criterion and to a brute-force, grid-search optimization of the WER on both a small and a large vocabulary audio-visual continuous speech recognition task. When estimating global stream weights, the MAXENT approach gives comparable results to the grid-search and the MCE. Estimation of state dependent weights is also considered: We observe significant improvements in both the MAXENT and MCE criteria, which, however, do not result in significant WER gains.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:ufrVoPGSRksC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robust multi-modal method for recognizing objects",
            "Publication year": 2000,
            "Publication url": "https://patents.google.com/patent/US6118887A/en",
            "Abstract": "A method for tracking heads and faces is disclosed wherein a variety of different representation models can be used to define individual heads and facial features in a multi-channel capable tracking algorithm. The representation models generated by the channels during a sequence of frames are ultimately combined into a representation comprising a highly robust and accurate tracked output. In a preferred embodiment, the method conducts an initial overview procedure to establish the optimal tracking strategy to be used in light of the particular characteristics of the tracking application.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:0EnyYjriUFMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scattering vs. discrete cosine transform features in visual speech processing.",
            "Publication year": 2015,
            "Publication url": "http://isca-speech.org/archive/avsp15/papers/av15_175.pdf",
            "Abstract": "Appearance-based feature extraction constitutes the dominant approach for visual speech representation in a variety of problems, such as automatic speechreading, visual speech detection, and others. To obtain the necessary visual features, typically a rectangular region-of-interest (ROI) containing the speaker\u2019s mouth is first extracted, followed, most commonly, by a discrete cosine transform (DCT) of the ROI pixel values and a feature selection step. The approach, although algorithmically simple and computationally efficient, suffers from lack of DCT invariance to typical ROI deformations, stemming, primarily, from speaker\u2019s head pose variability and small tracking inaccuracies. To address the problem, in this paper, the recently introduced scattering transform is investigated as an alternative to DCT within the appearance-based framework for ROI representation, suitable for visual speech applications. A number of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:9vf0nzSNQJEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Asynchronous stream modeling for large vocabulary audio-visual speech recognition",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/940794/",
            "Abstract": "Addresses the problem of audio-visual information fusion to provide highly robust speech recognition. We investigate methods that make different assumptions about asynchrony and conditional dependence across streams and propose a technique based on composite HMMs that can account for stream asynchrony and different levels of information integration. We show how these models can be trained jointly based on maximum likelihood estimation. Experiments, performed for a speaker-independent large vocabulary continuous speech recognition task and different integration methods, show that best performance is obtained by asynchronous stream integration. This system reduces the error rate at a 8.5 dB SNR with additive speech \"babble\" noise by 27 % relative over audio-only models and by 12 % relative over traditional audio-visual models using concatenative feature fusion.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:UeHWp8X0CEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "John McDonough, Matthias W\u00f6lfel, Ulrich Klee, 2 Maurizio Omologo, Alessio Brutti, Piergiorgio Svaizer",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=16267424715892307235&hl=en&oi=scholarr",
            "Abstract": "To realize the long-term goal of ubiquitous computing, technologi-cal advances in multi-channel acoustic analysis are needed in order to solve several basic problems, including speaker localization and tracking, speech activity detection (SAD) and distant-talking automatic speech recognition (ASR). The European Commission inte-grated project CHIL,\u201cComputers in the Human Interaction Loop\u201d, aims to make significant advances in these three technologies. In this work, we report the results of our initial automatic source lo-calization, speech activity detection, and speech recognition experiments on the CHIL seminar corpus, which is comprised of spontaneous speech collected by both near-and far-field microphones. In addition to the audio sensors, the seminars were also recorded by calibrated video cameras. This simultaneous audio-visual data capture enables the realistic evaluation of component technologies as was never possible with earlier data bases.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:LI9QrySNdTsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multi3: Multi-sensory perception system for multi-modal child interaction with multiple robots",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8461210/",
            "Abstract": "Child-robot interaction is an interdisciplinary research area that has been attracting growing interest, primarily focusing on edutainment applications. A crucial factor to the successful deployment and wide adoption of such applications remains the robust perception of the child's multi-modal actions, when interacting with the robot in a natural and untethered fashion. Since robotic sensory and perception capabilities are platform-dependent and most often rather limited, we propose a multiple Kinect-based system to perceive the child-robot interaction scene that is robot-independent and suitable for indoors interaction scenarios. The audio-visual input from the Kinect sensors is fed into speech, gesture, and action recognition modules, appropriately developed in this paper to address the challenging nature of child-robot interaction. For this purpose, data from multiple children are collected and used for module training \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:_Re3VWB3Y0AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improved ROI and within frame discriminant features for lipreading",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/958098/",
            "Abstract": "We study three aspects of designing appearance based visual features for automatic lipreading: (a) the choice of the video region of interest (ROI) on which image transform features are obtained; (b) the extraction of speech discriminant features at each frame; (c) the use of temporal information to improve visual speech modeling. With respect to (a), we propose a ROI that includes the speaker's jaw and cheeks, in addition to the traditionally used mouth/lip region. With respect to (b) and (c), we propose the use of a two-stage linear discriminant analysis, both within a single frame and across a large number of frames. On a large-vocabulary, continuous-speech, audio-visual database, the proposed visual features result in a 13% absolute reduction in visual-only word error rate over a baseline visual front end, and in an additional 28% relative improvement in audio-visual over audio-only phonetic classification accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The CHIL Audiovisual Corpus for Lecture and Meeting Analysis inside Smart Rooms",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=11234163584879901819&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:AXPGKjj_ei8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Steepest descent for efficient covariance tracking",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4544049/",
            "Abstract": "Recent research has advocated the use of a covariance matrix of image features for tracking objects instead of the conventional histogram object representation models used in popular algorithms. In this paper we extend the covariance tracker and propose efficient algorithms with an emphasis on both improving the tracking accuracy and reducing the execution time. The algorithms are compared to a baseline covariance tracker and the popular histogram-based mean shift tracker. Quantitative evaluations on a publicly available dataset demonstrate the efficacy of the presented methods. Our algorithms obtain significant speedups factors up to 330 while reducing the tracking errors by 86-90% relative to the baseline approach.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:NMxIlDl6LWMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A robotic edutainment framework for designing child-robot interaction scenarios",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3453892.3458048",
            "Abstract": "This paper presents the development of a child-robot interaction (CRI) system for edutainment scenarios, aiming to provide a framework for their design and to simplify access to social robots by educators with non-specialized technical knowledge in this challenging area. Our framework incorporates powerful robotic perception modules for action and emotion recognition of the interacting child, allowing the robot to exhibit empathy and be informed of the child\u2019s activity. Both developed modules are evaluated on respective datasets, outperforming the current state-of-the-art by a significant margin, while retaining low computational cost. The modules are complemented by off-the-shelf automatic speech recognition and synthesis components to further enable and enrich edutainmment-focused CRI. Moreover, the developed framework allows custom CRI scenario-building via a suitable graphical user interface \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:VaXvl8Fpj5cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automatic speech recognition and speech activity detection in the CHIL smart room",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11677482_29",
            "Abstract": "An important step to bring speech technologies into wide deployment as a functional component in man-machine interfaces is to free the users from close-talk or desktop microphones, and enable far-field operation in various natural communication environments. In this work, we consider far-field automatic speech recognition and speech activity detection in conference rooms. The experiments are conducted on the smart room platform provided by the CHIL project. The first half of the paper addresses the development of speech recognition systems for the seminar transcription task. In particular, we look into the effect of combining parallel recognizers in both single-channel and multi-channel settings. In the second half of the paper, we describe a novel algorithm for speech activity detection based on fusing phonetic likelihood scores and energy features. It is shown that the proposed technique is able to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:-f6ydRqryjwC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Audio-visual speech processing: Progress and challenges",
            "Publication year": 2006,
            "Publication url": "http://users.cecs.anu.edu.au/~vishci/VisHCI2006/Papers/VisHCI2006_Potamianos_Keynote2_Presentation.pdf",
            "Abstract": "We follow a statistical, appearance based face detection approach:\u25aa 2-class classification (into faces/non-faces).\u25aa\u201cFace template\u201d(eg, 11x11 pixel rectangle) ordered into vectors x.\u25aa A trainable scheme \u201cscores\u201d/classifies x into the 2 classes.\u25aa Pyramidal search (over locations, scales, orientations) provides face candidates x.\u25aa Can speed-up search by using skin-tone (based on color information on the R, G, B or transformed space), or location/scale (in the case of a video sequence). end start ratio",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:g5m5HwL7SMYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Long-time span acoustic activity analysis from far-field sensors in smart homes",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4960548/",
            "Abstract": "Smart homes for the aging population have recently started attracting the attention of the research community. One of the problems of interest is this of monitoring the activities of daily living (ADLs) of the elderly, in order to help identify critical problems, aiming to improve their protection and general well-being. In this paper, we report on our initial attempts to recognize such activities, based on input from networks of far-field microphones distributed inside the home. We propose two approaches to the problem: The first models the entire activity, which typically covers long time spans, with a single statistical model, for example a hidden Markov model (HMM), a Gaussian mixture model (GMM), or GMM super-vectors in conjunction with support vector machines (SVMs). The second is a two-step approach: It first performs acoustic event detection (AED) to locate distinctive events, characteristic of the ADLs, and it is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:ns9cj8rnVeAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A system for synergistically structuring news content from traditional media and the blogosphere",
            "Publication year": 2011,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.650.6905&rep=rep1&type=pdf",
            "Abstract": "News and social media are emerging as a dominant source of information for numerous applications. However, their vast unstructured content present challenges to efficient extraction of such information. In this paper, we present the SYNC3 system that aims to intelligently structure content from both traditional news media and the blogosphere. To achieve this goal, SYNC3 incorporates innovative algorithms that first model news media content statistically, based on fine clustering of articles into so-called \u201cnews events\u201d. Such models are then adapted and applied to the blogosphere domain, allowing its content to map to the traditional news domain. Furthermore, appropriate algorithms are employed to extract news event labels and relations between events, in order to efficiently present news content to the system end users.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:8AbLer7MMksC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Attention-Enhanced Sensorimotor Object Recognition",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8451158/",
            "Abstract": "Sensorimotor learning, namely the process of understanding the physical world by combining visual and motor information, has been recently investigated, achieving promising results for the task of 2D/3D object recognition. Following the recent trend in computer vision, powerful deep neural networks (NNs) have been used to model the \u201csensory\u201d and \u201cmotor\u201d information, namely the object appearance and affordance. However, the existing implementations cannot efficiently address the spatio-temporal nature of the human-object interaction. Inspired by recent work on attention-based learning, this paper introduces an attention-enhanced NN-based model that learns to selectively focus on parts of the physical interaction where the object appearance is corrupted by occlusions and deformations. The model's attention mechanism relies on the confidence of classifying an object based solely on its appearance. Three \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:tKAzc9rXhukC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The IBM Rich Transcription Spring 2006 speech-to-text system for lecture meetings",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/11965152_38",
            "Abstract": "We describe the IBM systems submitted to the NIST RT06s Speech-to-Text (STT) evaluation campaign on the CHIL lecture meeting data for three conditions: Multiple distant microphone (MDM), single distant microphone (SDM), and individual headset microphone (IHM). The system building process is similar to the IBM conversational telephone speech recognition system. However, the best models for the far-field conditions (SDM and MDM) proved to be the ones that use neither variance normalization nor vocal tract length normalization. Instead, feature-space minimum-phone error discriminative training yielded the best results. Due to the relatively small amount of CHIL-domain data, the acoustic models of our systems are built on publicly available meeting corpora, with maximum a-posteriori adaptation applied twice on CHIL data during training: First, at the initial speaker-independent model, and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:hFOr9nPyWt4C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Towards practical deployment of audio-visual speech recognition",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1326660/",
            "Abstract": "Much progress has been achieved during the past two decades in audio-visual automatic speech recognition (AVASR). However, challenges persist that hinder AVASR deployment in practical situations, most notably, robust and fast extraction of visual speech features. We review our efforts in overcoming this problem, based on an appearance-based visual feature representation of the speaker's mouth region. We cover three topics in particular. Firstly, we discuss AVASR in realistic, visually challenging domains, where lighting, background, and head-pose vary significantly. To enhance visual-front-end robustness in such environments, we employ an improved statistical-based face detection algorithm that significantly outperforms our baseline scheme. However, visual-only recognition remains inferior to visually \"clean\" (studio-like) data, thus demonstrating the importance of accurate mouth region extraction. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:hC7cP41nSMkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Handbook on Ambient Intelligence and Smart Environments (AISE)",
            "Publication year": 2010,
            "Publication url": "https://scholar.google.com/scholar?cluster=15262236108208859127&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:XiVPGOgt02cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Database and baseline system for detecting degraded traffic signs in urban environments",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7018395/",
            "Abstract": "We present a small database of \u201cnoisy\u201d traffic signs in cluttered urban environments that exhibit various forms of degradation, including vandalism and fading (discoloration). The database contains five types of international traffic signs that allow differentiation by means of color and shape, and it has been collected in two cities in Greece. We further present a baseline system for detecting and recognizing signs in this database, primarily employing color segmentation in the RGB color space, shape detection, and a number of problem specific heuristics. Our approach proves quite robust to the degraded traffic signs of our collected database, achieving an F-score of 0.91.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:eMMeJKvmdy0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A hierarchical approach with feature selection for emotion recognition from speech.",
            "Publication year": 2012,
            "Publication url": "https://www.researchgate.net/profile/Panagiotis-Giannoulis/publication/268205390_A_Hierarchical_Approach_with_Feature_Selection_for_Emotion_Recognition_from_Speech/links/5623917808ae70315b5dac75/A-Hierarchical-Approach-with-Feature-Selection-for-Emotion-Recognition-from-Speech.pdf",
            "Abstract": "We examine speaker independent emotion classification from speech, reporting experiments on the Berlin database across six basic emotions. Our approach is novel in a number of ways: First, it is hierarchical, motivated by our belief that the most suitable feature set for classification is different for each pair of emotions. Further, it uses a large number of feature sets of different types, such as prosodic, spectral, glottal flow based, and AM-FM ones. Finally, it employs a two-stage feature selection strategy to achieve discriminative dimensionality reduction. The approach results to a classification rate of 85%, comparable to the state-of-the-art on this dataset.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:u_35RYKgDlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Room-localized spoken command recognition in multi-room, multi-microphone environments",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0885230816303515",
            "Abstract": "The paper focuses on the design of a practical system pipeline for always-listening, far-field spoken command recognition in everyday smart indoor environments that consist of multiple rooms equipped with sparsely distributed microphone arrays. Such environments, for example domestic and multi-room offices, present challenging acoustic scenes to state-of-the-art speech recognizers, especially under always-listening operation, due to low signal-to-noise ratios, frequent overlaps of target speech, acoustic events, and background noise, as well as inter-room interference and reverberation. In addition, recognition of target commands often needs to be accompanied by their spatial localization, at least at the room level, to account for users in different rooms, providing command disambiguation and room-localized feedback. To address the above requirements, the use of parallel recognition pipelines is proposed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:N5tVd3kTz84C",
            "Publisher": "Academic Press"
        },
        {
            "Title": "End-to-End Convolutional Sequence Learning for ASL Fingerspelling Recognition.",
            "Publication year": 2019,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2422.pdf",
            "Abstract": "Although fingerspelling is an often overlooked component of sign languages, it has great practical value in the communication of important context words that lack dedicated signs. In this paper we consider the problem of fingerspelling recognition in videos, introducing an end-to-end lexicon-free model that consists of a deep auto-encoder image feature learner followed by an attention-based encoder-decoder for prediction. The feature extractor is a vanilla auto-encoder variant, employing a quadratic activation function. The learned features are subsequently fed into the attention-based encoder-decoder. The latter deviates from traditional recurrent neural network architectures, being a fully convolutional attention-based encoder-decoder that is equipped with a multi-step attention mechanism relying on a quadratic alignment function and gated linear units over the convolution output. The introduced model is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:ML0RJ9NH7IQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Joint audio-visual speech processing for recognition and enhancement",
            "Publication year": 2003,
            "Publication url": "https://www.isca-speech.org/archive_open/avsp03/av03_095.html",
            "Abstract": "Visual speech information present in the speaker\u2019s mouth region has long been viewed as a source for improving the robustness and naturalness of human-computer-interfaces (HCI). Such information can be particularly crucial in realistic HCI environments, where the acoustic channel is corrupted, and as a result, the performance of traditional automatic speech recognition (ASR) systems falls below usability levels. In this paper, we review two general approaches that utilize visual speech to improve ASR in acoustically challenging environments: One directly combines features extracted from the acoustic and visual channels, aiming at superior recognition performance of the resulting audio-visual ASR system. The other seeks to eliminate the noise present in the acoustic features, aiming at their audio-visual based enhancement, and thus resulting in improved speech recognition. We present a number of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An embedded system for in-vehicle visual speech activity detection",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4412866/",
            "Abstract": "We present a system for automatically detecting driver's speech in the automobile domain using visual-only information extracted from the driver's mouth region. The work is motivated by the desire to eliminate manual push-to-talk activation of the speech recognition engine in newly designed voice interfaces in the typically noisy car environment, aiming at reducing driver cognitive load and increasing naturalness of the interaction. The proposed system uses a camera mounted on the rearview mirror to monitor the driver, detect face boundaries and facial features, and finally employ lip motion clues to recognize visual speech activity. In particular, the designed algorithm has very low computational cost, which allows real-time implementation on currently available inexpensive embedded platforms, as described in the paper. Experiments are also reported on a small multi-speaker database collected in moving \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:M3NEmzRMIkIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio-visual speech enhancement with AVCDCN (audio-visual codebook dependent cepstral normalization)",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1191001/",
            "Abstract": "We introduce a non-linear enhancement technique called audio-visual codebook dependent cepstral normalization (AVCDCN) and we consider its use with both audio-only and audio-visual speech recognition. AVCDCN is inspired from CDCN, an audio-only enhancement technique that approximates the nonlinear effect of noise on speech with a piecewise constant function. Our experiments show that the use of visual information in AVCDCN allows significant performance gains over CDCN.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:KlAtU1dfN6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multi-microphone fusion for detection of speech and acoustic events in smart spaces",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6952875/",
            "Abstract": "In this paper, we examine the challenging problem of detecting acoustic events and voice activity in smart indoors environments, equipped with multiple microphones. In particular, we focus on channel combination strategies, aiming to take advantage of the multiple microphones installed in the smart space, capturing the potentially noisy acoustic scene from the far-field. We propose various such approaches that can be formulated as fusion at the signal, feature, or at the decision level, as well as combinations of the above, also including multi-channel training. We apply our methods on two multi-microphone databases: (a) one recorded inside a small meeting room, containing twelve classes of isolated acoustic events; and (b) a speech corpus containing interfering noise sources, simulated inside a smart home with multiple rooms. Our multi-channel approaches demonstrate significant improvements, reaching \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:t6usbXjVLHcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Deep view2view mapping for view-invariant lipreading",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8639698/",
            "Abstract": "Recently, visual-only and audio-visual speech recognition have made significant progress thanks to deep-learning based, trainable visual front-ends (VFEs), with most research focusing on frontal or near-frontal face videos. In this paper, we seek to expand the applicability of VFEs targeted on frontal face views to non-frontal ones, without making assumptions on the VFE type, and allowing systems trained on frontal-view data to be applied on mismatched, non-frontal videos. For this purpose, we adapt the \u201cpix2pix\u201d model, recently proposed for image translation tasks, to transform non-frontal speaker mouth regions to frontal, employing a convolutional neural network architecture, which we call \u201cview2view\u201d. We develop our approach on the OuluVS2 multiview lipreading dataset, allowing training of four such networks that map views at predefined non-frontal angles (up to profile) to frontal ones, which we subsequently \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:zLWjf1WUPmwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multimodal Sign Language Recognition via Temporal Deformable Convolutional Sequence Learning.",
            "Publication year": 2020,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2691.pdf",
            "Abstract": "In this paper we address the challenging problem of sign language recognition (SLR) from videos, introducing an end-to-end deep learning approach that relies on the fusion of a number of spatio-temporal feature streams, as well as a fully convolutional encoder-decoder for prediction. Specifically, we examine the contribution of optical flow, human skeletal features, as well as appearance features of handshapes and mouthing, in conjunction with a temporal deformable convolutional attention-based encoder-decoder for SLR. To our knowledge, this is the first use in this task of a fully convolutional multi-step attention-based encoder-decoder employing temporal deformable convolutional block structures. We conduct experiments on three sign language datasets and compare our approach to existing state-of-the-art SLR methods, demonstrating its superiority.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:TIZ-Mc8IlK0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automatic speechreading of impaired speech",
            "Publication year": 2001,
            "Publication url": "https://www.isca-speech.org/archive_open/avsp01/av01_177.html",
            "Abstract": "We investigate the use of visual, mouth-region information in improving automatic speech recognition (ASR) of the speech impaired. Given the video of an utterance by such a subject, we first extract appearance-based visual features from the mouth region-of-interest, and we use a feature fusion method to combine them with the subject\u2019s audio features into bimodal observations. Subsequently, we adapt the parameters of a speaker-independent, audio-visual hidden Markov model, trained on a large database of hearing subjects, to the audio-visual features extracted from the speech impaired videos. We consider a number of speaker adaptation techniques, and we study their performance in the case of a single speech impaired subject uttering continuous read speech, as well as connected digits. For both tasks, maximum-a-posteriori adaptation followed by maximum likelihood linear regression performs the best \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:aqlVkmm33-oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio-visual unit selection for the synthesis of photo-realistic talking-heads",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/871439/",
            "Abstract": "This paper investigates audio-visual unit selection for the synthesis of photo-realistic, speech-synchronized talking-head animations. These animations are synthesized from recorded video samples of a subject speaking in front of a camera, resulting in a photo-realistic appearance. The lip-synchronization is obtained by optimally selecting and concatenating variable-length video units of the mouth area. Synthesizing a new speech animation from these recorded units starts with audio speech and its phonetic annotation from a text-to-speech synthesizer. Then, optimal image units are selected from the recorded set using a Viterbi search through a graph of candidate image units. Costs are attached to the nodes and arcs of the graph that are computed from similarities in both the acoustic and visual domain. While acoustic similarities are computed by simple phonetic matching, visual similarities are estimated using a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:LkGwnXOMwfcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "IBM TJ Watson Research Center, Yorktown Heights, NY 10598, USA",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=11252384417435071564&hl=en&oi=scholarr",
            "Abstract": "Finding faces in visually challenging environments is crucial to many applications, such as audio-visual automatic speech recog-nition, video indexing, person recognition, and video surveillance. In this study, we investigate several algorithms to improve face detection accuracy in visually challenging environments using the IBM appearance based face detection system. The algorithms considered are trainable skintone pre-screening, Hamming windowing of the face images, DCT coefficient selection, and the AdaBoost technique. When these methods are combined, an up to 68% rela-tive reduction in face detection error is observed on visually chal-lenging datasets.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:lmc2jWPfTJgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A joint system for person tracking and face detection",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11573425_5",
            "Abstract": "Visual detection and tracking of humans in complex scenes is a challenging problem with a wide range of applications, for example surveillance and human-computer interaction. In many such applications, time-synchronous views from multiple calibrated cameras are available, and both frame-view and space-level human location information is desired. In such scenarios, efficiently combining the strengths of face detection and person tracking is a viable approach that can provide both levels of information required and improve robustness. In this paper, we propose a novel vision system that detects and tracks human faces automatically, using input from multiple calibrated cameras. The method uses an Adaboost algorithm variant combined with mean shift tracking applied on single camera views for face detection and tracking, and fuses the results on multiple camera views to check for consistency and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:TQgYirikUcIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Weighting schemes for audio-visual fusion in speech recognition",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/940795/",
            "Abstract": "We demonstrate an improvement in the state-of-the-art large vocabulary continuous speech recognition (LVCSR) performance, under clean and noisy conditions, by the use of visual information, in addition to the traditional audio one. We take a decision fusion approach for the audio-visual information, where the single-modality (audio- and visual- only) HMM classifiers are combined to recognize audio-visual speech. More specifically, we tackle the problem of estimating the appropriate combination weights for each of the modalities. Two different techniques are described: the first uses an automatically extracted estimate of the audio stream reliability in order to modify the weights for each modality (both clean and noisy audio results are reported), while the second is a discriminative model combination approach where weights on pre-defined model classes are optimized to minimize WER (clean audio only results).",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:qjMakFHDy7sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A multi-modal spoken dialog system for interactive tv",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1452392.1452429",
            "Abstract": "In this demonstration we present a novel prototype system that implements a multi-modal interface for control of the television. This system combines the standard TV remote control with a dialog management based natural language speech interface to allow users to efficiently interact with the TV, and to seamlessly alternate between the two modalities. One of the main objectives of this system is to make the unwieldy Electronic Program Guide information more navigable by the use of voice to filter and locate programs of interest.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:k_IJM867U9cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Computers in the human interaction loop",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-0-387-93808-0_40",
            "Abstract": "It is a common experience in our modern world, for us humans to be overwhelmed by the complexities of technological artifacts around us, and by the attention they demand. While technology provides wonderful support and helpful assistance, it also causes an increased preoccupation with technology itself and a related fragmentation of attention. But as humans, we would rather attend to a meaningful dialog and interaction with other humans, than to control the operations of machines that serve us. The cause for such complexity and distraction, however, is a natural consequence of the flexibility and choice of functions and features that technology has to offer. Thus flexibility of choice and the availability of desirable functions are in conflict with ease of use and our very ability to enjoy their benefits.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "Joint Object Affordance Reasoning and Segmentation in RGB-D Videos",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9459755/",
            "Abstract": "Understanding human-object interaction is a fundamental challenge in computer vision and robotics. Crucial to it is the ability to infer \u201cobject affordances\u201d from visual data, namely the types of interaction supported by an object of interest and the object parts involved. Such inference can be approached as an \u201caffordance reasoning\u201d task, where object affordances are recognized and localized as image heatmaps, and as an \u201caffordance segmentation\u201d task, where affordance labels are obtained at a more detailed, image pixel level. To tackle the two tasks, existing methods typically: (i) treat them independently; (ii) adopt static image-based models, ignoring the temporal aspect of human-object interaction; and / or (iii) require additional strong supervision concerning object class and location. In this paper, we focus on both tasks, while addressing all three aforementioned shortcomings. For this purpose, we propose a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:tuHXwOkdijsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Experiments on far-field multichannel speech processing in smart homes",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6622707/",
            "Abstract": "In this paper, we examine three problems that rise in the modern, challenging area of far-field speech processing. The developed methods for each problem, namely (a) multi-channel speech enhancement, (b) voice activity detection, and (c) speech recognition, are potentially applicable to a distant speech recognition system for voice-enabled smart home environments. The obtained results on real and simulated data, regarding the smart home speech applications, are quite promising due to the accomplished improvements made in the employed signal processing methods.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:eJXPG6dFmWUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multi-channel non-negative matrix factorization for overlapped acoustic event detection",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8553520/",
            "Abstract": "In this paper, we propose two multi-channel extensions of non-negative matrix factorization (NMF) for acoustic event detection. The first method performs decision fusion on the activation matrices produced from independent single-channel sparse- NMF solutions. The second method is a novel extension of single-channel NMF, incorporating in its objective function a multi-channel reconstruction error and a multi-channel class sparsity term on the activation matrices produced. This class sparsity constraint is used to guarantee that the NMF solutions at a given time will contain only a few classes activated across all channels. This indirectly forces the channels to seek solutions on which they agree, thus increasing robustness. We evaluate the proposed methods on a multi-channel database of overlapping acoustic events and various background noises collected inside a smart office space. Both proposed methods \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:tzM49s52ZIMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multistage information fusion for audio-visual speech recognition",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1394568/",
            "Abstract": "The paper looks into the information fusion problem in the context of audio-visual speech recognition. Existing approaches to audio-visual fusion typically address the problem in either the feature domain or the decision domain. We consider a hybrid approach that aims to take advantage of both the feature fusion and the decision fusion methodologies. We introduce a general formulation to facilitate information fusion at multiple stages, followed by an experimental study of a set of fusion schemes allowed by the framework. The proposed method is implemented on a real-time audio-visual speech recognition system, and evaluated on connected digit recognition tasks under varying acoustic conditions. The results show that the multistage fusion system consistently achieves lower word error rates than the reference feature fusion and decision fusion systems. It is further shown that removing the audio only channel \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:isC4tDSrTZIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The CHIL audiovisual corpus for lecture and meeting analysis inside smart rooms",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/article/10.1007/s10579-007-9054-4",
            "Abstract": "The analysis of lectures and meetings inside smart rooms has recently attracted much interest in the literature, being the focus of international projects and technology evaluations. A key enabler for progress in this area is the availability of appropriate multimodal and multi-sensory corpora, annotated with rich human activity information during lectures and meetings. This paper is devoted to exactly such a corpus, developed in the framework of the European project CHIL, \u201cComputers in the Human Interaction Loop\u201d. The resulting data set has the potential to drastically advance the state-of-the-art, by providing numerous synchronized audio and video streams of real lectures and meetings, captured in multiple recording sites over the past 4 years. It particularly overcomes typical shortcomings of other existing databases that may contain limited sensory or monomodal data, exhibit constrained human behavior \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:_FxGoFyzp5QC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Audio-only backoff in audio-visual speech recognition system",
            "Publication year": 2007,
            "Publication url": "https://patents.google.com/patent/US7251603B2/en",
            "Abstract": "Techniques for performing audio-visual speech recognition, with improved recognition performance, in a degraded visual environment. For example, in one aspect of the invention, a technique for use in accordance with an audio-visual speech recognition system for improving a recognition performance thereof includes the steps/operations of:(i) selecting between an acoustic-only data model and an acoustic-visual data model based on a condition associated with a visual environment; and (ii) decoding at least a portion of an input spoken utterance using the selected data model. Advantageously, during periods of degraded visual conditions, the audio-visual speech recognition system is able to decode (recognize) input speech data using audio-only data, thus avoiding recognition inaccuracies that may result from performing speech recognition based on acoustic-visual data models and degraded visual data.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:pqnbT2bcN3wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A joint system for single-person 2D-face and 3D-head tracking in CHIL seminars",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-69568-4_7",
            "Abstract": "We present the IBM systems submitted and evaluated within the CLEAR\u201906 evaluation campaign for the tasks of single person visual 3D tracking (localization) and 2D face tracking on CHIL seminar data. The two systems are significantly inter-connected to justify their presentation within a single paper as a joint vision system for single person 2D-face and 3D-head tracking, suitable for smart room environments with multiple synchronized, calibrated, stationary cameras. Indeed, in the developed system, face detection plays a pivotal role in 3D person tracking, being employed both in system initialization as well as in detecting possible tracking drift. Similarly, 3D person tracking determines the 2D frame regions where a face detector is subsequently applied. The joint system consists of a number of components that employ detection and tracking algorithms, some of which operate on input from all four corner \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:NaGl4SEjCO4C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Noisy audio feature enhancement using audio-visual speech data",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5745030/",
            "Abstract": "We investigate improving automatic speech recognition (ASR) in noisy conditions by enhancing noisy audio features using visual speech captured from the speaker's face. The enhancement is achieved by applying a linear filter to the concatenated vector of noisy audio and visual features, obtained by mean square error estimation of the clean audio features in a training stage. The performance of the enhanced audio features is evaluated on two ASR tasks: A connected digits task and speaker-independent, large-vocabulary, continuous speech recognition. In both cases and at sufficiently low signal-to-noise ratios (SNRs), ASR trained on the enhanced audio features significantly outperforms ASR trained on the noisy audio, achieving for example a 46% relative reduction in word error rate on the digits task at \u22123.5 dB SNR. However, the method fails to capture the full visual modality benefit to ASR, as demonstrated \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:MXK_kJrjxJIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Frame-dependent multi-stream reliability indicators for audio-visual speech recognition",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1198707/",
            "Abstract": "We investigate the use of local, frame-dependent reliability indicators of the audio and visual modalities, as a means of estimating stream exponents of multi-stream hidden Markov models for audio-visual automatic speech recognition. We consider two such indicators at each modality, defined as functions of the speech-class conditional observation probabilities of appropriate audio-or visual-only classifiers. We subsequently map the four reliability indicators into the stream exponents of a state-synchronous, two-stream hidden Markov model, as a sigmoid function of their linear combination. We propose two algorithms to estimate the sigmoid weights, based on the maximum conditional likelihood and minimum classification error criteria. We demonstrate the superiority of the proposed approach on a connected-digit audio-visual speech recognition task, under varying audio channel noise conditions. Indeed, the use \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:YOwf2qJgpHMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio-visual speech recognition using depth information from the Kinect in noisy video conditions",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2413097.2413100",
            "Abstract": "In this paper we build on our recent work, where we successfully incorporated facial depth data of a speaker captured by the Microsoft Kinect device, as a third data stream in an audio-visual automatic speech recognizer. In particular, we focus our interest on whether the depth stream provides sufficient speech information that can improve system robustness to noisy audio-visual conditions, thus studying system operation beyond the traditional scenarios, where noise is applied to the audio signal alone. For this purpose, we consider four realistic visual modality degradations at various noise levels, and we conduct small-vocabulary recognition experiments on an appropriate, previously collected, audiovisual database. Our results demonstrate improved system performance due to the depth modality, as well as considerable accuracy increase, when using both the visual and depth modalities over audio only speech \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:4fKUyHm3Qg0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Introduction to the special issue on multimodal processing in speech-based interactions",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4782037/",
            "Abstract": "The nine papers in this special issue focus on multimodal processing in speech-based interaction.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:ldfaerwXgEUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Acoustic fall detection using Gaussian mixture models and GMM supervectors",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4959522/",
            "Abstract": "We present a system that detects human falls in the home environment, distinguishing them from competing noise, by using only the audio signal from a single far-field microphone. The proposed system models each fall or noise segment by means of a Gaussian mixture model (GMM) supervector, whose Euclidean distance measures the pairwise difference between audio segments. A support vector machine built on a kernel between GMM supervectors is employed to classify audio segments into falls and various types of noise. Experiments on a dataset of human falls, collected as part of the Netcarity project, show that the method improves fall classification F-score to 67% from 59% of a baseline GMM classifier. The approach also effectively addresses the more difficult fall detection problem, where audio segment boundaries are unknown. Specifically, we employ it to reclassify confusable segments produced by a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:4DMP91E08xMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robust far-field spoken command recognition for home automation combining adaptation and multichannel processing",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6854664/",
            "Abstract": "The paper presents our approach to speech-controlled home automation. We are focusing on the detection and recognition of spoken commands preceded by a key-phrase as recorded in a voice-enabled apartment by a set of multiple microphones installed in the rooms. For both problems we investigate robust modeling, environmental adaptation and multichannel processing to cope with a) insufficient training data and b) the far-field effects and noise in the apartment. The proposed integrated scheme is evaluated in a challenging and highly realistic corpus of simulated audio recordings and achieves F-measure close to 0.70 for key-phrase spotting and word accuracy close to 98% for the command recognition task.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:bnK-pcrLprsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Childbot: Multi-robot perception and interaction with children",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2008.12818",
            "Abstract": "In this paper we present an integrated robotic system capable of participating in and performing a wide range of educational and entertainment tasks, in collaboration with one or more children. The system, called ChildBot, features multimodal perception modules and multiple robotic agents that monitor the interaction environment, and can robustly coordinate complex Child-Robot Interaction use-cases. In order to validate the effectiveness of the system and its integrated modules, we have conducted multiple experiments with a total of 52 children. Our results show improved perception capabilities in comparison to our earlier works that ChildBot was based on. In addition, we have conducted a preliminary user experience study, employing some educational/entertainment tasks, that yields encouraging results regarding the technical validity of our system and initial insights on the user experience with it.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:g3aElNc5_aQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The handbook of multimodal-multisensor interfaces, Volume 2: Signal processing, architectures, and detection of emotion and cognition",
            "Publication year": 2018,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=AttyDwAAQBAJ&oi=fnd&pg=PP2&dq=info:sNJFvZO3di4J:scholar.google.com&ots=WUlRhw_uyo&sig=wdxuvNMU54lIYebDNlzpm1UN_mU",
            "Abstract": "The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces: user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces that often include biosignals. This edited collection is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas. This second volume of the handbook begins with multimodal signal processing, architectures, and machine learning. It includes recent deep learning approaches for processing multisensorial and multimodal user data and interaction, as well as context-sensitivity. A further highlight is processing of information about users' states and traits, an exciting emerging capability in next-generation user interfaces. These chapters discuss real-time multimodal analysis of emotion and social signals from various modalities, and perception of affective expression by users. Further chapters discuss multimodal processing of cognitive state using behavioral and physiological signals to detect cognitive load, domain expertise, deception, and depression. This collection of chapters provides walk-through examples of system design and processing, information on tools and practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this rapidly expanding field. In the final section of this volume, experts exchange views on the timely and controversial challenge topic of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:vDijr-p_gm4C",
            "Publisher": "Morgan & Claypool"
        },
        {
            "Title": "The IBM RT06s evaluation system for speech activity detection in CHIL seminars",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/11965152_29",
            "Abstract": "In this paper, we describe the IBM system submitted to the NIST Rich Transcription Spring 2006 (RT06s) evaluation campaign for automatic speech activity detection (SAD). This SAD system has been developed and evaluated on CHIL lecture meeting data using far-field microphone sensors, namely a single distant microphone (SDM) configuration and a multiple distant microphone (MDM) condition. The IBM SAD system employs a three-class statistical classifier, trained on features that augment traditional signal energy ones with features that are based on acoustic phonetic likelihoods. The latter are obtained using a large speaker-independent acoustic model trained on meeting data. In the detection stage, after feature extraction and classification, the resulting sequence of classified states is further collapsed into segments belonging to only two classes, speech or silence, following two levels of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:j3f4tGmQtD8C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Introduction: scope, trends, and paradigm shift in the field of computer interfaces",
            "Publication year": 2017,
            "Publication url": "https://research.monash.edu/en/publications/introduction-scope-trends-and-paradigm-shift-in-the-field-of-comp",
            "Abstract": "Introduction: scope, trends, and paradigm shift in the field of computer interfaces \u2014 Monash \nUniversity Skip to main navigation Skip to search Skip to main content Monash University Logo \nHelp & FAQ Home Profiles Research Units Equipment Projects Research output Prizes \nActivities Press / Media Search by expertise, name or affiliation Introduction: scope, trends, and \nparadigm shift in the field of computer interfaces Sharon Oviatt, Bjorn Schuller, Philip Cohen, \nDaniel Sonntag, Gerasimos Potamianos, Antonio Kruger Research output: Chapter in \nBook/Report/Conference proceeding \u203a Chapter (Book) \u203a Other \u203a peer-review Overview Original \nlanguage English Title of host publication The Handbook of Multimodal-Multisensor Interfaces, \nVolume 1 Subtitle of host publication Foundations, User Modeling, and Common Modality \nCombinations Editors Sharon Oviatt, Bjorn Schuller, Philip R. Cohen, Daniel Sonntag, , Antonio (\u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:uWiczbcajpAC",
            "Publisher": "Association for Computing Machinery (ACM)"
        },
        {
            "Title": "Audio-visual automatic speech recognition: An overview",
            "Publication year": 2004,
            "Publication url": "https://www.academia.edu/download/42209245/Audio-Visual_Automatic_Speech_Recognitio20160206-14055-1vxds7c.pdf",
            "Abstract": "We have made significant progress in automatic speech recognition (ASR) for well-defined applications like dictation and medium vocabulary transaction processing tasks in relatively controlled environments. However, ASR performance has yet to reach the level required for speech to become a truly pervasive user interface. Indeed, even in \u201cclean\u201d acoustic environments, and for a variety of tasks, state of the art ASR system performance lags human speech perception by up to an order of magnitude (Lippmann, 1997). In addition, current systems are quite sensitive to channel, environment, and style of speech variations. A number of techniques for improving ASR robustness have met limited success in severely degraded environments, mismatched to system training (Ghitza, 1986; Nadas et al., 1989; Juang, 1991; Liu et al., 1993; Hermansky and Morgan, 1994; Neti, 1994; Gales, 1997; Jiang et al., 2001). Clearly, novel, non-traditional approaches, that use orthogonal sources of information to the acoustic input, are needed to achieve ASR performance closer to the human speech perception level, and robust enough to be deployable in field applications. Visual speech is the most promising source of additional speech information, and it is obviously not affected by the acoustic environment and noise.Human speech perception is bimodal in nature: Humans combine audio and visual information in deciding what has been spoken, especially in noisy environments. The visual modality benefit to speech intelligibility in noise has been quantified as far back as in Sumby and Pollack (1954). Furthermore, bimodal fusion of audio and visual stimuli in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:d1gkVwhDpl0C",
            "Publisher": "MIT Press"
        },
        {
            "Title": "The Handbook of Multimodal-Multisensor Interfaces, Volume 3: Language Processing, Software, Commercialization, and Emerging Directions",
            "Publication year": 2019,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=M1WfDwAAQBAJ&oi=fnd&pg=PP2&dq=info:2dRdDacRMjQJ:scholar.google.com&ots=hO44wJ2rfV&sig=bguN0S3Ux7AwV46G2quWhIRFnOk",
            "Abstract": "The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces-user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces. This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas. This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted in medicine, robotics, interaction with smart spaces, and similar areas. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:URolC5Kub84C",
            "Publisher": "Morgan & Claypool"
        },
        {
            "Title": "A comparison of model and transform-based visual features for audio-visual LVCSR",
            "Publication year": 2001,
            "Publication url": "http://www.cs.cmu.edu/~iainm/papers/icme01.pdf",
            "Abstract": "Four different visual speech parameterisation methods are compared on a large vocabulary, continuous, audio-visual speech recognition task using the IBM ViaVoiceTM audio-visual speech database. Three are direct mouth image region based transforms; discrete cosine and wavelet transforms, and principal component analysis. The fourth uses a statistical model of shape and appearance called an active appearance model, to track and obtain model parameters describing the entire face.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:eQOLeE2rZwMC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "Exploiting 3d hand pose estimation in deep learning-based sign language recognition from rgb videos",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-66096-3_18",
            "Abstract": "In this paper, we investigate the benefit of 3D hand skeletal information to the task of sign language (SL) recognition from RGB videos, within a state-of-the-art, multiple-stream, deep-learning recognition system. As most SL datasets are available in traditional RGB-only video lacking depth information, we propose to infer 3D coordinates of the hand joints from RGB data via a powerful architecture that has been primarily introduced in the literature for the task of 3D human pose estimation. We then fuse these estimates with additional SL informative streams, namely 2D skeletal data, as well as convolutional neural network-based hand- and mouth-region representations, and employ an attention-based encoder-decoder for recognition. We evaluate our proposed approach on a corpus of isolated signs of Greek SL and a dataset of continuous finger-spelling in American SL, reporting significant gains by the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:hMsQuOkrut0C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Kernel-based 3d tracking",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4270499/",
            "Abstract": "We present a computer vision system for robust object tracking in 3D by combining evidence from multiple calibrated cameras. This kernel-based 3D tracker is automatically bootstrapped by constructing 3D point clouds. These points clouds are then clustered and used to initialize the trackers and validate their performance. The framework describes a complete tracking system that fuses appearance features from all available camera sensors and is capable of automatic initialization and drift detection. Its elegance resides in its inherent ability to handle problems encountered by various 2D trackers, including scale selection, occlusion, view-dependence, and correspondence across views. Tracking results for an indoor smart room and a multi-camera outdoor surveillance scenario are presented. We demonstrate the effectiveness of this unified approach by comparing its performance to a baseline 3D tracker that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:roLk4NBRz8UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Efficient likelihood computation in multi-stream HMM based audio-visual speech recognition",
            "Publication year": 2004,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2004/i04_2297.pdf",
            "Abstract": "Multi-stream hidden Markov models have recently been introduced in the field of automatic speech recognition as an alternative to single-stream modeling of sequences of speech informative features. In particular, they have been very successful in audio-visual speech recognition, where features extracted from video of the speaker\u2019s lips are also available. However, in contrast to single-stream modeling, their use during decoding becomes computationally intensive, as it requires calculating class-conditional likelihoods of the added stream observations. In this paper, we propose a technique to reduce this overhead by drastically limiting the number of observation probabilities computed for the visual stream. The algorithm estimates a joint co-occurrence mapping of the Gaussian mixture components that separately model the audio and visual observations, and uses it to select the visual mixture components to be \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:hMod-77fHWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A multisensor system for high reliability people fall detection in home environment",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-90-481-3606-3_79",
            "Abstract": "This paper presents a hardware and software system for reliable fall detection in the home environment, with particular focus on the protection and assistance to older people. The prototype includes three different sensors: a 3D time-of-flight range camera, a wearable MEMS accelerometer and a microphone. These devices are connected with custom interface circuits to a central PC that collects and processes the information with a multi-threading approach. For each of the three sensors, an optimized algorithm for fall-detection has been developed and benchmarked on a collected multimodal database.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "Springer, Dordrecht"
        },
        {
            "Title": "On the joint use of nmf and classification for overlapping acoustic event detection",
            "Publication year": 2018,
            "Publication url": "https://www.mdpi.com/252028",
            "Abstract": "In this paper, we investigate the performance of classifier-based non-negative matrix factorization (NMF) methods for detecting overlapping acoustic events. We provide evidence that the performance of classifier-based NMF systems deteriorates significantly in overlapped scenarios in case mixed observations are unavailable during training. To this end, we propose a K-means based method for artificial generation of mixed data. The method of Mixture of Local Dictionaries (MLD) is employed for the building of the NMF dictionary using both the isolated and artificially mixed data. Finally an SVM classifier is trained for each of the isolated and mixed event classes, using the corresponding MLD-NMF activations from the training set. The proposed system, tested on two experiments with (a) synthetic and (b) real events, outperforms the state-of-the-art classifier-based NMF system in the overlapped scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:dQ2og3OwTAUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Robust detection of visual ROI for automatic speechreading",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/962715/",
            "Abstract": "We present our work on visual pruning in an audio-visual (AV) speech recognition scenario. Visual speech information has been successfully used in circumstances where audio-only recognition suffers (e.g. noisy environments). Tracking and extraction of region-of-interest (ROI) (e.g., speaker's mouth region) from video is an essential component of such systems. It is important for the visual front-end to handle tracking errors that result in noisy visual data and hamper performance. We present our robust visual front-end, investigate methods to prune visual noise and its effect on the performance of the AV speech recognition systems. Specifically, we estimate the \"goodness of ROI\" using Gaussian mixture models and our experiments indicate that significant performance gains are achieved with good quality visual data.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:r0BpntZqJG4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Perceptual interfaces for information interaction: joint processing of audio and visual information for human-computer interaction.",
            "Publication year": 2000,
            "Publication url": "http://www.andrewsenior.com/papers/NetiICSLP00_perceptual.pdf",
            "Abstract": "We are exploiting the human perceptual principle of sensory integration (the joint use of audio and visual information) to improve the recognition of human activity (speech recognition, speech event detection and speaker change), intent (intent to speak) and human identity (speaker recognition), particularly in the presence of acoustic degradation due to noise and channel. In this paper, we present experimental results in a variety of contexts that demonstrate the benefit of joint audio-visual processing.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:Zph67rFs4hoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exploiting visual information in automatic speech processing",
            "Publication year": 2005,
            "Publication url": "https://www.scholars.northwestern.edu/en/publications/exploiting-visual-information-in-automatic-speech-processing",
            "Abstract": "This chapter focuses on how the joint processing of visual and audio signals, both generated by a talking person, can provide valuable speech information to benefit a number of audiovisual speech processing applications crucial to human-computer interactions. The analysis of visual signals has been done followed by a description of various possible ways of representing and extracting the speech information available in them. It has been shown in the chapter that the obtained visual features can complement features extracted from the acoustic signal and that the two modality representations can be fused together to allow joint audiovisual speech processing. The general bimodal integration framework is subsequently applied to three problems-automatic speech recognition, talking face synthesis, and speaker identification and authentication. In all three cases, issues specific to the particular application have been discussed, several relevant systems that have been reported in the literature have been reviewed, and the results using the implementations developed at IBM Research and Northwestern University have been presented.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:_Ybze24A_UAC",
            "Publisher": "Elsevier Inc"
        },
        {
            "Title": "The IBM rich transcription 2007 speech-to-text systems for lecture meetings",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-68585-2_40",
            "Abstract": "The paper describes the IBM systems submitted to the NIST Rich Transcription 2007 (RT07) evaluation campaign for the speech-to-text (STT) and speaker-attributed speech-to-text (SASTT) tasks on the lecture meeting domain. Three testing conditions are considered, namely the multiple distant microphone (MDM), single distant microphone (SDM), and individual headset microphone (IHM) ones \u2013 the latter for the STT task only. The IBM system building process is similar to that employed last year for the STT Rich Transcription Spring 2006 evaluation (RT06s). However, a few technical advances have been introduced for RT07: (a) better speaker segmentation; (b) system combination via the ROVER approach applied over an ensemble of systems, some of which are built by randomized decision tree state-tying; and (c) development of a very large language model consisting of 152M n-grams, incorporating \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Robust audio-visual speech synchrony detection by generalized bimodal linear prediction",
            "Publication year": 2009,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2009/papers/i09_2251.pdf",
            "Abstract": "We study the problem of detecting audio-visual synchrony in video segments containing a speaker in frontal head pose. The problem holds a number of important applications, for example speech source localization, speech activity detection, speaker diarization, speech source separation, and biometric spoofing detection. In particular, we build on earlier work, extending our previously proposed time-evolution model of audio-visual features to include non-causal (future) feature information. This significantly improves robustness of the method to small timealignment errors between the audio and visual streams, as demonstrated by our experiments. In addition, we compare the proposed model to two known literature approaches for audio-visual synchrony detection, namely mutual information and hypothesis testing, and we show that our method is superior to both.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:RYcK_YlVTxYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A multi-sensor approach for people fall detection in home environment",
            "Publication year": 2008,
            "Publication url": "https://hal.inria.fr/inria-00326739/",
            "Abstract": "This paper presents a hardware and software framework for reliable fall detection in the home environment, with particular focus on the protection and assistance to the elderly. The integrated prototype includes three di\u00aeerent sensors: a 3D Time-Of-Flight range camera, a wearable MEMS accelerometer and a microphone. These devices are connected with custom interface circuits to a central PC that collects and processes the information with a multi-threading approach. For each of the three sensors, an optimized algorithm for fall-detection has been developed and benchmarked on a collected mulitimodal database. This work is expected to lead to a multi-sensory approach employing appropriate fusion techniques aiming to improve system precision and recall.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:TFP_iSt0sucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Advances in large vocabulary continuous speech recognition in Greek: Modeling and nonlinear features",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6811764/",
            "Abstract": "The main goal of this work is the development of an improved Large Vocabulary Continuous Speech Recognition (LVCSR) framework in Greek. Language modeling is carried out in a collection of journalistic text and in the acoustic signal processing, a nonlinear approach is implemented for deriving features of the AM-FM type. Experimentation is carried out in both clean and simulated far-field speech offering insight about the acoustic modeling under adverse conditions with reverberation and additive ambient noise. Beyond the baseline implementation, a first step is made in exploring how standard (MFCCs and PLPs) and modulation features (AM-FM) behave in a LVCSR framework when the input speech is distant, like in real life home applications.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:q3oQSFYPqjQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Information fusion and decision cascading for audio-visual speaker recognition based on time-varying stream reliability prediction",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1221235/",
            "Abstract": "We examine the techniques for multi-modal biometric information fusion for verification and identification of speakers, where the reliability of each data stream, either audio of video, is modeled with parameters that are time-varying and depend on the context created by its local behavior. The complementary nature and the time dependent relative reliability of audio and video data is studied in the context of verification and identification, on data collected during a user's interaction with an automated system. Of significance is that this data is not corrupted artificially. Particular focus is directed to verification and its ability to refine identification decisions, by indicating a level of confidence in the system decisions. Results show more striking effects for verification, when using time-dependent fusion, than for identification.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:_kc_bZDykSQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "System and method for likelihood computation in multi-stream HMM based speech recognition",
            "Publication year": 2012,
            "Publication url": "https://patents.google.com/patent/US8121840B2/en",
            "Abstract": "A system and method for speech recognition includes determining active Gaussians related to a first feature stream and a second feature stream by labeling at least one of the first and second streams, and determining active Gaussians co-occurring in the first stream and the second stream based upon joint probability. A number of Gaussians computed is reduced based upon Gaussians already computed for the first stream and a number of Gaussians co-occurring in the second stream. Speech is decoded based on the Gaussians computed for the first and second streams.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:70eg2SAEIzsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Speech recognition, audio-visual",
            "Publication year": 2006,
            "Publication url": "https://scholar.google.com/scholar?cluster=8732772689711266587&hl=en&oi=scholarr",
            "Abstract": "Audio-visual speech recognition refers to the automatic transcription of speech into text by exploiting information present in the video of the speaker's mouth region, in addition to the traditionally used acoustic signal. The use of visual information in automatic speech recognition is also known as automatic speechreading or lipreading, and has been motivated by the bimodality of human speech production and perception, coupled with the fact that audio-only speech recognition is not robust in noisy acoustic environments. Audio-visual speech recognition systems significantly outperform their audio-only counterparts, especially under ideal visual and noisy audio conditions. Incorporating visual information into speech recognition requires two new components: the visual front end, which detects the speaker's mouth area and extracts informative visual speech features from it, and the integration of the visual features \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:uLbwQdceFCQC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Improved dictionary selection and detection schemes in sparse-CNMF-based overlapping acoustic event detection",
            "Publication year": 2016,
            "Publication url": "http://cvsp.cs.ece.ntua.gr/publications/confr/GiannoulisEtAl_DictionarySelection-SparseCNMF-AED_DCASE2016.pdf",
            "Abstract": "In this paper, we investigate sparse convolutive non-negative matrix factorization (sparse-CNMF) for detecting overlapping acoustic events in single-channel audio, within the experimental framework of Task 2 of the DCASE\u201916 Challenge. In particular, our main focus lies on the efficient creation of the dictionary, as well as the detection scheme associated with the CNMF approach. Specifically, for the dictionary creation stage, we propose a shift-invariant method for its size reduction that outperforms standard CNMF-based dictionary building. Further, for detection, we develop a novel algorithm that combines information from the CNMF activation matrix and atom-based reconstruction residuals, achieving significant improvements over conventional detection based on activations alone. The resulting system, assisted by efficient background noise modeling, outperforms a traditional NMF baseline provided by the Challenge organizers, achieving a 24% relative reduction in the total error rate metric on the Challenge Task 2 test set.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:eq2jaN3J8jMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal classification of activities of daily living inside smart homes",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-02481-8_103",
            "Abstract": "Smart homes for the aging population have recently started attracting the attention of the research community. One of the problems of interest is this of monitoring the activities of daily living (ADLs) of the elderly aiming at their protection and well-being. In this work, we present our initial efforts to automatically recognize ADLs using multimodal input from audio-visual sensors. For this purpose, and as part of Integrated Project Netcarity, far-field microphones and cameras have been installed inside an apartment and used to collect a corpus of ADLs, acted by multiple subjects. The resulting data streams are processed to generate perception-based acoustic features, as well as human location coordinates that are employed as visual features. The extracted features are then presented to Gaussian mixture models for their classification into a set of predefined ADLs. Our experimental results show that both acoustic \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:O3NaXMp0MMsC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Hierarchical discriminant features for audio-visual LVCSR",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/940793/",
            "Abstract": "We propose the use of a hierarchical, two-stage discriminant transformation for obtaining audio-visual features that improve automatic speech recognition. Linear discriminant analysis (LDA), followed by a maximum likelihood linear transform (MLLT) is first applied to MFCC based audio-only features, as well as on visual only features, obtained by a discrete cosine transform of the video region of interest. Subsequently, a second stage of LDA and MLLT is applied to the concatenation of the resulting single modality features. The obtained audio-visual features are used to train a traditional HMM based speech recognizer. Experiments on the IBM ViaVoice/sup TM/ audio-visual database demonstrate that the proposed feature fusion method improves speaker-independent, large vocabulary, continuous speech recognition (LVCSR) for both clean and noisy audio conditions considered. A 24% relative word error rate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:IjCSPb-OGe4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio-Assisted Image Inpainting for Talking Faces",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9053184/",
            "Abstract": "The goal of our work is to complete missing areas of images of talking faces, exploiting information from both the visual and audio modalities. Existing image inpainting methods rely solely on visual content that doesn't always provide sufficient information for the task. To counter this, we propose a neural network that employs an encoder-decoder architecture with a bimodal fusion mechanism, thus taking into account both visual and audio content. Our proposed method demonstrates consistently superior performance over a baseline visual-only model, reaching for example up to 17% relative improvement in mean absolute error. The presented model is applicable to practical video editing tasks, such as object and overlay-text removal from talking faces, where existing lip and face generation works are not applicable as they require clean input.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:L7CI7m0gUJcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A unified approach to multi-pose audio-visual ASR",
            "Publication year": 2007,
            "Publication url": "https://eprints.qut.edu.au/12848/",
            "Abstract": "The vast majority of studies in the field of audio-visual automatic speech recognition (AVASR) assumes frontal images of a speaker's face, but this cannot always be guaranteed in practice. Hence our recent research efforts have concentrated on extracting visual speech information from non-frontal faces, in particular the profile view. The introduction of additional views to an AVASR system increases the complexity of the system, as it has to deal with the different visual features associated with the various views. In this paper, we propose the use of linear regression to find a transformation matrix based on synchronous frontal and profile visual speech data, which is used to normalize the visual speech in each viewpoint into a single uniform view. In our experiments for the task of multi-speaker lipreading, we show that this \"pose-invariant\" technique reduces train/test mismatch between visual speech features of different views, and is of particular benefit when there is more training data for one viewpoint over another (e.g. frontal over profile).",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:IWHjjKOFINEC",
            "Publisher": "Causal Productions Pty Ltd"
        },
        {
            "Title": "First experiments of automatic speech activity detection, source localization and speech recognition in the CHIL project",
            "Publication year": 2005,
            "Publication url": "https://www.academia.edu/download/39575059/hernando_first.pdf",
            "Abstract": "In the workspace of the future, a so-called \u201cambient intelligence\u201d will be realized through the widespread use of sensors (eg, cameras, microphones, directed audio devices) connected to computers that are unobtrusive to their human users. Towards this end of ubiquitous computing, technological advances in multi-channel acoustic analysis are needed in order to solve several basic problems, including speaker localization and tracking, speech activity detection (SAD) and distanttalking Automatic Speech Recognition (ASR)[1]. The long-term goal is the ability to monitor speakers and noise sources in a real reverberant environment, without any constraint on the number or the distribution of microphones in the space nor on the number of sound sources active at the same time. This problem is surpassingly difficult, given that the speech signals collected by a given set of microphones are severely degraded by both background noise and reverberation. The European Commission integrated project CHIL, Computers in the Human Interaction Loop, aims to make significant advances in the three technologies mentioned above, and to integrate them in several technology demonstrators for seminar and meeting scenarios. To this purpose, most of the CHIL partners have set-up an experimental room for data collection and demonstrator development, which includes a variety of sensors: close-talking microphones, table-top microphones, T-shaped microphone arrays each with four omnidirectional sensors, and a 64-channel Mark III microphone array developed at the National Institute of Standards and Technologies (NIST). For every sensor, the input \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:bEWYMUwI8FkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Hierarchical detection of sound events and their localization using convolutional neural networks with adaptive thresholds",
            "Publication year": 2019,
            "Publication url": "http://archive.nyu.edu/handle/2451/60726",
            "Abstract": "This paper details our approach to Task 3 of the DCASE\u201919 Challenge, namely sound event localization and detection (SELD). Our system is based on multi-channel convolutional neural networks (CNNs), combined with data augmentation and ensembling. Specifically, it follows a hierarchical approach that first determines adaptive thresholds for the multi-label sound event detection (SED) problem, based on a CNN operating on spectrograms over long duration windows. It then exploits the derived thresholds in an ensemble of CNNs operating on raw waveforms over shorter-duration sliding windows to provide event segmentation and labeling. Finally, it employs event localization CNNs to yield direction-of-arrival (DOA) source estimates of the detected sound events. The system is developed and evaluated on the microphone-array set of Task 3. Compared to the baseline of the Challenge organizers, on the development set it achieves relative improvements of 12% in SED error, 2% in F-score, 36% in DOA error, and 3% in the combined SELD metric, but trails significantly in frame-recall, whereas on the evaluation set it achieves relative improvements of 3% in SED, 51% in DOA, and 4% in SELD errors. Overall though, the system lags significantly behind the best Task 3 submission, achieving a combined SELD error of 0.2033 against 0.044 of the latter",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:BUYA1_V_uYcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Large-vocabulary audiovisual speech recognition: A summary",
            "Publication year": 2001,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.64.7946",
            "Abstract": "We compare automatic recognition with human perception of audio-visual speech, in the large-vocabulary, continuous speech recognition (LVCSR) domain. Specifically, we study the benefit of the visual modality for both machines and humans, when combined with audio degraded by speech-babble noise at various signal-to-noise ratios (SNRs). We first consider an automatic speechreading system with a pixel based visual front end that uses feature fusion for bimodal integration, and we compare its performance with an audio-only LVCSR system. We then describe results of human speech perception experiments, where subjects are asked to transcribe audio-only and audiovisual utterances at various SNRs. For both machines and humans, we observe approximately a 6 dB effective SNR gain compared to the audio-only performance at 10 dB, however such gains significantly diverge at other SNRs. Furthermore, automatic audio-visual recognition outperforms human audioonly speech perception at low SNRs. 1.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:XD-gHx7UXLsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improved face and feature finding for audio-visual speech recognition in visually challenging environments",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1327250/",
            "Abstract": "Visual information in a speaker's face is known to improve the robustness of automatic speech recognition (ASR). However, most studies in audio-visual ASR have focused on \"visually clean\" data to benefit ASR in noise. This paper is a follow up on a previous study that investigated audio-visual ASR in visually challenging environments. It focuses on visual speech front end processing, and it proposes an improved, appearance based face and feature detection algorithm that utilizes Gaussian mixture model classifiers. This method is shown to improve the accuracy of face and feature detection, and thus visual speech recognition, over our previously used baseline system. In turn, this translates to improved audio-visual ASR, resulting in a 10% relative reduction of the word-error-rate in noisy speech.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:4JMBOYKVnBMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The handbook of multimodal-multisensor interfaces, volume 1: Foundations, user modeling, and common modality combinations",
            "Publication year": 2017,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=owrIDgAAQBAJ&oi=fnd&pg=PP2&dq=info:Bq-FabQ2QB0J:scholar.google.com&ots=CTmh3ldg2J&sig=sGHG976F9Mn99gz4TMFD92aFNvE",
            "Abstract": "The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces\u2014user input involving new media (speech, multi-touch, gestures, writing) embedded in multimodal-multisensor interfaces. These interfaces support smart phones, wearables, in-vehicle and robotic applications, and many other areas that are now highly competitive commercially. This edited collection is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas. This first volume of the handbook presents relevant theory and neuroscience foundations for guiding the development of high-performance systems. Additional chapters discuss approaches to user modeling and interface designs that support user choice, that synergistically combine modalities with sensors, and that blend multimodal input and output. This volume also highlights an in-depth look at the most common multimodal-multisensor combinations\u2014for example, touch and pen input, haptic and non-speech audio output, and speech-centric systems that co-process either gestures, pen input, gaze, or visible lip movements. A common theme throughout these chapters is supporting mobility and individual differences among users. These handbook chapters provide walk-through examples of system design and processing, information on tools and practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:PR6Y55bgFSsC",
            "Publisher": "Morgan & Claypool"
        },
        {
            "Title": "Audio-visual speaker recognition using time-varying stream reliability prediction",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1200070/",
            "Abstract": "We examine a time-varying, context dependent, information fusion methodology for multi-stream authentication based on audio and video data collected simultaneously during a user's interaction with a system. Scores obtained from the two data streams are combined based on the relative local richness, as compared to the training data or derived model, and on the stability of each stream. The results show that the proposed technique outperforms the use of video or audio data alone as well as the use of fused data streams (via concatenation). Of particular note is that the performance improvements are achieved for clean, high quality speech, whereas previous efforts focused on degraded speech conditions.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Person tracking",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-1-84882-054-8_3",
            "Abstract": "One of the most basic building blocks for the understanding of human actions and interactions is the accurate detection and tracking of persons in a scene. In constrained scenarios involving at most one subject, or in situations where persons can be confined to a controlled monitoring space or required to wear markers, sensors, or microphones, these tasks can be solved with relative ease. However, when accurate localization and tracking have to be performed in an unobtrusive or discreet fashion, using only distantly placed microphones and cameras, in a variety of natural and uncontrolled scenarios, the challenges posed are much greater. The problems faced by video analysis are those of poor or uneven illumination, low resolution, clutter or occlusion, unclean backgrounds, and multiple moving and uncooperative users that are not always easily distinguishable.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:HoB7MX3m0LUC",
            "Publisher": "Springer, London"
        },
        {
            "Title": "Speaker adaptation for audio-visual speech recognition",
            "Publication year": 2000,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.2384",
            "Abstract": "In this paper, speaker adaptation is investigated for audiovisual automatic speech recognition# ASR# using the multistream hidden Markov model# HMM#. First, audio-only and visual-only HMM parameters are adapted bycombining maximum a posteriori and maximum likelihood linear regression adaptation. Subsequently, the audio-visual HMM stream exponents are adapted to better capture the reliabilityofeach modality for the speci# c speaker, by means of discriminative training. Various visual feature sets are compared, and features based on linear discriminant analysis are demonstrated to result in superior multispeaker and speaker-adapted recognition performance. In addition, visual feature mean normalization is shown to signi# cantly improve visual-only and audio-visual ASR performance. Adaptation experiments on a 49-subject database are reported. On average, a 28# relativeword error reduction is achieved by adapting the multi-speaker audiovisual HMM to each subject in the database. 1. INTRODU...",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:hkOj_22Ku90C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio-visual technologies for lecture and meeting analysis inside smart rooms",
            "Publication year": 2006,
            "Publication url": "http://inf-server.inf.uth.gr/~gpotamianos/PAPERS/Potamianos.VisHCI.1.pdf",
            "Abstract": "\u25aa Here, we focus on one such project, CHIL (\u201cComputers in the Human Interaction Loop\u201d).\u25aa CHIL is a very ambitious and multi-faceted project, as we briefly describe.\u25aa In particular though, we concentrate on basic technologies that \u201cperceive\u201d the human activities in the CHIL project scenarios; ie, algorithmic development & evaluation.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:zA6iFVUQeVQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Deep Learning Approach to Object Affordance Segmentation",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9054167/",
            "Abstract": "Learning to understand and infer object functionalities is an important step towards robust visual intelligence. Significant research efforts have recently focused on segmenting the object parts that enable specific types of human-object interaction, the so-called \"object affordances\". However, most works treat it as a static semantic segmentation problem, focusing solely on object appearance and relying on strong supervision and object detection. In this paper, we propose a novel approach that exploits the spatio-temporal nature of human-object interaction for affordance segmentation. In particular, we design an autoencoder that is trained using ground-truth labels of only the last frame of the sequence, and is able to infer pixel-wise affordance labels in both videos and static images. Our model surpasses the need for object labels and bounding boxes by using a soft-attention mechanism that enables the implicit \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:EYYDruWGBe4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audio-visual selection process for the synthesis of photo-realistic talking-head animations",
            "Publication year": 2011,
            "Publication url": "https://patents.google.com/patent/US7990384B2/en",
            "Abstract": "A system and method for generating photo-realistic talking-head animation from a text input utilizes an audio-visual unit selection process. The lip-synchronization is obtained by optimally selecting and concatenating variable-length video units of the mouth area. The unit selection process utilizes the acoustic data to determine the target costs for the candidate images and utilizes the visual data to determine the concatenation costs. The image database is prepared in a hierarchical fashion, including high-level features (such as a full 3D modeling of the head, geometric size and position of elements) and pixel-based, low-level features (such as a PCA-based metric for labeling the various feature bitmaps).",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:f2IySw72cVMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Robust multi-modal speech recognition in two languages utilizing video and distance information from the kinect",
            "Publication year": 2013,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-39330-3_5",
            "Abstract": "We investigate the performance of our audio-visual speech recognition system in both English and Greek under the influence of audio noise. We present the architecture of our recently built system that utilizes information from three streams including 3-D distance measurements. The feature extraction approach used is based on the discrete cosine transform and linear discriminant analysis. Data fusion is employed using state-synchronous hidden Markov models. Our experiments were conducted on our recently collected database under a multi-speaker configuration and resulted in higher performance and robustness in comparison to an audio-only recognizer.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:VOx2b1Wkg3QC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Audio-visual speech activity detection in a two-speaker scenario incorporating depth information from a profile or frontal view",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7846321/",
            "Abstract": "Motivated by increasing popularity of depth visual sensors, such as the Kinect device, we investigate the utility of depth information in audio-visual speech activity detection. A two-subject scenario is assumed, allowing to also consider speech overlap. Two sensory setups are employed, where depth video captures either a frontal or profile view of the subjects, and is subsequently combined with the corresponding planar video and audio streams. Further, multi-view fusion is regarded, using audio and planar video from a sensor at the complementary view setup. Support vector machines provide temporal speech activity classification for each visually detected subject, fusing the available modality streams. Classification results are further combined to yield speaker diarization. Experiments are reported on a suitable audio-visual corpus recorded by two Kinects. Results demonstrate the benefits of depth information \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:WqliGbK-hY8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Method for likelihood computation in multi-stream HMM based speech recognition",
            "Publication year": 2009,
            "Publication url": "https://patents.google.com/patent/US7480617B2/en",
            "Abstract": "A method for speech recognition includes determining active Gaussians related to a first feature stream and a second feature stream by labeling at least one of the first and second streams, and determining active Gaussians co-occurring in the first stream and the second stream based upon joint probability. A number of Gaussians computed is reduced based upon Gaussians already computed for the first stream and a number of Gaussians co-occurring in the second stream. Speech is decoded based on the Gaussians computed for the first and second streams.",
            "Abstract entirety": 1,
            "Author pub id": "DjcaM1MAAAAJ:NJ774b8OgUMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fingerspelled alphabet sign recognition in upper-body videos",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8902541/",
            "Abstract": "Fingerspelling is a crucial part of sign-based communication, however its recognition remains a challenging and mostly overlooked computer vision problem. To address it, this paper presents a system that recognizes the 24 static fingerspelled alphabet signs of the American Sign Language. The system consists of two algorithmic stages, comprising an efficient preprocessing phase that generates candidate hand-region proposals, followed by their deep-learning based classification. Specifically, the first stage exploits own earlier work on hand detection and segmentation in videos that also contain the signer's face, allowing face detection to drive skin-tone based hand segmentation, with motion further utilized to localize hands, extending it with a peak detection module that yields proposal regions likely to contain the signs of interest. These regions are then classified by a variant of a convolutional neural network \u2026",
            "Abstract entirety": 0,
            "Author pub id": "DjcaM1MAAAAJ:BwyfMAYsbu0C",
            "Publisher": "IEEE"
        }
    ]
}]