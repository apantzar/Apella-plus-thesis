[{
    "name": "\u0399\u03c9\u03ac\u03bd\u03bd\u03b7\u03c2 \u03a3\u03c4\u03c5\u03bb\u03b9\u03b1\u03bd\u03bf\u03cd",
    "romanize name": "Ioannis Stylianou",
    "School-Department": "\u0395\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03c2 \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "uoc",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 9558,
    "Scholar name": "Yannis Stylianou",
    "Scholar id": "6ZSjpdwAAAAJ",
    "Affiliation": "Professor of Speech Processing, University of Crete and Research Manager at Apple",
    "Citedby": 8218,
    "Interests": [
        "Signal Processing"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=6ZSjpdwAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Automatic glottal segmentation using local-based active contours",
            "Publication year": 2010,
            "Publication url": "http://hal.in2p3.fr/UNIV-GRENOBLE1/hal-00540517v1",
            "Abstract": "High-speed videoendoscopy is the most promising approach to directly assess vocal-fold vibrations. Yet, its application to clinics is limited by the vast amount of data that has to be evaluated both qualitatively and quantitatively. There is need to reduce the dimensionality of the spatio-temporal information, and to efficiently represent the high-speed data in a compact, handy and lossless way. In this issue, automatic segmentation of glottal area is a major challenge. Recently, several advanced methods have been proposed for glottal segmentation, based either on a region-growing approach (Yan et al., 2006; Lohscheller et al., 2007; Demeyer et al., 2009) or on an active-contours framework (Marendic et al., 2001; Allin et al., 2004; Moukalled et al., 2009). The choice of segmentation method is not trivial, as it depends on image quality, the features profile of the object of interest, and computational demands. In this \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:_xSYboBqXhAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Analysis and synthesis of speech using an adaptive full-band harmonic model",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6525352/",
            "Abstract": "Voice models often use frequency limits to split the speech spectrum into two or more voiced/unvoiced frequency bands. However, from the voice production, the amplitude spectrum of the voiced source decreases smoothly without any abrupt frequency limit. Accordingly, multiband models struggle to estimate these limits and, as a consequence, artifacts can degrade the perceived quality. Using a linear frequency basis adapted to the non-stationarities of the speech signal, the Fan Chirp Transformation (FChT) have demonstrated harmonicity at frequencies higher than usually observed from the DFT which motivates a full-band modeling. The previously proposed Adaptive Quasi-Harmonic model (aQHM) offers even more flexibility than the FChT by using a non-linear frequency basis. In the current paper, exploiting the properties of aQHM, we describe a full-band Adaptive Harmonic Model (aHM) along with detailed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:738O_yMBCRsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Detection of clicks based on group delay",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=2299543610779515145&hl=en&oi=scholarr",
            "Abstract": "In this paper we present a novel approach for the automatic detection of clicks from recordings of beaked whales based on the phase characteristics of minimum phase signals and especially using the group delay function. Group delay is estimated through the and first derivative of the Fourier Transform of a signal. A major advantage of the proposed approach is its robustness against additive noise while it doesn't require the definition of ad-hoc or adaptive thresholds for the detection of clicks. This method works on raw recordings which are usually quite noisy as well as on click enhanced recordings (after band-pass filtering or using operators like the Teager-Kaiser energy operator). Moreover, a click is just detected by searching the positive zero crossings over time of the slope of the phase spectrum. To evaluate the effectiveness of the proposed approach in detecting clicks, a one-minute recording has been \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:M3NEmzRMIkIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Speech Enhancement for Noise-Robust Speech Synthesis Using Wasserstein GAN.",
            "Publication year": 2019,
            "Publication url": "https://isca-speech.org/archive/Interspeech_2019/pdfs/2648.pdf",
            "Abstract": "The quality of speech synthesis systems can be significantly deteriorated by the presence of background noise in the recordings. Despite the existence of speech enhancement techniques for effectively suppressing additive noise under low signal-tonoise (SNR) conditions, these techniques have been neither designed nor tested in speech synthesis tasks where background noise has relatively lower energy. In this paper, we propose a speech enhancement technique based on generative adversarial networks (GANs) which acts as a preprocessing step of speech synthesis. Motivated by the speech enhancement generative adversarial network (SEGAN) approach and recent advances in deep learning, we propose to use Wasserstein GAN (WGAN) with gradient penalty and gated activation functions to the autoencoder network of SEGAN. We studied the impact of the proposed method on a data set consisting of 28 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:86PQX7AUzd4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal Speaker Identity Conversion",
            "Publication year": 2007,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.4928&rep=rep1&type=pdf",
            "Abstract": "Being able to convert a given the speech and facial movements of a given source speaker into those of another (identified) target speaker, is a challenging problem. In this paper we build on the experience gained in a previous eNTERFACE workshop to produce a working, although still very imperfect, identity conversion system. The conversion system we develop is based on the late fusion of two independently obtained conversion results: voice conversion and facial movement conversion. In an attempt to perform parallel conversion of the glottal source and excitation tract features of speech, we examine the usability of the ARX-LF source-filter model of speech. Given its high sensitivity to parameter modification, we then use the code-book based STASC model. For face conversion, we first build 3D facial models of the source and target speakers, using the MPEG-4 standard. Facial movements are then tracked using the Active Appearance Model approach, and facial movement mapping is obtained by imposing source FAPs on the 3D model of the target, and using the target FAPUs to interpret the source FAPs.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:bFI3QPDXJZMC",
            "Publisher": "eNTERFACE"
        },
        {
            "Title": "Prediction of dialogue success with spectral and rhythm acoustic features using dnns and svms",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8639580/",
            "Abstract": "In this paper we investigate the novel use of exclusively audio to predict whether a spoken dialogue will be successful or not, both in a subjective and in an objective manner. To achieve that, multiple spectral and rhythmic features are inputted to support vector machines and deep neural networks. We report results on data from 3267 spoken dialogues, using both the full user response as well as parts of it. Experiments show an average accuracy of 74% can be achieved using just 5 acoustic features, when analysing merely 1 user turn, which allows both a real-time but also a fairly accurate prediction of a dialogue successfulness only after one short interaction unit. From the features tested, those related to speech rate, signal energy and cepstrum are amongst the most informative. Results presented here outperform the state of the art in spoken dialogue success prediction through solely acoustic features.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:FPJr55Dyh1AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Adaptive gain control for enhanced speech intelligibility under reverberation",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7539275/",
            "Abstract": "Overlap-masking reduces speech intelligibility in reverberant environments. In contrast to additive noise, the masking signal depends on the past of the speech signal. An increase in output signal power is followed by an increase in reverberation power. Taking into consideration, the mechanics of reverberation is essential for the development of speech modifications that effectively increase intelligibility. This letter proposes a mathematical framework that optimizes the full-band signal power as a function of late reverberation power and the degree of signal nonstationarity. The prescribed signal gain is smoothed adaptively to suppress artifacts that may be introduced by rapid gain fluctuations due to frame-based processing. Compared to a reference method, it is shown that higher signal-to-late-reverberation ratio in nonstationary regions of the speech signal is achieved with less aggressive, on average, gain \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:nrtMV_XWKgEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Increasing speech intelligibility via spectral shaping with frequency warping and dynamic range compression plus transient enhancement.",
            "Publication year": 2013,
            "Publication url": "https://lpp.ilpga.fr/PDF/IS130174/IS130174.PDF",
            "Abstract": "In order to make speech (natural or synthetic) more intelligible for listeners in real-world noisy environments, various modifications have been proposed that exploit spectral and temporal signal features. Previously, an evaluation campaign involving several approaches illustrated that a Spectral Shaping (SS) and Dynamic Range Compression (DRC) method proved highly successful at increasing speech intelligibility. For the public follow-up campaign (ie, the Hurricane Challenge), this work introduces additional modifications into SSDRC in an attempt to further enhance intelligibility. First aiming to slow down the articulation rate, the speech is uniformly time stretched to effectively increase signal redundancy. Second, a frequency warping mechanism to expand vowel space is incorporated into the SS. Third, scaling to enhance the transient regions of speech is applied in the time-domain along with DRC. Objective \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:VOx2b1Wkg3QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spectral Based Short-Time Features for Voice Quality Assessment",
            "Publication year": 2009,
            "Publication url": "https://www.csd.uoc.gr/~sspl/MSc/Vasilakis.pdf",
            "Abstract": "In the context of voice quality assessment, phoniatricians are aided by the measurement of several phenomena that may reveal the existence of pathology in voice. Of the most prominent among such phenomena are these of jitter and shimmer. Jitter is defined as perturbations of the glottal cycle and shimmer is defined as perturbations of the glottal excitation amplitude. Both phenomena occur during voice production, especially in the case of vowel phonation. Acoustic analysis methods are usually employed to estimate jitter using the radiated speech signal as input. Most of these methods measure jitter in the time domain and are based on pitch period estimation, consequently, they are sensitive to the error of this estimation. Furthermore, the lack of robustness that is exhibited by pitch period estimators, it makes the use of continuous speech recordings as input problematic, and essentially limits jitter measurement to sustained vowel signals. Similarly for shimmer, time domain acoustic analysis methods are usually called to estimate the phenomenon in speech signals, based on estimation of peak amplitude per period. Moreover, these methods, for both phenomena, are affected by averaging and explicit or implicit use of low-pass information. The use of mathematical descriptions for jitter and shimmer, in order to transfer the estimation from the time domain to the frequency domain, may alleviate these problems. Using a mathematical model that couples two periodic events to achieve the local aperiodicity, allows jitter to be modeled as the shift of one of the two periodic events with respect to the other. Said model, when transformed to the frequency \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:CHSYGLWDkRkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Conditional vector quantization for voice conversion",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4218148/",
            "Abstract": "Voice conversion methods have the objective of transforming speech spoken by a particular source speaker, so that it sounds as if spoken by a different target speaker. The majority of voice conversion methods is based on transforming the short-time spectral envelope of the source speaker, based on derived correspondences between the source and target vectors using training speech data from both speakers. These correspondences are usually obtained by segmenting the spectral vectors of one or both speakers into clusters, using soft (GMM-based) or hard (VQ-based) clustering. Here, we propose that voice conversion performance can be improved by taking advantage of the fact that often the relationship between the source and target vectors is one-to-many. In order to illustrate this, we propose that a VQ approach namely constrained vector quantization (CVQ), can be used for voice conversion. Results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:RHpTSmoSYBkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fast inter-harmonic reconstruction for spectral envelope estimation in high-pitched voices",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6853374/",
            "Abstract": "During the production of voiced speech, the excitation signal performs a spectral subsampling of the filter transfer function. As a consequence, recovering the underlying spectral envelope (SE) becomes particularly difficult in high-pitched voices, where estimates using several conventional approaches are known to be contaminated by harmonics. To overcome such issues, this letter proposes to reconstruct inter-harmonics by a simple weigthed time-domain multiplication. Usual SE estimation methods can then be applied on the resulting signal. Both our objective and subjective experiments show that the proposed method provides similar or slightly better results when compared to more sophisticated approaches like true envelope or cubic spline interpolation between the harmonics. However, contrary to these latter techniques, its computational load is very low thanks to its simplicity.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:VL0QpB8kHFEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Combined estimation/coding of highband spectral envelopes for speech spectrum expansion",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1326024/",
            "Abstract": "The paper addresses the problem of expanding the bandwidth of narrowband speech signals, focusing on the estimation of highband spectral envelopes. It is well known that there is not enough mutual information between the two bands. We show that this happens because narrowband spectral envelopes have a one-to-many relationship with highband spectral envelopes. A combined estimation/coding scheme for the missing spectral envelope is proposed, which employs this relationship to produce a high quality highband reconstruction, provided that there is an appropriate excitation. Subjective tests using the TIMIT database indicate that 134 bits/sec for the highband spectral envelope are adequate for a DCR (degradation category rating) score of 4.41. This is an improvement of 22.8% over a typical estimation of highband envelopes using the usual mapping functions, in terms of DCR score.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:KlAtU1dfN6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Can modified casual speech reach the intelligibility of clear speech?",
            "Publication year": 2012,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2012/i12_0579.pdf",
            "Abstract": "Clear speech is a speaking style adopted by speakers in an attempt to maximize the clarity of their speech and is proven to be more intelligible than casual speech. This work focuses on modifying casual speech to sound as intelligible as clear speech. First, we examine the role of speaking rate for intelligibility. Clear and casual speech signals are time-scale stretched, matching the average duration of the casual and clear speech respectively. Next, spectral shaping and dynamic range compression are considered for increasing the loudness of the original casual speech while keeping the power of signals unaffected. Subjective tests with speech-in-noise conditions using speech shaped noise at-3, 0, and 5 dB SNR show that clear speech with high speaking rate is less intelligible than the original clear speech but still more intelligible than the unmodified casual speech. However, the intelligibility score for the time \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:fPk4N6BV_jEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Effective emotion recognition in movie audio tracks",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7953132/",
            "Abstract": "This paper addresses the problem of speech emotion recognition from movie audio tracks. The recently collected Acted Facial Expression in the Wild 5.0 database is used. The aim is to discriminate among angry, happy, and neutral. We extract a relatively small number of features, a subset of which is not commonly used for the emotion recognition task. Those features are fed as input to an ensemble classifier that combines random forests with support vector machines. An accuracy of 65.63% is reported, outperforming a baseline system that uses the K-nearest neighbor classifier and has an accuracy of 56.88%. To verify the suitability of the exploited features, the same ensemble classification schema is applied on the feature set similar those employed in Audio/Visual Emotion Challenge 2011. In the latter case, an accuracy of 61.25% is achieved using a large set of 1582 features, as opposed to just 86 features in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:4MWp96NkSFoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The importance of phase on voice quality assessment",
            "Publication year": 2014,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.705.5777&rep=rep1&type=pdf",
            "Abstract": "State of the art objective measures for quantifying voice quality mostly consider estimation of features extracted from the magnitude spectrum. Assuming that speech is obtained by exciting a minimum-phase (vocal tract filter) and a maximum-phase component (glottal source), the amplitude spectrum cannot capture the maximum phase characteristics. Since voice quality is connected to the glottal source, the extracted features should be linked with the maximum-phase component of speech. This work proposes a new metric based on the phase spectrum for characterizing the maximum-phase component of the glottal source. The proposed feature, the Phase Distortion Deviation, reveals the irregularities of the glottal pulses and therefore, can be used for detecting voice disorders. This is evaluated in a ranking problem of speakers with spasmodic dysphonia. Results show that the obtained ranking is highly correlated \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:SdhP9T11ey4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Methods for applying dynamic sinusoidal models to statistical parametric speech synthesis",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7178900/",
            "Abstract": "Sinusoidal vocoders can generate high quality speech, but they have not been extensively applied to statistical parametric speech synthesis. This paper presents two ways for using dynamic sinusoidal models for statistical speech synthesis, enabling the sinusoid parameters to be modelled in HMM-based synthesis. In the first method, features extracted from a fixed- and low-dimensional, perception-based dynamic sinusoidal model (PDM) are statistically modelled directly. In the second method, we convert both static amplitude and dynamic slope from all the harmonics of a signal, which we term the Harmonic Dynamic Model (HDM), to intermediate parameters (regularised cepstral coefficients) for modelling. During synthesis, HDM is then used to reconstruct speech. We have compared the voice quality of these two methods to the STRAIGHT cepstrum-based vocoder with mixed excitation in formal listening tests \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:_Ybze24A_UAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Similarity methods for computational ethnomusicology",
            "Publication year": 2010,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.710.1270&rep=rep1&type=pdf",
            "Abstract": "The field of computational ethnomusicology has drawn growing attention by researchers in the music information retrieval community. In general, subjects are considered that are related to the processing of traditional forms of music, often with the goal to support studies in the field of musicology with computational means.Tools have been proposed that make access to large digital collections of traditional music easier, for example by automatically detecting a specific kind of similarity between pieces or by automatically segmenting data into partitions that are either relevant or irrelevant for further investigation. In this thesis, the focus lies on music of the Eastern Mediterranean, and specifically on traditional music of Greece and Turkey. At the beginning of the thesis related work, the task was defined which directed the aspects of the necessary research activities.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:k_IJM867U9cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal Speaker Conversion\u2014his master\u2019s voice... and face\u2014",
            "Publication year": 2006,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.8799",
            "Abstract": "The goal of this project is to convert a given speaker\u2019s speech (the Source speaker) into another identified voice (the Target speaker) as well as analysing the face animation of the source to animate a 3D avatar imitating the source facial movements. We assume we have at our disposal a large amount of speech samples from the source and target voices with a reasonable amount of parallel data. Speech and video are processed separately and recombined at the end. Voice conversion is obtained in two steps: a voice mapping step followed by a speech synthesis step. In the speech synthesis step, we specifically propose to select speech frames directly from the large target speech corpus, in a way that recall the unit-selection principle used in state-of-the-art text-to-speech systems. The output of this four weeks work can be summarized as: a tailored source database, a set of open-source MATLAB and C files and finally audio and video files obtained by our conversion method. Experimental results show that we cannot aim to reach the target with our LPC synthesis method; further work is required to enhance the quality of the speech. Index Terms\u2014voice conversion, speech-to-speech conversion, speaker mapping, face tracking, cloning, morphing, avatar control.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:tuHXwOkdijsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A fully recurrent feature extraction for single channel speech enhancement",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2006.05233",
            "Abstract": "Convolutional neural network (CNN) modules are widely being used to build high-end speech enhancement neural models. However, the feature extraction power of vanilla CNN modules has been limited by the dimensionality constraint of the convolution kernels that are integrated - thereby, they have limitations to adequately model the noise context information at the feature extraction stage. To this end, adding recurrency factor into the feature extracting CNN layers, we introduce a robust context-aware feature extraction strategy for single-channel speech enhancement. As shown, adding recurrency results in capturing the local statistics of noise attributes at the extracted features level and thus, the suggested model is effective in differentiating speech cues even at very noisy conditions. When evaluated against enhancement models using vanilla CNN modules, in unseen noise conditions, the suggested model with recurrency in the feature extraction layers has produced a segmental SNR (SSNR) gain of up to 1.5 dB, an improvement of 0.4 in subjective quality in the Mean Opinion Score scale, while the parameters to be optimized are reduced by 25%.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:anf4URPfarAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the detection of the intelligibility advantage of clear speech vs. casual speech",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=14859307828959736656&hl=en&oi=scholarr",
            "Abstract": "This work focuses on speaking rate and pitch differences between clear and casual signals. Transforming the clear signal to match the casual signal in terms of the prosodic features, the effect of each one factor to the intelligibility advantage of clear speech is examined based on acoustic evaluations and objective measures.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:V3AGJWp-ZtQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Voice transformation: a survey",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4960401/",
            "Abstract": "Voice transformation refers to the various modifications one may apply to the sound produced by a person, speaking or singing. Voice transformation is usually seen as an add-on or an external system in speech synthesis systems since it may create virtual voices in a simple and flexible way. In this paper we review the state-of-the-art Voice transformation methodology showing its limitations in producing good speech quality and its current challenges. Addressing quality issues of current voice transformation algorithms in conjunction with properties of the speech production and speech perception systems we try to pave the way for more natural Voice Transformation algorithms in the future. Facing the challenges, will allow Voice transformation systems to be applied in important and versatile areas of speech technology; applications that are far beyond speech synthesis.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ufrVoPGSRksC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Photo-realistic expressive text to talking head synthesis.",
            "Publication year": 2013,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2013/i13_2667.pdf",
            "Abstract": "A controllable computer animated avatar that could be used as a natural user interface for computers is demonstrated. Driven by text and emotion input, it generates expressive speech with corresponding facial movements. To create the avatar, HMM-based text-to-speech synthesis is combined with active appearance model (AAM)-based facial animation. The novelty is the degree of control achieved over the expressiveness of both the speech and the face while keeping the controls simple. Controllability is achieved by training both the speech and facial parameters within a cluster adaptive training (CAT) framework. CAT creates a continuous, low dimensional eigenspace of expressions, which allows the creation of expressions of different intensity (including ones more intense than those in the original recordings) and combining different expressions to create new ones. Results on an emotion-recognition task \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:WA5NYHcadZ8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Universal Multi-Speaker Multi-Style Text-to-Speech via Disentangled Representation Learning Based on R\u00e9nyi Divergence Minimization",
            "Publication year": 2021,
            "Publication url": "https://scholar.google.com/scholar?cluster=7179543401071695124&hl=en&oi=scholarr",
            "Abstract": "In this paper, we present a universal multi-speaker, multi-style Text-to-Speech (TTS) synthesis system which is able to generate speech from text with speaker characteristics and speaking style similar to a given reference signal. Training is conducted on non-parallel data and generates voices in an unsupervised manner, ie, neither style annotation nor speaker label are required. To avoid leaking content information into the style embeddings (referred to as \u201ccontent leakage\u201d) and leaking speaker information into style embeddings (referred to as \u201cstyle leakage\u201d) we suggest a novel R\u00e9nyi Divergence based Disentangled Representation framework through adversarial learning. Similar to mutual information minimization, the proposed approach explicitly estimates via a variational formula and then minimizes the R\u00e9nyi divergence between the joint distribution and the product of marginals for the content-style and style \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:OTTXONDVkokC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the recovery of time-varying spectral envelope information from aqhm-derived spectra",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5947579/",
            "Abstract": "Spectral envelopes of speech signals are typically obtained by making stationarity assumptions about the signal which are not always valid. The Adaptive Quasi-Harmonic Model (AQHM), a non-stationary signal model, is capable of capturing the time-varying quasi harmonics in voiced speech. This paper suggests the use of AQHM in a multi-layer scheme which results in a high-resolution time-frequency representation of speech. This representation is then used for the recovery of the evolving spectral envelope and thus, a time-frequency spectral envelope estimation algorithm is introduced related to the Papoulis-Gerchberg algorithm for data extrapolation. Results on voiced speech sounds show that the estimated spectral envelopes are smoother than those estimated by state-of-the-art spectral envelope estimators, while maintaining the important spectral details of the speech spectrum.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:vRqMK49ujn8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Weighted generative adversarial network for many-to-many voice conversion",
            "Publication year": 2019,
            "Publication url": "http://publications.rwth-aachen.de/record/769831/files/769831.pdf",
            "Abstract": "The goal of voice conversion (VC) is to convert speech from a source speaker to that of a target, without changing phonetic contents. VC usually relies on parallel data for training, which limits its practical applications. Existing approaches are also limited in handling multiple speakers, since different models should be built independently for every speaker pair. To tackle that, a variant of Generative Adversarial Network (StarGAN-VC) were introduced that allows many-to-many mapping instead of learning all the pairwise transformations. Moreover, StarGAN-VC can handle non-parallel data, ie, speakers do not need to utter the same sentences. In this paper, we suggest an algorithmic variation of StarGAN training where suitable weights are introduced. Weights which modify the Generator\u2019s gradient value aim to put more power to fake samples that fool the Discriminator. The suggested algorithm results in a stronger Generator. We refer to this variation as weighted-StarGAN (weS-tarGAN). In weStarGAN, the convergence of the training performance is accelerated. More importantly, the proposed algorithm achieves significant improvement against baseline StarGAN-VC concerning speech subjective quality for both speech quality and speaker similarity.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:Ri6SYOTghG4C",
            "Publisher": "Universit\u00e4tsbibliothek der RWTH Aachen"
        },
        {
            "Title": "Normalized modulation spectral features for cross-database voice pathology detection",
            "Publication year": 2009,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2009/papers/i09_0935.pdf",
            "Abstract": "In this paper, we employ normalized modulation spectral analysis for voice pathology detection. Such normalization is important when there is a mismatch between training and testing conditions, or in other words, employing the detection system in real (testing) conditions. Modulation spectra usually produce a high-dimensionality space. For classification purposes, the size of the original space is reduced using Higher Order Singular Value Decomposition (SVD). Further, we select most relevant features based on the mutual information between subjective voice quality and computed features, which leads to an adaptive to the classification task modulation spectra representation. For voice pathology detection, the adaptive modulation spectra is combined with an SVM classifier. To simulate the real testing conditions; one for training and the other for testing. We address the difference of signal characteristics between \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:hMod-77fHWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Effectiveness of Near-End Speech Enhancement Under Equal-Loudness and Equal-Level Constraints.",
            "Publication year": 2016,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2016/pdfs/0594.PDF",
            "Abstract": "Most recently proposed near-end speech enhancement methods have been evaluated with the overall power (RMS) of the speech held constant. While significant intelligibility gains have been reported in various noisy conditions, an equal-RMS constraint may lead to enhancement solutions that increase the loudness of the original speech. Comparable effects might be produced simply by increasing the power of the original speech, which also leads to an increase in loudness. Here we suggest modifying the equal-RMS constraint to one of equal loudness between the original and the modified signals, based on a loudness model for time-varying sounds. Four state-of-the-art speechin-noise intelligibility enhancement systems were evaluated under the equal-loudness constraint, using intelligibility tests with normal-hearing listeners. Results were compared with those obtained under the equal-RMS constraint. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:yB1At4FlUx8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "An unsupervised learning approach to neural-net-supported WPE dereverberation",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8683542/",
            "Abstract": "Reverberation degrades signal quality and increases word error rates in automatic speech recognition (ASR). Reverberation suppression is, thus, a key component in listening enhancement devices and ASR front end. The weighted prediction error (WPE) is a prominent and effective method that gained popularity in recent ASR challenges. The need for iterative optimization in WPE leads to high computational cost and instabilities for short signals. Neural net (NN) supported WPE was proposed to alleviate these issues. However, NN training requires parallel data, i.e., reverberant and \"clean\" (direct sound plus early reflections) speech, which is not available in general. We show that the supporting network can be trained efficiently, without any supervision, using reverberant speech only. Consequently, adaptation to unseen environments is largely simplified. Network training involves the complete de-reverberation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:ClCfbGk0d_YC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Near and far field speech-in-noise intelligibility improvements based on a time\u2013frequency energy reallocation approach",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7501607/",
            "Abstract": "An algorithm designed to enhance the intelligibility of speech signals before they are presented in noisy environments was evaluated. The processed and unprocessed speech had the same root-mean square level. Spectral energy was redistributed to increase the signal-to-noise ratio (SNR) in the mid- and high-frequency bands, while the softer segments of speech were increased in level by applying time-domain dynamic range compression. Noise level adaptation was introduced to increase the subjective quality of signals at high SNRs. Evaluations were conducted both in near field (headphones) and in far field (outdoor) conditions using listeners with normal hearing and two types of background. The results showed: (a) In the near field test, the proposed algorithm yielded significant intelligibility improvements relative to the unprocessed speech for both stationary and nonstationary backgrounds; (b) In the far \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:t7zJ5fGR-2UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Extraction of speech-relevant information from modulation spectrograms",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-71505-4_5",
            "Abstract": "In this work, we adopt an information theoretic approach - the Information Bottleneck method to extract the relevant modulation frequencies across both dimensions of a spectrogram, for speech / non-speech discrimination (music, animal vocalizations, environmental noises). A compact representation is built for each sound ensemble, consisting of the maximally informative features. We demonstrate the effectiveness of a simple thresholding classifier which is based on the similarity of a sound to each characteristic modulation spectrum.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Evaluation of near-end speech enhancement under equal-loudness constraint for listeners with normal-hearing and mild-to-moderate hearing loss",
            "Publication year": 2017,
            "Publication url": "https://asa.scitation.org/doi/abs/10.1121/1.4973533",
            "Abstract": "Four algorithms designed to enhance the intelligibility of speech when noise is added after processing were evaluated under the constraint that the speech should have the same loudness before and after processing, as determined using a loudness model. The algorithms applied spectral modifications and two of them included dynamic-range compression. On average, the methods with dynamic-range compression required the least level adjustment to equate loudness for the unprocessed and processed speech. Subjects with normal-hearing (experiment 1) and mild-to-moderate hearing loss (experiment 2) were tested using unmodified and enhanced speech presented in speech-shaped noise (SSN) and a competing speaker (CS). The results showed (a) the algorithms with dynamic-range compression yielded the largest intelligibility gains in both experiments and for both types of background; (b) the algorithms \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:zLWjf1WUPmwC",
            "Publisher": "Acoustical Society of America"
        },
        {
            "Title": "Voice transformation",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-49127-9_24",
            "Abstract": "Voice transformation                                                         refers to the various modifications one may apply to the sound produced by a person, speaking or singing. In this chapter we give a description of various ways in which one can modify a voice and provide details on how to implement these modifications using a simple, but quite efficient, parametric model based on a harmonic representation of speech. By discussing the quality issues of current voice transformation algorithms in conjunction with the properties of speech production and perception systems we try to pave the way for more-natural voice transformation algorithms in the future.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Evaluating the outcome of phonosurgery: comparing the role of VHI and VoiSS questionnaires in the Greek language",
            "Publication year": 2012,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0892199711000774",
            "Abstract": "The objective was to study the role of the Greek version of Voice Handicap Index (VHI) in comparison with Voice Symptom Scale (VoiSS) in terms of measuring voice surgery outcome in patients with benign laryngeal lesions.Nonrandomized prospective.Forty-six patients operated for benign laryngeal lesions were enrolled in the present study. All patients were assessed according to the European Laryngological Society guidelines. In terms of self-evaluation, patients answered the Greek versions of both VHI and VoiSS, preoperatively and 6 weeks postoperatively, and the results were statistically analyzed.The strongest correlation was observed between the functional subscale of VHI and the impairment subscale of VoiSS, as well as, between the emotional subscales of both VHI and VoiSS, pre- and postoperatively. A statistically significant change in subscale \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:pqnbT2bcN3wC",
            "Publisher": "Mosby"
        },
        {
            "Title": "Adaptive sinusoidal modeling of percussive musical instrument sounds",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6811508/",
            "Abstract": "Percussive musical instrument sounds figure among the most challenging to model using sinusoids particularly due to the characteristic attack that features a sharp onset and transients. Attack transients present a highly nonstationary inharmonic behaviour that is very difficult to model with traditional sinusoidal models which use slowly varying sinusoids, commonly introducing an artifact known as pre-echo. In this work we use an adaptive sinusoidal model dubbed eaQHM to model percussive sounds from musical instruments such as plucked strings or percussion and investigate how eaQHM handles the sharp onsets and the nonstationary inharmonic nature of the attack transients. We show that adaptation renders a virtually perceptually identical sinusoidal representation of percussive sounds from different musical instruments, improving the Signal to Reconstruction Error Ratio (SRER) obtained with a traditional \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:eJXPG6dFmWUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fast analysis/synthesis of harmonic signals",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1660826/",
            "Abstract": "Harmonic models are commonly used in signal processing. The analysis of harmonic signals requires the solution of a symmetric Toeplitz system of equations. Levinson-based Toeplitz solvers have a O(n  2 ) complexity. This paper proposes an O(n) algorithm by encoding the inverse matrices required for the solution of the linear system to a few parameters in order to obtain an approximate solution for the harmonic model. For speech related applications, the proposed algorithm is 2-30 times faster than the Levinson algorithm, while degradation is minimal and memory requirements are very low",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:b0M2c_1WBrUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Author and Subject Index Vol. 61, No. 3, 2009",
            "Publication year": 2009,
            "Publication url": "https://www.karger.com/Article/Abstract/227417",
            "Abstract": "Author Index Vol. 61, No. 3, 2009 Subject Index Vol. 61, No. 3, 2009 Page 1 Fax +41 61 306 12 \n34 E-Mail karger@karger.ch www.karger.com Author Index Vol. 61, No. 3, 2009 de Bruijn, MJ \n180 Dejonckere, PH 171 Fourcin, A. 126 Fraile, R. 146 Fredouille, C. 146 Godino-Llorente, JI \n146 Hor\u00e1\u010dek, J. 137 Kob, M. 125, 171 Kuik, DJ 180 Langendijk, JA 180 Laukkanen, A.-M. 137 \nLeemans, CR 180 \u00a9 2009 S. Karger AG, Basel Accessible online at: www.karger.com/fpl \nMurphy, P. 137 Neuman, K. 125 Osma-Ruiz, V. 146 Quen\u00e9, H. 180 S\u00e1enz-Lech\u00f3n, N. 146 \n\u0160idlof, P. 137 Stylianou, Y. 153 \u0160vec, JG 137 ten Bosch, L. 180 Vasilakis, M. 153 Verdonck-de \nLeeuw, IM 180 Subject Index Vol. 61, No. 3, 2009 Acoustic analysis 180 Automatic detection of \nlaryngeal pathologies 146 Beat spectrum 153 Biomechanics of voice modeling 137 Connected \nspeech 126 Continuous speech 153 Fundamental frequency 137 Gender in voice --\u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:L7CI7m0gUJcC",
            "Publisher": "Karger Publishers"
        },
        {
            "Title": "The Toshiba entry to the CHiME 2018 challenge",
            "Publication year": 2018,
            "Publication url": "https://www.isca-speech.org/archive_v0/CHiME_2018/pdfs/CHiME_2018_paper_doddipatla.pdf",
            "Abstract": "This paper summarises the Toshiba entry to the single-array track of the CHiME 2018 speech recognition challenge. The system is based on conventional acoustic modelling (AM), where phonetic targets are tied to features at the frame-level, and use the provided tri-gram language model. The system is ranked in category A that focuses on acoustic robustness. Array signals are first enhanced using speaker dependent generalised eigenvalue (GEV) based beamforming. Two different acoustic representations are then extracted from the enhanced signals: i) log Mel filter-bank and ii) subband temporal envelope (STE) features. Separate acoustic models, trained on each set, are used for lattice combination. The AM combines convolutional and recurrent architectures in a single CNN-BLSTM model. Speaker adaptation, limited to vocal tract length normalisation (VTLN), de-reverberation and speaker suppression are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:XoXfffV-tXoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Complex cepstrum factorization for statistical parametric synthesis",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6854320/",
            "Abstract": "This paper presents a study on complex cepstrum-based speech factorization for acoustic modeling in statistical parametric synthesizers. The factorization is conducted assuming that both vocal tract resonance and glottal flow effect are fully represented by the complex cepstrum. We investigated four different forms to represent the complex cepstrum in the acoustic models and compared their performances in terms of objective measures between reconstructed and natural waveforms and final quality of the synthesized speech. According to experimental results, the all-pass/minimum-phase and real cepstrum/phase cepstrum decompositions are the best ones in terms of preserving the complex cepstrum information after the parameter generation process.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:WqliGbK-hY8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Audiovisual Speech Synthesis using Tacotron2",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2008.00620",
            "Abstract": "Audiovisual speech synthesis is the problem of synthesizing a talking face while maximizing the coherency of the acoustic and visual speech. In this paper, we propose and compare two audiovisual speech synthesis systems for 3D face models. The first system is the AVTacotron2, which is an end-to-end text-to-audiovisual speech synthesizer based on the Tacotron2 architecture. AVTacotron2 converts a sequence of phonemes representing the sentence to synthesize into a sequence of acoustic features and the corresponding controllers of a face model. The output acoustic features are used to condition a WaveRNN to reconstruct the speech waveform, and the output facial controllers are used to generate the corresponding video of the talking face. The second audiovisual speech synthesis system is modular, where acoustic speech is synthesized from text using the traditional Tacotron2. The reconstructed acoustic speech signal is then used to drive the facial controls of the face model using an independently trained audio-to-facial-animation neural network. We further condition both the end-to-end and modular approaches on emotion embeddings that encode the required prosody to generate emotional audiovisual speech. We analyze the performance of the two systems and compare them to the ground truth videos using subjective evaluation tests. The end-to-end and modular systems are able to synthesize close to human-like audiovisual speech with mean opinion scores (MOS) of 4.1 and 3.9, respectively, compared to a MOS of 4.1 for the ground truth generated from professionally recorded videos. While the end-to-end system gives a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:HtEfBTGE9r8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Analysis/synthesis of speech based on an adaptive quasi-harmonic plus noise model",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5495692/",
            "Abstract": "Decomposition of speech into a deterministic part and a stochastic part is a typical modeling. Usually, the deterministic part in voiced speech is modeled as a sum of time-varying sinusoids while the stochastic part is modeled as modulated noise. The estimation of sinusoidal parameters assumes that locally speech is a stationary signal. However, this is not true leading to biased amplitude and phase estimation. In this paper, we develop a scheme for speech analysis and synthesis which is able to deal with locally nonstationary frames. Thus, deterministic part it modeled using an adaptive quasi-harmonic model while stochastic part is modeled as time-modulated and frequency-modulated noise. Results show that the reconstructed signal is almost indistinguishable from the original.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:RGFaLdJalmkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An investigation of the application of dynamic sinusoidal models to statistical parametric speech synthesis",
            "Publication year": 2014,
            "Publication url": "https://www.researchgate.net/profile/Qiong_Hu4/publication/266793761_An_investigation_of_the_application_of_dynamic_sinusoidal_models_to_statistical_parametric_speech_synthesis/links/543bc4cb0cf2d6698be32897/An-investigation-of-the-application-of-dynamic-sinusoidal-models-to-statistical-parametric-speech-synthesis.pdf",
            "Abstract": "This paper applies a dynamic sinusoidal synthesis model to statistical parametric speech synthesis (HTS). For this, we utilise regularised cepstral coefficients to represent both the static amplitude and dynamic slope of selected sinusoids for statistical modelling. During synthesis, a dynamic sinusoidal model is used to reconstruct speech. A preference test is conducted to compare the selection of different sinusoids for cepstral representation. Our results show that when integrated with HTS, a relatively small number of sinusoids selected according to a perceptual criterion can produce quality comparable to using all harmonics. A Mean Opinion Score (MOS) test shows that our proposed statistical system is preferred to one using mel-cepstra from pitch synchronous spectral analysis.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ZuybSZzF8UAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multi-stream spectral representation for statistical parametric speech synthesis",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7472661/",
            "Abstract": "In statistical parametric speech synthesis such as Hidden Markov Model (HMM) based synthesis, one of the problems is in the over-smoothing of parameters, which leads to a muffled sensation in the synthesised output. In this paper, we propose an approach in which the high frequency spectrum is modelled separately from the low frequency spectrum. The high frequency band, which does not carry much linguistic information, is clustered using a very large decision tree so as to generate parameters as close as possible to natural speech samples. The boundary frequency can be adjusted at synthesis time for each state. Subjective listening tests show that the proposed approach is significantly preferred over the conventional approach using a single spectrum stream. Samples synthesised using the proposed approach sound less muffled and more natural.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:uJ-U7cs_P_0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Chirp rate estimation of speech based on a time-varying quasi-harmonic model",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4960501/",
            "Abstract": "The speech signal is usually considered as stationary during short analysis time intervals. Though this assumption may be sufficient in some applications, it is not valid for high-resolution speech analysis and in applications such as speech transformation and objective voice function assessment for detection of voice disorders. In speech, there are non stationary components, for instance time-varying amplitudes and frequencies, which may change quickly over short time intervals. In this paper, a previously suggested time-varying quasi-harmonic model is extended in order or to estimate the chirp rate for each sinusoidal component, thus successfully tracking fast variations in frequency and amplitude. The parameters of the model are estimated through linear Least Squares and the model accuracy is evaluated on synthetic chirp signals. Experiments on speech signals indicate that the new model is able to efficiently \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:J_g5lzvAfSwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Singer identification in rembetiko music",
            "Publication year": 2007,
            "Publication url": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:1040356",
            "Abstract": "In this paper, the problem of the automatic identification of a singer is investigated using methods known from speaker identification. Ways for using world models are presented and the usage of Cepstral Mean Subtraction (CMS) is evaluated. In order to minimize the difference due to musical style we use a novel data set, consisting of samples from greek Rembetiko music, being very similar in style. The data set also explores for the first time the influence of the recording quality, by including many historical gramophone recordings. Experimental evaluations show the benefits of world models for frame selection and CMS, resulting in an average classification accuracy of about 81% among 21 different singers.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "Sound and music Computing network"
        },
        {
            "Title": "Nonlinear speech features for the objective detection of discontinuities in concatenative speech synthesis",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/11520153_21",
            "Abstract": "abstr An objective distance measure which is able to predict audible discontinuities in concatenative speech synthesis systems is very important. Previous results showed that linear approaches are not very effective to detect audible discontinuities. The best result was obtained by using the Kullback-Leibler distance on power spectra with the rate of 37%. In this paper, we present two nonlinear approaches for the detection of discontinuities. The first method is based on a nonlinear harmonic model for speech while the second method is based on the demodulation of speech in an amplitude and a frequency component using the Teager energy operator. Results show that detection rate can exceed 70%, which is an improvement of about 95% over previous published results.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:XiVPGOgt02cC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Simple Spectral Techniques to Enhance the Intelligibility of Speech using a Harmonic Model",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=6733685317319099309&hl=en&oi=scholarr",
            "Abstract": "We have designed a tool to increase the intelligibility of speech by manipulating the parameters of a harmonic speech model. The system performs the transformation in two steps. In the first step, it modifies the spectral slope, which is closely related to the vocal effort. In the second step, it amplifies low-energy parts of the signal using dynamic range compression techniques. Such a system has two main advantages: its simplicity and the fact that it can be easily integrated into the synthesis engine of a speech synthesizer trained from Mel-cepstral coefficients.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:tOudhMTPpwUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tremor in Speakers With Spasmodic Dysphonia",
            "Publication year": 2011,
            "Publication url": "https://www.torrossa.com/gs/resourceProxy?an=2469234&publisher=FF3888",
            "Abstract": "The objective of this work is the estimation of vocal tremor in patients with spasmodic dysphonia before and after treatment, and the comparison of their tremor characteristics with those estimated from healthy speakers. As an outcome, a new tremor attribute is introduced, the deviation of the modulation level and a novel method is proposed for classifying speakers according to the prevalence of tremor in their voice. Results are consistent with subjective evaluations on patients who suffer from spasmodic dysphonia and confirm that the proposed method can be used for accurate estimation and objective ranking of the severity of tremor.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:_B80troHkn4C",
            "Publisher": "Firenze University Press"
        },
        {
            "Title": "Intelligibility enhancement of casual speech for reverberant environments inspired by clear speech properties",
            "Publication year": 2015,
            "Publication url": "https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_0065.pdf",
            "Abstract": "Clear speech has been shown to have an intelligibility advantage over casual speech in noisy and reverberant environments. This work validates spectral and time domain modifications to increase the intelligibility of casual speech in reverberant environments by compensating particular differences between the two speaking styles. To compensate spectral differences, a frequency-domain filtering approach is applied to casual speech. In time domain, two techniques for time-scaling casual speech are explored:(1) uniform time-scaling and (2) pause insertion and phoneme elongation based on loudness and modulation criteria. The effect of the proposed modifications is evaluated through subjective listening tests in two reverberant conditions with reverberation time 0.8 s and 2s. The combination of spectral transformation and uniform time-scaling is shown to be the most successful in increasing the intelligibility of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:7T2F9Uy0os0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Comunicaci \u0301on enriquecida a lo largo de la vida",
            "Publication year": 2019,
            "Publication url": "https://www.research.ed.ac.uk/en/publications/enriched-communication-across-the-lifespan",
            "Abstract": "Speech is a hugely efficient means of communication: a reduced capacity in listening or speaking creates a significant barrier to social inclusion at all points through the lifespan, in education, work and at home. Hearing devices and speech synthesis can help address this reduced capacity but their use imposes greater listener effort. The goal of the EU-funded ENRICH project is to modify or augment speech with additional information to make it easier to process. Enrichment reduces listening burden by minimising cognitive load, while maintaining or improving intelligibility. ENRICH investigates the relationship between cognitive effort and natural and synthetic speech. Non-intrusive metrics for listening effort will be developed and used to design modification techniques which result in low-burden speech. The value of various enrichment approaches will be evaluated with individuals and cohorts with typically sub-optimal communication ability, such as children, hearing-or speech-impaired adults, non-native listeners and individuals engaged in simultaneous tasks.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:YohjEiUPhakC",
            "Publisher": "Universidad de Alicante"
        },
        {
            "Title": "Three dimensions of pitched instrument onset detection",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5337997/",
            "Abstract": "In this paper, we suggest a novel group delay based method for the onset detection of pitched instruments. It is proposed to approach the problem of onset detection by examining three dimensions separately: phase (i.e., group delay), magnitude and pitch. The evaluation of the suggested onset detectors for phase, pitch and magnitude is performed using a new publicly available and fully onset annotated database of monophonic recordings which is balanced in terms of included instruments and onset samples per instrument, while it contains different performance styles. Results show that the accuracy of onset detection depends on the type of instruments as well as on the style of performance. Combining the information contained in the three dimensions by means of a fusion at decision level leads to an improvement of onset detection by about 8% in terms of F-measure, compared to the best single dimension.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Towards Neural-Based Single Channel Speech Enhancement for Hearing Aids",
            "Publication year": 2019,
            "Publication url": "http://publications.rwth-aachen.de/record/770005/files/770005.pdf",
            "Abstract": "Advancements in machine learning techniques have promoted the use of deep neural networks (DNNs) for supervised speech enhancement. However, the DNN\u2019s benefits of non-explicit noise statistics and nonlinear modeling capacity come at the expense of increased computational complexity for training and inference which is an issue for real-time restricted applications, like hearing aids. Contrary to the conventional approach which separately models the feature extraction and temporal dependency through a sequence of convolutional layers followed by a fully-connected recurrent layer, this work promotes the use of convolutional recurrent network layers for single-channel speech enhancement. Thereby, temporal correlations among inherently extracted spectral feature vectors are exploited, while further reducing the parameter set to be estimated relative to the conventional method. The proposed method is compared to a recent low algorithmic delay architecture. The models were trained in a speaker independent fashion on the NSDTSEA data set composed of different environmental noises. While objective speech quality and intelligibility measures of the two architectures are similar, the number of network parameters in the suggested enhancement method being reduced by 66%. This reduction is highly beneficial for storage and computation constraint applications.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:kh2fBNsKQNwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audiovisual Speech Synthesis using Tacotron2",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3462244.3479883",
            "Abstract": "Audiovisual speech synthesis involves synthesizing a talking face while maximizing the coherency of the acoustic and visual speech. To solve this problem, we propose using AVTacotron2, which is an end-to-end text-to-audiovisual speech synthesizer based on the Tacotron2 architecture. AVTacotron2 converts a sequence of phonemes into a sequence of acoustic features and the corresponding controllers of a face model. The output acoustic features are passed through a WaveRNN model to reconstruct the speech waveform. The speech waveform and the output facial controllers are used to generate the corresponding video of the talking face. As a baseline, we use a modular system, where acoustic speech is synthesized from text using the traditional Tacotron2. The reconstructed acoustic speech is then used to drive the controls of the face model using an independently trained audio-to-facial-animation neural \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:XvxMoLDsR5gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Time-scale modifications based on a full-band adaptive harmonic model",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6639262/",
            "Abstract": "In this paper, a simple method for time-scale modifications of speech based on a recently suggested model for AM-FM decomposition of speech signals, is presented. This model is referred to as the adaptive Harmonic Model (aHM). A full-band speech analysis/synthesis system based on the aHM representation is built, without the necessity of separating a deterministic and/or a stochastic component from the speech signal. The aHM models speech as a sum of harmonically related sinusoids that can adapt to the local characteristics of the signal and provide accurate instantaneous amplitude, frequency, and phase trajectories. Because of the high quality representation and reconstruction of speech, aHM can provide high quality time-scale modifications. Informal listenings show that the synthetic time-scaled waveforms are natural and free of some common artifacts encountered in other state-of-the-art models, such \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:1sJd4Hv_s6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Initial investigation of speech synthesis based on complex-valued neural networks",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7472755/",
            "Abstract": "Although frequency analysis often leads us to a speech signal in the complex domain, the acoustic models we frequently use are designed for real-valued data. Phase is usually ignored or modelled separately from spectral amplitude. Here, we propose a complex-valued neural network (CVNN) for directly modelling the results of the frequency analysis in the complex domain (such as the complex amplitude). We also introduce a phase encoding technique to map real-valued data (e.g. cepstra or log amplitudes) into the complex domain so we can use the same CVNN processing seamlessly. In this paper, a fully complex-valued neural network, namely a neural network where all of the weight matrices, activation functions and learning algorithms are in the complex domain, is applied for speech synthesis. Results show its ability to model both complex-valued and real-valued data.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:j8SEvjWlNXcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Implementation of simple spectral techniques to enhance the intelligibility of speech using a harmonic model",
            "Publication year": 2012,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2012/i12_0639.pdf",
            "Abstract": "We have designed a system that increases the intelligibility of speech signals in noise by manipulating the parameters of a harmonic speech model. The system performs the transformation in two steps: in the first step, it modifies the spectral slope, which is closely related to the vocal effort; in the second step, it amplifies low-energy parts of the signal using dynamic range compression techniques. Objective and subjective measures involving speech-shaped noise confirm the effectiveness of these simple methods. As the harmonic model has been used in previous works to implement the waveform generation module of high-quality statistical synthesizers, the system presented here can provide the synthesis engine with a higher degree of control on the intelligibility of the resulting artificial speech.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:70eg2SAEIzsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "GMM-based multimodal biometric verification",
            "Publication year": 2005,
            "Publication url": "https://www.eurecom.fr/fr/publication/1861/download/sec-publi-1861.pdf",
            "Abstract": "In this work, we describe how biometric data can be used for person identification and verification. We rely on three categories of traits, that is speech, signature, and face. These distinguishing features or characteristics of a person, on their own, do not provide satisfactory results using well-known techniques. This is the case especially when the number of enrolled persons is large. For this reason, we develop techniques for making good use of all the three traits. In particular, we choose to follow late fusion of the scores of each single trait. The results of these techniques are quite better than using only one trait. Another goal of this work is the creation of a high quality multilingual database with video, audio, and signatures from forty seven persons.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:qUcmZB5y_30C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Predicting dialogue success, naturalness, and length with acoustic features",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7953110/",
            "Abstract": "Statistical methods for Spoken Dialogue Systems have been shown to reduce the cost of development, while successfully handling a variety of applications. However, such systems are usually trained with simulated users or paid subjects in controlled settings. While this may be sufficient to jump-start learning in the various sub-components, learning is very much dependent on the complete knowledge that we have about the interaction. Relatively few works have focused on this problem, and we here propose to extract low-level audio descriptors and use them as input to various classifiers, namely support vector machines, Gaussian process regressors, and random forests, to predict metrics that are constituents of user satisfaction from acoustic features. While our approach is not directly comparable to the current state of the art, results show that models using the proposed feature set outperform models that use \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:p__nRnzSRKYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The harmonic model codec (hmc) framework for voip",
            "Publication year": 2007,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2007/i07_1681.pdf",
            "Abstract": "A framework for joint source/channel coding of speech is presented. It is based on a harmonic representation of the speech signal and facilitates efficient quantization of harmonic amplitudes and phases both in a single description and a multiple description setting. Furthermore, it combines high-quality packet loss concealment with efficient source coding and multiple description coding. Two proof-of-concept codecs are presented; a single description codec that is equivalent to iLBC in terms of bitrate and quality but more robust in conditions of increased packet losses and a multiple description codec that is capable of accepting loss rates up to 40% for a DCR score of 3.8.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:u_35RYKgDlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dimensionality reduction of modulation frequency features for speech discrimination",
            "Publication year": 2008,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2008/i08_0646.pdf",
            "Abstract": "We describe a dimensionality reduction method for modulation spectral features, which keeps the time-varying information of interest to the classification task. Due to the varying degrees of redundancy and discriminative power of the acoustic and modulation frequency subspaces, we first employ a generalization of SVD to tensors (Higher Order SVD) to reduce dimensions. Projection of modulation spectral features on the principal axes with the higher energy in each subspace results in a compact feature set. We further estimate the relevance of these projections to speech discrimination based on mutual information to the target class. Reconstruction of modulation spectrograms from the \u201cbest\u201d 22 features back to the initial dimensions, shows that modulation spectral features close to syllable and phoneme rates as well as pitch values of speakers are preserved.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:maZDTaKrznsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Linear dynamical models in speech synthesis",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6853606/",
            "Abstract": "Hidden Markov models (HMMs) are becoming the dominant approach for text-to-speech synthesis (TTS). HMMs provide an attractive acoustic modeling scheme which has been exhaustively investigated and developed for many years. Modern HMM-based speech synthesizers have approached the quality of the best state-of-the-art unit selection systems. However, we believe that statistical parametric speech synthesis has not reached its potential, since HMMs are limited by several assumptions which do not apply to the properties of speech. We, therefore, propose in this paper to use Linear Dynamical Models (LDMs) instead of HMMs. LDMs can better model the dynamics of speech and can produce a naturally smoother trajectory of the synthesized speech. We perform a series of experiments using different system configurations to check on the performance of LDMs for speech synthesis. We show that LDM \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:5awf1xo2G04C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improving speech intelligibility in noise environments by spectral shaping and dynamic range compression",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=5900015191264670283&hl=en&oi=scholarr",
            "Abstract": "3. ResultsFor testing the system, we used the first 20 Harvard sentences and two types of noise: Speech Shaped Noise (SSN) at SNR:-9dB,-4 dB and 1 dB, and Competing Speaker noise (CS) at SNR:-21 dB,-14 dB,-7 dB. For evaluation and comparison purposes, the extended SII suggested in [1] and the frequency dependent SNR recovery system suggested in [2], were implemented. Fig. 1 shows the results in terms of SII for the original signal without modifications (Orig), the suggested system (SSDRC) and the system presented in [2](referred to as SNR-R). Performance of sub-systems (Spectral Shaping (SS), Dynamic Range Compression (DRC)) is also shown. The final system combines SS and DRC in a cascade form. Overall, the suggested system (SS-DRC) outperforms SNR-R for all SNR levels and for both types of noise. All modified signals (either modified by SNR-R or SS-DRC) report better SII score than \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:K3LRdlH-MEoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Minimum mean squared error based warped complex cepstrum analysis for statistical parametric speech synthesis.",
            "Publication year": 2013,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2013/i13_2336.pdf",
            "Abstract": "This paper presents an approach for complex cepstrum analysis based on the minimum mean squared error criterion, and describes its application to statistical parametric speech synthesis. The proposed method alleviates some of the issues associated with conventional complex cepstrum analysis, such as choice of the window, phase unwrapping, and the need for accurate pitch marks. Given initial estimates of warped complex cepstra and respective analysis instants, the method iteratively optimizes the complex cepstrum on a warped quefrency domain by minimizing the mean squared error between the natural and the reconstructed speech waveforms. When applied to statistical parametric speech synthesis, the optimized complex cepstrum results in better performance in terms of synthesized speech quality, specially for emotional databases, when compared with the complex cepstrum calculated through \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:Mojj43d5GZwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Video and audio based detection of filled hesitation pauses in classroom lectures",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7077658/",
            "Abstract": "In this paper we study the detection of hesitation filled pauses in oral presentations of university lectures taught in the Greek language and recorded using a tablet PC via a specialized software. We suggest a hierarchical approach fusing video data with audio data for increasing the precision rate in our detection system. The detection method works at frame level rather than the usual segmental level for more accurate synchronization of audio and video data after removing the detected hesitations. Audio characteristics are modeled using Gaussian Mixture Models while the stationarity of the recorded video is taken into account. This efficient video and audio combination yields higher precision and recall rates comparing with other works in the literature. On a dataset of approximately 7 hours the precision rate is 99.6% while the recall rate is 84.7% when audio and video data are taken into account.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:35N4QoGY0k4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Generalizing Steady State Suppression for Enhanced Intelligibility under Reverberation",
            "Publication year": 2016,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2016/pdfs/1026.PDF",
            "Abstract": "Speech intelligibility in reverberant environments decreases due to overlap-masking. Unlike additive noise, the masking signal is not independent from the information bearing signal. A mathematical framework for intelligibility-enhancing signal modification prior to presentation in reverberant environments is presented in this paper. The optimal solution generalizes steady state suppression and adjusts the short-term signal power as a function of late reverberation power and signal importance. The signal modification operates in a full-band setting and preserves the time scale of the unmodified signal. Gain smoothing based on an adaptive rate-of-change constraint reduces processing artifacts and enhances performance. Subjective validation shows that the proposed method effectively reduces the impact of overlap-masking. Speech intelligibility at a reverberation time of 1.8 s was improved significantly compared \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:0KyAp5RtaNEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Text-to-speech synthesis",
            "Publication year": 2003,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=yl6AnaKtVAkC&oi=fnd&pg=PA323&dq=info:YFWFKt1C3d4J:scholar.google.com&ots=_VUk07zIHk&sig=fDQ3rHdupyGFk1IXP8fafZlwkmg",
            "Abstract": "This chapter gives an introduction to state-of-the-art text-to-speech (TTS) synthe\u2014sis systems, showing both the natural language processing and (although with fewer details) the digital signal processing problems involved. Section 17.1 gives a brief user-oriented description of a general TTS system and comments on its commercial applications. Section 17.2 gives a fairly general functional diagram of a modern TTS system and introduces its components. Section 17.3 brie\ufb02y describes its morphosyntactic module. Section 17.4 examines why sentence-level phonetization cannot be achieved by a sequence of dictionary look-ups, and describes possible implementations of the phonetizer. Section 17.5 is devoted to prosody generation. It brie\ufb02y outlines how intonation and duration can approximately be computed from text. Section 17.6 gives a conceptual introduction to the two main existing categories of techniques for waveform generation:(1) synthesis by rule and (2) concatenative synthesis.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:YOwf2qJgpHMC",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "A hybrid quasi-harmonic/CELP wideband speech coding scheme for unit selection TTS synthesis",
            "Publication year": 2011,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2011/i11_2537.pdf",
            "Abstract": "This paper suggests a new wideband speech coding model to efficiently compress acoustic inventories for concatenative unit selection text-to-speech (TTS) synthesis system. To fulfill the requirements of TTS synthesizer such as partial segment decoding and random access capability, a non-predictive scheme was adopted which combines the adaptive Quasi-Harmonic Model (aQHM) with the innovative codebook (ICB) model. aQHM plays a major role in modeling pitch harmonic components, and ICB compensates, in a closed-loop way, for the modeling error of aQHM. This is especially important in transient or unvoiced regions. To further improve the coding efficiency, a hybrid coding framework is also suggested. Results from a large French speech database show that the proposed algorithm provides similar speech quality to the high quality AMR-WB codec while it supports the random access capability.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:q3oQSFYPqjQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the detection of discontinuities in concatenative speech synthesis",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-71505-4_6",
            "Abstract": "Last decade considerable work has been done in finding an objective distance measure which is able to predict audible discontinuities in concatenative speech synthesis. Speech segments in concatenative synthesis are extracted from disjoint phonetic contexts and discontinuities in spectral shape and phase mismatches tend to occur at unit boundaries. Many feature sets most of them of spectral nature and distances were tested. However there were significant discrepancies among the results. In this paper, we tested most of the distances that were proposed using the same listening experiment. Best score were given by AM&FM decomposition of the speech signal using Fisher\u2019s linear discriminant.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ns9cj8rnVeAC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Evaluating the Intelligibility Benefits of Neural Speech Enrichment for Listeners with Normal Hearing and Hearing Impairment using the Greek Harvard Corpus",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2011.06548",
            "Abstract": "In this work we evaluate a neural based speech intelligibility booster based on spectral shaping and dynamic range compression (SSDRC), referred to as WaveNet-based SSDRC (wSSDRC), using a recently designed Greek Harvard-style corpus. The corpus has been developed according to the format of the Harvard/IEEE sentences and offers the opportunity to apply neural speech enhancement models and examine their performance gain for Greek listeners. wSSDRC has been successfully tested for English material and speakers in the past. In this paper we revisit wSSDRC to perform a full scale evaluation of the model with Greek listeners under the condition of equal energy before and after modification. Both normal hearing (NH) and hearing impaired (HI) listeners evaluated the model under speech shaped noise (SSN) at listener-specific SNRs matching their Speech Reception Threshold (SRT) - a point at which 50 % of unmodified speech is intelligible. The analysis statistics show that the wSSDRC model has produced a median intelligibility boost of 39% for NH and 38% for HI, relative to the plain unprocessed speech.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:sNmaIFBj_lkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Modulation Spectral Features for Objective Voice Quality Assessment: The Breathiness Case",
            "Publication year": 2009,
            "Publication url": "https://www.torrossa.com/gs/resourceProxy?an=2430475&publisher=FF3888",
            "Abstract": "In this paper, we employ normalized modula-tion spectral features for objective voice quality assessment regarding breathiness. Modulation spectra usually produce a high-dimensionality space. For classification purposes, the size of the original space is reduced using Higher Order Singular Value Decomposition (SVD). Further, we select most relevant features based on the mutual information between the degree of breathiness and the computed features, which leads to an adaptive to the classification task modulation spectral representation. The adaptive modulation spectral features are used as input to a Naive Bayes (NB) classifier. By combining two NB classifiers based on different feature sets a global classification rate of 79% for breathiness was achieved.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:PELIpwtuRlgC",
            "Publisher": "Firenze University Press"
        },
        {
            "Title": "Parataxis: Morphological similarity in traditional music",
            "Publication year": 2010,
            "Publication url": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:1040361",
            "Abstract": "In this paper an automatic system for the detection of similar phrases in music of the Eastern Mediterranean is proposed. This music follows a specific structure, which is referred to as parataxis. The proposed system can be applied to audio signals of complex mixtures that contain the lead melody together with instrumental accompaniment. It is shown that including a lead melody estimation into a stateof-the-art system for cover song detection leads to promising results on a dataset of transcribed traditional dances from the island of Crete in Greece. Furthermore, a general framework that includes also rhythmic aspects is proposed. The proposed method represents a simple framework for the support of ethnomusicological studies on related forms of traditional music.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:RYcK_YlVTxYC",
            "Publisher": "ISMIR"
        },
        {
            "Title": "Enhancing the intelligibility of statistically generated synthetic speech by means of noise-independent modifications",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6914585/",
            "Abstract": "When speaking devices such as smartphones, tablet-PCs, or GPS systems are used in noisy outdoor environments, the intelligibility of speech significantly drops. This is even more pronounced when synthetic speech is used. This article describes how a statistical parametric speech synthesis system trained on an ordinary synthesis database can be designed to generate highly intelligible speech, even at very low signal-to-noise ratios. By using a simple and flexible vocoder based on a full-band harmonic model, the proposed system applies deterministic noise-independent modifications at several levels: speaking rate, average fundamental frequency level and range, energy contour over time, formant sharpness, and intensity of specific spectral bands. The degree of intelligibility achieved by the system has been evaluated by means of a large-scale subjective test, the results of which show that the suggested \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:hkOj_22Ku90C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Weighting Time-Frequency Representation of Speech Using Auditory Saliency for Automatic Speech Recognition.",
            "Publication year": 2018,
            "Publication url": "https://www.researchgate.net/profile/Cong-Thanh_Do/publication/327385588_Weighting_Time-Frequency_Representation_of_Speech_Using_Auditory_Saliency_for_Automatic_Speech_Recognition/links/5b8b992c92851c1e1241f215/Weighting-Time-Frequency-Representation-of-Speech-Using-Auditory-Saliency-for-Automatic-Speech-Recognition.pdf",
            "Abstract": "This paper proposes a new method for weighting twodimensional (2D) time-frequency (TF) representation of speech using auditory saliency for noise-robust automatic speech recognition (ASR). Auditory saliency is estimated via 2D auditory saliency maps which model the mechanism for allocating human auditory attention. These maps are used to weight TF representation of speech, namely the 2D magnitude spectrum or spectrogram, prior to features extraction for ASR. Experiments on Aurora-4 corpus demonstrate the effectiveness of the proposed method for noise-robust ASR. In multi-stream ASR, relative word error rate (WER) reduction of up to 5.3% and 4.0% are observed when comparing the multi-stream system using the proposed method with the baseline single-stream system not using TF representation weighting and that using conventional spectral masking noise-robust technique, respectively. Combining the multi-stream system using the proposed method and the single-stream system using the conventional spectral masking technique reduces further the WER.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:HbR8gkJAVGIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A fast algorithm for improved intelligibility of speech-in-noise based on frequency and time domain energy reallocation.",
            "Publication year": 2015,
            "Publication url": "https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_0060.pdf",
            "Abstract": "This paper presents a fast and effective algorithm for enhancing speech intelligibility in additive noise conditions under the constraint of equal signal power before and after enhancement. Speech energy is reallocated in time, using dynamic range compression, and in frequency by boosting the signal to noise ratio in high frequencies, increasing the contrast between consecutive spectral peaks and valleys for the mid-frequencies, while maintaining the spectral energy in low frequencies. The algorithm has 90% lower computational load than similar and recently suggested state-of-the art approaches, while in large formal speech-in-noise intelligibility tests, the algorithm has shown to perform equally well to these methods in terms of intelligibility gains.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ZfRJV9d4-WMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Assessing Speaker Interpolation in Neural Text-to-Speech",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-87802-3_33",
            "Abstract": "This paper presents a study on voice interpolation in the framework of neural text-to-speech. Two main approaches are considered. The first one consists of adding three independent speaker embeddings at 3 different positions within the model. The second one substitutes the embedding vectors by convolutional layers, kernels of which are computed on the fly from reference spectrograms. The interpolation between speakers is done by linear interpolation between the speaker embeddings in the first case, and between convolution kernels in the second. Finally, we propose a new method for evaluating interpolation smoothness using agreements between interpolation weights, objective and subjective speaker similarities. The results indicate that both methods are able to produce smooth interpolation to some extent, with the one based on learned speaker embeddings yielding better results.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:9c2xU6iGI7YC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Voice pathology detection based eon short-term jitter estimations in running speech",
            "Publication year": 2009,
            "Publication url": "https://www.karger.com/Article/Abstract/219951",
            "Abstract": "In this paper, we investigate the use of jitter estimation over short time intervals (short-term jitter) for voice pathology detection in the case of running or continuous speech. Short-term jitter estimations are provided by the spectral jitter estimator (SJE), which is based on a mathematical description of the jitter phenomenon. The SJE has been shown to be robust against errors in pitch period estimations, which makes it a good candidate for measuring jitter in continuous speech. On two large databases of sustained vowel recordings from healthy and pathological voices, we suggest a threshold for the SJE for pathology detection based on cross-database validation. Applying that to a database of continuous speech (reading text) from normophonic and dysphonic speakers, a second threshold and new features are suggested for monitoring jitter in continuous speech. Detection performance of the suggested thresholds \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:ZeXyd9-uunAC",
            "Publisher": "Karger Publishers"
        },
        {
            "Title": "Will this dialogue be unsuccessful? prediction using audio features",
            "Publication year": 2017,
            "Publication url": "https://spiral.imperial.ac.uk/handle/10044/1/51854",
            "Abstract": "This paper proposes a method to improve statistical spoken dialogue systems and specifically it aims to provide a way for early detection of unsuccessful dialogues using the audio stream. If an interaction is predicted as unsuccessful, this information could be used to update the policy or to forward the call to a human agent. A dataset of interactions between Amazon Mechanical Turk workers and a statistical spoken dialogue system is used. A total of 702 dialogues are recorded. Then, mel-frequency cepstral coefficients (MFCCs) are extracted from the user\u2019s speech signal, forming a \u201cfeature image\u201d which is then given as input to a convolutional neural network comprising of 9 layers. The reported accuracy is 94.7%, and the system manages to predict that a dialogue will be unsuccessful for the 97.9% of the cases. With respect to accuracy, there is an improvement of 17.2%, compared to our previous work on predicting dialogue quality. We observe that for our task, convolutional neural networks can model temporal correlations given context information and that the cepstral domain is a useful and compact representation for convolutional neural networks.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:kuK5TVdYjLIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Device and method for a spoken dialogue system",
            "Publication year": 2018,
            "Publication url": "https://patents.google.com/patent/US9865257B2/en",
            "Abstract": "A controller for a dialog manager, the dialog manager being configured to receive a representation of an input utterance from a user and control the flow of conversation with the user. The controller is configured to implement a parameterized policy for defining the behavior of a dialog manager. The parameterized policy is configured to operate with an ontology-independent parameter as an input. The controller has a processor for parameterizing an end user ontology such that the parameterized policy can define the behavior of a dialog manager for the end user ontology. The processor is configured to define a slot of the end user ontology in terms of at least one of the ontology-independent parameters such that it is suitable for being used as an input to the parameterized policy.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:MLfJN-KU85MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Domain complexity and policy learning in task-oriented dialogue systems",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-92108-2_8",
            "Abstract": "In the present paper, we conduct a comparative evaluation of a multitude of information-seeking domains, using two well-known but fundamentally different algorithms for policy learning: GP-SARSA and DQN. Our goal is to gain an understanding of how the nature of such domains influences performance. Our results indicate several main domain characteristics that play an important role in policy learning performance in terms of task success rates.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:vbGhcppDl1QC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Modulation Enhancement of Temporal Envelopes for Increasing Speech Intelligibility in Noise.",
            "Publication year": 2016,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2016/pdfs/0500.PDF",
            "Abstract": "In this paper, speech intelligibility is enhanced by manipulating the modulation spectrum of the signal. First, the signal is decomposed into Amplitude Modulation (AM) and Frequency Modulation (FM) components using a high resolution adaptive quasi-harmonic model of speech. Then, the AM part of midrange frequencies of speech spectrum is modified by applying a transforming function which follows the characteristics of the clear style of speaking. This results in increasing the modulation depth of the temporal envelopes of casual speech as in clear speech. The modified AM components of speech are then combined with the original FM parts to synthesize the final processed signal. Subjective listening tests evaluating the intelligibility of speech in noise showed that the suggested approach increases the intelligibility of speech by 40% on average, while it is comparable with recently suggested state-of-the-art \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:uc_IGeMz5qoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Combining perceptually-motivated spectral shaping with loudness and duration modification for intelligibility enhancement of HMM-based synthetic speech in noise.",
            "Publication year": 2013,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2013/i13_3567.pdf",
            "Abstract": "This paper presents our entry to a speech-in-noise intelligibility enhancement evaluation: the Hurricane Challenge. The system consists of a Text-To-Speech voice manipulated through a combination of enhancement strategies, each of which is known to be individually successful: a perceptually-motivated spectral shaper based on the Glimpse Proportion measure, dynamic range compression, and adaptation to Lombard excitation and duration patterns. We achieved substantial intelligibility improvements relative to unmodified synthetic speech: 4.9 dB in competing speaker and 4.1 dB in speech-shaped noise. An analysis conducted across this and other two similar evaluations shows that the spectral shaper and the compressor (both of which are loudness boosters) contribute most under higher SNR conditions, particularly for speech-shaped noise. Duration and excitation Lombard-adapted changes are more \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:tS2w5q8j5-wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improve the accuracy of TDOA measurement using the Teager-Kaiser Energy operator",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4786987/",
            "Abstract": "In this paper we present a method to improve the accuracy of the time difference of arrivals (TDOA) estimators for reducing the uncertainty in localizing sperm whales using passive acoustics. More precisely we suggest the use of Teager-Kaiser energy operator for the enhancement of recordings made by hydrophones before applying an estimator of TDOA. Using data from the Atlantic undersea test and evaluation center (AUTEC) and standard estimators for TDOA like the generalized cross correlation (GCC) approaches or statistical methods based on LMS, we show that the suggested pre-processing method significantly improves the accuracy of these estimators. Simulating various signal to noise ratio (SNR) conditions, we are able to show the robustness of the pre-processing approach against additive noise.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:yD5IFk8b50cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Speech-nonspeech discrimination using the information bottleneck method and spectro-temporal modulation index.",
            "Publication year": 2007,
            "Publication url": "http://www.csd.uoc.gr/~mmarkaki/MMILab-MMarkaki_files/is2007_mmarkaki_stylianou.pdf",
            "Abstract": "In this work, we adopt an information theoretic approach-the Information Bottleneck method-to extract the relevant spectrotemporal modulations for the task of speech/non-speech discrimination-non-speech events include music, noise and animal vocalizations. A compact representation (a \u201ccluster prototype\u201d) is built for each class consisting of the maximally informative features with respect to the classification task. We assess the similarity of a sound to each representative cluster using the spectro-temporal modulation index (STMI) adapted to handle the contribution of different frequency bands. A simple threshold check is then used for discriminating speech from non-speech events. Conducted experiments have shown that the proposed method has low complexity and high accuracy of discrimination in low SNR conditions compared to recently proposed methods for the same task.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:a0OBvERweLwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the modeling of voiceless stop sounds of speech using adaptive quasi-harmonic models",
            "Publication year": 2012,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2012/i12_0859.pdf",
            "Abstract": "In this paper, the performance of the recently proposed adaptive signal models on modeling speech voiceless stop sounds is presented. Stop sounds are transient parts of speech that are highly non-stationary in time. State-of-the-art sinusoidal models fail to model them accurately and efficiently, thus introducing an artifact known as the pre-echo effect. The adaptive QHM and the extended adaptive QHM (eaQHM) are tested to confront this effect and it is shown that highly accurate, pre-echo-free representations of stop sounds are possible using adaptive schemes. Results on a large database of voiceless stops show that, on average, eaQHM improves by 100% the Signal to Reconstruction Error Ratio (SRER) obtained by the standard sinusoidal model.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ZHo1McVdvXMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "AM-FM estimation for speech based on a time-varying sinusoidal model",
            "Publication year": 2009,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2009/papers/i09_0104.pdf",
            "Abstract": "In this paper we present a method based on a time-varying sinusoidal model for a robust and accurate estimation of amplitude and frequency modulations (AM-FM) in speech. The suggested approach has two main steps. First, speech is modeled as a sinusoidal model with time-varying amplitudes. Specifically, the model makes use of a first order time polynomial with complex coefficients for capturing instantaneous amplitude and frequency (phase) components. Next, the model parameters are updated by using the previously estimated instantaneous phase information. Thus, an iterative scheme for AM-FM decomposition of speech is suggested which was validated on synthetic AM-FM signals and tested on reconstruction of voiced speech signals where the signal-to-error reconstruction ratio (SERR) was used as measure. Compared to the standard sinusoidal representation, the suggested approach found to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:bEWYMUwI8FkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Linking loudness increases in normal and lombard speech to decreasing vowel formant separation.",
            "Publication year": 2013,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.9563&rep=rep1&type=pdf",
            "Abstract": "The increased vocal effort associated with the Lombard reflex produces speech that is perceived as louder and judged to be more intelligible in noise than normal speech. Previous work illustrates that, on average, Lombard increases in loudness result from boosting spectral energy in a frequency band spanning the range of formants F1-F3, particularly for voiced speech. Observing additionally that increases in loudness across spoken sentences are spectro-temporally localized, the goal of this work is to further isolate these regions of maximal loudness by linking them to specific formant trends, explicitly considering here the vowel formant separation. For both normal and Lombard speech, this work illustrates that, as loudness increases in frequency bands containing formants (eg F1-F2 or F2-F3), the observed separation between formant frequencies decreases. From a production standpoint, these results seem to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:OU6Ihb5iCvQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cluster adaptive training of average voice models",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6853602/",
            "Abstract": "Hidden Markov model based text-to-speech systems may be adapted so that the synthesised speech sounds like a particular person. The average voice model (AVM) approach uses linear transforms to achieve this while multiple decision tree cluster adaptive training (CAT) represents different speakers as points in a low dimensional space. This paper describes a novel combination of CAT and AVM for modelling speakers. CAT yields higher quality synthetic speech than AVMs but AVMs model the target speaker better. The resulting combination may be interpreted as a more powerful version of the AVM. Results show that the combination achieves better target speaker similarity when compared with both AVM and CAT while the speech quality is in-between AVM and CAT.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:LjlpjdlvIbIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Musical genre classification using nonnegative matrix factorization-based features",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4432640/",
            "Abstract": "Nonnegative matrix factorization (NMF) is used to derive a novel description for the timbre of musical sounds. Using NMF, a spectrogram is factorized providing a characteristic spectral basis. Assuming a set of spectrograms given a musical genre, the space spanned by the vectors of the obtained spectral bases is modeled statistically using mixtures of Gaussians, resulting in a description of the spectral base for this musical genre. This description is shown to improve classification results by up to 23.3% compared to MFCC-based models, while the compression performed by the factorization decreases training time significantly. Using a distance-based stability measure this compression is shown to reduce the noise present in the data set resulting in more stable classification models. In addition, we compare the mean squared errors of the approximation to a spectrogram using independent component analysis and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:IjCSPb-OGe4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Progress in nonlinear speech processing",
            "Publication year": 2007,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=z7ZqCQAAQBAJ&oi=fnd&pg=PP2&dq=info:Wo-VCXZzzngJ:scholar.google.com&ots=NW0CGeWRb3&sig=H_yBfiSkN_IGOWYWGlVJjU7Z-jg",
            "Abstract": "This book constitutes of the major results of the EU COST (European Cooperation in the field of Scientific and Technical Research) Action 277: NSP, Nonlinear Speech Processing, running from April 2001 to June 2005. Coverage includes such areas as speech analysis for speech synthesis, speech recognition, speech-non speech discrimination and voice quality assessment, speech enhancement, and emotional state detection.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:zA6iFVUQeVQC",
            "Publisher": "Springer"
        },
        {
            "Title": "Speech intelligibility enhancement based on a non-causal Wavenet-like model",
            "Publication year": 2018,
            "Publication url": "https://pdfs.semanticscholar.org/0b7f/7d4ac60b3368920573d140fdd4f986c938e6.pdf",
            "Abstract": "Low speech intelligibility in noisy listening conditions makes more difficult our communication with others. Various strategies have been suggested to modify a speech signal before it is presented in a noisy listening environment with the goal to increase its intelligibility. A state-of-the art approach, referred to as Spectral Shaping and Dynamic Range Compression (SSDRC), relies on modifying spectral and temporal structure of the clean speech and has been shown to considerably improve the intelligibility of speech in noisy listening conditions. In this paper, we present a non-causal Wavenet-like model for mapping clean speech samples to samples generated by SSDRC. A successful non-linear mapping function has the potential to be used a) in improving the intelligibility of noisy speech and b) in the Wavenet-based speech synthesizers as a model based intelligibility improvement layer. Objective and subjective results show that the Wavenet-based mapping function is able to reproduce the intelligibility gains of SSDRC, while by far it improves the quality of the modified signal compared to the quality obtained by SSDRC.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:q3CdL3IzO_QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Applying the harmonic plus noise model in concatenative speech synthesis",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/890068/",
            "Abstract": "This paper describes the application of the harmonic plus noise model (HNM) for concatenative text-to-speech (TTS) synthesis. In the context of HNM, speech signals are represented as a time-varying harmonic component plus a modulated noise component. The decomposition of a speech signal into these two components allows for more natural-sounding modifications of the signal (e.g., by using different and better adapted schemes to modify each component). The parametric representation of speech using HNM provides a straightforward way of smoothing discontinuities of acoustic units around concatenation points. Formal listening tests have shown that HNM provides high-quality speech synthesis while outperforming other models for synthesis (e.g., TD-PSOLA) in intelligibility, naturalness, and pleasantness.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:d1gkVwhDpl0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Discontinuity detection in concatenated speech synthesis based on nonlinear speech analysis",
            "Publication year": 2005,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.1272&rep=rep1&type=pdf",
            "Abstract": "An objective distance measure which is able to predict audible discontinuity in concatenated speech synthesis systems is very important. Previous works were primarily based on features estimated by linear and/or stationary models of speech. In this paper, we introduce two nonlinear approaches for the detection of discontinuity. The first method is based on a nonlinear harmonic model of speech while the second method is based on the demodulation of speech in an amplitude and a frequency component using the Teager energy operator. Fisher\u2019s linear discriminant was used for the separation of signals with audible discontinuity from those perceived as continuous. When we combined the two methods using Fisher\u2019s linear discriminant a detection rate of 56.5% was achieved which is an 90% improvement over previously published results on the same database.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:dhFuZR0502QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On spectral and time domain energy reallocation for speech-in-noise intelligibility enhancement",
            "Publication year": 2014,
            "Publication url": "https://scholar.google.com/scholar?cluster=11915668733558949032&hl=en&oi=scholarr",
            "Abstract": "This paper addresses the problem of increasing speech-innoise intelligibility under the constraint of energy preservation. Two recently proposed algorithms which have been shown to be very successful in this problem according to two large formal listening tests are reviewed and a hybrid system which combines the properties of the two methods is suggested. The first technique, which is a frequency domain approach, is reimplemented providing clarifications on its energy reallocation strategy. Based on objective measures well correlated with human perception, we show that our implementation performs similarly to the original approach. Moreover, this is combined with a dynamic range compression algorithm from the second method to allow reallocation of energy over time as well. Experiments with speech shaped noise (SSN) and competing speaker (CS) noise maskers at various SNRs indicate that the hybrid \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:PR6Y55bgFSsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Combining speakers of multiple languages to improve quality of neural voices",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2108.07737",
            "Abstract": "In this work, we explore multiple architectures and training procedures for developing a multi-speaker and multi-lingual neural TTS system with the goals of a) improving the quality when the available data in the target language is limited and b) enabling cross-lingual synthesis. We report results from a large experiment using 30 speakers in 8 different languages across 15 different locales. The system is trained on the same amount of data per speaker. Compared to a single-speaker model, when the suggested system is fine tuned to a speaker, it produces significantly better quality in most of the cases while it only uses less than  of the speaker's data used to build the single-speaker model. In cross-lingual synthesis, on average, the generated quality is within  of native single-speaker models, in terms of Mean Opinion Score.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:mlAyqtXpCwEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised Normal-to-Lombard Spectral Envelope Transformation; Examining Loudness, Voicing & Stationarity",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=3229088646375613799&hl=en&oi=scholarr",
            "Abstract": "When speaking in noisy environments, humans modify their speech in order to make it more intelligible: this phenomenon is known as the Lombard effect [1],[2]. It has been shown that, among the various Lombard modifications, those to the spectral envelope account for the largest increases in speech intelligibility [3]. The present work examines and seeks to exploit the spectral envelope differences between Normal and Lombard speech for multiple (4 male, 4 female) speakers of the GRID corpus in an unsupervised context, ie, in the absence of segmentation or phonetic labeling. Our goals are twofold: 1) to transform the Normal speech spectral envelope towards that of the Lombard; 2) to isolate acoustic criteria that help to identify and better understand important spectral differences between Normal and Lombard speech. 1",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:SP6oXDckpogC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A}{P} ublic {A} udio {I} dentification {E} valuation {F} ramework for {B} roadcast {M} onitoring}",
            "Publication year": 2011,
            "Publication url": "https://scholar.google.com/scholar?cluster=12504862453110177075&hl=en&oi=scholarr",
            "Abstract": "@ article {Ramona2011, Abstract={This paper presents the first public framework for the evaluation of audio fingerprinting techniques. Although the domain of audio identification is very active, both in the industry and the academic world, there is nowadays no common basis to compare the proposed techniques. This is because corpuses and evaluation protocols differ between the authors. The framework we present here corresponds to a use-case in which audio excerpts have to be detected in a radio broadcast stream. This scenario indeed naturally provides a large variety of audio distortions that makes this task a real challenge for fingerprinting systems. Scoring metrics are discussed, with regard to this particular scenario. We then describe a whole evaluation framework including an audio corpus, along with the related groundtruth annotation, and a toolkit for the computation of the score metrics. An example of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:D_sINldO8mEC",
            "Publisher": "Springer Verlag"
        },
        {
            "Title": "Voice activity detection: Merging source and filter-based information",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7307972/",
            "Abstract": "Voice Activity Detection (VAD) refers to the problem of distinguishing speech segments from background noise. Numerous approaches have been proposed for this purpose. Some are based on features derived from the power spectral density, others exploit the periodicity of the signal. The goal of this letter is to investigate the joint use of source and filter-based features. Interestingly, a mutual information-based assessment shows superior discrimination power for the source-related features, especially the proposed ones. The features are further the input of an artificial neural network-based classifier trained on a multi-condition database. Two strategies are proposed to merge source and filter information: feature and decision fusion. Our experiments indicate an absolute reduction of 3% of the equal error rate when using decision fusion. The final proposed system is compared to four state-of-the-art methods on 150 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:tzM49s52ZIMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Towards a linear dynamical model based speech synthesizer",
            "Publication year": 2015,
            "Publication url": "https://www.researchgate.net/profile/Vassilios-Diakoloukas-2/publication/303881878_Towards_a_Linear_Dynamical_Model_based_Speech_Synthesizer/links/575a89e008aec91374a5ecfb/Towards-a-Linear-Dynamical-Model-based-Speech-Synthesizer.pdf",
            "Abstract": "We present recent developments towards building a speech synthesis system completely based on Linear Dynamical Models (LDMs). Specifically, we describe a decision tree-based context clustering approach to LDM-based speech synthesis and an algorithm for parameter generation using global variance with LDMs. In order to capture the speech dynamics, LDMs need coarser phoneme segmentation than the 5-state segmentation usually used in Hidden Markov Model (HMM)-based speech synthesis. Therefore, using LDMs to evaluate the clustering of longer phoneme segments improves the linguistic-to-acoustic mapping and leads to trajectories of synthetic speech parameters without discontinuities and closer to the natural ones. It also decreases the footprint of the system since the total number of decision tree leaves is smaller than the total number of leaves usually produced in a typical HMM-based \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:NJ774b8OgUMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Wrapped Gaussian mixture models for modeling and high-rate quantization of phase data of speech",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4806283/",
            "Abstract": "The harmonic representation of speech signals has found many applications in speech processing. This paper presents a novel statistical approach to model the behavior of harmonic phases. Phase information is decomposed into three parts: a minimum phase part, a translation term, and a residual term referred to as dispersion phase. Dispersion phases are modeled by wrapped Gaussian mixture models (WGMMs) using an expectation-maximization algorithm suitable for circular vector data. A multivariate WGMM-based phase quantizer is then proposed and constructed using novel scalar quantizers for circular random variables. The proposed phase modeling and quantization scheme is evaluated in the context of a narrowband harmonic representation of speech. Results indicate that it is possible to construct a variable-rate harmonic codec that is equivalent to iLBC at approximately 13 kbps.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:blknAaTinKkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Modulation spectral features for objective voice quality assessment",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5463313/",
            "Abstract": "In this paper, we employ normalized modulation spectral features for objective voice quality assessment regarding grade (hoarseness). Modulation spectra usually produce a high-dimensionality space. For classification purposes, the size of the original space is reduced using Higher Order Singular Value Decomposition (SVD). Further, we select most relevant features based on the mutual information between subjective voice quality (the degree of hoarseness) and the computed features, which leads to an adaptive to the classification task modulation spectral representation. The adaptive modulation spectral features are used as input to a Naive Bayes (NB) classifier. By combining two NB classifiers based on different feature sets a global classification rate of 73.93% for hoarseness was achieved.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:rO6llkc54NcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Maximum Voiced Frequency Estimation: Exploiting Amplitude and Phase Spectra",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2006.00521",
            "Abstract": "Maximum Voiced Frequency (MVF) is used in various speech models as the spectral boundary separating periodic and aperiodic components during the production of voiced sounds. Recent studies have shown that its proper estimation and modeling enhance the quality of statistical parametric speech synthesizers. Contrastingly, these same methods of MVF estimation have been reported to degrade the performance of singing voice synthesizers. This paper proposes a new approach for MVF estimation which exploits both amplitude and phase spectra. It is shown that phase conveys relevant information about the harmonicity of the voice signal, and that it can be jointly used with features derived from the amplitude spectrum. This information is further integrated into a maximum likelihood criterion which provides a decision about the MVF estimate. The proposed technique is compared to two state-of-the-art methods, and shows a superior performance in both objective and subjective evaluations. Perceptual tests indicate a drastic improvement in high-pitched voices.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:NXb4pA-qfm4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automatic classification of systolic heart murmurs",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6637861/",
            "Abstract": "This paper describes a system for discriminating innocent from pathologic systolic heart murmurs in children based on auscultation recordings. For sound signal analysis the use of reassigned spectrogram is suggested. Both dimensions and noise of the time-frequency representation were significantly reduced using higher order singular value decomposition. Optimal dimensions were selected through cross-validation experiments on a database of auscultation recordings with systolic murmurs from the University Hospital of Heraklion. The database only consisted with recordings of high misclassification rate by general practitioners. Using support vector machines for classification, the suggested approach achieved an Equal Error Rate of 6.71 \u00b1 1.18% and an Area Under the Curve score of 0.9758 \u00b1 0.0053 (95% confidence intervals). The performance of the suggested classification system is comparable to the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:BrmTIyaxlBUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A study of time-frequency features for CNN-based automatic heart sound classification for pathology detection",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0010482518301744",
            "Abstract": "This study concerns the task of automatic structural heart abnormality risk detection from digital phonocardiogram (PCG) signals aiming at pediatric heart disease screening applications. Recently, various systems based on convolutional neural networks trained on time-frequency representations of segmental PCG frames have been presented that outperform systems using hand-crafted features. This study focuses on the segmentation and time-frequency representation components of the CNN-based designs. We consider the most commonly used features (MFCC and Mel-Spectrogram) used in state-of-the-art systems and a time-frequency representation influenced by domain-knowledge, namely sub-band envelopes as an alternative feature. Via tests carried on two high quality databases with a large set of possible settings, we show that sub-band envelopes are preferable to the most commonly used features \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:SpbeaW3--B0C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "A phase based detector of whale clicks",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4786986/",
            "Abstract": "In this paper we present an alternative way to usual energy based approaches for detecting whale clicks. We suggest the use of the phase spectrum since the information about the location of clicks is very well represented in phase spectra. The method is referred to as the phase slope function. It is shown that the phase slope function is robust to additive noise while it offers simplicity in click detection since it is independent of the click source level. We further discuss its properties regarding the mono-pulse and multi-pulse character of clicks by introducing the notion of center of gravity for clicks. To evaluate the suggested phase based whale click detector we labeled clicks by hand in recordings of sperm and beaked whales provided by the Atlantic Undersea Test and Evaluation Center (AUTEC). Conducting detection tests demonstrate that 88% (on average) of the hand labeled mono-pulsed clicks were detected \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:uWQEDVKXjbEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Part D Text-to-Speech Synthesis",
            "Publication year": 2007,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=Slg10ekZBkAC&oi=fnd&pg=PA411&dq=info:0StS2O-lHxIJ:scholar.google.com&ots=wg5CH6WtKt&sig=DaKghZ1wOA827Q7Smmc0CNzH0VE",
            "Abstract": "Part D Text-to-Speech Synthesis Page 436 kV Text-** 5-' rt Part D Text-to-Speech Synthesis Ed. \nby S. Narayanan 19 Basic Principles of Speech Synthesis J. Schroeter, Florham Park, USA 20 \nRule-Based Speech Synthesis R. Carlson, Stockholm, Sweden B. Granstrom, Stockholm, \nSweden 21 Corpus-Based Speech Synthesis T. Dutoit, Mons, Belgium 22 Linguistic Processing \nfor Speech Synthesis R. Sproat, Urbana, USA 23 Prosodic Processing J. van Santen, Beaverton, \nUSA T. Mishra, Beaverton, USA E. Klabbers, Beaverton, USA 2k Voice Transformation Y. \nStylianou, Heraklion, Greece 25 Expressive/Affective Speech Synthesis N. Campbell, Keihanna \nScience City, Japan Page 437 412 S. Narayanan University of Southern California Street \nDepartment of Electrical Engineering Shrikanth Narayanan is Andrew J. Viterbi Professor \nof Engineering at the University of Southern California. Previously, he was with Los , CA\u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:TIZ-Mc8IlK0C",
            "Publisher": "Springer Science & Business Media"
        },
        {
            "Title": "Identifying tenseness of lombard speech using phase distortion",
            "Publication year": 2012,
            "Publication url": "https://www.researchgate.net/profile/Gilles-Degottex/publication/256303767_Identifying_Tenseness_of_Lombard_Speech_Using_Phase_Distortion/links/0c96053ae9381a8b43000000/Identifying-Tenseness-of-Lombard-Speech-Using-Phase-Distortion.pdf",
            "Abstract": "The \u201cLombard effect\u201d describes speakers\u2019 tendency to increase their vocal effort when communicating in noise [1]. Most often, the Lombard effect is examined in terms of acoustic parameters such as pitch, duration, and spectral amplitude (eg, tilt/slope, formants)[2, 3]. However, these parameters offer limited insight into voice quality, such as \u201ctenseness\u201d associated with increased vocal effort. Acoustically, one of the most significant indicators of tenseness relates to features of the glottal excitation signal: specifically, perceived tenseness of a voice is linked to a decrease in the glottal spectral tilt (ie, slope)[4]. Unlike typical analyses of the Lombard effect, the work in [5] explicitly examines glottal source parameters. Unfortunately, glottal source estimation is a challenging and delicate problem. Consequently, the present work seeks to offer an alternative analysis framework that can also isolate contributions of the excitation source (from the vocal tract), but without explicit glottal source modelling. In particular, phase distortion (defined below) is used to highlight differences in voice quality, focusing specifically on the relative tenseness of Lombard speech compared to Normal speech. The phase distortion is defined here as the group-delay with the influence of linear phase component removed. To calculate it, a harmonic model with pitch-synchronous analysis [6] is used. First, the minimum phase contribution of the vocal tract filter is removed from the instantaneous phase of the sinusoidal parameters. Then, using phase difference and antidifference operators, the linear phase component is also removed from the measurement. What remains after this \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:kRWSkSYxWN8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Emotional speech classification using adaptive sinusoidal modelling",
            "Publication year": 2014,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2014/i14_1361.pdf",
            "Abstract": "Automatic classification of emotional speech is a challenging task with applications in synthesis and recognition. In this paper, an adaptive sinusoidal model (aSM), called the extended adaptive Quasi-Harmonic Model-eaQHM, is applied on emotional speech analysis for classification purposes. The parameters of the model (amplitude and frequency) are used as features for the classification. Using a well known database of narrowband expressive speech (SUSAS), we develop two separate Vector Quantizers (VQ) for the classification, one for the amplitude and one for the frequency features. It is shown that the eaQHM can outperform the standard Sinusoidal Model in classification scores. However, single feature classification is inappropriate for higher-rate classification. Thus, we suggest a combined amplitude-frequency classification scheme, where the classification scores of each VQ are weighted and ranked \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:1yQoGdGgb4wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Adaptive gain control and time warp for enhanced speech intelligibility under reverberation",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7952244/",
            "Abstract": "Moderate and severe reverberation reduce speech intelligibility as a result of the overlap-masking effect, which constitutes the simultaneous observation of multiple delayed and attenuated copies of the speech signal. Recent progress has been made in ameliorating the degradation in intelligibility by adaptively controlling the signal gain as a function of both the signal statistics and the properties of the environment. While the intelligibility gain is at present small, it is significant and serves as a clear indication that reduced masking in non-stationary portions of the signal, under appropriate smoothness constraints, correlates well with an increase in intelligibility. A multi-modal modification framework is expected to improve performance further by i) introducing additional means to reduce masking in designated portions of the speech signal and ii) reducing signal distortion introduced by the use of a single modification \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:ILKRHgRFtOwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Detection of creak clicks of sperm whales in low SNR conditions",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1513203/",
            "Abstract": "A function of a nonlinear operator referred to as Teager-Kaiser Energy operator is described to detect creak clicks of sperm whales in adverse conditions. Clicks are considered as a sum of transient signals mixed with interference signals and background Gaussian noise of unknown variance. The method has been applied to synthetic as well as to real recordings of creak clicks. On synthetic data the detection rate gives excellent results. For low SNR a detection rate close to 100% is achieved while it works very well in detecting clicks in real creak sounds.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improving the modeling of the noise part in the harmonic plus noise model of speech",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4518683/",
            "Abstract": "Harmonic + noise model (HNM) is a hybrid model of speech with a harmonic component and a noise component. While the harmonic part describes efficiently the periodicities in speech signals (voiced parts), modeling of the noise part introduces artifacts primarily because of the specific time-domain characteristics of noise in voiced speech. In this paper, we concentrated on the modeling of noise in voiced frames. To model the temporal characteristics of noise, we study three time envelopes in the context of HNM; triangular envelope, Hilbert envelope and energy envelope. Listening tests showed a clear preference for the Energy envelope and Hilbert envelope for male voices and to a lesser extent the same conclusions can be drawn for female voices.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:Zph67rFs4hoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A mathematical model for accurate measurement of jitter.",
            "Publication year": 2007,
            "Publication url": "https://www.torrossa.com/gs/resourceProxy?an=2251331&publisher=FF3888#page=19",
            "Abstract": "Jitter is a fundamental metric of voice quality. The majority of jitter estimators produce an average value over a duration of several pitch periods. This paper proposes a method for short-time jitter measurement, based on a mathematical model which describes the coupling of two periodic phenomena. The movement of one of the two periodic phenomena with respect to the other is what is considered as jitter and what the proposed method measures. Through tests with synthetic jitter signals it has been verified that the suggested method provides accurate local estimates of jitter. Further evaluation was conducted on actual normal and pathological voice signals from the Massachusetts Eye and Ear Infirmary (MEEI) Disordered Voice Database. Compared with corresponding parameters from the Multi-Dimension Voice Program (MDVP) and the Praat system, the proposed method outperformed both in normal vs \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:GnPB-g6toBAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Full-band quasi-harmonic analysis and synthesis of musical instrument sounds with adaptive sinusoids",
            "Publication year": 2016,
            "Publication url": "https://www.mdpi.com/138594",
            "Abstract": "Sinusoids are widely used to represent the oscillatory modes of musical instrument sounds in both analysis and synthesis. However, musical instrument sounds feature transients and instrumental noise that are poorly modeled with quasi-stationary sinusoids, requiring spectral decomposition and further dedicated modeling. In this work, we propose a full-band representation that fits sinusoids across the entire spectrum. We use the extended adaptive Quasi-Harmonic Model (eaQHM) to iteratively estimate amplitude-and frequency-modulated (AM\u2013FM) sinusoids able to capture challenging features such as sharp attacks, transients, and instrumental noise. We use the signal-to-reconstruction-error ratio (SRER) as the objective measure for the analysis and synthesis of 89 musical instrument sounds from different instrumental families. We compare against quasi-stationary sinusoids and exponentially damped sinusoids. First, we show that the SRER increases with adaptation in eaQHM. Then, we show that full-band modeling with eaQHM captures partials at the higher frequency end of the spectrum that are neglected by spectral decomposition. Finally, we demonstrate that a frame size equal to three periods of the fundamental frequency results in the highest SRER with AM\u2013FM sinusoids from eaQHM. A listening test confirmed that the musical instrument sounds resynthesized from full-band analysis with eaQHM are virtually perceptually indistinguishable from the original recordings. View Full-Text",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:-_dYPAW6P2MC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Multi-stream spectral representation for statistical parametric speech synthesis",
            "Publication year": 2019,
            "Publication url": "https://patents.google.com/patent/US10446133B2/en",
            "Abstract": "There is provided a speech synthesizer comprising a processor configured to receive one or more linguistic units, convert said one or more linguistic units into a sequence of speech vectors for synthesizing speech, and output the sequence of speech vectors. Said conversion comprises modelling higher and lower spectral frequencies of the speech data as separate high and low spectral streams by applying a first set of one or more statistical models to the higher spectral frequencies and a second set of one or more statistical models to the lower spectral frequencies.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:lmc2jWPfTJgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On Finding the Relevant User Reviews for Advancing Conversational Faceted Search.",
            "Publication year": 2018,
            "Publication url": "https://iris.unica.it/bitstream/11584/263585/3/paper1.pdf#page=29",
            "Abstract": "Faceted Search (FS) is a widely used exploratory search paradigm which is commonly applied over multidimensional or graph data. However sometimes the structured data are not sufficient for answering a user\u2019s query. User comments (or reviews) is a valuable source of information that could be exploited in such cases for aiding the user to explore the information space and to decide what options suits him/her better (either through question answering or query-oriented sentiment analysis). To this end in this paper we introduce and comparatively evaluate methods for locating the more relevant user comments that are related with the user\u2019s focus in the context of a conversational faceted search system. Specifically we introduce a dictionary-based method, a word embedding-based method, and one combination of them. The analysis and the experimental results showed that the combined method outperforms the other methods, without significantly affecting the overall response time.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:S16KYo8Pm5AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Speech-in-noise intelligibility improvement based on spectral shaping and dynamic range compression",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=3455381884290858685&hl=en&oi=scholarr",
            "Abstract": "In this paper, we suggest a non-parametric way to improve the intelligibility of speech in noise. The signal is enhanced before presented in a noisy environment, under the constraint of equal global signal power before and after modifications. Two systems are combined in a cascade form to enhance the quality of the signal first in frequency (spectral shaping) and then in time (dynamic range compression). Experiments with speech shaped (SSN) and competing speaker (CS) types of noise at various low SNR values, show that the suggested approach outperforms state-of-the art methods in terms of the Speech Intelligibility Index (SII). In terms of SNR gain there is an improvement of 7 dB (SSN) and 8 dB (CS) over these methods. A formal listening test confirm the efficiency of the suggested system in enhancing speech intelligibility in noise.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:4JMBOYKVnBMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Evaluation of modulation frequency features for speaker verification and identification",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7077731/",
            "Abstract": "In this paper, we suggest the use of mutual information to explore the information provided by the modulation spectrum for speaker verification and identification purposes. The initial representation is first transformed to a lower-dimensional domain using Higher Order SVD and then, the mutual information between speaker identity and features in the transformed domain is computed. Projection of the relevant features back to the original dimensions reveals the modulation spectral components which discriminate speakers. Simulations carried out on YOHO database show that the relevance of modulation spectral features is speaker-dependent.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:M05iB0D1s5AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Adaptive AM\u2013FM signal decomposition with application to speech analysis",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5445040/",
            "Abstract": "In this paper, we present an iterative method for the accurate estimation of amplitude and frequency modulations (AM-FM) in time-varying multi-component quasi-periodic signals such as voiced speech. Based on a deterministic plus noise representation of speech initially suggested by Laroche (\u201cHNM: A simple, efficient harmonic plus noise model for speech,\u201d Proc. WASPAA, Oct., 1993, pp. 169-172), and focusing on the deterministic representation, we reveal the properties of the model showing that such a representation is equivalent to a time-varying quasi-harmonic representation of voiced speech. Next, we show how this representation can be used for the estimation of amplitude and frequency modulations and provide the conditions under which such an estimation is valid. Finally, we suggest an adaptive algorithm for nonparametric estimation of AM-FM components in voiced speech. Based on the estimated \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:3fE2CSJIrl8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sinusoidal speech synthesis using deep neural networks",
            "Publication year": 2015,
            "Publication url": "https://scholar.google.com/scholar?cluster=17196067551252809452&hl=en&oi=scholarr",
            "Abstract": "Recent work has shown deep neural networks (DNN) can improve text-to-speech synthesis performance compared to competing state-of-art statistical parametric methods. This work has so far made use of source-filter vocoders exclusively. However, sinusoidal vocoders offer an alternative kind of speech waveform model, and have also been shown to generate high quality speech. In this paper, we discuss in detail two methods to integrate a sinusoidal model into a DNN or HMM-based synthesis system. In the first method, we use mel-cepstra as an intermediate representation of the spectral envelope for each frame during statistical modelling. In the second method, we use a fixed number of sinusoid parameters and model them directly. We find the DNNs always outperform their HMM-based equivalent, confirming other recently reported observations. Meanwhile, we also compare both of our sinusoidal methods with the state-ofart STRAIGHT vocoder. In our experiments, we find that our sinusoidal-DNN-based system which uses an intermediate spectral parameterisation gives marginally the best performance in objective tests. Furthermore, subjective listener ratings also show it outperforms the state-of-the-art STRAIGHT-based equivalent. We conclude from our results that sinusoidal models are indeed highly suited for statistical parametric synthesis, and preferably when used in conjunction with DNNs.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:_Re3VWB3Y0AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using modulation spectra for voice pathology detection and classification",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5334850/",
            "Abstract": "In this paper, we consider the use of Modulation Spectra for voice pathology detection and classification. To reduce the high-dimensionality space generated by Modulation spectra we suggest the use of Higher Order Singular Value Decomposition (SVD) and we propose a feature selection algorithm based on the Mutual Information between subjective voice quality and computed features. Using SVM with a radial basis function (RBF) kernel as classifier, we conducted experiments on a database of sustained vowel recordings from healthy and pathological voices. For voice pathology detection, the suggested approach achieved a detection rate of 94.1% and an Area Under the Curve (AUC) score of 97.8%. For voice pathology classification, an average detection rate and AUC of 88.6% and 94.8%, respectively, was achieved in classifying polyp against keratosis leukoplakia, adductor spasmodic dysphonia and vocal \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:4TOpqqG69KYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Pitch modifications of speech based on an adaptive harmonic model",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6855143/",
            "Abstract": "In this paper, a simple method for pitch-scale modifications of speech based on a recently suggested model for AM-FM decomposition of speech signals, is presented. This model is referred to as the adaptive Harmonic Model (aHM). The aHM models speech as a sum of harmonically related sinusoids that can adapt to the local characteristics of the signal. It was shown that this model provides high quality reconstruction of speech and thus, it can also provide high quality pitch-scale modifications. For the latter, the amplitude envelope is estimated using the Discrete All-Pole (DAP) method, and the phase envelope estimation is performed by utilizing the concept of relative phase. Formal listening tests on a database of several languages show that the synthetic pitch-scaled waveforms are natural and free of some common artefacts encountered in other state-of-the-art models, such as HNM and STRAIGHT.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:9vf0nzSNQJEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Voice pathology detection and discrimination based on modulation spectral features",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5680950/",
            "Abstract": "In this paper, we explore the information provided by a joint acoustic and modulation frequency representation, referred to as modulation spectrum, for detection and discrimination of voice disorders. The initial representation is first transformed to a lower dimensional domain using higher order singular value decomposition (HOSVD). From this dimension-reduced representation a feature selection process is suggested using an information-theoretic criterion based on the mutual information between voice classes (i.e., normophonic/dysphonic) and features. To evaluate the suggested approach and representation, we conducted cross-validation experiments on a database of sustained vowel recordings from healthy and pathological voices, using support vector machines (SVMs) for classification. For voice pathology detection, the suggested approach achieved a classification accuracy of 94.1\u00b10.28% (95 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:2P1L_qKh6hAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fusion of multiple parameterisations for DNN-based sinusoidal speech synthesis with multi-task learning",
            "Publication year": 2015,
            "Publication url": "https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_0854.pdf",
            "Abstract": "It has recently been shown that deep neural networks (DNN) can improve the quality of statistical parametric speech synthesis (SPSS) when using a source-filter vocoder. Our own previous work has furthermore shown that a dynamic sinusoidal model (DSM) is also highly suited to DNN-based SPSS, whereby sinusoids may either be used themselves as a \u201cdirect parameterisation\u201d(DIR), or they may be encoded using an \u201cintermediate spectral parameterisation\u201d(INT). The approach in that work was effectively to replace a decision tree with a neural network. However, waveform parameterisation and synthesis steps that have been developed to suit HMMs may not fully exploit DNN capabilities. Here, in contrast, we investigate ways to combine INT and DIR at the levels of both DNN modelling and waveform generation. For DNN training, we propose to use multi-task learning to model cepstra (from INT) and log \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:W5xh706n7nkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Perceptual and objective detection of discontinuities in concatenative speech synthesis",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/941045/",
            "Abstract": "Concatenative speech synthesis systems attempt to minimize audible signal discontinuities between two successive concatenated units. An objective distance measure which is able to predict audible discontinuities is therefore very important, particularly in unit selection synthesis, for which units are selected from among a large inventory at run time. In this paper, we describe a perceptual test to measure the detection rate of concatenation discontinuity by humans, and then we evaluate 13 different objective distance measures based on their ability to predict the human results. Criteria used to classify these distances include the detection rate, the Bhattacharyya measure of separability of two distributions, and receiver operating characteristic (ROC) curves. Results show that the Kullback-Leibler distance on power spectra has the higher detection rate followed by the Euclidean distance on Mel-frequency cepstral \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:UeHWp8X0CEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improving intelligibility in noise of HMM-generated speech via noise-dependent and-independent methods",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6639193/",
            "Abstract": "In order to improve the intelligibility of HMM-generated Text-to-Speech (TTS) in noise, this work evaluates several speech enhancement methods, exploring combinations of noise-independent and - dependent approaches as well as algorithms previously developed for natural speech. We evaluate one noise-dependent method proposed for TTS, based on the glimpse proportion measure, and three approaches originally proposed for natural speech - one that estimates the noise and is based on the speech intelligibility index, and two noise-independent methods based on different spectral shaping techniques followed by dynamic range compression. We demonstrate how these methods influence the average spectra for different phone classes. We then present results of a listening experiment with speech-shaped noise and a competing speaker. A few methods made the TTS voice even more intelligible than the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:g5m5HwL7SMYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Speaker conditional wavernn: Towards universal neural vocoder for unseen speaker and recording conditions",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2008.05289",
            "Abstract": "Recent advancements in deep learning led to human-level performance in single-speaker speech synthesis. However, there are still limitations in terms of speech quality when generalizing those systems into multiple-speaker models especially for unseen speakers and unseen recording qualities. For instance, conventional neural vocoders are adjusted to the training speaker and have poor generalization capabilities to unseen speakers. In this work, we propose a variant of WaveRNN, referred to as speaker conditional WaveRNN (SC-WaveRNN). We target towards the development of an efficient universal vocoder even for unseen speakers and recording conditions. In contrast to standard WaveRNN, SC-WaveRNN exploits additional information given in the form of speaker embeddings. Using publicly-available data for training, SC-WaveRNN achieves significantly better performance over baseline WaveRNN on both subjective and objective metrics. In MOS, SC-WaveRNN achieves an improvement of about 23% for seen speaker and seen recording condition and up to 95% for unseen speaker and unseen condition. Finally, we extend our work by implementing a multi-speaker text-to-speech (TTS) synthesis similar to zero-shot speaker adaptation. In terms of performance, our system has been preferred over the baseline TTS system by 60% over 15.5% and by 60.9% over 32.6%, for seen and unseen speakers, respectively.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:a9-T7VOCCH8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the use of WaveNet as a statistical vocoder",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8462393/",
            "Abstract": "In this paper, we explore the possibility of using the WaveNet architecture as a statistical vocoder. In that case, the generation of speech waveforms is locally conditioned only by acoustic features. Focusing on the single speaker case at the moment, we investigate the impact of the local conditions as well as that of the amount of data available for training. Furthermore, variations of the WaveNet architecture are considered and discussed in the context of our work. We compare our work against a very recent work which also used WaveNet architecture as a speech vocoder using the same speech data. More specifically, we used two female and two male speakers from the CMU-ARCTIC database to contrast the use of cepstrum coefficients and filter-bank features as local conditioners with the goal to improve the overall quality for both male and female speakers. In the paper we also discuss the impact of the size of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:i2xiXl-TujoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Introduction to the special section on voice transformation",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5485204/",
            "Abstract": "VOICE Transformation aims at the control of non-lin-guistic information of speech signals such as voice quality and voice individuality. It refers to the various modifications one may apply to the sound produced by a person, speaking or singing. Voice Transformation covers a wide area of research from speech production modeling and understanding to perception of speech, from natural language processing, modeling, and control of speaking style, to pattern recognition and statistical signal processing. While there are common research interests with speaker-dependent technologies (eg, speaker recognition/verification), Voice Transformation goes beyond these technologies; not only the cues that are relevant to voice individuality should be detected but the corresponding features need to be modified in a way that the transformed speech signal sounds natural. Speech models suggested for Voice Transformation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:xtRiw3GOFMkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "P8-Active Speech Modifications",
            "Publication year": 2012,
            "Publication url": "https://www.academia.edu/download/38041611/FINAL_report_14-11-12.pdf",
            "Abstract": "In many intelligibility studies, it was demonstrated that the speaking style referred to as clear speech is significantly more intelligible than conversational (or casual) speech. This intelligibility gain exists for both normal-hearing and hearing-impaired listeners (eg elderly persons and linguistically inexperienced listeners like non-native (L2) speakers and children). Also, in a two-way conversation in which one person is affected by an adverse listening condition and one is not (eg between one person speaking to another via telephone where the other is in a noisy club, or in a cafeteria, in the street etc.), the person who is not affected still manages to make adaptations (on acoustic-phonetic and linguistic levels) that are quite specifically tailored to counteract the specific communication barrier that the other person is experiencing. These adaptations show that clear speech is not defined in a uniform way, but that there are different styles of clear speech depending on the adverse condition that the speech is heard in. In this context, Active Speech Modifications refer to the speaking-style adaptations or strategies a speaker applies in order to maximize communication effectiveness. Identification and effective manipulations of the most prominent acoustic-phonetic characteristics of different styles of clear speech can allow for the development of new, signal based, active speech modification algorithms to increase intelligibility. The algorithms can consequently improve speech intelligibility in many situations, such as in the design of hearing aids, telephony, and other speech signal processing technologies and applications (ie, speech synthesis, recognition \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:dQ2og3OwTAUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Theoretical models I",
            "Publication year": 2007,
            "Publication url": "https://fupress.com/Archivio/pdf/2591.pdf",
            "Abstract": "CONTENTS Page 1 Models and analysis of vocal emissions for biomedical applications: \n5th international workshop: December 13-15, 2007: Firenze, Italy, ed. by C. Manfredi, ISBN \n978 88-8453-673-3 (print) ISBN 978-88-8453-674-7 (online) \u00a9 Firenze university press, \n2007. Contents VII CONTENTS Foreword.....................................................................................................................................XI \nTheoretical models I S. Ben Elhadj Fraj, F. Grenez, J. Schoentgen, Towards the simulation \nof pathological voice qualities ........................................................................................................................................... \n3 M. Vasilakis, Y. Stylianou, A mathematical model for accurate measurement of jitter ........................... \n7 W. Wokurek, Towards a temporal high-resolution formant analysis ......................................................11 \nPathology detection/classification I PJ Murphy, Physical and perceptual correlates of voice \nusing acoustic analysis............................... 17 , , .\u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:epqYDVWIO7EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Special Session: Phase Importance in Speech Processing Applications",
            "Publication year": 2014,
            "Publication url": "https://www.spsc.tugraz.at/sites/default/files/Intro.pdf",
            "Abstract": "\u00bdD. Wang, J. Lim,\u201d The unimportance of phase in speech enhancement,\u201d TASL 30 (4), pp. 679-681, 1982. \u00beK. K. Paliwal et al.,\u201d The importance of phase in speech enhancement\u201d Speech Communication, 53 (4), pp. 465-494, 2011.\u00bf P. Mowlaee, R. Saiedi, R. Martin,\u201d Phase estimation for signal reconstruction in single-channel speech separation\u201d INTERSPEECH 2012.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:4fGpz3EwCPoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automatic acoustic detection of birds through deep learning: the first Bird Audio Detection challenge",
            "Publication year": 2019,
            "Publication url": "https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13103",
            "Abstract": "  Assessing the presence and abundance of birds is important for monitoring specific species as well as overall ecosystem health. Many birds are most readily detected by their sounds, and thus, passive acoustic monitoring is highly appropriate. Yet acoustic monitoring is often held back by practical limitations such as the need for manual configuration, reliance on example sound libraries, low accuracy, low robustness, and limited ability to generalise to novel acoustic conditions. Here, we report outcomes from a collaborative data challenge. We present new acoustic monitoring datasets, summarise the machine learning techniques proposed by challenge teams, conduct detailed performance evaluation, and discuss how such approaches to detection can be integrated into remote monitoring projects. Multiple methods were able to attain performance of around 88% area under the receiver operating characteristic \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:Dip1O2bNi0gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Connections between Reassigned Spectrum and Least Squares Estimation for Sinusoidal Models",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8903116/",
            "Abstract": "The parameter estimation of sinusoidal signals, especially the frequency estimation is for decades a very challenging problem. Among the various frequency estimation methods, this paper compares and connects the reassigned spectrum and an iterative, nonlinear Least Squares method referred to as iQHM (iterative Quasi Harmonic Model). Interestingly, there are subtle connections between these two -seemingly different- iterative methods both in frequency as well in time domain. Moreover, inspired by the optimal performance of reassigned spectrum for mono-component sinusoidal signals, a variant of iQHM is proposed. The new method improves the performance of the original iQHM approach in frequency estimation by increasing the region of convergence by 40% on average.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:GtLg2Ama23sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the estimation of the speech harmonic model",
            "Publication year": 2008,
            "Publication url": "http://www.csd.uoc.gr/~hannover/LabsiteDraft/MMILab_files/HarmonicEstimation_itrw08.pdf",
            "Abstract": "In this paper we present and compare four time-domain approaches for estimating the parameters of a harmonic speech model. The classic approach of Least Squares is directly compared with a Total Least Squares approach trying to overcome errors in the estimation of the fundamental frequency of the model. Both of these approaches are suboptimal since they split the estimation problem into two subproblems; to the estimation of amplitudes and phases and to the estimation of fundamental frequency. To improve the accuracy of the parameters estimation of the harmonic model two iterative non linear approaches are then presented, based on the Steepest Descent and Newton-Gauss optimization algorithms, where all parameters of the harmonic model are estimated simultaneously. The approach based on the Newton-Gauss optimization algorithm provided the best accuracy as this is measured by the Signal-to-Noise Ratio criterion.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Beat tracking using group delay based onset detection",
            "Publication year": 2008,
            "Publication url": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:1040355",
            "Abstract": "This paper introduces a novel approach to estimate onsets in musical signals based on the phase spectrum and specifically using the average of the group delay function. A frame-by-frame analysis of a music signal provides the evolution of group delay over time, referred to as phase slope function. Onsets are then detected simply by locating the positive zero-crossings of the phase slope function. The proposed approach is compared to an amplitude-based onset detection approach in the framework of a state-of-the-art system for beat tracking. On a data set of music with less percussive content, the beat tracking accuracy achieved by the system is improved by 82% when the suggested phase-based onset detection approach is used instead of the amplitudebased approach, while on a set of music with stronger percussive characteristics both onset detection approaches provide comparable results of accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:hqOjcs7Dif8C",
            "Publisher": "ISMIR"
        },
        {
            "Title": "A maximum likelihood approach to the detection of moments of maximum excitation and its application to high-quality speech parameterization",
            "Publication year": 2015,
            "Publication url": "https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_0603.pdf",
            "Abstract": "This paper presents an algorithm to detect moments of maximum excitation (MME) in speech. It assumes a model in which speech can be represented as a sequence of pulses located at the MME convolved with a time-varying minimum-phase impulse response. By considering that in the glottal cycle speech concentrates more energy at the MME than at other instants, the locations and amplitudes of the excitation pulses are determined through maximum likelihood estimation. The suggested approach provides a fully automatic and consistent method for the detection of MME in speech without relying on ad hoc procedures which usually do not work well across different speech styles without a required amount of adjustments. Experiments with speech parameterization, in the context of complex cepstrum analysis and synthesis, have shown that the proposed MME-based processing can improve signal to error \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:tKAzc9rXhukC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Iterative estimation of phase using complex cepstrum representation",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7472627/",
            "Abstract": "This paper presents a method to iteratively estimate phase information from speech in the cepstrum domain. It assumes that correct markings of pitch periods, which may not correspond to glottal closure instants (GCI), are available and can be used to extract the smooth spectral envelope of speech. By using this information, the minimum-phase cepstrum is derived and used as prior information in a modified version of a previously proposed scheme of complex cepstrum analysis based on the mean squared error. Experiments with an emotional database show that the proposed method achieves better performance in terms of continuous phase spectrum estimation, when compared with approaches that rely on accurate GCI markings and high-resolution phase unwrapping mechanisms. In addition, similar results to the full optimization of the complex cepstrum vector are reached, at a lower computational complexity.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:evX43VCCuoAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Pitched instrument onset detection based on auditory spectra",
            "Publication year": 2009,
            "Publication url": "https://openaccess.city.ac.uk/id/eprint/2095/1/",
            "Abstract": "In this paper, a novel method for onset detection of music signals using auditory spectra is proposed. The auditory spectrogram provides a time-frequency representation that employs a sound processing model resembling the human auditory system. Recent work on onset detection employs DFT-based features, such as the spectral flux and group delay function. The spectral flux and group delay are introduced in the auditory framework and an onset detection algorithm is proposed. Experiments are conducted on a dataset covering 11 pitched instrument types, consisting of 1829 onsets in total. Results indicate the superiority of the auditory representations over the DFT-based ones, with the auditory spectral flux exhibiting an onset detection improvement by 2% in terms of F-measure when compared to the DFT-based feature.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:EUQCXRtRnyEC",
            "Publisher": "International Society for Music Information Retrieval"
        },
        {
            "Title": "Stochastic modeling and quantization of harmonic phases in speech using wrapped gaussian mixture models",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4218302/",
            "Abstract": "Harmonic sinusoidal representations of speech have proven to be useful in many speech processing tasks. This work focuses on the phase spectra of the harmonics and provides a methodology to analyze and subsequently to model the statistics of the harmonic phases. To do so, we propose the use of a wrapped Gaussian mixture model (WGMM), a model suitable for random variables that belong to circular spaces, and provide an expectation-maximization algorithm for training. The WGMM is then used to construct a phase quantizer. The quantizer is employed in a prototype variable rate narrow-band VoIP sinusoidal codec that is equivalent to iLBC in terms of PESQ-MOS, at ~13 kbps.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:NMxIlDl6LWMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Spectral jitter modeling and estimation",
            "Publication year": 2009,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S174680940900010X",
            "Abstract": "This paper suggests a new method for short-time jitter estimation based on a mathematical model that describes the coupling of two periodical phenomena. Specifically, jitter is modeled as the movement of one of the two periodic phenomena with respect to the other. The proposed method measures this movement indirectly by taking into account the spectral properties of the suggested model. Experiments with synthetic jitter signals showed that the suggested method produces accurate local estimates of jitter. Further evaluation was conducted on actual normal and pathological voice signals using two databases and jitter estimations from the Multi-Dimension Voice Program (MDVP) and the Praat system. Compared with the corresponding parameters from these two systems, the proposed method outperformed both in normal vs. pathological voice discrimination accuracy by at least 4%. Furthermore, it was shown \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:j3f4tGmQtD8C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Fusion strategies for speech and handwriting modalities in HCI",
            "Publication year": 2005,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/5684/0000/Fusion-strategies-for-speech-and-handwriting-modalities-in-HCI/10.1117/12.585869.short",
            "Abstract": "In this paper we present a strategy for handling of multimodal signals from pen-based mobile devices for Human to Computer Interaction (HCI), where our focus is on the modalities of spoken and handwritten inputs. Each modality for itself is quite well understood, as the exhaustive literature demonstrates, although still a number of challenges exist, like recognition result improvements. Among the potentials in multimodal HCI are improvements in recognition and robustness as well as seamless men-machine communication based on fusion of different modalities by exploiting redundancies among these modalities. However, such valuable fusion of both modalities still poses some problems. Open problems today include design approaches for fusion strategies and with the increasing number of mobile and pen-based computers, particularly techniques for fusion of handwriting and speech appear to have a great \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:HDshCWvjkbEC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "A fixed dimension and perceptually based dynamic sinusoidal model of speech",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6854810/",
            "Abstract": "This paper presents a fixed- and low-dimensional, perceptually based dynamic sinusoidal model of speech referred to as PDM (Perceptual Dynamic Model). To decrease and fix the number of sinusoidal components typically used in the standard sinusoidal model, we propose to use only one dynamic sinusoidal component per critical band. For each band, the sinusoid with the maximum spectral amplitude is selected and associated with the centre frequency of that critical band. The model is expanded at low frequencies by incorporating sinusoids at the boundaries of the corresponding bands while at the higher frequencies a modulated noise component is used. A listening test is conducted to compare speech reconstructed with PDM and state-of-the-art models of speech, where all models are constrained to use an equal number of parameters. The results show that PDM is clearly preferred in terms of quality over \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:tkaPQYYpVKoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automatic glottal segmentation using local-based active contours and application to glottovibrography",
            "Publication year": 2012,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167639311001282",
            "Abstract": "The use of high-speed videoendoscopy (HSV) for the assessment of vocal-fold vibrations dictates the development of efficient techniques for glottal image segmentation. We present a new glottal segmentation method using a local-based active contour framework. The use of local-based features and the exploitation of the vibratory pattern allows for dealing effectively with image noise and cases where the glottal area consists of multiple regions. A scheme for precise glottis localization is introduced, which facilitates the segmentation procedure. The method has been tested on a database of 60 HSV recordings. Comparisons with manual verification resulted in less than 1% difference on the average glottal area. These errors mainly come from detection failure in the posterior or anterior parts of the glottal area. Comparisons with automatic threshold-based glottal detection point out the necessity of complete \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:NaGl4SEjCO4C",
            "Publisher": "North-Holland"
        },
        {
            "Title": "On combining information from modulation spectra and mel-frequency cepstral coefficients for automatic detection of pathological voices",
            "Publication year": 2011,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.3109/14015439.2010.528788",
            "Abstract": "This work presents a novel approach for the automatic detection of pathological voices based on fusing the information extracted by means of mel-frequency cepstral coefficients (MFCC) and features derived from the modulation spectra (MS). The system proposed uses a two-stepped classification scheme. First, the MFCC and MS features were used to feed two different and independent classifiers; and then the outputs of each classifier were used in a second classification stage. In order to establish the best configuration which provides the highest accuracy in the detection, the fusion of information was carried out employing different classifier combination strategies. The experiments were carried out using two different databases: the one developed by The Massachusetts Eye and Ear Infirmary Voice Laboratory, and a database recorded by the Universidad Polit\u00e9cnica de Madrid. The results show that the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:TQgYirikUcIC",
            "Publisher": "Taylor & Francis"
        },
        {
            "Title": "On the Quality and Intelligibility of Noisy Speech Processed for Near-End Listening Enhancement.",
            "Publication year": 2017,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2017/pdfs/1225.PDF",
            "Abstract": "Most current techniques for near-end speech intelligibility enhancement have focused on processing clean input signals, however, in realistic environments, the input is often noisy. Processing noisy speech for intelligibility enhancement using algorithms developed for clean signals can lower the perceptual quality of the samples when they are listened in quiet. Here we address the quality loss in these conditions by combining noise reduction with a multi-band version of a state-of-the-art intelligibility enhancer for clean speech that is based on spectral shaping and dynamic range compression (SSDRC). Subjective quality and intelligibility assessments with noisy input speech showed that:(a) In quiet near-end conditions, the proposed system outperformed the baseline SSDRC in terms of Mean Opinion Score (MOS);(b) In speech-shaped near-end noise, the proposed system improved the intelligibility of unprocessed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:KUbvn5osdkgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Removing linear phase mismatches in concatenative speech synthesis",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/905997/",
            "Abstract": "Many current text-to-speech (TTS) systems are based on the concatenation of acoustic units of recorded speech. While this approach is believed to lead to higher intelligibility and naturalness than synthesis-by-rule, it has to cope with the issues of concatenating acoustic units that have been recorded at different times and in a different order. One important issue related to the concatenation of these acoustic units is their synchronization. In terms of signal processing this means removing linear phase mismatches between concatenated speech frames. This paper presents two novel approaches to the problem of synchronization of speech frames with an application to concatenative speech synthesis. Both methods are based on the processing of phase spectra without, however, decreasing the quality of the output speech, in contrast to previously proposed methods. The first method is based on the notion of center of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:0EnyYjriUFMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Iterative estimation of sinusoidal signal parameters",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5411753/",
            "Abstract": "While the problem of estimating the amplitudes of sinusoidal components in signals, given an estimation of their frequencies, is linear and tractable, it is biased due to the unavoidable, in practice, errors in the estimation of frequencies. These errors are of great concern for processing signals with many sinusoidal like components as is the case of speech and audio. In this letter, we suggest using a time-varying sinusoidal representation which is able to iteratively correct frequency estimation errors. Then the corresponding amplitudes are computed through Least Squares. Experiments conducted on synthetic and speech signals show the suggested model's effectiveness in correcting frequency estimation errors and robustness in additive noise conditions.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Adaptation of an expressive single speaker deep neural network speech synthesis system",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8461888/",
            "Abstract": "One of the advantages of statistical parametric speech synthesis is the ability to alter some of the characteristics of the speech e.g. change the speaker, expression etc. In this paper we present a technique to adapt an expressive single speaker deep neural network (DNN) speech synthesis model to a new speaker, allowing for both neutral and expressive speech in the new speaker's voice. Experiments show that the proposed adaptation technique achieves higher MOS scores on both neutral and expressive speech, and higher speaker similarity and slightly lower expression similarity scores on the expressive speech when compared with another DNN speaker adaptation technique.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:-FonjvnnhkoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Learning domain-independent dialogue policies via ontology parameterisation",
            "Publication year": 2015,
            "Publication url": "https://www.aclweb.org/anthology/W15-4654.pdf",
            "Abstract": "This paper introduces a novel approach to eliminate the domain dependence of dialogue state and action representations, such that dialogue policies trained based on the proposed representation can be transferred across different domains. The experimental results show that the policy optimised in a restaurant search domain using our domain-independent representations can be deployed to a laptop sale domain, achieving a task success rate very close (96.4% relative) to that of the policy optimised on in-domain dialogues.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ML0RJ9NH7IQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cumulant GAN",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2006.06625",
            "Abstract": "In this paper, we propose a novel loss function for training Generative Adversarial Networks (GANs) aiming towards deeper theoretical understanding as well as improved performance for the underlying optimization problem. The new loss function is based on cumulant generating functions giving rise to \\emph{Cumulant GAN}. Relying on a recently-derived variational formula, we show that the corresponding optimization problem is equivalent to R{\\'e}nyi divergence minimization, thus offering a (partially) unified perspective of GAN losses: the R{\\'e}nyi family encompasses Kullback-Leibler divergence (KLD), reverse KLD, Hellinger distance and -divergence. Wasserstein GAN is also a member of the proposed cumulant GAN. In terms of stability, we rigorously prove the exponential convergence of cumulant GAN to the Nash equilibrium for a linear discriminator, Gaussian distributions and the standard gradient descent algorithm. Finally, we experimentally demonstrate that image generation is generally more robust relative to Wasserstein GAN and it is substantially improved in terms of inception score when weaker discriminators are considered.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:0izLItjtcgwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards Scalable Information-Seeking Multi-Domain Dialogue",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8461754/",
            "Abstract": "Multi-domain dialogue systems face challenges such as scaling algorithms to handle large ontologies, or transferring trained policy models to unseen domains. We attempt to address these challenges by proposing a dialogue management architecture that has an abstracted view of the world but yet is able to focus on relevant parts of the ontology at runtime. Specifically, we train a sub-domain identifier neural network that learns which features are relevant to the current turn and the immediate future, thus filtering out irrelevant information from the ontology and consequently the belief space at each dialogue turn. We then train a policy network that needs: a) to adapt to the sub-domain identifier's output; and b) to learn what information will carry over from previous turns and when it needs to be updated. We evaluate our method on a large information-seeking ontology that contains latent sub-domains. Our results in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:oNZyr7d5Mn4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Assessing the intelligibility impact of vowel space expansion via clear speech-inspired frequency warping.",
            "Publication year": 2013,
            "Publication url": "https://lpp.ilpga.fr/PDF/IS130177/IS130177.PDF",
            "Abstract": "Among the key acoustic features attributed with the intelligibility gain of Clear speech are the observed reduction in speaking rate and expansion of vowel space, representing greater articulation and vowel discrimination. Considering the slower speaking rate, previous works have attempted to assess the intelligibility impact of time-scaling casual speech to mimic Clear speech. In a complementary fashion, this work addresses the latter of the key traits observed in Clear speech, notably vowel space expansion. Specifically, a novel Clear speechinspired frequency warping method is described and shown to successfully achieve vowel space expansion when applied to casual speech. The intelligibility impact resulting from this expansion is then evaluated objectively and subjectively through formal listening tests. Much like the relevant time-scaling works, the frequency warping that expands vowel space is not shown \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:AXPGKjj_ei8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards a robust dynamic frequency warping textindependent voice conversion system",
            "Publication year": 2012,
            "Publication url": "https://www.ehu.eus/documents/1305439/1443388/IberSPEECH2012+DFW.pdf",
            "Abstract": "In this work we investigate several issues related to the use of Dynamic Frequency Warping in the context of text-independent voice conversion. For this type of systems, given an average spectral representation of each acoustic/phonetic class, dynamic programming is applied to compute the best alignment path between the frequency axis of the source and target speakers. In order to increase the robustness of the system, we suggest estimating such average spectral information using the Multi-Frame Analysis framework, while we compare several different local slope constraints for the dynamic programming procedure. Objective measurements show that the suggested approach provides better results than a state-of-the-art histogram-based solution in transforming the source spectral envelope towards the target one for all the dynamic programming constraints we considered.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:t6usbXjVLHcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Mathematical Model for Accurate Measurement of Jitter",
            "Publication year": 2007,
            "Publication url": "https://www.torrossa.com/gs/resourceProxy?an=2296668&publisher=FF3888",
            "Abstract": "Jitter is a fundamental metric of voice quality. The majority of jitter estimators produce an average value over a duration of several pitch periods. This paper proposes a method for short-time jitter measurement, based on a mathematical model which describes the coupling of two periodic phenomena. The movement of one of the two periodic phenomena with respect to the other is what is considered as jitter and what the proposed method measures. Through tests with synthetic jitter signals it has been verified that the suggested method provides accurate local estimates of jitter. Further evaluation was conducted on actual normal and pathological voice signals from the Massachusetts Eye and Ear Infirmary (MEEI) Disordered Voice Database. Compared with corresponding parameters from the Multi-Dimension Voice Program (MDVP) and the Praat system, the proposed method outperformed both in normal vs \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:JoZmwDi-zQgC",
            "Publisher": "Firenze University Press"
        },
        {
            "Title": "A factorial sparse coder model for single channel source separation",
            "Publication year": 2010,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2010/i10_0386.pdf",
            "Abstract": "We propose a probabilistic factorial sparse coder model for single channel source separation in the magnitude spectrogram domain. The mixture spectrogram is assumed to be the sum of the sources, which are assumed to be generated frame-wise as the output of sparse coders plus noise. For dictionary training we use an algorithm which can be described as non-negative matrix factorization with \u21130 sparseness constraints. In order to infer likely source spectrogram candidates, we approximate the intractable exact inference by maximizing the posterior over a plausible subset of solutions. We compare our system to the factorial-max vector quantization model, where the proposed method shows a superior performance in terms of signal-tointerference ratio. Finally, the low computational requirements of the algorithm allows close to real time applications.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:f2IySw72cVMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Neural text-to-speech adaptation from low quality public recordings",
            "Publication year": 2019,
            "Publication url": "https://www.isca-speech.org/archive_v0/SSW_2019/pdfs/SSW10_O_2-1.pdf",
            "Abstract": "Neural Text-to-Speech (TTS) synthesis is able to generate highquality speech with natural prosody. However, these systems typically require a large amount of data, preferably recorded in a clean and noise-free environment. We focus on creating target voices from low quality public recordings and our findings show that even with a large amount of data from a specific speaker, it is challenging to train a speaker-dependent neural TTS model. In order to improve the voice quality, while simultaneously reducing the amount of data required, we introduce meta-learning to adapt the neural TTS front-end. We propose three approaches for multi-speaker systems:(1) a lookup-table-based system,(2) a speaker representation derived from the Personalized Hey Siri (PHS) system, and (3) a system with no speaker encoder. Results show that: i) By using a significantly smaller number of target voice recordings, the proposed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:PVjk1bu6vJQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "High-resolution sinusoidal modeling of unvoiced speech",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7472626/",
            "Abstract": "In this paper, a recently proposed high-resolution Sinusoidal Model, dubbed the extended adaptive Quasi-Harmonic Model (eaQHM), is applied on modeling unvoiced speech sounds. Unvoiced speech sounds are parts of speech that are highly non-stationary in the time-frequency plane. Standard sinusoidal models fail to model them accurately and efficiently, thus introducing artefacts, while the reconstructed signals do not attain the quality and naturalness of the originals. Motivated by recently proposed non-stationary transforms, such as the Fan-Chirp Transform (FChT), eaQHM is tested to confront these effects and it is shown that highly accurate, artefact-free representations of unvoiced sounds are possible using the non-stationary properties of the model. Experiments on databases of unvoiced sounds show that, on average, eaQHM improves the Signal to Reconstruction Error Ratio (SRER) obtained by the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:fEOibwPWpKIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Enriched communication across the lifespan",
            "Publication year": 2019,
            "Publication url": "http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6113",
            "Abstract": "Speech is a hugely efficient means of communication: a reduced capacity in listening or speaking creates a significant barrier to social inclusion at all points through the lifespan, in education, work and at home. Hearing devices and speech synthesis can help address this reduced capacity but their use imposes greater listener effort. The goal of the EU-funded ENRICH project is to modify or augment speech with additional information to make it easier to process. Enrichment reduces listening burden by minimising cognitive load, while maintaining or improving intelligibility. ENRICH investigates the relationship between cognitive effort and natural and synthetic speech. Non-intrusive metrics for listening effort will be developed and used to design modification techniques which result in low-burden speech. The value of various enrichment approaches will be evaluated with individuals and cohorts with typically sub-optimal communication ability, such as children, hearingor speech-impaired adults, non-native listeners and individuals engaged in simultaneous tasks.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:rmuvC79q63oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A novel method for the extraction of vocal tremor",
            "Publication year": 2009,
            "Publication url": "https://www.torrossa.com/gs/resourceProxy?an=2430632&publisher=FF3888",
            "Abstract": "Vocal tremor is defined as slow modulation of fundamental frequency or its amplitude [1, 2]. Even though vocal tremor may be attributed to neurological diseases, it may also be a natural stochastic modulation of voice. Many studies try to measure these modulations assuming that they are stationary. Hence, their analysis is limited to small intervals loosing important information about vocal tremor. We propose a novel method for the estimation of the modulations which is able to adapt to nonstationary environments. The method is mainly based on an AM-FM signal decomposition algorithm which is able to estimate the instantaneous components of speech signals. Results confirm that the method successfully extract the modulations of large speech segments and robustly estimate the time-varying modulation frequency and the timevarying modulation level of vocal tremor.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ldfaerwXgEUC",
            "Publisher": "Firenze University Press"
        },
        {
            "Title": "Robust full-band adaptive sinusoidal analysis and synthesis of speech",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6854808/",
            "Abstract": "Recent advances in speech analysis have shown that voiced speech can be very well represented using quasi-harmonic frequency tracks and local parameter adaptivity to the underlying signal. In this paper, we revisit the quasi-harmonicity approach through the extended adaptive Quasi-Harmonic Model - eaQHM, and we show that the application of a continuous f 0  estimation method plus an adaptivity scheme can yield high resolution quasi-harmonic analysis and perceptually indistinguishable resynthesized speech. This method assumes an initial harmonic model which successively converges to quasi-harmonicity. Formal listening tests showed that eaQHM is robust against f 0  estimation artefacts and can provide a higher quality in resynthesizing speech, compared to a recently developed model, called the adaptive Harmonic Model (aHM), and the standard Sinusoidal Model (SM).",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ye4kPcJQO24C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Introduction to the Special Issue on The listening talker: context-dependent speech production and perception",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1016/j.csl.2013.11.002",
            "Abstract": "Introduction to the Special Issue on The listening talker: context-dependent speech \nproduction and perception: Computer Speech and Language: Vol 28, No 2 ACM Digital \nLibrary home ACM home Google, Inc. (search) Advanced Search Browse About Sign in \nRegister Advanced Search Journals Magazines Proceedings Books SIGs Conferences \nPeople More Search ACM Digital Library SearchSearch Advanced Search Computer \nSpeech and Language Periodical Home Latest Issue Archive Authors Affiliations Award \nWinners More HomeBrowse by TitlePeriodicalsComputer Speech and LanguageVol. , No. \nIntroduction to the Special Issue on The listening talker: context-dependent speech \nproduction and perception article Introduction to the Special Issue on The listening talker: \ncontext-dependent speech production and perception Share on Authors: Martin Cooke \nView Profile , Simon King View Profile , Bastiaan , & : :\u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:olpn-zPbct0C",
            "Publisher": "Academic Press Ltd."
        },
        {
            "Title": "Interspeech 2014 special session: Phase importance in speech processing applications",
            "Publication year": 2014,
            "Publication url": "http://www.cs.joensuu.fi/pages/franti/sipu/pub/i14_1623.pdf",
            "Abstract": "In many speech processing applications, the spectral amplitude is the dominant information while the use of phase spectrum is not so widely spread. In this paper, we present an overview on why speech phase spectrum has been neglected in the conventional techniques used in different applications including: speech separation/enhancement, automatic speech and speaker recognition and speech synthesis. We proceed with giving highlights on the recent progress carried out in demonstrating the importance of phase in different applications and how it impacts on the overall performance. The paper is an introduction to the",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:tYavs44e6CUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards a voice conversion system based on frame selection",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4218150/",
            "Abstract": "The subject of this paper is the conversion of a given speaker's voice (the source speaker) into another identified voice (the target one). We assume we have at our disposal a large amount of speech samples from source and target voice with at least a part of them being parallel. The proposed system is built on a mapping function between source and target spectral envelopes followed by a frame selection algorithm to produce final spectral envelopes. Converted speech is produced by a basic LP analysis of the source and LP synthesis using the converted spectral envelopes. We compared three types of conversion: without mapping, with mapping and using the excitation of the source speaker and finally with mapping using the excitation of the target. Results show that the combination of mapping and frame selection provide the best results, and underline the interest to work on methods to convert the LP excitation.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ULOm3_A8WrAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the properties of a time-varying quasi-harmonic model of speech",
            "Publication year": 2008,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2008/i08_1044.pdf",
            "Abstract": "In this paper we present the properties of a parametric speech model based on a deterministic plus noise representation of speech initially suggested by Laroche et al.[1]. Aiming at a high resolution analysis of speech signals for voice quality control (transformation) and assessment, we focus on the deterministic representation and we reveal the properties of the model showing that such a representation is equivalent to a time-varying quasi-harmonic representation of speech. Results show that the model is appropriate in estimating accurately linear amplitude modulations and modeling the inharmonicity of speech.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:MXK_kJrjxJIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bird detection in audio: a survey and a challenge",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7738875/",
            "Abstract": "Many biological monitoring projects rely on acoustic detection of birds. Despite increasingly large datasets, this detection is often manual or semi-automatic, requiring manual tuning/postprocessing. We review the state of the art in automatic bird sound detection, and identify a widespread need for tuning-free and species-agnostic approaches. We introduce new datasets and an IEEE research challenge to address this need, to make possible the development of fully automatic algorithms for bird sound detection.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:uWiczbcajpAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A non-causal FFTNet architecture for speech enhancement",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2006.04469",
            "Abstract": "In this paper, we suggest a new parallel, non-causal and shallow waveform domain architecture for speech enhancement based on FFTNet, a neural network for generating high quality audio waveform. In contrast to other waveform based approaches like WaveNet, FFTNet uses an initial wide dilation pattern. Such an architecture better represents the long term correlated structure of speech in the time domain, where noise is usually highly non-correlated, and therefore it is suitable for waveform domain based speech enhancement. To further strengthen this feature of FFTNet, we suggest a non-causal FFTNet architecture, where the present sample in each layer is estimated from the past and future samples of the previous layer. By suggesting a shallow network and applying non-causality within certain limits, the suggested FFTNet for speech enhancement (SE-FFTNet) uses much fewer parameters compared to other neural network based approaches for speech enhancement like WaveNet and SEGAN. Specifically, the suggested network has considerably reduced model parameters: 32% fewer compared to WaveNet and 87% fewer compared to SEGAN. Finally, based on subjective and objective metrics, SE-FFTNet outperforms WaveNet in terms of enhanced signal quality, while it provides equally good performance as SEGAN. A Tensorflow implementation of the architecture is provided at 1 .",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:5qfkUJPXOUwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Expressive visual text to speech and expression adaptation using deep neural networks",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7953092/",
            "Abstract": "In this paper, we present an expressive visual text to speech system (VTTS) based on a deep neural network (DNN). Given an input text sentence and a set of expression tags, the VTTS is able to produce not only the audio speech, but also the accompanying facial movements. The expressions can either be one of the expressions in the training corpus or a blend of expressions from the training corpus. Furthermore, we present a method of adapting a previously trained DNN to include a new expression using a small amount of training data. Experiments show that the proposed DNN-based VTTS is preferred by 57.9% over the baseline hidden Markov model based VTTS which uses cluster adaptive training.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:BwyfMAYsbu0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Progress in Nonlinear Speech Processing",
            "Publication year": 2007,
            "Publication url": "https://scholar.google.com/scholar?cluster=13351832398573833629&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:2KloaMYe4IUC",
            "Publisher": "Springer-Verlag Berlin Heidelberg"
        },
        {
            "Title": "A full-band adaptive harmonic representation of speech",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=7324468756316805370&hl=en&oi=scholarr",
            "Abstract": "In this paper we present a full-band Adaptive Harmonic Model (aHM) that is able to accurately reconstruct stationary and non stationary parts of speech. The model does not require any voiced/unvoiced decision, neither an accurate estimation of the pitch contour. Its robustness is based on the previously suggested adaptive Quasi-Harmonic model (aQHM), which provides a mechanism for frequency correction and adaptivity of its basis functions to the characteristics of the input signal. The suggested method overcomes limitations of the initial method based on aQHM in detecting frequency tracks over time, especially at mid and high frequencies, by employing a bandlimited iterative procedure for the re-estimation of the fundamental frequency. Listening tests show that reconstructed speech using aHM is mainly indistinguishable from the original signal, outperforming standard sinusoidal models (SM) and the aQHM \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:BqipwSGYUEgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spoken dialogue for information navigation",
            "Publication year": 2018,
            "Publication url": "https://www.aclweb.org/anthology/W18-5025.pdf",
            "Abstract": "Aiming to expand the current research paradigm for training conversational AI agents that can address real-world challenges, we take a step away from traditional slot-filling goal-oriented spoken dialogue systems (SDS) and model the dialogue in a way that allows users to be more expressive in describing their needs. The goal is to help users make informed decisions rather than being fed matching items. To this end, we describe the Linked-Data SDS (LD-SDS), a system that exploits semantic knowledge bases that connect to linked data, and supports complex constraints and preferences. We describe the required changes in language understanding and state tracking, and the need for mined features, and we report the promising results (in terms of semantic errors, effort, etc) of a preliminary evaluation after training two statistical dialogue managers in various conditions.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:xtoqd-5pKcoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Discrimination of speech from nonspeeech in broadcast news based on modulation frequency features",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S016763931000141X",
            "Abstract": "In audio content analysis, the discrimination of speech and non-speech is the first processing step before speaker segmentation and recognition, or speech transcription. Speech/non-speech segmentation algorithms usually consist of a frame-based scoring phase using MFCC features, combined with a smoothing phase. In this paper, a content based speech discrimination algorithm is designed to exploit long-term information inherent in modulation spectrum. In order to address the varying degrees of redundancy and discriminative power of the acoustic and modulation frequency subspaces, we first employ a generalization of SVD to tensors (Higher Order SVD) to reduce dimensions. Projection of modulation spectral features on the principal axes with the higher energy in each subspace results in a compact set of features with minimum redundancy. We further estimate the relevance of these projections to speech \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:M3ejUd6NZC8C",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Bit-Erasure Channel Decoding for GMM-based Multiple Description Coding",
            "Publication year": 2007,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2007/i07_1685.pdf",
            "Abstract": "Multiple Description Coding (MDC) is a plausible way to use the diversity of packet networks to increase the robustness of the transmission to packet losses. The redundancy that is introduced via MDC can also be used to increase the robustness of the transmission to bit-errors. This paper presents a novel decoding method for GMM (Gaussian Mixture Model)-based MDC in the presence of detected bit-errors. Particularly for speech transmission over bit-erasure channels, is is shown that the proposed method considerably improves the quality of the received speech spectral envelopes when one side-description is damaged. In highly correlated descriptions, for example, single and double bit-errors can almost be corrected.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:P5F9QuxV20EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A scale transform based method for rhythmic similarity of music",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4959584/",
            "Abstract": "This paper introduces scale transforms to measure rhythmic similarity between two musical pieces. The rhythm of a piece of music is described by the scale transform magnitude, computed by transforming the sample autocorrelation of its onset strength signal to the scale domain. Then, two pieces can be compared without the impact of tempo differences by using simple distances between these descriptors like the cosine distance. A widely used dance music dataset has been chosen for proof of concept. On this data set, the proposed method based on scale transform achieves classification results as high as other state of the art approaches. On a second data set, which is characterized by much larger intra-class tempo variance, the scale transform based measure improves classification compared to previously presented measures by 41%.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:aqlVkmm33-oC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Coding with side information techniques for LSF reconstruction in voice over IP",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1415070/",
            "Abstract": "In applications like VoIP, speech codecs have to deal with excessive packet losses, caused by network errors and/or delays. In this paper, a new method for the reconstruction of lost speech spectral envelopes is presented, which is based on a statistical estimation function. We suggest the usage of a minimal \"corrective\" bitstream and propose coding with side information (CSI) techniques for an efficient forward error correction (FEC) strategy. The proposed methods are tested on multiple scenarios of missing frames. Objective results indicate that with only 4 bits per lost frame, a spectral distortion reduction of 0.77-1.14 dB is achieved, compared to results obtained by current state-of-the-art estimation methods. Compared to \"predictive\" estimation methods, the use of the jitter buffer as side information and 4 bits per lost frame provide a 42% reduction of spectral distortion for single packet losses, and a 32% reduction \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:R3hNpaxXUhUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robust excitation-based features for automatic speech recognition",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7178855/",
            "Abstract": "In this paper we investigate the use of noise-robust features characterizing the speech excitation signal as complementary features to the usually considered vocal tract based features for Automatic Speech Recognition (ASR). The proposed Excitation-based Features (EBF) are tested in a state-of-the-art Deep Neural Network (DNN) based hybrid acoustic model for speech recognition. The suggested excitation features expand the set of periodicity features previously considered for ASR, expecting that these features help in a better discrimination of the broad phonetic classes (e.g., fricatives, nasal, vowels, etc.). Our experiments on the AMI meeting transcription system showed that the proposed EBF yield a relative word error rate reduction of about 5% when combined with conventional PLP features. Further experiments led on Aurora4 confirmed the robustness of the EBF to both additive and convolutive noises, with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:uLbwQdceFCQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the robustness of the quasi-harmonic model of speech",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5495700/",
            "Abstract": "In this paper we discuss the robustness of the Quasi-Harmonic model, QHM, previously suggested for speech analysis and AM-FM decomposition of speech. Assuming a frame by frame analysis, QHM suggests an iterative estimator for the actual frequencies of the speech components at the center of analysis window. In this paper, we show that this is a biased estimator and then, we compute analytically and numerically the bias of the estimator showing its dependence on the type and length of the analysis window. Moreover, we analyze the robustness of the QHM estimator in white Gaussian noise, showing that the suggested iterative estimator asymptotically attains the corresponding Cramer-Rao lower bound even in adverse noisy conditions. Examples of synthetic signals are provided to support our analysis.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:NhqRSupF_l8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Corpus-based techniques in the AT&T NextGen synthesis system",
            "Publication year": 2000,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.4104&rep=rep1&type=pdf",
            "Abstract": "The AT&T text-to-speech (TTS) synthesis system has been used as a framework for experimenting with a perceptuallyguided data-driven approach to speech synthesis, with primary focus on data-driven elements in the\\back end\". Statistical training techniques applied to a large corpus are used to make decisions about predicted speech events and selected speech inventory units. Our recent advances in automatic phonetic and prosodic labeling and a new faster harmonic plus noise model (HNM) and unit preselection implementations have significantly improved TTS quality and speeded up both development time and runtime.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:Y0pCki6q_DkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Benefits of the WaveNet-Based Speech Intelligibility Enhancement for Normal and Hearing Impaired Listeners",
            "Publication year": 2019,
            "Publication url": "https://scholar.google.com/scholar?cluster=3143659827486221390&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:6ZxmRoH8BuwC",
            "Publisher": "Universit\u00e4tsbibliothek der RWTH Aachen"
        },
        {
            "Title": "Scale transform in rhythmic similarity of music",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5430891/",
            "Abstract": "As a special case of the Mellin transform, the scale transform has been applied in various signal processing areas, in order to get a signal description that is invariant to scale changes. In this paper, the scale transform is applied to autocorrelation sequences derived from music signals. It is shown that two such sequences, when derived from similar rhythms with different tempo, differ mainly by a scaling factor. By using the scale transform, the proposed descriptors are robust to tempo changes, and are specially suited for the comparison of pieces with different tempi but similar rhythm. As music with such characteristics is widely encountered in traditional forms of music, the performance of the descriptors in a classification task of Greek traditional dances and Turkish traditional songs is evaluated. On these datasets accuracies compared to non-tempo robust approaches are improved by more than 20%, while on a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:iH-uZ7U-co4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Statistical synthesizer with embedded prosodic and spectral modifications to generate highly intelligible speech in noise.",
            "Publication year": 2013,
            "Publication url": "https://isca-speech.org/archive/archive_papers/interspeech_2013/i13_3557.pdf",
            "Abstract": "This paper describes a statistical parametric speech synthesizer that, despite having been trained on an ordinary synthesis database and without any adaptation data, is able to generate highly intelligible speech in noisy environments. By using a simple and flexible vocoder based on a harmonic model, it applies several noise-independent modifications to durations, pitch level and range, energy contour, formant sharpness, and intensity of particular spectral bands. The system has been evaluated by means of a large subjective test, the results of which show that the suggested approach clearly outperforms the reference TTS systems and even unmodified natural speech in some conditions.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:HE397vMXCloC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Simple and artefact-free spectral modifications for enhancing the intelligibility of casual speech",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6854483/",
            "Abstract": "In this paper, the problem of modifying casual speech to reach the intelligibility level of clear speech is addressed. Unlike other studies, in this work modifications on casual speech both consider intelligibility and speech quality. To achieve this, the authors focus on human-like modifications inspired by clear speech. An acoustic analysis performed on clear and casual speech reveals energy differences on specific frequency bands between the two speaking styles. Then, a simple method is used to boost these frequency regions on casual speech. The proposed method, called mix-filtering, uses a multi-band filtering scheme to isolate the information of these frequency bands and then, add this information to the original signal. Our method is compared in terms of intelligibility and quality with unmodified casual speech and with a highly intelligible spectral modification technique, namely the Spectral Shaping and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:eq2jaN3J8jMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "LISTA D5. 1\u2013The Hurricane Challenge: An evaluation framework for the assessment of modified speech",
            "Publication year": 2012,
            "Publication url": "https://cordis.europa.eu/docs/projects/cnect/0/256230/080/deliverables/001-D51InterimEvaluation.pdf",
            "Abstract": "This document describes the motivation, design and implementation of an evaluation framework for testing the intelligibility of modified speech in noise.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:LO7wyVUgiFcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Single-model multi-domain dialogue management with deep learning",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-92108-2_9",
            "Abstract": "We present a Deep Learning approach to dialogue management for multiple domains. Instead of training multiple models (e.g. one for each domain), we train a single domain-independent policy network that is applicable to virtually any information-seeking domain. We use the Deep Q-Network algorithm to train our dialogue policy, and evaluate against simulated and paid human users. The results show that our algorithm outperforms previous approaches while being more practical and scalable.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:EYYDruWGBe4C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Global variance in speech synthesis with linear dynamical models",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7491224/",
            "Abstract": "Linear Dynamical Models (LDMs) have been used in speech synthesis recently as an alternative to hidden Markov models (HMMs). Among the advantages of LDMs are the ability to capture the dynamics of speech and the achievement of synthesized speech quality similar to HMM-based speech systems on a smaller footprint. However, such as in the HMM case, LDMs produce over-smoothed trajectories of speech parameters, resulting in muffled quality of synthetic speech. Inspired by a similar problem found in HMM-based speech synthesis, where the naturalness of the synthesized speech is greatly improved when the global variance (GV) is compensated, this paper proposes a novel speech parameter generation algorithm that considers GV in LDM-based speech synthesis. Experimental results show that the application of GV during parameter generation significantly improves speech quality.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:z_wVstp3MssC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Acknowledgment of Guest Reviewers",
            "Publication year": 2014,
            "Publication url": "https://www.jvoice.org/article/S0892-1997(14)00034-4/abstract",
            "Abstract": "Acknowledgment of Guest Reviewers Page 1 Acknowledgment of Guest Reviewers Journal of \nVoice would like to thank Evelyn Abberton, MD Nassima Babaci Abdelli-Beruh, PhD Richard \nKenneth Adler, PhD, CCC-SLP Ofer Amir, PhD Thomas L. Carroll, MD Michele Castellengo, \nPhD, LAM-IJLRA Swapna Chandran, MD Paul Corthals, PhD Lise Crevier-Buchman, MD, \nPhD Daniel Jacob Croake, MM, MS Georgia Dacakis, BAppSc (SpPath), MEd Maria Dietrich, \nPhD Julia Gerhard, DMA, CCC-SLP Vijaya Kumar Guntupalli, PhD, CCC-SLP Marco Antonio \nGuzman, SLP Michael Hammer, MA, PhD, CCC-SLP Adrienne Hancock, PhD Edie Renee \nHapner, PhD James Harnsberger, PhD Matthew Hoffman, PhD Jaromir Hor\u00e1cek, DrSc Eric \nHunter, PhD Aaron Johnson, PhD Jennifer Johnson, CCC-SLP Michael P. Karnell, PhD Lisa \nN. Kelchner, PhD Francisco Lacerda, PhD Thomas Law, BAppSc, MSc Wendy DeLeo , PhD Y-\u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:JQOojiI6XY0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Modulation spectral features for objective voice quality assessment: the breathiness case.",
            "Publication year": 2009,
            "Publication url": "https://www.torrossa.com/gs/resourceProxy?an=2414251&publisher=FF3888#page=53",
            "Abstract": "In this paper, we employ normalized modula-tion spectral features for objective voice quality assessment regarding breathiness. Modulation spectra usually produce a high-dimensionality space. For classification purposes, the size of the original space is reduced using Higher Order Singular Value Decomposition (SVD). Further, we select most relevant features based on the mutual information between the degree of breathiness and the computed features, which leads to an adaptive to the classification task modulation spectral representation. The adaptive modulation spectral features are used as input to a Naive Bayes (NB) classifier. By combining two NB classifiers based on different feature sets a global classification rate of 79% for breathiness was achieved.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:LPZeul_q3PIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improved Automatic Speech Recognition Using Subband Temporal Envelope Features and Time-Delay Neural Network Denoising Autoencoder.",
            "Publication year": 2017,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2017/pdfs/1096.PDF",
            "Abstract": "This paper investigates the use of perceptually-motivated subband temporal envelope (STE) features and time-delay neural network (TDNN) denoising autoencoder (DAE) to improve deep neural network (DNN)-based automatic speech recognition (ASR). STEs are estimated by full-wave rectification and low-pass filtering of band-passed speech using a Gammatone filter-bank. TDNNs are used either as DAE or acoustic models. ASR experiments are performed on Aurora-4 corpus. STE features provide 2.2% and 3.7% relative word error rate (WER) reduction compared to conventional log-mel filter-bank (FBANK) features when used in ASR systems using DNN and TDNN as acoustic models, respectively. Features enhanced by TDNN DAE are better recognized with ASR system using DNN acoustic models than using TDNN acoustic models. Improved ASR performance is obtained when features enhanced by \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:g3aElNc5_aQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Rhythmic similarity in traditional Turkish music",
            "Publication year": 2009,
            "Publication url": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:1040358",
            "Abstract": "In this paper, the problem of automatically assigning a piece of traditional Turkish music into a class of rhythm referred to as usul is addressed. For this, an approach for rhythmic similarity measurement based on scale transforms has been evaluated on a set of MIDI data. Because this task is related to time signature estimation, the accuracy of the proposed method is evaluated and compared with a state of the art time signature estimation approach. The results indicate that the proposed method can be successfully applied to audio signals of Turkish music and that it captures relevant properties of the individual usul.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:-f6ydRqryjwC",
            "Publisher": "ISMIR"
        },
        {
            "Title": "Conditional vector quantization for speech coding",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4067017/",
            "Abstract": "In many speech-coding-related problems, there is available information and lost information that must be recovered. When there is significant correlation between the available and the lost information source, coding with side information (CSI) can be used to benefit from the mutual information between the two sources. In this paper, we consider CSI as a special VQ problem which will be referred to as conditional vector quantization (CVQ). A fast two-step divide-and-conquer solution is proposed. CVQ is then used in two applications: the recovery of highband (4-8 kHz) spectral envelopes for speech spectrum expansion and the recovery of lost narrowband spectral envelopes for voice over IP. Comparisons with alternative approaches like estimation and simple VQ-based schemes show that CVQ provides significant distortion reductions at very low bit rates. Subjective evaluations indicate that CVQ provides \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:mVmsd5A6BfQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Laryngeal image processing of vocal folds motion",
            "Publication year": 2020,
            "Publication url": "https://www.mdpi.com/649726",
            "Abstract": "This review provides a comprehensive compilation, from a digital image processing point of view of the most important techniques currently developed to characterize and quantify the vibration behaviour of the vocal folds, along with a detailed description of the laryngeal image modalities currently used in the clinic. The review presents an overview of the most significant glottal-gap segmentation and facilitative playbacks techniques used in the literature for the mentioned purpose, and shows the drawbacks and challenges that still remain unsolved to develop robust vocal folds vibration function analysis tools based on digital image processing. View Full-Text",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:8d8msizDQcsC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Approaching speech intelligibility enhancement with inspiration from Lombard and Clear speaking styles",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0885230813000788",
            "Abstract": "Lombard and Clear speech represent two acoustically and perceptually distinct speaking styles that humans employ to increase intelligibility. For Lombard speech, increased spectral energy in a band spanning the range of formants is consistent, effectively augmenting loudness, while vowel space expansion is exhibited in Clear speech, indicating greater articulation. On the other hand, analyses in the first part of this work illustrate that Clear speech does not exhibit significant spectral energy boosting, nor does the Lombard effect invoke an expansion of vowel space. Accordingly, though these two acoustic phenomena are largely attributed with the respective intelligibility gains of the styles, present analyses would suggest that they are mutually exclusive in human speech production. However, these phenomena can be used to inspire signal processing algorithms that seek to exploit and ultimately compound their \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:eflP2zaiRacC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Rhythmic similarity of music based on dynamic periodicity warping",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4518085/",
            "Abstract": "This paper introduces a new way to measure rhythmic similarity between two musical pieces using periodicity spectra. In order to detect similarity for pieces of different tempi, the linearity of the warping path between their spectra serves as a measure of their rhythmic similarity. Using a modified kNN classification approach on two datasets, the proposed measure provides comparable classification accuracy (82.1%) to the best of widely used measures (85.5%) for the first dataset; For the second dataset, which is characterized by a large variance of tempi, the proposed measure outperforms all reference measures, reaching an accuracy of 69.0%, while the best of the other measures reaches 53.8%. Moreover, the presented technique works fully automatically, and no information regarding tempo is needed.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:7PzlFSSx8tAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Speech\u2014Nonspeech discrimination based on speech-relevant spectrogram modulations",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7099066/",
            "Abstract": "In this work, we adopt an information theoretic approach - the Information Bottleneck method - to extract the relevant modulation frequencies across both dimensions of a spectrogram, for speech / non-speech discrimination (music, animal vocalizations, environmental noises). A compact representation is built for each sound ensemble, consisting of the maximally informative features. We demonstrate the effectiveness of a simple thresholding classifier which is based on the similarity of a sound to each characteristic modulation spectrum. When we assess the performance of the classification system at various SNR conditions using F-measure, results are equally good to a recently proposed method based on the same features but having significantly greater complexity.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:SeFeTyx0c_EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fast and accurate phase unwrapping",
            "Publication year": 2015,
            "Publication url": "https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_1171.pdf",
            "Abstract": "More and more speech technology and signal processing applications make use of the phase information. A proper estimation and representation of the phase goes inextricably along with a correct phase unwrapping, which refers to the problem of finding the instance of the phase function chosen to ensure continuity. This paper proposes a new technique of phase unwrapping which is based on two mathematical considerations: i) a property of the unwrapped phase at Nyquist frequency, ii) the modified Schur-Cohn\u2019s algorithm which allows a fast calculation of the root distribution of polynomials with respect to the unit circle. The proposed method is compared to five state-of-the-art phase unwrappers on a large dataset of both synthetic random and real speech signals. By leveraging the two aforementioned considerations, the proposed approach is shown to perform an exact estimation of the unwrapped phase at a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:Fu2w8maKXqMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Stochastic modeling of spectral adjustment for high quality pitch modification",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/859118/",
            "Abstract": "We present a new algorithm for adjusting the magnitude spectrum when the fundamental frequency (F/sub 0/) of a speech signal is altered. The algorithm exploits the correlation between F/sub 0/ and the magnitude spectrum of speech as represented by line spectral frequencies (LSFs). This correlation is class-dependent, and thus a broad classification of the input is achieved by a Gaussian mixture model (GMM). The within-class dependencies of LSFs on F/sub 0/ values are captured by constructing their joint probability densities using a series of GMMs, one for each speech class. The proposed system is used for post-processing the pitch modified signal. Perceptual tests showed that the addition of this post-processing system improves the naturalness of the pitch modified signal for large pitch modification factors.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:LkGwnXOMwfcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Glottal inverse filtering using stabilised weighted linear prediction",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5947581/",
            "Abstract": "This paper presents and evaluates an inverse filtering technique of the speech signal which is based on the Stabilized Weighted Lin ear Prediction (SWLP) of speech. SWLP emphasizes the speech samples that fit the underlying speech production model well, by imposing temporal weighting of the square of the residual signal. The performance of SWLP is compared to the conventional Linear Prediction based inverse filtering techniques, such as the Autocorrelation and Closed Phase Covariance method. All the inverse filtering approaches are evaluated on a database of speech signals generated by a physical model of the voice production system. Results show that the estimated glottal flows using SWLP are closer to the original glottal flow than those estimated by the Autocorrelation approach, while its performance is comparable to the Closed Phase Covariance approach.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:abG-DnoFyZgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Advances in phase-aware signal processing in speech communication",
            "Publication year": 2016,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167639316300784",
            "Abstract": "During the past three decades, the issue of processing spectral phase has been largely neglected in speech applications. There is no doubt that the interest of speech processing community towards the use of phase information in a big spectrum of speech technologies, from automatic speech and speaker recognition to speech synthesis, from speech enhancement and source separation to speech coding, is constantly increasing. In this paper, we elaborate on why phase was believed to be unimportant in each application. We provide an overview of advancements in phase-aware signal processing with applications to speech, showing that considering phase-aware speech processing can be beneficial in many cases, while it can complement the possible solutions that magnitude-only methods suggest. Our goal is to show that phase-aware signal processing is an important emerging field with high potential in the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:35r97b3x0nAC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Speech processing to improve the perception of speech in background noise for children with auditory processing disorder and typically developing peers",
            "Publication year": 2018,
            "Publication url": "https://journals.sagepub.com/doi/abs/10.1177/2331216518756533",
            "Abstract": "Auditory processing disorder (APD) may be diagnosed when a child has listening difficulties but has normal audiometric thresholds. For adults with normal hearing and with mild-to-moderate hearing impairment, an algorithm called spectral shaping with dynamic range compression (SSDRC) has been shown to increase the intelligibility of speech when background noise is added after the processing. Here, we assessed the effect of such processing using 8 children with APD and 10 age-matched control children. The loudness of the processed and unprocessed sentences was matched using a loudness model. The task was to repeat back sentences produced by a female speaker when presented with either speech-shaped noise (SSN) or a male competing speaker (CS) at two signal-to-background ratios (SBRs). Speech identification was significantly better with SSDRC processing than without, for both groups \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:kz9GbA2Ns4gC",
            "Publisher": "SAGE Publications"
        },
        {
            "Title": "Biometrics: Different approaches for using gaussian mixture models in handwriting",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11552055_26",
            "Abstract": "In this work in progress paper we discuss an established as well as a new approache to the use of Gaussian Mixture Models (GMMs) for handwriting biometrics. The technique of GMMs is well explored for the domain of speech processing and we evaluate ways to use them for handwriting biometrics, too.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Analysis of emotional speech using an adaptive sinusoidal model",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6952538/",
            "Abstract": "Processing of emotional (or expressive) speech has gained attention over recent years in the speech community due to its numerous applications. In this paper, an adaptive sinusoidal model (aSM), dubbed extended adaptive Quasi-Harmonic Model - eaQHM, is employed to analyze emotional speech in accurate, robust, continuous, timevarying parameters (amplitude, frequency, and phase). It is shown that these parameters can adequately and accurately represent emotional speech content. Using a well known database of narrowband expressive speech (SUSAS) we show that very high Signal-to-Reconstruction-Error Ratio (SRER) values can be obtained, compared to the standard sinusoidal model (SM). Formal listening tests on a smaller wideband speech database show that the eaQHM outperforms SM from a perceptual resynthesis quality point of view. Finally, preliminary emotion classification tests show \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:N5tVd3kTz84C",
            "Publisher": "IEEE"
        },
        {
            "Title": "An extension of the adaptive quasi-harmonic model",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6288944/",
            "Abstract": "In this paper, we present an extension of a recently developed AM-FM decomposition algorithm, which will be referred to as the extended adaptive Quasi-Harmonic Model (eaQHM). It was previously shown that the adaptive Quasi-Harmonic Model (aQHM) [1] is an efficient AM-FM decomposition algorithm with applications in speech analysis. In this paper, we show that a simple extension of the aQHM algorithm to include not only frequency but also amplitude adaptation results in higher performance in terms of Signal-to-Reconstruction-Error Ratio (SRER). To support our hypothesis, eaQHM is tested both on synthetic signals and on a subset of the ARCTIC database of speech. Overall, compared with aQHM, eaQHM improves the SRER by more than 2 dB, on average.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:isC4tDSrTZIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fast least-squares solution for sinusoidal, harmonic and quasi-harmonic models",
            "Publication year": 2010,
            "Publication url": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2010/i10_1700.pdf",
            "Abstract": "Sinusoidal model and its variants are commonly used in speech processing. In the literature, there are various methods for the estimation of the unknown parameters of sinusoidal model such as Fourier transform based on FFT algorithm and Least Squares (LS) method. Least Squares method is more accurate and actually optimum for Gaussian noise, thus, more appropriate for high-quality signal processing, however, it is slower compared with FFT-based algorithms. In this paper, we study the source of computational load of LS solution and propose various computational improvements. We show that the complexity of LS solution as well the execution time are highly improved.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:nb7KW1ujOQ8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automated Pause Insertion for Improved Intelligibility Under Reverberation.",
            "Publication year": 2016,
            "Publication url": "https://www.isca-speech.org/archive_v0/Interspeech_2016/pdfs/0960.PDF",
            "Abstract": "Speech intelligibility in reverberant environments is reduced because of overlap-masking. Signal modification prior to presentation in such listening environments, eg, with a public announcement system, can be employed to alleviate this problem. Time-scale modifications are particularly effective in reducing the effect of overlap-masking. A method for introducing linguistically-motivated pauses is proposed in this paper. Given the transcription of a sentence, pause strengths are predicted at word boundaries. Pause duration is obtained by combining the pause strength and the time it takes late reverberation to decay to a level where a target signal-to-late-reverberation ratio criterion is satisfied. Considering a moderate reverberation condition and both binary and continuous pause strengths, a formal listening test was performed. The results show that the proposed methodology offers a significant intelligibility \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:ipzZ9siozwsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Detection of sperm whale clicks based on the Teager\u2013Kaiser energy operator",
            "Publication year": 2006,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0003682X06001095",
            "Abstract": "The development of an algorithm for automatic detection of sperm whale clicks in large recordings is described. It is based on the Teager\u2013Kaiser (TK) energy operator and it is able to detect efficiently creaks as well as regular clicks. A matching filter is first used as a pre-processor in order to overcome the difficulties caused by the multi-pulse structure of regular clicks. Next, the TK energy operator is applied to the output of the matching filter. A first selection of clicks is performed based on statistical measurements on the TK output, while the final selection is carried out by a forward\u2013backward search algorithm. The proposed system has been tested on a total duration of 25 min of data containing regular clicks as well as creak clicks, where the location of clicks has been marked by hand. An average rate of 94.05% of correct detections was achieved by comparison with the hand-labeled data created from the tested files \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:_FxGoFyzp5QC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Evaluating how well filtered white noise models the residual from sinusoidal modeling of musical instrument sounds",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6701840/",
            "Abstract": "Nowadays, sinusoidal modeling commonly includes a residual obtained by the subtraction of the sinusoidal model from the original sound. This residual signal is often further modeled as filtered white noise. In this work, we evaluate how well filtered white noise models the residual from sinusoidal modeling of musical instrument sounds for several sinusoidal algorithms. We compare how well each sinusoidal model captures the oscillatory behavior of the partials by looking into how \u201cnoisy\u201d their residuals are. We performed a listening test to evaluate the perceptual similarity between the original residual and the modeled counterpart. Then we further investigate whether the result of the listening test can be explained by the fine structure of the residual magnitude spectrum. The results presented here have the potential to subsidize improvements on residual modeling.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:mvPsJ3kp5DgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Towards a Neural-Based Single Channel Speech Enhancement Model for Hearing-Aids",
            "Publication year": 2019,
            "Publication url": "https://scholar.google.com/scholar?cluster=822399699918033985&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:DUooU5lO8OsC",
            "Publisher": "Universit\u00e4tsbibliothek der RWTH Aachen"
        },
        {
            "Title": "Dysphonia detection based on modulation spectral features and cepstral coefficients",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5495020/",
            "Abstract": "In this paper, we combine modulation spectral features with mel-frequency cepstral coefficients for automatic detection of dysphonia. For classification purposes, dimensions of the original modulation spectra are reduced using higher order singular value decomposition (HOSVD). Most relevant features are selected based on their mutual information to discrimination between normophonic and dysphonic speakers made by experts. Features that highly correlate with voice alterations are associated then with a support vector machine (SVM) classifier to provide an automatic decision. Recognition experiments using two different databases suggest that the system provides complementary information to the standard mel-cepstral features.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:JV2RwH3_ST0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "LISTA D4. 2\u2013Sound class and intonation structure extraction",
            "Publication year": 2012,
            "Publication url": "https://cordis.europa.eu/docs/projects/cnect/0/256230/080/deliverables/001-D42SoundClassandIntonationStructureExtraction.pdf",
            "Abstract": "It is known that gross sound classes (eg, vowels, fricatives, nasals) are treated differently with respect to the degree of modification that humans apply to maintain intelligibility under quiet and noisy conditions. Therefore, speech modifications should not be applied monolithically to the signal, but instead should be context-sensitive, dependent on the units and structures in the signal. This deliverable will focus on analysis of both prosodic and segmental properties of speech. More specifically, one goal is to develop a detector of sound classes. Using mel frequency cepstrum coefficients (MFCC) and their derivatives, a broad phonetic class detector has been developed in Matlab. Training and testing of the detector was performed using the TIMIT database. A second goal is to build a hierarchical description of speech rhythm and intonation based on prosodic structure analysis tools. For this purpose, the rhythmogram representation has been employed as well as a recent approach to intonation extraction (prosogram). Similar to the broad phonetic class detector, the hierarchical description of speech has been developed in Matlab using some utilities of R for visualization purposes. The results from both segmental and prosodic analyses will be helpful in determining the modification strategy given a segment of speech.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:2VqYfGB8ITEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Enhancing Speech Intelligibility in Text-To-Speech Synthesis using Speaking Style Conversion",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2008.05809",
            "Abstract": "The increased adoption of digital assistants makes text-to-speech (TTS) synthesis systems an indispensable feature of modern mobile devices. It is hence desirable to build a system capable of generating highly intelligible speech in the presence of noise. Past studies have investigated style conversion in TTS synthesis, yet degraded synthesized quality often leads to worse intelligibility. To overcome such limitations, we proposed a novel transfer learning approach using Tacotron and WaveRNN based TTS synthesis. The proposed speech system exploits two modification strategies: (a) Lombard speaking style data and (b) Spectral Shaping and Dynamic Range Compression (SSDRC) which has been shown to provide high intelligibility gains by redistributing the signal energy on the time-frequency domain. We refer to this extension as Lombard-SSDRC TTS system. Intelligibility enhancement as quantified by the Intelligibility in Bits (SIIB-Gauss) measure shows that the proposed Lombard-SSDRC TTS system shows significant relative improvement between 110% and 130% in speech-shaped noise (SSN), and 47% to 140% in competing-speaker noise (CSN) against the state-of-the-art TTS approach. Additional subjective evaluation shows that Lombard-SSDRC TTS successfully increases the speech intelligibility with relative improvement of 455% for SSN and 104% for CSN in median keyword correction rate compared to the baseline TTS method.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:4hFrxpcac9AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Ld-sds: Towards an expressive spoken dialogue system based on linked-data",
            "Publication year": 2017,
            "Publication url": "https://arxiv.org/abs/1710.02973",
            "Abstract": "In this work we discuss the related challenges and describe an approach towards the fusion of state-of-the-art technologies from the Spoken Dialogue Systems (SDS) and the Semantic Web and Information Retrieval domains. We envision a dialogue system named LD-SDS that will support advanced, expressive, and engaging user requests, over multiple, complex, rich, and open-domain data sources that will leverage the wealth of the available Linked Data. Specifically, we focus on: a) improving the identification, disambiguation and linking of entities occurring in data sources and user input; b) offering advanced query services for exploiting the semantics of the data, with reasoning and exploratory capabilities; and c) expanding the typical information seeking dialogue model (slot filling) to better reflect real-world conversational search scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:gsN89kCJA0AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks.",
            "Publication year": 2019,
            "Publication url": "https://www.researchgate.net/profile/Dipjyoti-Paul/publication/335829286_Non-Parallel_Voice_Conversion_Using_Weighted_Generative_Adversarial_Networks/links/5ec26e8ba6fdcc90d67e1dce/Non-Parallel-Voice-Conversion-Using-Weighted-Generative-Adversarial-Networks.pdf",
            "Abstract": "In this paper, we suggest a novel way to train Generative Adversarial Network (GAN) for the purpose of non-parallel, many-to-many voice conversion. The goal of voice conversion (VC) is to transform speech from a source speaker to that of a target speaker without changing the phonetic contents. Based on ideas from Game Theory, we suggest to multiply the gradient of the Generator with suitable weights. Weights are calculated so that they increase the power of fake samples that fool the Discriminator resulting in a stronger Generator. Motivated by a recently presented GAN based approach for VC, StarGAN-VC, we suggest a variation to StarGAN, referred to as Weighted StarGAN (WeStarGAN). The experiments are conducted on standard CMU ARCTIC database. WeStarGAN-VC approach achieves significantly better relative performance and is clearly preferred over recently proposed StarGAN-VC method in terms of speech subjective quality and speaker similarity with 75% and 65% preference scores, respectively.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:_axFR9aDTf0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "How can marine biologists track sperm whales in the oceans?",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-0-387-74535-0_6",
            "Abstract": "Whale watching has become a trendy occupation these last years. It is carried on in the waters of some 40 countries, plus Antarctica. Far beyond its touristic aspects, being able to spot the position of whales in real time has several important scientific applications, such as censusing (estimation of animal population), behavior studies, and mitigation efforts concerning fatal collisions between marine mammals and ships and exposure of marine mammals to loud sounds of anthropogenic origin (seismic surveys, oil and gas exploitation and drilling, naval and other uses of sonar).",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:D03iK_w7-QYC",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "Singing voice detection using modulation frequency feature.",
            "Publication year": 2008,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.385.109&rep=rep1&type=pdf",
            "Abstract": "In this paper, a feature set derived from modulation spectra is applied to the task of detecting singing voice in historical and recent recordings of Greek Rembetiko. A generalization of SVD to tensors, Higher Order SVD (HOSVD), is applied to reduce the dimensions of the feature vectors. Projection onto the \u201csignificant\u201d principal axes of the acoustic and modulation frequency subspaces, results in a compact feature set, which is evaluated using an SVM classifier on a set of hand labeled musical mixtures. Fusion of the proposed features with MFCCs and delta coefficients reduces the optimal detection cost from 11.11% to 9.01%.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:hFOr9nPyWt4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Maximum voiced frequency estimation: Exploiting amplitude and phase spectra",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6840999/",
            "Abstract": "Maximum Voiced Frequency (MVF) is used in various speech models as the spectral boundary separating periodic and aperiodic components during the production of voiced sounds. Recent studies have shown that its proper estimation and modeling enhance the quality of statistical parametric speech synthesizers. Contrastingly, these same methods of MVF estimation have been reported to degrade the performance of singing voice synthesizers. This paper proposes a new approach for MVF estimation which exploits both amplitude and phase spectra. It is shown that phase conveys relevant information about the harmonicity of the voice signal, and that it can be jointly used with features derived from the amplitude spectrum. This information is further integrated into a maximum likelihood criterion which provides a decision about the MVF estimate. The proposed technique is compared to two state-of-the-art \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:dTyEYWd-f8wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the implementation of the harmonic plus noise model for concatenative speech synthesis",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/859120/",
            "Abstract": "In concatenative speech synthesis systems, speech models are usually used to represent the speech signal. Recently, the harmonic plus noise model (HNM) has been proposed for concatenative speech synthesis with promising results. One main drawback of HNM is its complexity. In this paper, we review four different methods of reducing the complexity of HNM. These include, straight-forward synthesis(SF), synthesis using inverse fast Fourier transform (IFFT), synthesis using recurrence relations for trigonometric functions (RR), and synthesis based on delayed multi-resampled cosine functions (DMRC). DMRC was shown to outperform all the other techniques reducing the complexity of HNM synthesizer by 95% compared to the current version of the HNM which is based on the SF method. Informal listening tests showed that the version of HNM based on the DMRC method provides higher quality of speech \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:_kc_bZDykSQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Evaluating the intelligibility benefit of speech modifications in known noise conditions",
            "Publication year": 2013,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167639313000046",
            "Abstract": "The use of live and recorded speech is widespread in applications where correct message reception is important. Furthermore, the deployment of synthetic speech in such applications is growing. Modifications to natural and synthetic speech have therefore been proposed which aim at improving intelligibility in noise. The current study compares the benefits of speech modification algorithms in a large-scale speech intelligibility evaluation and quantifies the equivalent intensity change, defined as the amount in decibels that unmodified speech would need to be adjusted by in order to achieve the same intelligibility as modified speech. Listeners identified keywords in phonetically-balanced sentences representing ten different types of speech: plain and Lombard speech, five types of modified speech, and three forms of synthetic speech. Sentences were masked by either a stationary or a competing speech masker \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:mB3voiENLucC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "A hybrid system for Audio segmentation and speech-endpoint detection of broadcast news",
            "Publication year": 2007,
            "Publication url": "http://www.csd.uoc.gr/~mmarkaki/MMILab-MMarkaki_files/specom07_ERT_v2.pdf",
            "Abstract": "A hybrid speech/nonspeech detector is proposed for the preprocessing of broadcast news. During the first stage speech/nonspeech classification of uniform overlapping segments is performed. The accuracy in the detection of boundaries is determined by the degree of overlap of the audio segments and it is 250 ms in our case. Extracted speech segments are further processed on a frame basis using the entropy of the signal spectrum. Speech endpoint detection is accomplished with an accuracy of 10 ms. The combination of the two methods in one speech/nonspeech detection system, exhibits the robustness and accuracy required for subsequent processing stages like broadcast speech transcription and speaker diarization.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:pyW8ca7W8N0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A statistical approach to musical genre classification using non-negative matrix factorization",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4217503/",
            "Abstract": "This paper introduces a new feature set based on a non-negative matrix factorization approach for the classification of musical signals into genres, only using synchronous organization of music events (vertical dimension of music). This feature set generates a vector space to describe the spectrogram representation of a music signal. The space is modeled statistically by a mixture of Gaussians (GMM). A new signal is classified by considering the likelihoods over all the estimated feature vectors given these statistical models, without constructing a model for the signal itself. Cross-validation tests on two commonly utilized datasets for this task show the superiority of the proposed features compared to the widely used MFCC type of representation based on classification accuracies (over 9% of improvement), as well as on a stability measure introduced in this paper for GMM.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:IWHjjKOFINEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Modeling speech based on harmonic plus noise models",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/11520153_11",
            "Abstract": "Hybrid models of speech have received increasing interest from the speech processing community. Splitting the speech signal into a periodic and a non-periodic part increases the quality of prosodic modifications necessary in concatenative speech synthesis systems. This paper focuses on the decomposition of the speech signal into a periodic and a non-periodic part based on a Harmonic plus Noise Model, HNM; three versions of HNM are discussed with respect to their effectiveness in decomposing the speech signal into a periodic and a non-periodic part. While the harmonic part is modeled explicitely, the non-periodic part (or noise part) is obtained by subtracting in the time domain the harmonic part from the original speech signal. Three versions of HNM are discussed. The objective of the discussion is to determine which of these versions could be useful for prosodic modifications and synthesis of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:eQOLeE2rZwMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "A case study on the importance of belief state representation for dialogue policy management",
            "Publication year": 2018,
            "Publication url": "https://spiral.imperial.ac.uk/handle/10044/1/61853",
            "Abstract": "A key component of task-oriented dialogue systems is the belief state representation, since it directly affects the policy learning efficiency. In this paper, we propose a novel, binary, compact, yet scalable belief state representation. We compare the stan- dard verbose belief state representation (268 dimensions) with the domain-independent representation (57 dimensions) and the proposed representation (13 or 4 dimensions). To test those representations, the recently introduced Advantage Actor Critic (A2C) algorithm is exploited. The latter has not been tested before for any representation apart from the verbose one. We study the effect of the belief state representation within A2C un- der 0%, 15%, 30%, and 45% semantic error rate and conclude that the novel binary representation in general outperforms both the domain-independent and the verbose belief state represen- tation. Further, the robustness of the binary representation is tested under more realistic scenarios with mismatched semantic error rates, within the A2C and DQN algorithms. The results indicate that the proposed compact, binary representation per- forms better or similarly to the other representations, being an efficient and promising alternative to the full belief.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:foquWX3nUaYC",
            "Publisher": "ISCA"
        },
        {
            "Title": "Improved face-to-face communication using noise reduction and speech intelligibility enhancement",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7178943/",
            "Abstract": "Significant improvements in intelligibility of speech in noise can be obtained by modifying the speech signal in the time and/or frequency domains. However, most speech intelligibility enhancement algorithms are designed to use clean speech as an input, and their performance suffers once the input speech signal-to-noise ratio decreases, a common case in face-to-face communication environments such as restaurants or caf\u00e9s. In this work we investigate whether a particularly successful speech intelligibility enhancement system-spectral shaping and dynamic range compression-and various front-end noise reduction methods might be suitable in such environments. Our evaluations suggest that such a complete system would provide an increase in speech intelligibility equivalent to a gain of 10 dB input signal-to-noise ratio in the more challenging face-to-face communication environments.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:kzcrU_BdoSEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Effectiveness of a loudness model for time-varying sounds in equating the loudness of sentences subjected to different forms of signal processing",
            "Publication year": 2016,
            "Publication url": "https://asa.scitation.org/doi/abs/10.1121/1.4955005",
            "Abstract": "A model for the loudness of time-varying sounds [Glasberg and Moore (2012). J. Audio. Eng. Soc. 50, 331\u2013342] was assessed for its ability to predict the loudness of sentences that were processed to either decrease or increase their dynamic fluctuations. In a paired-comparison task, subjects compared the loudness of unprocessed and processed sentences that had been equalized in (1) root-mean square (RMS) level; (2) the peak long-term loudness predicted by the model; (3) the mean long-term loudness predicted by the model. Method 2 was most effective in equating the loudness of the original and processed sentences.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:XD-gHx7UXLsC",
            "Publisher": "Acoustical Society of America"
        },
        {
            "Title": "Tremor in speakers with spasmodic dysphonia.",
            "Publication year": 2011,
            "Publication url": "https://www.torrossa.com/gs/resourceProxy?an=2469192&publisher=FF3888#page=151",
            "Abstract": "The objective of this work is the estimation of vocal tremor in patients with spasmodic dysphonia before and after treatment, and the comparison of their tremor characteristics with those estimated from healthy speakers. As an outcome, a new tremor attribute is introduced, the deviation of the modulation level and a novel method is proposed for classifying speakers according to the prevalence of tremor in their voice. Results are consistent with subjective evaluations on patients who suffer from spasmodic dysphonia and confirm that the proposed method can be used for accurate estimation and objective ranking of the severity of tremor.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:WbkHhVStYXYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised acoustic analyses of normal and Lombard speech, with spectral envelope transformation to improve intelligibility",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=18135502716126549420&hl=en&oi=scholarr",
            "Abstract": "The \u201cLombard effect\u201d describes how humans modify their speech in noisy environments to make it more intelligible. The present work analyzes Normal and Lombard speech from multiple speakers in an unsupervised context, using meaningful acoustic criteria for speech classification (according to voicing and stationarity) and evaluation (using loudness and intelligibility). These acoustic analyses using generalized classes offer alternative and informative interpretations of the Lombard effect. For example, the Lombard increase in intelligibility is shown to be isolated primarily to voiced speech. Also, while transients are shown to be less intelligible overall, the Lombard effect does not appear to distinguish between stationary and transient speech. In addition to these analyses, following recently published results illustrating that Lombard spectral modifications account for the largest increases in intelligibility, this work \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:lSLTfruPkqcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reply to \u201cComments on \u2018Iterative Estimation of Sinusoidal Signal Parameters\u2019\u201d",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5593205/",
            "Abstract": "The method proposed in , referred to as iterative Quasi-Harmonic Model (iQHM), is an iterative frequency estimation technique. It is based on a linearized version of the frequency error between the true frequency and the initially provided frequency, while the estimation of its unknown parameters is performed by Least Squares (LS) method. In , a relationship between iQHM and Gauss-Newton (GN) method was presented. More specifically it was claimed that iQHM is actually equivalent to an approximate Gauss-Newton method (AGN). In this correspondence, we show that iQHM is actually equivalent to a sequential version of the GN method.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:KxtntwgDAa4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A simple and fast way of generating a harmonic signal",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/841155/",
            "Abstract": "Harmonic models are widely used for text-to-speech (TTS) systems based on a concatenation of acoustic units. The fast generation of a harmonic signal is an important issue in reducing the complexity of TTS systems based on these models. In this letter, we propose a novel method of generating a harmonic signal based on delayed multi-resampled cosine functions (DMRCs). The DMRC method is compared with the direct (straightforward) synthesis method, SF, the use of the inverse fast Fourier transform, and synthesis using recurrence relations for trigonometric functions. DMRC was shown to outperform all the other techniques, reducing the complexity of the SF method by 95%.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:r0BpntZqJG4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Auditory spectrum-based pitched instrument onset detection",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5393059/",
            "Abstract": "In this paper, a method for onset detection of music signals using auditory spectra is proposed. The auditory spectrogram provides a time-frequency representation that employs a sound processing model resembling the human auditory system. Recent work on onset detection employs DFT-based features describing spectral energy and phase differences, as well as pitch-based features. These features are often combined for maximizing detection performance. Here, the spectral flux and phase slope features are derived in the auditory framework and a novel fundamental frequency estimation algorithm based on auditory spectra is introduced. An onset detection algorithm is proposed, which processes and combines the aforementioned features at the decision level. Experiments are conducted on a dataset covering 11 pitched instrument types, consisting of 1829 onsets in total. Results indicate that auditory \u2026",
            "Abstract entirety": 0,
            "Author pub id": "6ZSjpdwAAAAJ:YFjsv_pBGBYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Training generative adversarial networks with weights",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8902934/",
            "Abstract": "The impressive success of Generative Adversarial Networks (GANs) is often overshadowed by the difficulties in their training. Despite the continuous efforts and improvements, there are still open issues regarding their convergence properties. In this paper, we propose a simple training variation where suitable weights are defined and assist the training of the Generator. We provide theoretical arguments which indicate that the proposed algorithm is better than the baseline algorithm in the sense of creating a stronger Generator at each iteration. Performance results showed that the new algorithm is more accurate and converges faster in both synthetic and image datasets resulting in improvements ranging between 5% and 50%.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:ruyezt5ZtCIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Speech-in-noise intelligibility improvement based on power recovery and dynamic range compression",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6334220/",
            "Abstract": "The ability to detect speech in noise plays a significant role in our communication with others. In this work we suggest to modify the original speech signal before this is presented in the noisy environment by combining a signal to noise ratio recovery approach with dynamic range compression in order to improve the intelligibility of the speech in noise. The modification is performed under the constraint of equal global signal power before and after modifications. Experiments with speech shaped (SSN) and competing speaker (CS) types of noise at various low SNR values, show that the suggested approach outperforms state-of-the-art methods in terms of the Speech Intelligibility Index (SII) as well as in informal listening tests. Comparing with a state-of-the-art method there is an improvement of 4 dB and 8 dB in terms of SNR gain, for the SSN and the CS types of noise, respectively.",
            "Abstract entirety": 1,
            "Author pub id": "6ZSjpdwAAAAJ:HoB7MX3m0LUC",
            "Publisher": "IEEE"
        }
    ]
}]