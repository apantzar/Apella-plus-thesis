[{
    "name": "Theodoros Evgeniou",
    "romanize name": "Theodoros Evgeniou",
    "School-Department": " Decision Sciences and Technology Management",
    "University": "Insead",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 7818,
    "Scholar name": "Theodoros Evgeniou",
    "Scholar id": "GEbzNGIAAAAJ",
    "Affiliation": "INSEAD",
    "Citedby": 11471,
    "Interests": [
        "https://github.com/tevgeniou"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=GEbzNGIAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Customer relationship management in a networked world",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=13242162879101665054&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:ZeXyd9-uunAC",
            "Publisher": "INSEAD"
        },
        {
            "Title": "Can metafeatures help improve explanations of prediction models when using behavioral and textual data?",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/article/10.1007/s10994-021-05981-0",
            "Abstract": "Machine learning models built on behavioral and textual data can result in highly accurate prediction models, but are often very difficult to interpret. Linear models require investigating thousands of coefficients, while the opaqueness of nonlinear models makes things worse. Rule-extraction techniques have been proposed to combine the desired predictive accuracy of complex \u201cblack-box\u201d models with global explainability. However, rule-extraction in the context of high-dimensional, sparse data, where many features are relevant to the predictions, can be challenging, as replacing the black-box model by many rules leaves the user again with an incomprehensible explanation. To address this problem, we develop and test a rule-extraction methodology based on higher-level, less-sparse \u201cmetafeatures\u201d. We empirically validate the quality of the explanation rules in terms of fidelity, stability, and accuracy over a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:WqliGbK-hY8C",
            "Publisher": "Springer US"
        },
        {
            "Title": "Predicting progression to Alzheimer\u2019s disease from clinical and imaging data: a reproducible study",
            "Publication year": 2019,
            "Publication url": "https://hal.inria.fr/hal-02142315/",
            "Abstract": "Various machine learning approaches have been developed for predicting progression to Alzheimer\u2019s disease (AD) in patients with mild cognitive impairment (MCI) from MRI and PET data. Objective comparison of these approaches is nearly impossible because of differences at all steps, from  data management to image processing and evaluation procedures. Moreover, with a few exceptions, these papers rarely compare their results to that obtained with clinical/cognitive data only, a critical point to demonstrate the practical utility of neuroimaging in this context. We previously proposed a framework for the reproducible evaluation of ML algorithms for AD classification. This framework was applied to AD classification using unimodal neuroimaging data (T1 MRI and FDG PET). Here, we extend our previous work  to the combination of multimodal clinical and neuroimaging data for predicting progression to AD among MCI patients.  All the code is publicly available at: https://gitlab.icm-institute.org/aramislab/AD-ML.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:mvPsJ3kp5DgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Low-rank matrix factorization with attributes",
            "Publication year": 2006,
            "Publication url": "https://arxiv.org/abs/cs/0611124",
            "Abstract": "We develop a new collaborative filtering (CF) method that combines both previously known users' preferences, i.e. standard CF, as well as product/user attributes, i.e. classical function approximation, to predict a given user's interest in a particular product. Our method is a generalized low rank matrix completion problem, where we learn a function whose inputs are pairs of vectors -- the standard low rank matrix completion problem being a special case where the inputs to the function are the row and column indices of the matrix. We solve this generalized matrix completion problem using tensor product kernels for which we also formally generalize standard kernel properties. Benchmark experiments on movie ratings show the advantages of our generalized matrix completion method over the standard matrix completion one with no information about movies or people, as well as over standard multi-task or single task learning methods.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Call for Papers\u2014Marketing Science Special Issue on Big Data: Integrating Marketing, Statistics, and Computer Science: Deadline: December 16, 2013",
            "Publication year": 2013,
            "Publication url": "https://pubsonline.informs.org/doi/pdf/10.1287/mksc.2013.0794",
            "Abstract": "Call for Papers\u2014Marketing Science Special Issue on Big Data: Integrating Marketing, Statistics, \nand Computer Science Page 1 Vol. 32, No. 4, July\u2013August 2013, p. 678 ISSN 0732-2399 (print) \nISSN 1526-548X (online) http://dx.doi.org/10.1287/mksc.2013.0794 \u00a9 2013 INFORMS Call for \nPapers Marketing Science Special Issue on Big Data: Integrating Marketing, Statistics, and \nComputer Science Special Issue Editors Pradeep Chintagunta, University of Chicago Dominique \nHanssens, University of California, Los Angeles John Hauser, Massachusetts Institute of \nTechnology Special Issue Associate Editors Sinan Aral, MIT Anand Bodapati, University of \nCalifornia, Los Angeles Eric Bradlow, University of Pennsylvania Theodoros Evgeniou, \nINSEAD David Godes, University of Maryland Dan Goldstein, Microsoft Research PK Kannan, \nUniversity of Maryland Peter Lenk, University of Michigan Rob McCulloch, University of , , \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:HoB7MX3m0LUC",
            "Publisher": "INFORMS"
        },
        {
            "Title": "Regularization and statistical learning theory for data analysis",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S016794730100069X",
            "Abstract": "Problems of data analysis, like classification and regression, can be studied in the framework of Regularization Theory as ill-posed problems, or through Statistical Learning Theory in the learning-from-example paradigm. In this paper we highlight the connections between these two approaches and discuss techniques, like support vector machines and regularization networks, which can be justified in this theoretical framework and proved to be useful in a number of image analysis applications.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:W7OEmFMy1HYC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Learning with kernel machine architectures",
            "Publication year": 2000,
            "Publication url": "https://dspace.mit.edu/bitstream/handle/1721.1/86442/46804360-MIT.pdf?sequence=2",
            "Abstract": "This thesis studies the problem of supervised learning using a family of machines, namely kernel learning machines. A number of standard learning methods belong to this family, such as Regularization Networks (RN) and Support Vector Machines (SVM). The thesis presents a theoretical justification of these machines within a unified framework based on the statistical learning theory of Vapnik. The generalization performance of RN and SVM is studied within this framework, and bounds on the generalization error of these machines are proved. In the second part, the thesis goes beyond standard one-layer learning machines, and probes into the problem of learning using hierarchical learning schemes. In particular it investigates the question: what happens when instead of training one machine using the available examples we train many of them, each in a different way, and then combine the machines? Two types of ensembles are defined: voting combinations and adaptive combinations. The statistical properties of these hierarchical learning schemes are investigated both theoretically and experimentally: bounds on their generalization performance are proved, and experiments characterizing their behavior are shown.Finally, the last part of the thesis discusses the problem of choosing data representations for learning. It is an experimental part that uses the particular problem of object detection in images as a framework to discuss a number of issues that arise when kernel machines are used in practice.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:Zph67rFs4hoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Onboarding AI",
            "Publication year": 2021,
            "Publication url": "https://hal.archives-ouvertes.fr/hal-03276433/document",
            "Abstract": "In a 2018 Workforce Institute survey of 3,000 managers across eight industrialized nations, the majority of respondents described AI as a valuable productivity tool. It\u2019s easy to see why: AI brings tangible benefits in processing speed, accuracy, and consistency (machines don\u2019t make mistakes because they\u2019re tired), which is why many professionals now rely on it. Medical specialists in many fields, for example, use AI tools to help diagnose illness and make decisions about treatment. But the same respondents to that survey also expressed fears that AI would take their jobs. They are not alone. The UK\u2019s Guardian newspaper recently reported that \u201cmore than 6 million workers [in the UK alone] fear being replaced by machines.\u201d We hear these fears echoed by the academics and executives we talk to at conferences and seminars. The advantages of AI can be cast in a much darker light: why would we need humans when machines can do a better job? The prevalence of such fears suggests that organizations looking to reap the benefits of AI need to be careful about how they introduce it to the people expected to work with it. As Accenture\u2019s CIO, Andrew Wilson, argues,\u201cthe greater the degree of organizational focus on people helping AI, and AI helping people, the greater the value achieved.\u201d Accenture\u2019s research confirms this: they find that companies using AI to improve human productivity (rather than replace humans directly) significantly outperform--in decision making speed, scalability, and effectiveness, and other performance dimensions.We need, in other words, to set AI up to succeed rather than to fail\u2013just as we have to do when adopting new \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:PR6Y55bgFSsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Eliciting consumer preferences using robust adaptive choice questionnaires",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4358937/",
            "Abstract": "We propose a framework for designing adaptive choice-based conjoint questionnaires that are robust to response error. It is developed based on a combination of experimental design and statistical learning theory principles. We implement and test a specific case of this framework using Regularization Networks. We also formalize within this framework the polyhedral methods recently proposed in marketing. We use simulations as well as an online market research experiment with 500 participants to compare the proposed method to benchmark methods. Both experiments show that the proposed adaptive questionnaires outperform existing ones in most cases. This work also indicates the potential of using machine learning methods in marketing.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:M3ejUd6NZC8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Combination of Forecasts of the M3-Competition",
            "Publication year": 2002,
            "Publication url": "https://flora.insead.edu/fichiersti_wp/inseadwp2002/2002-114.pdf",
            "Abstract": "We study combinations of forecasting methods using 14 individual methods from the M3-competition. We first replicate previous work on combinations of forecasts, fixed over series, done with the data of the M-competition. We then perform a variation which shows that if one chooses a different combination of forecasts for each time series independently, then, on average across the time series: a) it is no longer true that the worst combination of a feweven up to 11-methods is better than an average single method; and b) the best possible single method in this case is better than any best possible combination. We also investigate if the different methods selected in the best possible combinations for each time series are more or less frequently used than others. We then explore the question whether choosing a good combination is easier than choosing a good single method. To this purpose we use a simple criterion \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "INSEAD"
        },
        {
            "Title": "Adoption of new economy practices by SMEs in Eastern Europe",
            "Publication year": 2003,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0263237303000094",
            "Abstract": "We study the key drivers of adoption of e-business practices by small and medium-sized enterprises (SMEs) in Eastern Europe. We discuss the results of a survey of over 900 SME managers in four Eastern European countries and Cyprus. The survey was built on the basis of a framework for studying the adoption of new economy practices that captures the dynamic interrelationships between technological transformations, firms\u2019 organisational and knowledge-creating capabilities, emerging market and industry structures, and public institutions and regulatory frameworks. Based on the framework and the survey results, we offer some reflections on new economy practices adoption.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:eQOLeE2rZwMC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "How Should Artificial Intelligence Explain Itself? Understanding Preferences for Explanations Generated by XAI Algorithms",
            "Publication year": 2021,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3877426",
            "Abstract": "Explaining firm decisions made by algorithms in customer-facing applications is increasingly required by regulators and expected by customers. While the emerging field of Explainable Artificial Intelligence (XAI) has mainly focused on developing algorithms that generate such explanations, there has not yet been sufficient consideration of customers\u2019 preferences for various types and formats of explanations. We discuss theoretically and study empirically people\u2019s preferences for explanations of algorithmic decisions. We focus on three main attributes that describe automatically-generated explanations from existing XAI algorithms (format, complexity, and specificity), and capture differences across contexts (online targeted advertising vs. loan applications) as well as heterogeneity in users\u2019 cognitive styles. Despite their popularity among academics, we find that counterfactual explanations are not popular among users, unless they follow a negative outcome (eg, loan application was denied). We also find that users are willing to tolerate some complexity in explanations. Finally, our results suggest that preferences for specific (vs. more abstract) explanations are related to the level at which the decision is construed by the user, and to the deliberateness of the user\u2019s cognitive style.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:N5tVd3kTz84C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Regularized Robust Portfolio Estimation",
            "Publication year": 2014,
            "Publication url": "https://www.taylorfrancis.com/chapters/edit/10.1201/b17558-14/regularized-robust-portfolio-estimation-theodoros-evgeniou-massimiliano-pontil-diomidis-spinellis-nick-nassuphis?context=ubx",
            "Abstract": "Diomidis Spinellis Department of Management Science and Technology, Athens University of Economics and BusinessNick Nassuphis 31 St. Martin\u2019s Lane, London11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 11.2 Finding Robust Autocorrelation Portfolios . . . . . . . . . . . . . . . . . . . . . . 23911.2.1 Financial Time Series: Notation and Definitions . . . . . . . . 239 11.2.2 Regularization Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 11.2.3 Interpretation of the Case \u2192\u221e . . . . . . . . . . . . . . . . . . . . . . . 241 11.2.4 Connection to Slow Feature Analysis . . . . . . . . . . . . . . . . . . . 24211.3 Optimization Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242 11.4 Robust Canonical Correlation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 244 11.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24611.5.1 Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 11.5.2 S \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:JQOojiI6XY0C",
            "Publisher": "Chapman and Hall/CRC"
        },
        {
            "Title": "Share Buybacks and Abnormal Returns",
            "Publication year": 2015,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2664098",
            "Abstract": "We examine the behavior of stock returns after share buyback announcements. In line with the existing literature, we find evidence of abnormal returns after buyback announcements. A market neutral portfolio that is long equally weighted (with daily rebalancing) all companies that announced within the most recent month and short the IWM ETF/market using a rolling \u03b2 estimated from the recent 250 days has average annual\" abnormal\" returns of 11.6% with a Sharpe ratio of 1.3 over the period from 2000-01-20 to 2014-12-31. Moreover, small value-stocks that under performed pre buyback announcement date outperform large growth-stocks that over performed pre buyback announcement date. A portfolio of the first type of companies, in which we hold stocks for one month after buyback announcement, shows annual\" abnormal\" returns relative to the IWM market index of 16.3% with a Sharpe ratio of 0.8 over the same period. A portfolio of the second type of companies has returns and a Sharpe ratio of 7.6% and 0.4, respectively, over the same period. Finally, we provide evidence that buybacks could potentially provide a signal for timing/predicting the overall market.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:_xSYboBqXhAC",
            "Publisher": "INSEAD Working Paper"
        },
        {
            "Title": "Disjunctions of conjunctions, cognitive simplicity, and consideration sets",
            "Publication year": 2010,
            "Publication url": "https://journals.sagepub.com/doi/abs/10.1509/jmkr.47.3.485",
            "Abstract": "The authors test methods, based on cognitively simple decision rules, that predict which products consumers select for their consideration sets. Drawing on qualitative research, the authors propose disjunctions-of-conjunctions (DOC) decision rules that generalize well-studied decision models, such as disjunctive, conjunctive, lexicographic, and subset conjunctive rules. They propose two machine-learning methods to estimate cognitively simple DOC rules. They observe consumers' consideration sets for global positioning systems for both calibration and validation data. They compare the proposed methods with both machine-learning and hierarchical Bayes methods, each based on five extant compensatory and noncompensatory rules. For the validation data, the cognitively simple DOC-based methods predict better than the ten benchmark methods on an information theoretic measure and on hit rates. The results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:kNdYIx-mwKoC",
            "Publisher": "SAGE Publications"
        },
        {
            "Title": "Regularization networks and support vector machines",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/article/10.1023/A:1018946025316",
            "Abstract": "Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples \u2013 in particular, the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classification is treated as a special case.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:u5HHmVD_uO8C",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Beyond conjoint analysis: Advances in preference measurement",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/article/10.1007/s11002-008-9046-1",
            "Abstract": "We identify gaps and propose several directions for future research in preference measurement. We structure our argument around a framework that views preference measurement as comprising three interrelated components: (1) the problem that the study is ultimately intended to address; (2) the design of the preference measurement task and the data collection approach; (3) the specification and estimation of a preference model, and the conversion into action. Conjoint analysis is only one special case within this framework. We summarize cutting edge research and identify fruitful directions for future investigations pertaining to the framework\u2019s three components and to their integration.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:UebtZRa9Y70C",
            "Publisher": "Springer US"
        },
        {
            "Title": "FACULTY OF APPLIED ECONOMICS",
            "Publication year": 2017,
            "Publication url": "https://fosterprovost.com/wp-content/uploads/2019/07/Benchmarking-Study-of-Classification-Techniques-for-Behavioral-Data.pdf",
            "Abstract": "The predictive power in ubiquitous big, behavioral data has been emphasized by previous academic research. The ultra-high dimensional and sparse characteristics, however, pose significant challenges on state-of-the-art classification techniques. Moreover, no consensus exists regarding a feasible trade-off between classification performance and computational complexity. This work provides a contribution in this direction through a systematic benchmarking study. Forty-three fine-grained behavioral data sets are analyzed with 11 classification techniques. Statistical performance comparisons enriched with learning curve analyses demonstrate two important findings. Firstly, an inherent AUC-time trade-off becomes clear, making the choice for an appropriate classifier dependent on time restrictions and data set characteristics. Logistic regression achieves the best AUC, however in the worst amount of time. Also, L2 regularization proves better than sparse L1-regularization. An attractive trade-off is found in a similarity-based technique called PSN. Secondly, the results illustrate that significant value lies in collecting and analyzing even more data, both in the instance and in the feature dimension, contrasting findings on traditional data. The results of this study provide guidance for researchers and practitioners for the selection of appropriate classification techniques, sample sizes and data features, while also providing focus in scalable algorithm design in the face of large, behavioral data.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:olpn-zPbct0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Direct-to-consumer medical machine learning and artificial intelligence applications",
            "Publication year": 2021,
            "Publication url": "https://www.nature.com/articles/s42256-021-00331-0",
            "Abstract": "Direct-to-consumer medical artificial intelligence/machine learning applications are increasingly used for a variety of diagnostic assessments, and the emphasis on telemedicine and home healthcare during the COVID-19 pandemic may further stimulate their adoption. In this Perspective, we argue that the artificial intelligence/machine learning regulatory landscape should operate differently when a system is designed for clinicians/doctors as opposed to when it is designed for personal use. Direct-to-consumer applications raise unique concerns due to the nature of consumer users, who tend to be limited in their statistical and medical literacy and risk averse about their health outcomes. This creates an environment where false alarms can proliferate and burden public healthcare systems and medical insurers. While similar situations exist elsewhere in medicine, the ease and frequency with which artificial \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:eq2jaN3J8jMC",
            "Publisher": "Nature Publishing Group"
        },
        {
            "Title": "A new approach to collaborative filtering: Operator estimation with spectral regularization",
            "Publication year": 2009,
            "Publication url": "https://www.jmlr.org/papers/volume10/abernethy09a/abernethy09a.pdf",
            "Abstract": "We present a general approach for collaborative filtering (CF) using spectral regularization to learn linear operators mapping a set of \u201cusers\u201d to a set of possibly desired \u201cobjects\u201d. In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects\u2014a feature currently lacking in existing regularization-based CF approaches\u2014using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "JMLR. org"
        },
        {
            "Title": "Stability of Randomized Learning Algorithms.",
            "Publication year": 2005,
            "Publication url": "https://www.jmlr.org/papers/volume6/elisseeff05a/elisseeff05a.pdf",
            "Abstract": "We extend existing theory on stability, namely how much changes in the training data influence the estimated models, and generalization performance of deterministic learning algorithms to the case of randomized algorithms. We give formal definitions of stability for randomized algorithms and prove non-asymptotic bounds on the difference between the empirical and expected error as well as the leave-one-out and expected error of such algorithms that depend on their random stability. The setup we develop for this purpose can be also used for generally studying randomized learning algorithms. We then use these general results to study the effects of bagging on the stability of a learning method and to prove non-asymptotic bounds on the predictive performance of bagging which have not been possible to prove with the existing theory of stability for deterministic learning algorithms. 1",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:8k81kl-MbHgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using the Timing of Past Responses in Targeting Models to Address Dilution",
            "Publication year": 2020,
            "Publication url": "https://faculty.insead.edu/spyros-zoumpoulis/documents/Dilution%20Mkt%20Sci%20Submission.pdf",
            "Abstract": "Dilution of a market can hinder the performance of models designed to target different customers with different marketing actions. We propose using the timing of past responses as a measure of dilution. Using data from two field experiments, we show that including information about the timing of past responses can significantly improve the performance of targeting policies. The timing of past response is particularly valuable for targeting future firm actions when there is more variation in the effectiveness of those actions.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:Mojj43d5GZwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reproducible evaluation of Alzheimer's Disease classification from MRI and PET data",
            "Publication year": 2018,
            "Publication url": "https://hal.inria.fr/hal-01761666/document",
            "Abstract": "Various publications have proposed machine learning approaches to classify and predict Alzheimer\u2019s disease (AD) from neuroimaging data (eg Rathore et al, 2017; Jie et al, 2015; Cuingnet et al, 2013; Young et al, 2013; Fan et al, 2008; Kl\u00f6ppel et al, 2008). The vast majority make use of the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) public dataset. However, such studies usually differ in terms of: i) subsets of subjects; ii) image processing pipelines; iii) feature extraction and selection; iv) machine learning algorithms; v) crossvalidation procedures and vi) reported evaluation metrics. These differences make it, in practice, impossible to determine which methods perform the best and difficult to assess which contributions provide a real classification improvement, eg a specific image processing or classification algorithm. We propose a framework for the reproducible evaluation of machine learning approaches in AD. The main contributions are a framework for management of three public datasets: ADNI, the Australian Imaging Biomarker and Lifestyle study (AIBL) and the Open Access Series of Imaging Studies (OASIS) and a modular set of preprocessing pipelines, feature extraction and classification methods,",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:fQNAKQ3IYiAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Regularized multi--task learning",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1014052.1014067",
            "Abstract": "Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:u_35RYKgDlwC",
            "Publisher": "ACM"
        },
        {
            "Title": "A better way to onboard AI",
            "Publication year": 2020,
            "Publication url": "http://publications.ut-capitole.fr/42843/",
            "Abstract": "In a 2018 Workforce Institute survey of 3,000 managers across eight industrialized nations, the majority of respondents described artificial intelligence as a valuable productivity tool. But respondents to that survey also expressed fears that AI would take their jobs. They are not alone. The Guardian recently reported that in the UK \u201cmore than 6 million workers fear being replaced by machines AI\u2019s advantages can be cast in a dark light: Why would humans be needed when machines can do a better job? To allay such fears, employers must set AI up to succeed rather than to fail. The authors draw on their own and others\u2019 research and consulting on AI and information systems implementation, along with organizational studies of innovation and work practices, to present a four-phase approach to implementing AI. It allows organizations to cultivate people\u2019s trust\u2014a key condition for adoption\u2014and to work toward a distributed cognitive system in which humans and artificial intelligence both continually improve",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:AXPGKjj_ei8C",
            "Publisher": "Graduate School of Business Administration, Harvard University"
        },
        {
            "Title": "SEAIR framework accounting for a personalized risk prediction score: application to the Covid-19 epidemic",
            "Publication year": 2020,
            "Publication url": "http://www.ipol.im/pub/art/2020/305/",
            "Abstract": "The aim of the present work is to provide an SEAIR framework which takes a personalized risk prediction score as an additional input. Each individual is categorized depending on his actual status with respect to the disease-moderate or severe symptoms-, and the level of risk predicted-low or high. This idea leads to a 4-fold extension of the ODE model in classical SEAIR. This model offers the possibility for policy-makers to explore differentiated containment strategies, by varying sizes for the low risk segment and varying dates for'progressive release'of the population, while exploring the discriminative capacity of the risk score, for instance through its AUC. Differential contact rates for low-risk/high-risk compartments are also included in the model. The demo allows to select contact rates and time-depending exit strategies. The hard-coded parameters correspond to the data for the Covid-19 epidemic in France, and the risk refers to the probability of being admitted in ICU upon infection. Some examples of simulations are provided.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:tkaPQYYpVKoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Epidemic Models for COVID-19 during the First Wave from February to May 2020: a Methodological Review",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2109.01450",
            "Abstract": "We review epidemiological models for the propagation of the COVID-19 pandemic during the early months of the outbreak: from February to May 2020. The aim is to propose a methodological review that highlights the following characteristics: (i) the epidemic propagation models, (ii) the modeling of intervention strategies, (iii) the models and estimation procedures of the epidemic parameters and (iv) the characteristics of the data used. We finally selected 80 articles from open access databases based on criteria such as the theoretical background, the reproducibility, the incorporation of interventions strategies, etc. It mainly resulted to phenomenological, compartmental and individual-level models. A digital companion including an online sheet, a Kibana interface and a markdown document is proposed. Finally, this work provides an opportunity to witness how the scientific community reacted to this unique situation.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:hkOj_22Ku90C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reproducible evaluation of classification methods in Alzheimer's disease: Framework and application to MRI and PET data",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1053811918307407",
            "Abstract": "A large number of papers have introduced novel machine learning and feature extraction methods for automatic classification of Alzheimer's disease (AD). However, while the vast majority of these works use the public dataset ADNI for evaluation, they are difficult to reproduce because different key components of the validation are often not readily available. These components include selected participants and input data, image preprocessing and cross-validation procedures. The performance of the different approaches is also difficult to compare objectively. In particular, it is often difficult to assess which part of the method (e.g. preprocessing, feature extraction or classification algorithms) provides a real improvement, if any. In the present paper, we propose a framework for reproducible and objective classification experiments in AD using three publicly available datasets (ADNI, AIBL and OASIS). The framework \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:LPZeul_q3PIC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Learning multiple tasks with kernel methods.",
            "Publication year": 2005,
            "Publication url": "https://www.jmlr.org/papers/volume6/evgeniou05a/evgeniou05a.pdf",
            "Abstract": "We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:XiSMed-E-HIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Agents, system and method for dynamic pricing in a reputation-brokered, agent-mediated marketplace",
            "Publication year": 2002,
            "Publication url": "https://patents.google.com/patent/US20020138402A1/en",
            "Abstract": "Agent-mediated commerce method and system, and agents for use therein. Seller agents may offer services at prices that vary over time, based on past experiences. Buyer agents may be configured by their users according to time and constraints, budget and the importance of a specific task. Buyer agents try, probabilistically, to maximize their owners' utilities (in part, by estimating the expected performance of each seller based on the reputation of that seller in the relevant marketplace. Buying agents may reveal only their time constraints and descriptions of the tasks (services) desired to the sellers. Seller agents bid for the offered tasks and base their bids at least partly on their owners' reputations, their time availability, the difficulty of the task and the current demand on the marketplace. Seller reputations are updated in a collaborative fashion based on seller performance. Seller agents employ dynamic pricing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:RHpTSmoSYBkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Pandemic Lock-down, Isolation, and Exit Policies Based on Machine Learning Predictions",
            "Publication year": 2021,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3588401",
            "Abstract": "The COVID-19 pandemic highlighted the criticality of research on pandemic management when medical solutions, such as vaccines are not available. We present a framework to combine a standard epidemiological SEIR model (susceptible-exposed-infected-removed) with equally standard machine learning classification models for clinical severity risk, defined by the risk of an individual needing intensive care (ICU) if infected. We then simulate isolation and exit policies using COVID-19 data and estimates for France as of spring 2020. We show that policies considering clinical risk predictions could relax isolation restrictions for millions of the lowest-risk population months faster while abiding to the ICU capacity at all times. Exit policies without risk predictions would exceed the ICU capacity by a multiple, or they should isolate a substantial portion of population for over a year to not overwhelm the medical system. Sensitivity analyses further decompose the impact of various elements of our models on the observed effects.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:PELIpwtuRlgC",
            "Publisher": "INSEAD Working Paper"
        },
        {
            "Title": "Optimization-based and machine-learning methods for conjoint analysis: Estimation and question design",
            "Publication year": 2007,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=KjRNGs91qhIC&oi=fnd&pg=PA230&dq=info:LeptYw_2I-MJ:scholar.google.com&ots=k4uG4oYbOO&sig=jaOb99oILWDe42MDbhLqBpTJwNE",
            "Abstract": "Soon after the introduction of conjoint analysis into marketing by Green and Rao (1972), Srinivasan and Shocker (1973a, 1973b) introduced a conjoint analysis estimation method, Linmap, based on linear programming. Linmap has been applied successfully in many situations and has proven to be a viable alternative to statistical estimation (Jain, et al. 1979, Wittink and Cattin 1981). Recent modification to deal with \u201cstrict pairs\u201d has improved the estimation accuracy with the result that, on occasion, the modified Linmap predicts holdout data better than statistical estimation based on hierarchical Bayes methods (Srinivasan 1998, Hauser, et al. 2006).The last few years have seen a Renaissance of mathematical programming approaches to the design of questions for conjoint analysis and to the estimation of conjoint partworths. These methods have been made possible due to faster computers, web-based questionnaires, and new tools in both mathematical programming and machine learning. Empirical applications and Monte Carlo simulations with these methods show promise. While the development and philosophy of such approaches is nascent, the approaches show tremendous promise for predictive accuracy, efficient question design, and ease of computation.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:_kc_bZDykSQC",
            "Publisher": "New York, NY: Springer"
        },
        {
            "Title": "Uncovering sparsity and heterogeneity in firm-level return predictability using machine learning",
            "Publication year": 2021,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3604921",
            "Abstract": "We develop an approach that combines the estimation of monthly firm-level expected returns with an assignment of firms to (possibly) latent groups, both based upon observable characteristics, using machine learning principles with linear models. The best performing methods are flexible two-stage sparse models that capture group-membership predictive relationships. Our results uncover sparsity together with firms' heterogeneity based on their characteristics, improving both predictability and interpretability. We propose statistical tests based on nonparametric bootstrapping for our results, and detail how different characteristics may matter for different groups of firms, making comparisons to the existing literature.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:XiVPGOgt02cC",
            "Publisher": "INSEAD Working Paper"
        },
        {
            "Title": "Economics of dynamic pricing in a reputation brokered agent mediated marketplace",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/article/10.1023/A:1011523612549",
            "Abstract": "We present a framework to study the microeconomic effects in a reputation brokered Agent mediated Knowledge Marketplace, when we introduce dynamic pricing algorithms. We study the market with computer simulations of multiagent interactions. In this marketplace, the seller reputations are updated in a collaborative fashion based on the performance of the user in the delegated tasks. To the best of our knowledge this is the first agent mediated marketplace where the agents use dynamic pricing based on \u201cdynamically\u201d updated reputations. The framework can be used to investigate the different equilibria reached, based on the level of intelligence of the selling agents, the level of price-importance elasticity of the buying agents, and the level of unemployment in the marketplace. Preliminary experiments addressing these issues are presented.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:hqOjcs7Dif8C",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Dynamic experiments for estimating preferences: An adaptive method of eliciting time and risk parameters",
            "Publication year": 2013,
            "Publication url": "https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1120.1570",
            "Abstract": "We present a method that dynamically designs elicitation questions for estimating risk and time preference parameters. Typically these parameters are elicited by presenting decision makers with a series of static choices between alternatives, gambles, or delayed payments. The proposed method dynamically (i.e., adaptively) designs such choices to optimize the information provided by each choice, while leveraging the distribution of the parameters across decision makers (heterogeneity) and capturing response error. We explore the convergence and the validity of our approach using simulations. The simulations suggest that the proposed method recovers true parameter values well under various circumstances. We then use an online experiment to compare our approach to a standard one used in the literature that requires comparable task completion time. We assess predictive accuracy in an out-of-sample task \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:RGFaLdJalmkC",
            "Publisher": "INFORMS"
        },
        {
            "Title": "An SEAIR model with personalised risk prediction scores and application to the Covid-19 epidemic",
            "Publication year": 2015,
            "Publication url": "https://scholar.google.com/scholar?cluster=13075478679967599854&hl=en&oi=scholarr",
            "Abstract": "The aim of the present work is to provide an SEAIR framework which takes a personalised risk prediction score as an additional input. Each individual is categorised depending on his actual status with respect to the disease-mild or severe symptoms-, and the level of risk predicted-low or high. This idea leads to a 4-fold extension of the ODE model in classical SEAIR. This model offers the possibility for policy-makers to explore differentiated containment strategies, by varying sizes for the low risk segment and varying dates for \u2018progressive release\u2019of the population, while taking into account the discriminative capacity of the risk score through its AUC. Differential contact rates for low-risk/high-risk compartments are also included in the model. The demo allows to select contact rates and time-depending exit strategies. The hardcoded parameters correspond to the data for the Covid-19 epidemic in France, and the risk \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:_B80troHkn4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Beware explanations from AI in health care",
            "Publication year": 2021,
            "Publication url": "https://www.science.org/doi/abs/10.1126/science.abg1834",
            "Abstract": "The benefits of explainable artificial intelligence are not what they appear",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:ZuybSZzF8UAC",
            "Publisher": "American Association for the Advancement of Science"
        },
        {
            "Title": "Content contributor management and network effects in a UGC environment",
            "Publication year": 2012,
            "Publication url": "https://pubsonline.informs.org/doi/abs/10.1287/mksc.1110.0639",
            "Abstract": "The success of any user-generated content website depends crucially on its asset of content contributors. How firms should invest in the acquisition and retention of content contributors represents a novel question that is particularly important for these websites. We develop a vector autoregressive (VAR) model to measure the financial values of the retention and acquisition of both contributors and content consumers. In our empirical application to a customer-to-customer marketplace, we find that contributor (seller) acquisition has the largest financial value because of their strong network effects on content consumers (buyers) and other contributors. However, the wear-in of contributors' financial values takes longer because the network effects need time to be fully realized. Our simulation-based studies (i) shed light on the value implications of \u201cenhancing network effects\u201d and (ii) quantify the revenue contributions of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:hC7cP41nSMkC",
            "Publisher": "INFORMS"
        },
        {
            "Title": "The European artificial intelligence strategy: implications and challenges for digital health",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S2589750020301126",
            "Abstract": "In February, 2020, the European Commission published a white paper on artificial intelligence (AI) as well as an accompanying communication and report. The paper sets out policy options to facilitate a secure and trustworthy development of AI and considers health to be one of its most important areas of application. We illustrate that the European Commission's approach, as applied to medical AI, presents some challenges that can be detrimental if not addressed. In particular, we discuss the issues of European values and European data, the update problem of AI systems, and the challenges of new trade-offs such as privacy, cybersecurity, accuracy, and intellectual property rights. We also outline what we view as the most important next steps in the Commission's iterative process. Although the European Commission has done good work in setting out a European approach for AI, we conclude that this approach \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:WA5NYHcadZ8C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Image representations and feature selection for multimedia database search",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1209008/",
            "Abstract": "The success of a multimedia information system depends heavily on the way the data is represented. Although there are \"natural\" ways to represent numerical data, it is not clear what is a good way to represent multimedia data, such as images, video, or sound. We investigate various image representations where the quality of the representation is judged based on how well a system for searching through an image database can perform-although the same techniques and representations can be used for other types of object detection tasks or multimedia data analysis problems. The system is based on a machine learning method used to develop object detection models from example images that can subsequently be used for examples to detect-search-images of a particular object in an image database. As a base classifier for the detection task, we use support vector machines (SVM), a kernel based learning \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:tOudhMTPpwUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Statistical learning theory: A primer",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/article/10.1023/A:1008110632619",
            "Abstract": "In this paper we first overview the main concepts of Statistical Learning Theory, a framework in which learning from examples can be studied in a principled way. We then briefly discuss well known as well as emerging learning techniques such as Regularization Networks and Support Vector Machines which can be justified in term of the same induction principle.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:YsMSGLbcyi4C",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Implementation of collaborative e-supply-chain initiatives: an initial challenging and final success case from grocery retailing",
            "Publication year": 2009,
            "Publication url": "https://journals.sagepub.com/doi/abs/10.1057/jit.2008.11",
            "Abstract": "We discuss the challenges of implementing an Internet-based platform for creating collaborative supply chains using a case study in the retail sector. The case presents how an Internet-based collaboration platform was implemented to address the strategic issue of increasing shelf availability and customer service in grocery retailing, an issue that has emerged as one of the major confrontations for the whole sector over the past years. The case presented shows the challenges of executing such strategic collaborative supply-chain initiatives which, although arguably beneficial, can be hindered by IT adoption failures. A longitudinal view of the case is presented, from an initial pilot back in 2001 to the final success in 2005. We discuss the particular challenges of the execution of Internet-enabled collaborative supply-chain initiatives as well as possible managerial actions through a simple framework we develop based \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:L8Ckcad2t8MC",
            "Publisher": "SAGE Publications"
        },
        {
            "Title": "Barriers to information management",
            "Publication year": 2005,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0263237305000472",
            "Abstract": "As the amount of data increases, organizations need to consider how to use their information assets successfully, an organizational capability that we call information intelligence. Starting from a particular area of information management, that of using market research data, and based on our interviews with market research as well as business intelligence executives, in this paper we discuss what are some key barriers to successfully using and extracting value from information. We identify three fundamental types of barriers to information intelligence and discuss specific ones in each type. One implicit message of this work is that when thinking about information management, managers can learn not only from past IT experiences, for example from business intelligence and knowledge management initiatives, but also from market research as well as decision science principles.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:Se3iqnhoufwC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Algorithms on regulatory lockdown in medicine",
            "Publication year": 2019,
            "Publication url": "https://science.sciencemag.org/content/366/6470/1202.summary",
            "Abstract": "Prioritize risk monitoring to address the \u201cupdate problem\u201d",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:1qzjygNMrQYC",
            "Publisher": "American Association for the Advancement of Science"
        },
        {
            "Title": "Network Centrality and Managerial Market-Timing Ability",
            "Publication year": 2020,
            "Publication url": "https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/network-centrality-and-managerial-markettiming-ability/1B53C7E17D703C90B19649ABD036FA1D",
            "Abstract": "We document that long-run excess returns following announcements of share buyback authorizations and insider purchases are a U-shaped function of firm centrality in the input\u2013output trade-flow network. These results conform to a model of investors endowed with a large but finite capacity for analyzing firms. Additional links weaken insiders\u2019 informational advantage in peripheral firms (simple firms whose cash flows depend on few economic links), provided investors\u2019 capacity is large enough, but eventually amplify that advantage in central firms (firms with many links) as a result of investors\u2019 limited capacity. These findings shed light on the sources of managerial market-timing ability.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:eMMeJKvmdy0C",
            "Publisher": "Cambridge University Press"
        },
        {
            "Title": "Volatility and the buyback anomaly",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0929119917307022",
            "Abstract": "The buyback anomaly survives when using the five factor Fama and French (2015) and the four factor Stambaugh and Yuan (2017) models: buyback announcements are followed by positive long-term excess returns that are positively related to (idiosyncratic) volatility, inconsistent with the low volatility anomaly. The results are consistent with the costly arbitrage hypothesis (Stambaugh et al., 2015) as well as with the market timing hypothesis: the option to take advantage of undervalued stock is more valuable when firm value is more uncertain or is more driven by company-specific information. Combining volatility with undervaluation indicators proposed by Peyer and Vermaelen (2009) improves the predictability of excess returns after buyback announcements.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:uWQEDVKXjbEC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Link discovery using graph feature tracking",
            "Publication year": 2010,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.231.978&rep=rep1&type=pdf",
            "Abstract": "We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:dhFuZR0502QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ai Regulation Is Coming How to prepare for the inevitable",
            "Publication year": 2021,
            "Publication url": "https://scholar.google.com/scholar?cluster=2904002185623723281&hl=en&oi=scholarr",
            "Abstract": "For years public concern about technological risk has focused on the misuse of personal data. But as firms embed more and more artificial intelligence in products and processes, attention is shifting to the potential for bad or biased decisions by algorithms particularly the complex, evolving kind that diagnose cancers, drive cars, or approve loans. Inevitably, many governments will feel regulation is essential to protect consumers from that risk.This article explains the moves regulators are most likely to make and the three main challenges businesses need to consider as they adopt and integrate AI. The first is ensuring fairness. That requires evaluating the impact of AI outcomes on people's lives, whether decisions are mechanical or subjective, and how equitably the AI operates across varying markets. The second is transparency. Regulators are very likely to require firms to explain how the software makes decisions \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:dQ2og3OwTAUC",
            "Publisher": "HARVARD BUSINESS SCHOOL PUBLISHING CORPORATION"
        },
        {
            "Title": "Three simple ideas for predicting progression to Alzheimer's disease",
            "Publication year": 2018,
            "Publication url": "https://hal.univ-lille.fr/LIB/hal-01891996v1",
            "Abstract": "In spite of the amount of research done in the prediction of the progression of mild cognitive impaired (MCI) subjects to Alzheimer's disease (AD), there is still room for further improvement. Sophisticated methods have been proposed, some reaching classification accuracies of up to 85%. In the present paper, we propose a combination of simple ideas to determine if they allow to obtain similar accuracies when predicting MCI to AD conversion. We present three approaches making use of ADNI database. We set a performance baseline using only demographic and clinical data (gender, education level, APOE4, MMSE, CDR sum of boxes, ADASCog) that provides a balanced accuracy of 76% (AUC of 0.84). When using imaging data, an important finding is that when an SVM is trained for discriminating between cognitive normal (CN) subjects and AD patients, and the resulting classifier is applied to MCI subjects to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:BrmTIyaxlBUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Share Buyback and Equity Issue Anomalies Revisited",
            "Publication year": 2016,
            "Publication url": "http://www.dimfunds.com/wp-content/uploads/2016/03/Share-Buyback-and-Equity-Anomaly-Revisited-Feb-2016.pdf",
            "Abstract": "We re-examine the behavior of stock returns after share buyback and equity issuance announcements using the five-factor model proposed by Fama and French (2015a). We confirm the findings of Fama and French (2015b) that the equity issue anomaly, ie, the fact that equity issues are followed by negative long-term excess returns, disappears after replacing the Fama and French (1993) three-factor model with the five-factor model. However, long term positive excess returns after buyback announcements, first reported by Ikenberry, Lakonishok, and Vermaelen (1995), remain economically and statistically significant. Moreover, the Undervaluation Index proposed by Peyer and Vermaelen (2009) remains a useful proxy for the likelihood that the buyback is driven by undervaluation. The buyback anomaly is robust over time and across sectors. Firms with low correlations with the market seem to be better at market timing, which is consistent with the hypothesis that the repurchase is driven by superior companyspecific information. Firms with high pre-announce stock returns volatility also have higher abnormal returns, which is consistent with the hypothesis that a repurchase authorization is an option to take advantage of undervaluation as argued by Ikenberry and Vermaelen (1996). This result is also consistent with Stambaugh, Yu, and Yuan (2015) who find a positive relation between stock returns and idiosyncratic volatility for undervalued stocks, and unlike the well-known low volatility anomaly. Finally, based on these findings, we develop an enhanced Undervaluation Index that improves abnormal returns relative to the index developed by \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:dshw04ExmUIC",
            "Publisher": "INSEAD Working Paper 2016/21/DSC/FIN"
        },
        {
            "Title": "Leave one out error, stability, and generalization of voting combinations of classifiers",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/article/10.1023/B:MACH.0000019805.88351.60",
            "Abstract": "We study the leave-one-out and generalization errors of voting combinations of learning machines. A special case considered is a variant of bagging. We analyze in detail combinations of kernel machines, such as support vector machines, and present theoretical estimates of their leave-one-out error. We also derive novel bounds on the stability of combinations of any classifiers. These bounds can be used to formally show that, for example, bagging increases the stability of unstable learning machines. We report experiments supporting the theoretical findings.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:UeHWp8X0CEIC",
            "Publisher": "Kluwer Academic Publishers-Plenum Publishers"
        },
        {
            "Title": "A simple algorithm for learning stable machines",
            "Publication year": 2002,
            "Publication url": "http://www.frontiersinai.com/ecai/ecai2002/pdf/p0513.pdf",
            "Abstract": "We present an algorithm for learning stable machines which is motivated by recent results in statistical learning theory. The algorithm is similar to Breiman\u2019s bagging despite some important differences in that it computes an ensemble combination of machines trained on small random sub-samples of an initial training set. A remarkable property is that it is often possible to just use the empirical error of these combinations of machines for model selection. We report experiments using support vector machines and neural networks validating the theory.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The need for a system view to regulate artificial intelligence/machine learning-based software as medical device",
            "Publication year": 2020,
            "Publication url": "https://www.nature.com/articles/s41746-020-0262-2?vid=195",
            "Abstract": "Artificial intelligence (AI) and Machine learning (ML) systems in medicine are poised to significantly improve health care, for example, by offering earlier diagnoses of diseases or recommending optimally individualized treatment plans. However, the emergence of AI/ML in medicine also creates challenges, which regulators must pay attention to. Which medical AI/ML-based products should be reviewed by regulators? What evidence should be required to permit marketing for AI/ML-based software as a medical device (SaMD)? How can we ensure the safety and effectiveness of AI/ML-based SaMD that may change over time as they are applied to new data? The US Food and Drug Administration (FDA), for example, has recently proposed a discussion paper to address some of these issues. But it misses an important point: we argue that regulators like the FDA need to widen their scope from evaluating medical AI/ML \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:5ugPr518TE4C",
            "Publisher": "Nature Publishing Group"
        },
        {
            "Title": "Dynamic pricing in a reputation\u2010brokered agent\u2010mediated marketplace",
            "Publication year": 2000,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/1099-1174(200012)9:4%3C271::AID-ISAF189%3E3.0.CO;2-1",
            "Abstract": "We describe an agent\u2010mediated marketplace, with dynamically changing reputation ratings. In this marketplace, the seller reputations are updated in a collaborative fashion based on the performance of the user in the delegated tasks. We study the market with computer simulations of multiagent interactions, where sellers learn how to price their services dynamically. We first present some simple dynamic pricing methods and we investigate the different equilibria reached, based on the level of intelligence of the selling agents, the level of price\u2013importance elasticity of the buying agents, and the level of unemployment in the marketplace. We then compare the equilibria reached with a theoretically \u2018optimal\u2019 equilibrium that we show to exist. Based on the results of this comparison we design a new dynamic pricing algorithm that we experimentally show to be almost optimal for reputation\u2010brokered agent\u2010mediated \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:YOwf2qJgpHMC",
            "Publisher": "John Wiley & Sons, Ltd."
        },
        {
            "Title": "Emerging machine learning techniques in signal processing",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/content/pdf/10.1155/2008/830381.pdf",
            "Abstract": "In the era of knowledge-based society and machine automation, there is a strong interest in machine learning (ML) techniques in a wide range of applications. The attention paid to ML methods within the DSP community is not new. Speech recognition is an example of an area where DSP and machine learning have been combined to develop efficient and robust speech recognizers. Channel equalization is another area at the intersection of ML and DSP techniques. After all, deciding upon the transmitted information symbol is nothing but a class assignment task. In cognitive radio, DSP techniques and ML methods can work together for developing algorithms for the efficient utilization of the radio spectrum. Image/video/audio coding, recognition, and retrieval are some additional typical examples where DSP and ML tie together. Another problem at the heart of the DSP community interests is the regression task \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:70eg2SAEIzsC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "Algorithmic Stability and Model Selection for Bagging using Small Sub-samples.",
            "Publication year": 2001,
            "Publication url": "https://scholar.google.com/scholar?cluster=5644761901578385349&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Artificial intelligence to support the integration of variable renewable energy sources to the power system",
            "Publication year": 2021,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0306261921002646",
            "Abstract": "The power sector is increasingly relying on variable renewable energy sources (VRE) whose share in energy production is expected to further increase. A key challenge for adopting these energy sources is their high integration costs. Artificial intelligence (AI) solutions and data-intensive technologies are already used in different parts of the electricity value chain and, due to the growing complexity and data generation potential of the future smart grid, have the potential to create significant value in the system. However, different uncertainties or lack of understanding about its impact often hinder the commitment of decision makers to invest in AI and data intensive technologies, also in the energy sector. While previous work has outlined a number of ways AI solutions can be used in the power sector, the goal of this article is to consider the value creation potential of AI in terms of managing VRE integration costs. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:SdhP9T11ey4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A regularization approach for prediction of edges and node features in dynamic graphs",
            "Publication year": 2012,
            "Publication url": "https://arxiv.org/abs/1203.5438",
            "Abstract": "We consider the two problems of predicting links in a dynamic graph sequence and predicting functions defined at each node of the graph. In many applications, the solution of one problem is useful for solving the other. Indeed, if these functions reflect node features, then they are related through the graph structure. In this paper, we formulate a hybrid approach that simultaneously learns the structure of the graph and predicts the values of the node-related functions. Our approach is based on the optimization of a joint regularization objective. We empirically test the benefits of the proposed method with both synthetic and real data. The results indicate that joint regularization improves prediction performance over the graph evolution and the node features.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:ns9cj8rnVeAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "To combine or not to combine: selecting among forecasts and their combinations",
            "Publication year": 2005,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0169207004000494",
            "Abstract": "Much research shows that combining forecasts improves accuracy relative to individual forecasts. In this paper we present experiments, using the 3003 series of the M3-competition, that challenge this belief: on average across the series, the best individual forecasts, based on post-sample performance, perform as well as the best combinations. However, this finding lacks practical value since it requires that we identify the best individual forecast or combination using post sample data. So we propose a simple model-selection criterion to select among forecasts, and we show that, using this criterion, the accuracy of the selected combinations is significantly better and less variable than that of the selected individual forecasts. These results indicate that the advantage of combining forecasts is not that the best possible combinations perform better than the best possible individual forecasts, but that it is less risky in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Two-stage models: Identifying non-compensatory heuristics for the consideration set then adaptive polyhedral methods within the consideration set",
            "Publication year": 2007,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.537.8387&rep=rep1&type=pdf",
            "Abstract": "In most product categories, consumers simplify their choices by forming a \u201cconsideration set\u201d of products (or services) that they will seriously evaluate before making a final choice (Hauser and Wernerfelt 1990, Roberts and Lattin 1991). There is evidence that simply knowing which products are in the consideration set can explain 80% of the uncertainty that could be explained with a logit-based model (Hauser 1978). This two-stage process is well-established in the academic literature as a realistic description of the process by which consumers make decisions (Payne 1976). Indeed there has been recent interest in analytic models in which the consideration stage is unobserved, but inferred from final choices (Gilbride and Allenby 2004, Jedidi and Kohli 2005).The consideration set is often motivated by recognizing that it is optimal for consumers to balance search costs (evaluating all products) with opportunity costs (evaluating only those products most likely to be chosen). Because the products identified in the first stage (consideration) will be evaluated again in the second stage (choice), it is not unreasonable that consumers use heuristic processes in the consideration stage (possibly in the choice stage, too) that focus on a relatively few important features and do so in a simple (\u201cfirst cut\u201d) non-compensatory manner (Payne, Bettman and Johnson 1988, Gigerenzer and Goldstein 1996). This is particularly true when there are a large number of alternatives in the first-stage consideration decision (Payne, Bettman and Johnson 1993). There is evidence that such heuristics might be more efficient and lead to better selections than more-complex \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:ULOm3_A8WrAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Implementing Ai Principles: Frameworks, Processes, and Tools",
            "Publication year": 2021,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3783124",
            "Abstract": "In recent years, a number of international organisations, regulators, governments, academics, and as well businesses have worked on developing principles of Artificial Intelligence (AI). Alongside the development of these principles, there is an on-going discussion on how to regulate AI in order to best align risk management with optimising potential value creation of these technologies. Risk managing AI systems will likely become a regulatory and social expectations requirement, for all sectors and for both business and government. However emphasis on how to implement the proposed AI principles and upcoming regulations in practice is more recent, and appropriate tools to achieve this still need to be identified and developed. For example, implementing so-called Responsible AI requires the development of new processes, frameworks and tools, among others. We review the current state and identify possible gaps.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:dTyEYWd-f8wC",
            "Publisher": "INSEAD Working Paper"
        },
        {
            "Title": "Uncertainty, Skill, and Analysts' Dynamic Forecasting Behavior",
            "Publication year": 2010,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1636634",
            "Abstract": "We study the risk-taking behavior of stock analysts under varying market conditions. We examine how the risk analysts take by providing bold forecasts that deviate from consensus depends on the degree of uncertainty in the environment as well as the analysts\u2019 skill level. We provide evidence that low-skill analysts become significantly bolder, hence taking more risk, and significantly less accurate when market uncertainty increases. These findings are consistent with previous experimental results that show that skill levels moderate differential attitudes towards uncertainty. We further provide evidence that the difference in an analyst\u2019s boldness between times of low and high uncertainty is a signal of the analyst\u2019s skill that predicts future forecasting accuracy over and above standard skill measures such as past forecasting accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:IWHjjKOFINEC",
            "Publisher": "INSEAD Working Paper"
        },
        {
            "Title": "Competitive dynamics in forecasting: The interaction of skill and uncertainty",
            "Publication year": 2013,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/bdm.1766",
            "Abstract": "The outcomes in many competitive tasks depend upon both skill and luck. Behavioral theories on risk taking in tournaments indicate that low\u2010skilled individuals may have incentives to take more risks than high\u2010skilled ones. We build on these theories and suggest, in addition, that when luck is more important in determining outcomes, the increase in risk taking is larger for low\u2010skilled than high\u2010skilled individuals. We test this hypothesis by analyzing stock analysts' forecasts of companies' earnings per share under market conditions that vary in volatility and thus imply different levels of luck in outcomes. Specifically, noting that forecasts that deviate widely from the consensus\u2014which is observable by the analyst\u2014potentially carry career\u2010related rewards but also reputational risks, we examine the degree of deviation from consensus exhibited by analysts of different skill levels (measured by both past forecasting \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:O3NaXMp0MMsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A benchmarking study of classification techniques for behavioral data",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s41060-019-00185-1?p=.&error=cookies_not_supported&code=3581747c-af6d-4dad-ab2b-c57c294db0a0",
            "Abstract": "The predictive power of increasingly common large-scale, behavioral data has been demonstrated by previous research. Such data capture human behavior through the actions and/or interactions of people. Their sparsity and ultra-high dimensionality pose significant challenges to state-of-the-art classification techniques. Moreover, no prior work has systematically explored the choice of methods with respect to the trade-off between classification performance and computational expense. This paper provides a contribution in this direction through a benchmarking study. Eleven classification models are compared on forty-one fine-grained behavioral data sets. Statistical performance comparisons enriched with learning curve analyses demonstrate two important findings. First, there is an inherent generalization performance versus time trade-off, rendering the choice of an appropriate classifier dependent on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:tS2w5q8j5-wC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "Image representations for object detection using kernel classifiers",
            "Publication year": 2000,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.35.9820&rep=rep1&type=pdf",
            "Abstract": "This paper presents experimental comparisons of various image representations for object detection using kernel classifiers. In particular it discusses the use of support vector machines (SVM) for object detection using as image representations raw pixel values, projections onto principal components, and Haar wavelets. General linear transformations of the images through the choice of the kernel of the SVM are considered. Experiments showing the effects of histogram equalization, a non-linear transformation, are presented. Image representations derived from probabilistic models of the class of images considered, through the choice of the kernel of the SVM, are also evaluated. Finally, we present a feature selection method using SVMs, and show experimental results.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Image representations and feature selection for multimedia database search",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1209008/",
            "Abstract": "The success of a multimedia information system depends heavily on the way the data is represented. Although there are \"natural\" ways to represent numerical data, it is not clear what is a good way to represent multimedia data, such as images, video, or sound. We investigate various image representations where the quality of the representation is judged based on how well a system for searching through an image database can perform-although the same techniques and representations can be used for other types of object detection tasks or multimedia data analysis problems. The system is based on a machine learning method used to develop object detection models from example images that can subsequently be used for examples to detect-search-images of a particular object in an image database. As a base classifier for the detection task, we use support vector machines (SVM), a kernel based learning \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:ufrVoPGSRksC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Disjunctions of Conjunctions, Cognitive Simplicity and Consideration Sets Web Appendix",
            "Publication year": 2009,
            "Publication url": "http://web.mit.edu/~hauser/www/Papers/Hauser_Toubia%20Web_Appendix%20JMR%202010.pdf",
            "Abstract": "D covariance matrix used in estimation HB compensatory f indexes features, F is the total number of features h indexes respondents (mnemonic to households), H is the total number of respondents I the identity matrix of size equal to the total number of aspects j indexes profiles, J is the total number of profiles i indexes levels within features, L is the total number of levels mjp binary indicator of whether profile j matches pattern p j m \u043e binary vector describing profile j by the patterns it matches n size of the consideration set Mj percent of respondents in the sample (\u201cmarket\u201d) that consider profile j p indexes patterns; also used for significance level in t-tests when clear in context P maximum number of patterns [LAD-DOC (P, S) estimation] Q number of partworths (compensatory model) s size of a pattern (number of aspects in a conjunction)",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:SP6oXDckpogC",
            "Publisher": "Unknown"
        },
        {
            "Title": "M&A Portfolios and Market Returns",
            "Publication year": 2015,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2546255",
            "Abstract": "M&A activity and the returns of targets and acquirers may provide information about both the companies involved and the overall market conditions. This paper shows evidence that a number of M&A related measurements, such as the number of live deals or the beta of a portfolio of cash targets, may provide predictive information about future returns of the S&P index. It also describes portfolios of M&A targets or acquirers constructed while taking into account risk management constraints, including concentration risk, amount invested, or leverage, as well as deal characteristics and potential competing bids or term improvements, and explores how the performance of these portfolios changes as these constraints are modified.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:cFHS6HbyZ2cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Understanding Consumer Preferences for Explanations Generated by XAI Algorithms",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2107.02624",
            "Abstract": "Explaining firm decisions made by algorithms in customer-facing applications is increasingly required by regulators and expected by customers. While the emerging field of Explainable Artificial Intelligence (XAI) has mainly focused on developing algorithms that generate such explanations, there has not yet been sufficient consideration of customers' preferences for various types and formats of explanations. We discuss theoretically and study empirically people's preferences for explanations of algorithmic decisions. We focus on three main attributes that describe automatically-generated explanations from existing XAI algorithms (format, complexity, and specificity), and capture differences across contexts (online targeted advertising vs. loan applications) as well as heterogeneity in users' cognitive styles. Despite their popularity among academics, we find that counterfactual explanations are not popular among users, unless they follow a negative outcome (e.g., loan application was denied). We also find that users are willing to tolerate some complexity in explanations. Finally, our results suggest that preferences for specific (vs. more abstract) explanations are related to the level at which the decision is construed by the user, and to the deliberateness of the user's cognitive style.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:VL0QpB8kHFEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Counterfactual explanation algorithms for behavioral and textual data",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1912.01819",
            "Abstract": "We study the interpretability of predictive systems that use high-dimensonal behavioral and textual data. Examples include predicting product interest based on online browsing data and detecting spam emails or objectionable web content. Recently, counterfactual explanations have been proposed for generating insight into model predictions, which focus on what is relevant to a particular instance. Conducting a complete search to compute counterfactuals is very time-consuming because of the huge dimensionality. To our knowledge, for behavioral and text data, only one model-agnostic heuristic algorithm (SEDC) for finding counterfactual explanations has been proposed in the literature. However, there may be better algorithms for finding counterfactuals quickly. This study aligns the recently proposed Linear Interpretable Model-agnostic Explainer (LIME) and Shapley Additive Explanations (SHAP) with the notion of counterfactual explanations, and empirically benchmarks their effectiveness and efficiency against SEDC using a collection of 13 data sets. Results show that LIME-Counterfactual (LIME-C) and SHAP-Counterfactual (SHAP-C) have low and stable computation times, but mostly, they are less efficient than SEDC. However, for certain instances on certain data sets, SEDC's run time is comparably large. With regard to effectiveness, LIME-C and SHAP-C find reasonable, if not always optimal, counterfactual explanations. SHAP-C, however, seems to have difficulties with highly unbalanced data. Because of its good overall performance, LIME-C seems to be a favorable alternative to SEDC, which failed for some nonlinear models to find \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:V3AGJWp-ZtQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Six Barriers to Information Intelligence",
            "Publication year": 2005,
            "Publication url": "http://flora.insead.edu/fichiersti_wp/inseadwp2004/2004-68.pdf",
            "Abstract": "Every organization needs to become information intelligent. This is a fundamental challenge whether one is searching for the business magic that leads to successful innovations, sequencing the human genome, or sorting through the fragmented signals that might warn of a 9/11 type catastrophe. For example, The 9/11 Commission Report, Executive Summary (2004), states:",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:bEWYMUwI8FkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Regularized robust portfolio estimation",
            "Publication year": 2015,
            "Publication url": "https://scholar.google.com/scholar?cluster=10721878713622125398&hl=en&oi=scholarr",
            "Abstract": "Given a vector-valued time series, we study the problem of learning the weights of a linear combination of the series\u2019 components (eg, a portfolio), which has large autocorrelation, and discuss the extension to the problem of learning two combinations, which have large cross-correlation. Both problems have been studied from different perspectives in various areas, ranging from computational neuroscience [27], to computer vision [20, 14], to information retrieval [15], among others. In this chapter, we address these problems from the point of view of robust optimization (see, eg,[5, 8] and references therein) and regularization, and highlight their application to the context of financial time series analysis; see, eg,[25].The autocorrelation (or cross-correlation) function is a quantity difficult to measure, as it depends on the lag-1 autocovariance matrix of the time series, which is typically unstable. To mitigate this problem, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:dfsIfKJdRG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Support vector machines with clustering for training with very large datasets",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-46014-4_31",
            "Abstract": "We present a method for training Support Vector Machines (SVM) classifiers with very large datasets. We present a clustering algorithm that can be used to preprocess standard training data and show how SVM can be simply extended to deal with clustered data, that is effectively training with a set of weighted examples. The algorithm computes large clusters for points which are far from the decision boundary and small clusters for points near the boundary. This implies that when SVMs are trained on the preprocessed clustered data set nearly the same decision boundary is found but the computational time decreases significantly. When the input dimensionality of the data is not large, for example of the order of ten, the clustering algorithm can significantly decrease the effective number of training examples, which is a useful feature for training SVM on large data sets. Preliminary experimental results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:KlAtU1dfN6UC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "A comparison of instance-level counterfactual explanation algorithms for behavioral and textual data: SEDC, LIME-C and SHAP-C",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s11634-020-00418-3",
            "Abstract": "Predictive systems based on high-dimensional behavioral and textual data have serious comprehensibility and transparency issues: linear models require investigating thousands of coefficients, while the opaqueness of nonlinear models makes things worse. Counterfactual explanations are becoming increasingly popular for generating insight into model predictions. This study aligns the recently proposed linear interpretable model-agnostic explainer and Shapley additive explanations with the notion of counterfactual explanations, and empirically compares the effectiveness and efficiency of these novel algorithms against a model-agnostic heuristic search algorithm for finding evidence counterfactuals using 13 behavioral and textual data sets. We show that different search methods have different strengths, and importantly, that there is much room for future research.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:Y5dfb0dijaUC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Metafeatures-based Rule-Extraction for Classifiers on Behavioral and Textual Data",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2003.04792",
            "Abstract": "Machine learning models on behavioral and textual data can result in highly accurate prediction models, but are often very difficult to interpret. Rule-extraction techniques have been proposed to combine the desired predictive accuracy of complex \"black-box\" models with global explainability. However, rule-extraction in the context of high-dimensional, sparse data, where many features are relevant to the predictions, can be challenging, as replacing the black-box model by many rules leaves the user again with an incomprehensible explanation. To address this problem, we develop and test a rule-extraction methodology based on higher-level, less-sparse metafeatures. A key finding of our analysis is that metafeatures-based explanations are better at mimicking the behavior of the black-box prediction model, as measured by the fidelity of explanations.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:wbdj-CoPYUoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Share buybacks and gender diversity",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0929119917301189",
            "Abstract": "We find that board gender diversity increases the likelihood that firms announce a buyback but long-term excess returns are significantly smaller when there is larger female representation on the board. This is consistent with the governance hypothesis: gender diversity makes it more likely that firms buy back stock to reduce agency costs of free cash flow. But because gender diversity improves the quality of public information disclosure repurchases are less driven by market timing. Moreover, when the quality of monitoring is lower because board members sit on many other boards, long-term excess returns are larger.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:u9iWguZQMMsC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Network Centrality and Managerial Market Timing Ability: Evidence from Open-Market Repurchase Announcements",
            "Publication year": 2018,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2801993",
            "Abstract": "We document a U-shaped relation between long-run excess returns after buyback authorization announcements and firm centrality in the input-output trade flow network. We rationalize this finding in a model in which investors are endowed with a large but finite capacity for analyzing firms. Investors, therefore, have a better (resp. worse) understanding of peripheral (central) firms-whose cash flows depend on few (many) economic links-than the management, ie the insiders. Hence, additional links weaken insiders' informational advantage in peripheral firms but amplify that advantage in central firms. These results provide direct support for the market timing hypothesis of buybacks.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:p2g8aNsByqUC",
            "Publisher": "INSEAD Working Paper"
        },
        {
            "Title": "Customer relationship management and networked healthcare in the pharmaceutical industry",
            "Publication year": 2004,
            "Publication url": "https://journals.sagepub.com/doi/pdf/10.1057/palgrave.jmm.5040188",
            "Abstract": "It is commonly believed, within the industry at least, that the pharmaceutical marketing function is unique. The traditional marketing mix variables such as product life cycle, price, distribution and communication are scrutinised and tightly controlled by regulators and payers; and in addition there are multiple customer groups to manage. The main pharmaceutical customer remains physicians, but patients, who are the endusers of the products, play an important role, as do insurers, governments and managed healthcare (MHC) organisations. Traditional pharmaceutical marketing strategy is based on the deployment of large salesforces. The salesforce model, the cornerstone of the industry, involves pharmaceutical sales representatives (reps)\u2018detailing\u2019a portfolio of drugs to physicians. The salesforce is usually constituted as a matrix structure, where reps are part of territorial sales teams. This model proved quite \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:3fE2CSJIrl8C",
            "Publisher": "SAGE Publications"
        },
        {
            "Title": "A note on the generalization performance of kernel classifiers with margin",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-40992-0_23",
            "Abstract": "We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the V\u03b3 dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "When machine learning goes off the rails",
            "Publication year": 2021,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3881422",
            "Abstract": "Products and services that rely on machine learning\u2014computer programs that constantly absorb new data and adapt their decisions in response\u2014don\u2019t always make ethical or accurate choices. Sometimes they cause investment losses, for instance, or biased hiring or car accidents. And as such offerings proliferate across markets, the companies creating them face major new risks. Executives need to understand and mitigate the technology\u2019s potential downside.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:JoZmwDi-zQgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bounds on the generalization performance of kernel machine ensembles",
            "Publication year": 2000,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.34.9760&rep=rep1&type=pdf",
            "Abstract": "We study the problem of learning using combinations of machines. In particular we present new theoretical bounds on the generalization performance of voting ensembles of kernel machines. Special cases considered are bagging and support vector machines. We present experimental results supporting the theoretical bounds, and describe characteristics of kernel machines ensembles suggested from the experimental findings. We also show how such ensembles can be used for fast training with very large datasets.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:0EnyYjriUFMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multi-task feature learning",
            "Publication year": 2007,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=Tbn1l9P1220C&oi=fnd&pg=PA41&dq=info:0fbPBiN4LUgJ:scholar.google.com&ots=V5ndBgks0V&sig=mJxwjfU_hTLc9W5ao4z5yr4iBV0",
            "Abstract": "We present a method for learning a low-dimensional representation which is shared across a set of multiple related tasks. The method builds upon the wellknown 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks. We show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the latter step we learn commonacross-tasks representations and in the former step we learn task-speci\ufb01c functions using these representations. We report experiments on a simulated and a real data set which demonstrate that the proposed method dramatically improves the performance relative to learning each task independently. Our algorithm can also be used, as a special case, to simply select\u2013not learn\u2013a few common features across the tasks.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:Tiz5es2fbqcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Convex multi-task feature learning",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/article/10.1007/s10994-007-5040-8",
            "Abstract": " We present a method for learning sparse representations shared across multiple tasks. This method is a generalization of the well-known single-task 1-norm regularization. It is based on a novel non-convex regularizer which controls the number of learned features common across the tasks. We prove that the method is equivalent to solving a convex optimization problem for which there is an iterative algorithm which converges to an optimal solution. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the former step it learns task-specific functions and in the latter step it learns common-across-tasks sparse representations for these functions. We also provide an extension of the algorithm which learns sparse nonlinear representations using kernels. We report experiments on simulated and real data sets which demonstrate that the proposed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:9yKSN-GCB0IC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Building Competitiveness and Business Performance with ICT: How Investments in New Technologies Can Make Companies More Competitive (working paper)",
            "Publication year": 2013,
            "Publication url": "https://scholar.google.com/scholar?cluster=12327395638010845952&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:vRqMK49ujn8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Convex multi-task feature learning",
            "Publication year": 2007,
            "Publication url": "https://www.academia.edu/download/6805418/mtl_feat.pdf",
            "Abstract": "We present a method for learning sparse representations shared across multiple tasks. This method is a generalization of the well-known singletask 1-norm regularization. It is based on a novel non-convex regularizer which controls the number of learned features common across the tasks. We prove that the method is equivalent to solving a convex optimization problem for which there is an iterative algorithm which converges to an optimal solution. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the former step it learns task-specific functions and in the latter step it learns common-across-tasks sparse representations for these functions. We also provide an extension of the algorithm which learns sparse nonlinear representations using kernels. We report experiments on simulated and real data sets which demonstrate that the proposed method can both improve the performance relative to learning each task independently and lead to a few learned features common across related tasks. Our algorithm can also be used, as a special case, to simply select\u2013not learn\u2013a few common variables across the tasks3.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:ye4kPcJQO24C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Generalized robust conjoint estimation",
            "Publication year": 2005,
            "Publication url": "https://pubsonline.informs.org/doi/abs/10.1287/mksc.1040.0100",
            "Abstract": "We introduce methods from statistical learning theory to the field of conjoint analysis for preference modeling. We present a method for estimating preference models that can be highly nonlinear and robust to noise. Like recently developed polyhedral methods for conjoint analysis, our method is based on computationally efficient optimization techniques. We compare our method with standard logistic regression, hierarchical Bayes, and the polyhedral methods using standard, widely used simulation data. The experiments show that the proposed method handles noise significantly better than both logistic regression and the recent polyhedral methods and is never worse than the best method among the three mentioned above. It can also be used for estimating nonlinearities in preference models faster and better than all other methods. Finally, a simple extension for handling heterogeneity shows promising results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "INFORMS"
        },
        {
            "Title": "Epidemic models for personalised COVID-19 isolation and exit policies using clinical risk predictions",
            "Publication year": 2020,
            "Publication url": "https://www.medrxiv.org/content/medrxiv/early/2020/05/12/2020.04.29.20084707.full.pdf",
            "Abstract": "Background: In mid April 2020, with more than 2.5 billion people in the world following social distancing measures due to COVID-19, governments are considering relaxing lock-down. We combined individual clinical risk predictions with epidemic modelling to examine simulations of  isolation and exit policies. Methods: We developed a method to include personalised risk predictions in epidemic models based on data science principles. We extended a standard susceptible-exposed-infected-removed (SEIR) model to account for predictions of severity, defined by the risk of an individual needing intensive care in case of infection. We studied example isolation policies using simulations with the risk-extended epidemic model, using COVID-19 data and estimates in France as of mid April 2020 (4 000 patients in ICU, around 7 250 total ICU beds occupied at the peak of the outbreak, 0.5% percent of patients requiring ICU upon infection).  We considered scenarios varying in the discrimination performance of a risk prediction model, in the degree of social distancing, and in the severity rate upon infection. Confidence intervals were obtained using an Approximate Bayesian Computation approach. The framework may be used with other epidemic models, with other risk predictions, and for other epidemic outbreaks. Findings: Based on the data for France as of mid April 2020, simulations indicated that an exit policy considering clinical risk predictions starting on May 11, as planned by the government, could enable to immediately relax restrictions for an extra 10% (6 700 000 people) or more of the lowest-risk population, and consequently relax the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:bnK-pcrLprsC",
            "Publisher": "Cold Spring Harbor Laboratory Press"
        },
        {
            "Title": "When machine learning goes off the rails a guide to managing the risks",
            "Publication year": 2021,
            "Publication url": "https://pennstate.pure.elsevier.com/en/publications/when-machine-learning-goes-off-the-rails-a-guide-to-managing-the-",
            "Abstract": "When machine learning goes off the rails a guide to managing the risks \u2014 Penn State Skip to \nmain navigation Skip to search Skip to main content Penn State Home Penn State Logo Help & \nFAQ Home Researchers Research output Research Units Core Facilities Grants & Projects \nPrizes Activities Search by expertise, name or affiliation When machine learning goes off the \nrails a guide to managing the risks Boris Babic, I. Glenn Cohen, Theodoros Evgeniou, Sara \nGerke Research output: Contribution to journal \u203a Article \u203a peer-review Overview Original \nlanguage English (US) Pages (from-to) 1-10 Number of pages 10 Journal Harvard business \nreview Volume 2021 Issue number January-February State Published - Jan 1 2021 All Science \nJournal Classification (ASJC) codes Business and International Management Business, \nManagement and Accounting(all) Economics and Econometrics Strategy and Management of to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:UHK10RUVsp4C",
            "Publisher": "Harvard Business School Publishing"
        },
        {
            "Title": "Iteratively refining SVMs",
            "Publication year": 2015,
            "Publication url": "https://scholar.google.com/scholar?cluster=13008995763305784676&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:1yQoGdGgb4wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Analysts\u2019 dynamic forecasting behavior: The interaction of skill and luck",
            "Publication year": 2011,
            "Publication url": "https://scholar.google.com/scholar?cluster=10917047017993545421&hl=en&oi=scholarr",
            "Abstract": "Many activities resemble contests where outcomes depend on both the skill of competitors and random factors, ie, luck. We analyze risk taking behavior in contests that involve forecasts made by stock analysts operating under market conditions that vary in uncertainty (captured by volatility) and thus imply different levels of luck in outcomes. Noting that analysts can take risks if their forecasts deviate from the consensus of their peers, we examine how such risk taking depends on the degree of volatility in the forecasting environment as well as the analysts\u2019 skill level. We show that low-skill analysts take significantly more risk by making forecasts that deviate from consensus when market volatility increases. These forecasts are ex post significantly less accurate. We further provide evidence that the difference in analysts\u2019 risk-taking between times of low and high volatility is a signal of the analyst\u2019s skill that predicts future \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:TFP_iSt0sucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Information integration and information strategies for adaptive enterprises",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0263237302000920",
            "Abstract": "Enterprise information systems, such as Enterprise Resource Planning (ERP), have often been criticized for their rigidity. Alternatively, global, matrixed enterprises often follow a federated information systems approach. The first type of enterprises, which we call the standardized enterprises, lack flexibility, while the second type, which we call decentralized enterprises, lack visibility. To create what we shall call an adaptive enterprise the information systems strategy should achieve both these goals: visibility and flexibility. I discuss the problems associated with the lack of either of these two, and how information integration technology, within the enterprise application integration space, can lead to the creation of adaptive enterprises.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Estimation of Risk and Time Preferences: Response Error, Heterogeneity, Adaptive Questionnaires, and Experimental Evidence from Mortgagers",
            "Publication year": 2010,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1655775",
            "Abstract": "We develop a methodology for the measurement of the parameters of cumulative prospect theory and time discounting models based on tools from the preference measurement literature. These parameters are typically elicited by presenting decision makers with a series of choices between hypothetical alternatives, gambles or delayed payments. We present a method for adaptively designing the sets of hypothetical choices presented to decision makers, and a method for estimating the preference function parameters which capture interdependence across decision makers as well as response error. We apply our questionnaire design and estimation methods to a study of the characteristics of homeowners who owe more on their mortgage than the current value of the underlying real estate asset. Our estimates indicate that such homeowners have larger discount rates and present bias than others, but do not differ in their risk preferences.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:HDshCWvjkbEC",
            "Publisher": "INSEAD Working Paper"
        },
        {
            "Title": "Ionescu-somers, A., 27",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=6743918149447799502&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:ldfaerwXgEUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Machine Learning Strategies for Complex Tasks",
            "Publication year": 2000,
            "Publication url": "http://eis.bris.ac.uk/~enicgc/pubs/2000/ieee_ras.pdf",
            "Abstract": "In this paper we begin by reviewing recent research on kernel methods. This subject provides a systematic and principled approach to machine learning tasks such as classification, regression, novelty detection, and query learning. Advanced robots are examples of complex autonomous systems which must be able to complete sophisticated operations involving one or more of these tasks. For example, regression is relevant to modeling the coordinate transformations of manipulator kinematics. Analysis of images in a scene may require detection of novel objects and classification of known objects. We outline past work illustrating successful application of kernel methods to object recognition in scenes. The complex machine vision and control operations inherent in humanoid robotics suggests the area is an excellent test-bed for co-operatively integrating different machine learning tasks and stimulating future research directions.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:-f6ydRqryjwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reproducible evaluation of methods for predicting progression to Alzheimer's disease from clinical and neuroimaging data",
            "Publication year": 2019,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10949/109490V/Reproducible-evaluation-of-methods-for-predicting-progression-to-Alzheimers-disease/10.1117/12.2512430.short",
            "Abstract": "Various machine learning methods have been proposed for predicting progression of patients with mild cognitive impairment (MCI) to Alzheimer\u2019s disease (AD) using neuroimaging data. Even though the vast majority of these works use the public dataset ADNI, reproducing their results is complicated because they often do not make available elements that are essential for reproducibility, such as selected participants and input data, image preprocessing and cross-validation procedures. Comparability is also an issue. Specially, the influence of different components like preprocessing, feature extraction or classification algorithms on the performance is difficult to evaluate. Finally, these studies rarely compare their results to models built from clinical data only, a critical aspect to demonstrate the utility of neuroimaging. In our previous work,1, 2 we presented a framework for reproducible and objective classification \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:q3oQSFYPqjQC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "Iteratively refining SVMs using priors",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7363740/",
            "Abstract": "Research on scalable machine learning algorithms has gained a considerable amount of traction since the exponential growth in data assets during the past decades. Many Big Data applications resort to somewhat \"simple\" data modelling techniques due to the computational constraints associated with more complex models. Simple models, while being very efficient to estimate, often fail to capture some of the finer details of more complex datasets. In this manuscript, we explore the idea that complex large scale classification can be tractable using a process of iterative refining. In such a process, we focus on non-linearities of the data only after having first found an approximate linear model. This knowledge is then incorporated into the nonlinear model implicitly, allowing the non-linear model to focus on important parts of the data after a rough first estimation. This in turn reduces overall training time and allows for a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:CHSYGLWDkRkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Making Sense of Customer Relationship Management Strategies in a Technology-Driven World",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1057/9780230524545_13",
            "Abstract": "In a world where rapid technological changes lead to continuously innovative forms of interactivity and connectivity among companies and customers, Customer Relationship Management (CRM) has emerged as a key managerial issue that companies increasingly need to master. The available CRM strategic options are now numerous. This chapter provides a framework that can enable managers to have a better understanding of the current status of CRM and future trends in their industry. We introduce a model that looks simultaneously at the two main levers of change in the market today: the increasing interactivity with customers and the networking effect among the market elements, namely customers and companies. Using examples from today\u2019s market we point out that scoring high in all CRM dimensions is not necessarily ideal for each and every company and we identify the key factors that should be \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:mB3voiENLucC",
            "Publisher": "Palgrave Macmillan, London"
        },
        {
            "Title": "Yet another ADNI machine learning paper? Paving the way towards fully-reproducible research on classification of Alzheimer\u2019s disease",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-67389-9_7",
            "Abstract": "In recent years, the number of papers on Alzheimer\u2019s disease classification has increased dramatically, generating interesting methodological ideas on the use machine learning and feature extraction methods. However, practical impact is much more limited and, eventually, one could not tell which of these approaches are the most efficient. While over 90% of these works make use of ADNI an objective comparison between approaches is impossible due to variations in the subjects included, image pre-processing, performance metrics and cross-validation procedures. In this paper, we propose a framework for reproducible classification experiments using multimodal MRI and PET data from ADNI. The core components are: (1) code to automatically convert the full ADNI database into BIDS format; (2) a modular architecture based on Nipype in order to easily plug-in different classification and feature extraction tools; (3 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:sSrBHYA8nusC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "A better way to onboard ai understand it as a tool to assist rather than replace people.",
            "Publication year": 2020,
            "Publication url": "https://nyuscholars.nyu.edu/en/publications/a-better-way-to-onboard-ai-understand-it-as-a-tool-to-assist-rath",
            "Abstract": "A better way to onboard ai understand it as a tool to assist rather than replace people. \u2014 NYU \nScholars Skip to main navigation Skip to search Skip to main content NYU Scholars Logo Help \n& FAQ Home Profiles Research Units Research output Search by expertise, name or affiliation \nA better way to onboard ai understand it as a tool to assist rather than replace people. Boris \nBabic, Daniel L. Chen, Theodoros Evgeniou, Anne Laure Fayard Research output: Contribution \nto journal \u203a Comment/debate \u203a peer-review Overview Original language English (US) Pages \n(from-to) 2-11 Number of pages 10 Journal Harvard Business Review Volume 2020 Issue \nnumber July-August State Published - Jul 1 2020 ASJC Scopus subject areas Business \nand International Management Business, Management and Accounting(all) Economics and \nEconometrics Strategy and Management Management of Technology and Innovation files .\u2026",
            "Abstract entirety": 0,
            "Author pub id": "GEbzNGIAAAAJ:5awf1xo2G04C",
            "Publisher": "Harvard Business School Publishing"
        },
        {
            "Title": "Scalable Delivery of Dynamic Content Using a Cooperative Edge Cache Grid",
            "Publication year": 2007,
            "Publication url": "https://scholar.google.com/scholar?cluster=18419079692545376007&hl=en&oi=scholarr",
            "Abstract": "We propose a framework for designing adaptive choice-based conjoint questionnaires that are robust to response error. It is developed based on a combination of experimental design and statistical learning theory principles. We implement and test a specific case of this framework using Regularization Networks. We also formalize within this framework the polyhedral methods recently proposed in marketing. We use simulations, as well as an online market research experiment with 500 participants, to compare the proposed method to benchmark methods. Both experiments show that the proposed adaptive questionnaires outperform the existing ones in most cases. This work also indicates the potential of using machine-learning methods in marketing.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:eflP2zaiRacC",
            "Publisher": "Institute of Electrical and Electronics Engineers, Inc., 345 E. 47 th St. NY NY 10017-2394 USA"
        },
        {
            "Title": "A convex optimization approach to modeling consumer heterogeneity in conjoint estimation",
            "Publication year": 2007,
            "Publication url": "https://pubsonline.informs.org/doi/abs/10.1287/mksc.1070.0291",
            "Abstract": "We propose and test a new approach for modeling consumer heterogeneity in conjoint estimation based on convex optimization and statistical machine learning. We develop methods both for metric and choice data. Like hierarchical Bayes (HB), our methods shrink individual-level partworth estimates towards a population mean. However, while HB samples from a posterior distribution that is influenced by exogenous parameters (the parameters of the second-stage priors), we minimize a convex loss function that depends only on endogenous parameters. As a result, the amounts of shrinkage differ between the two approaches, leading to different estimation accuracies. In our comparisons, based on simulations as well as empirical data sets, the new approach overall outperforms standard HB (i.e., with relatively diffuse second-stage priors) both with metric and choice data.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:5nxA0vEk-isC",
            "Publisher": "INFORMS"
        },
        {
            "Title": "Learning with Kernel Machines and their Ensembles",
            "Publication year": 2000,
            "Publication url": "https://scholar.google.com/scholar?cluster=18056393928918184463&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:BqipwSGYUEgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using Past Responders to Target Non-Responders",
            "Publication year": 2017,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3038737",
            "Abstract": "Firms must often decide how to target households that did not respond to past promotions. For example, when prospecting for new customers, households that purchase are no longer eligible, and so the remaining households are an increasingly pure pool of non-responders. Past response data reflects the behavior of households that responded, and these households generally differ from the rest in unobservable ways. We show that despite this, the decisions of the past responders in a group can help firms target the remaining non-responders. In particular, the timing of past responses can help to reveal whether a geographic region is (a) exhausted of future responders, or (b) the future responders just require additional exposures. We use this insight to develop three different timing measures. The measures are calculated and validated using a sequence of mailings in a large-scale field experiment. The measures perform well, even when some regions have only a handful of past responses to calibrate them. They confirm that the decisions of past responders can help firms target non-responders.",
            "Abstract entirety": 1,
            "Author pub id": "GEbzNGIAAAAJ:geHnlv5EZngC",
            "Publisher": "INSEAD Working Paper"
        }
    ]
}]