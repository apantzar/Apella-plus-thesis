[{
    "name": "\u039c\u03af\u03bd\u03c9\u03c2 \u0393\u03b1\u03c1\u03bf\u03c6\u03b1\u03bb\u03ac\u03ba\u03b7\u03c2",
    "romanize name": "Minos Garofalakis",
    "School-Department": "\u0397\u03bb\u03b5\u03ba\u03c4\u03c1\u03bf\u03bd\u03b9\u03ba\u03ce\u03bd \u039c\u03b7\u03c7 \u03ba\u03b1\u03b9 \u039c\u03b7\u03c7 \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "tuc",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 3674,
    "Scholar name": "Minos Garofalakis",
    "Scholar id": "WemX9rAAAAAJ",
    "Affiliation": "Director, IMSI, Athena Research Center & Professor, School of ECE, Technical Univ. of Crete",
    "Citedby": 15206,
    "Interests": [
        "Big data analytics",
        "Data streams",
        "Data mining and machine learning",
        "Data Management",
        "Database Systems"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=WemX9rAAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Data-Independent Space Partitionings for Summaries",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3452021.3458316",
            "Abstract": "Histograms are a standard tool in data management for describing multidimensional data. It is often convenient or even necessary to define data independent histograms, to partition space in advance without observing the data itself. Specific motivations arise in managing data when it is not suitable to frequently change the boundaries between histogram cells. For example, when the data is subject to many insertions and deletions; when data is distributed across multiple systems; or when producing a privacy-preserving representation of the data. The baseline approach is to consider an equiwidth histogram, ie, a regular grid over the space. However, this is not optimal for the objective of splitting the multidimensional space into (possibly overlapping) bins, such that each box can be rebuilt using a set of non-overlapping bins with minimal excess (or deficit) of volume. Thus, we investigate how to split the space into \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:vDZJ-YLwNdEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Optimal configuration of OSPF aggregates",
            "Publication year": 2002,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.2638&rep=rep1&type=pdf",
            "Abstract": "Open Shortest Path First (OSPF) is a popular protocol for routing within an Autonomous System (AS) domain. In order to scale for large networks containing hundreds and thousands of subnets, OSPF supports a twolevel hierarchical routing scheme through the use of OSPF areas. Each area consists of a set of interconnected subnets and traffic across areas is handled by routers attached to two or more areas, known as Area Border Routers (ABRs). OSPF ABRs are typically configured to aggregate the subnet addresses in their areas and to advertise these aggregates in the remainder of the network instead of individual subnet addresses. Address aggregation within areas is a crucial requirement for scaling OSPF to large AS domains, as it results in significant reductions in routing table sizes, smaller link-state databases, and less network traffic to synchronize the router link-state databases. On the other hand, address aggregation also implies loss of information about the length of the shortest path from the ABR to each subnet; typically, an ABR associates each advertised aggregate with a single weight that is common for all subnets covered by the aggregate. This, in turn, can lead to the selection of significantly suboptimal OSPF routing paths between source-destination subnet pairs that span different areas. In this paper, we address the important practical problem of configuring OSPF aggregates to minimize the error in OSPF shortest path computations due to subnet aggregation. We first develop an optimal dynamic programming algorithm that, given an upper bound k on the number of aggregates to be advertised by the ABRs and a weight \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:cFHS6HbyZ2cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Determination of physical topology of a communication network",
            "Publication year": 2004,
            "Publication url": "https://patents.google.com/patent/US6697338B1/en",
            "Abstract": "Physical connectivity is determined between elements such as switches and routers in a multiple subnet communication network. Each element has one or more interfaces each of which is physically linked with an interface of another network element. Address sets are generated for each interface of the network elements, wherein members of a given address set correspond to network elements that can be reached from the corresponding interface for which the given address set was generated. The members of first address sets generated for corresponding interfaces of a given network element, are compared with the members of second address sets generated for corresponding interfaces of network elements other than the given element. A set of candidate connections between an interface of the given network element and one or more interfaces of other network elements, are determined. If more than one \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:YFjsv_pBGBYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Approximating Multidimensional Range Counts with Maximum Error Guarantees",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9458917/",
            "Abstract": "We address the problem of compactly approximating multidimensional range counts with a guaranteed maximum error and propose a novel histogram-based summary structure, termed SliceHist. The key idea is to operate a grid histogram in an approximately rank-transformed space, where the data points are more uniformly distributed and each grid slice contains only a small number of points. Then, the points of each slice are summarised again using the same technique. As each query box partially intersects only few slices and each grid slice has few data points, the summary is able to achieve tight error guarantees. In experiments and through analysis of non-asymptotic formulas we show that SliceHist is not only competitive with existing heuristics in terms of performance, but additionally offers tight error guarantees.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:artPoR2Yc-kC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Scalable ranked publish/subscribe",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/1453856.1453906",
            "Abstract": "Publish/subscribe (pub/sub) systems are designed to efficiently match incoming events (e.g., stock quotes) against a set of subscriptions (e.g., trader profiles specifying quotes of interest). However, current pub/sub systems only support a simple binary notion of matching: an event either matches a subscription or it does not; for instance, a stock quote will either match or not match a trader profile. In this paper, we argue that this simple notion of matching is inadequate for many applications where only the \"best\" matching subscriptions are of interest. For instance, in targeted Web advertising, an incoming user (\"event\") may match several different advertiser-specified user profiles (\"subscriptions\"), but given the limited advertising real-estate, we want to quickly discover the best (e.g., most relevant) ads to display.To address this need, we initiate a study of ranked pub/sub systems. We focus on the case where \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:maZDTaKrznsC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Extended wavelets for multiple measures",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1242524.1242527",
            "Abstract": "Several studies have demonstrated the effectiveness of the Haar wavelet decomposition as a tool for reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast, accurate approximate answers to user queries. Although originally designed for minimizing the overall mean-squared (i.e., L2-norm) error in the data approximation, recently proposed methods also enable the use of Haar wavelets in minimizing other error metrics, such as the relative error in data value reconstruction, which is arguably the most important for approximate query answers. Relatively little attention, however, has been paid to the problem of using wavelet synopses as an approximate query answering tool over complex tabular datasets containing multiple measures, such as those typically found in real-life OLAP applications. Existing decomposition approaches will either operate on each measure \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:JV2RwH3_ST0C",
            "Publisher": "ACM"
        },
        {
            "Title": "Fast approximate wavelet tracking on streams",
            "Publication year": 2011,
            "Publication url": "https://patents.google.com/patent/US7885911B2/en",
            "Abstract": "The first fast solution to the problem of tracking wavelet representations of one-dimensional and multi-dimensional data streams based on a stream synopsis, the Group-Count Sketch (GCS) is provided. By imposing a hierarchical structure of groups over the data and applying the GCS, our algorithms can quickly recover the most important wavelet coefficients with guaranteed accuracy. A tradeoff between query time and update time is established, by varying the hierarchical structure of groups, allowing the right balance to be found for specific data streams. Experimental analysis confirmed this tradeoff, and showed that all the methods significantly outperformed previously known methods in terms of both update time and query time, while maintaining a high level of accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:JoZmwDi-zQgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Functional Geometric Monitoring for Distributed Streams.",
            "Publication year": 2019,
            "Publication url": "http://users.softnet.tuc.gr/~minos/Papers/edbt19.pdf",
            "Abstract": "The explosion in the amount of data generated online is entering its next phase, as the Internet of Things (IoT) is set to increase the number of networked data sources by orders of magnitude in the near future. Thus, there is a clear need for ever more scalable techniques for distributed stream processing, where the tsunami of data generated by networked nodes is filtered and summarized in near-real time at (or, near) the source, drastically reducing the required communication costs.Motivated by such needs, there has been significant research effort on the distributed functional monitoring problem, over the past decade. Much effort has concentrated on worst-case communication complexity for particular types of important queries, such as frequency moments, heavy hitters, percentiles, distinct elements etc. Early on, it became apparent that many of these problems can have very bad worst-case performance, unless certain assumptions were made, in particular with respect to monotonicity, about the input streams and/or the monitored functions.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:nZcligLrVowC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Processing complex aggregate queries over data streams",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/564691.564699",
            "Abstract": "Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (ie, data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed. In this paper, we consider the problem of approximately answering general aggregate SQL queries over continuous data streams with limited memory. Our method relies on randomizing techniques that compute small\" sketch\" summaries of the streams that can then be used to provide approximate answers to aggregate queries with provable guarantees on the approximation error. We also demonstrate how \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:9yKSN-GCB0IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Grammar and method for integrating XML data from multiple sources",
            "Publication year": 2015,
            "Publication url": "https://patents.google.com/patent/US8949710B2/en",
            "Abstract": "A grammar for mapping a first grouping of XML data into a second grouping of XML data and a method for accomplishing same to incorporate the first grouping into the second grouping. The grammar includes a first rule for computing a first child element attribute and a second rule for computing a second parent element attribute. The first rule and second rule vary according to a production of an element type of the first grouping. The element types include PCDATA, disjunctive, conjunctive and Kleene star, each having a unique rule set for defining inherited and synthesized attributes of the parent and child elements. The method includes the step of executing a mapping of a first grouping having at least one parent element and a set of corresponding child elements into a second grouping in accordance with the grammar rules based on the production of the element type.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:sSrBHYA8nusC",
            "Publisher": "Unknown"
        },
        {
            "Title": "33rd International Conference on",
            "Publication year": 2007,
            "Publication url": "https://www.vldb.org/conf/2007/frontmatter/title.pdf",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:q3oQSFYPqjQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fractional XSketch Synopses for XML Databases",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-30081-6_14",
            "Abstract": "A key step in the optimization of declarative queries over XML data is estimating the selectivity of path expressions, ie, the number of elements reached by a specific navigation pattern through the XML data graph. Recent studies have introduced XSketch structural graph synopses as an effective, space-efficient tool for the compile-time estimation of complex path-expression selectivities over graph-structured, schema-less XML data. Briefly, XSketch es exploit localized graph stability and well-founded statistical assumptions to accurately approximate the path and branching distribution in the underlying XML data graph. Empirical results have demonstrated the effectiveness of XSketch summaries over real-life and synthetic data sets, and for a variety of path-expression workloads. In this paper, we introduce fractional XSketch es (f XSketches) a simple, yet intuitive and very effective generalization of the basic XSketch \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:ZHo1McVdvXMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Building decision trees with constraints",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/article/10.1023/A:1022445500761",
            "Abstract": "Classification is an important problem in data mining. Given a database of records, each with a class label, a classifier generates a concise and meaningful description for each class that can be used to classify subsequent records. A number of popular classifiers construct decision trees to generate class models. Frequently, however, the constructed trees are complex with hundreds of nodes and thus difficult to comprehend, a fact that calls into question an often-cited benefit that decision trees are easy to interpret. In this paper, we address the problem of constructing \u201csimple\u201d decision trees with few nodes that are easy for humans to interpret. By permitting users to specify constraints on tree size or accuracy, and then building the \u201cbest\u201d tree that satisfies the constraints, we ensure that the final tree is both easy to understand and has good accuracy. We develop novel branch-and-bound algorithms for pushing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:j3f4tGmQtD8C",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Filtering, punctuation, windows and synopses",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/0-387-25229-0_3",
            "Abstract": "This chapter addresses some of the problems raised by the high-volume, nonterminating nature of many data streams. We begin by outlining challenges for query processing over such streams, such as outstripping CPU or memory resources, operators that wait for the end of input and unbounded query state. We then consider various techniques for meeting those challenges. Filtering attempts to reduce stream volume in order to save on system resources. Punctuations incorporate semantics on the structure of a stream into the stream itself, and can help unblock query operators and reduce the state they must retain. Windowing modifies a query so that processing takes place on finite subsets of full streams. Synopses are compact, efficiently maintained summaries of data that can provide approximate answers to particular queries.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:P5F9QuxV20EC",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "Deterministic wavelet thresholding for general-error metrics",
            "Publication year": 2010,
            "Publication url": "https://patents.google.com/patent/US8055088B2/en",
            "Abstract": "Novel, computationally efficient schemes for deterministic wavelet thresholding with the objective of optimizing maximum-error metrics are provided. An optimal low polynomial-time algorithm for one-dimensional wavelet thresholding based on a new dynamic-programming (DP) formulation is provided that can be employed to minimize the maximum relative or absolute error in the data reconstruction. Directly extending a one-dimensional DP algorithm to multi-dimensional wavelets results in a super-exponential increase in time complexity with the data dimensionality. Thus, novel, polynomial-time approximation schemes (with tunable approximation guarantees for the target maximum-error metric) for deterministic wavelet thresholding in multiple dimensions are also provided.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:r_AWSJRzSzQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Admission control system and method for media-on-demand servers",
            "Publication year": 2001,
            "Publication url": "https://patents.google.com/patent/US6330609B1/en",
            "Abstract": "In a server system having a predetermined total bandwidth providing data files to a plurality of clients in response to requests received from the clients, a method for providing admission control comprises the steps of allocating a plurality of channel partitions to a plurality of channel groups such that each channel group includes one or more of the channel partitions. The system then obtains a channel group number based on the length of the data file requested by one of the clients and transmits the requested data file when a channel group corresponding to the obtained channel group number contains a vacant channel partition.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "System and method for optimizing open shortest path first aggregates and autonomous network domain incorporating the same",
            "Publication year": 2006,
            "Publication url": "https://patents.google.com/patent/US7082473B2/en",
            "Abstract": "Systems and method for selecting open shortest path first (OSPF) aggregates and aggregate weights for a particular area. In one embodiment, an aggregate selecting system includes:(1) a database for containing data pertaining to candidate OSPF aggregates and corresponding weights and (2) an aggregate selector, associated with the database, that selects at least a subset of the OSPF aggregates such that the shortest path length between the particular source and destination subnets resulting from advertisement of a set of weighted aggregates approaches the shortest path length between the particular source and destination subnets irrespective of the advertisement. In one embodiment, a weight selection system includes:(1) a database for containing data pertaining to candidate OSPF aggregates and (2) a weight assigner, associated with the database, that assigns, for the OSPF aggregates, weights based \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:bFI3QPDXJZMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "FERARI: a prototype for complex event processing over streaming multi-cloud platforms",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2882903.2899395",
            "Abstract": "In this demo, we present FERARI, a prototype that enables real-time Complex Event Processing (CEP) for large volume event data streams over distributed topologies. Our prototype constitutes, to our knowledge, the first complete, multi-cloud based end-to-end CEP solution incorporating: a) a user-friendly, web-based query authoring tool,(b) a powerful CEP engine implemented on top of a streaming cloud platform,(c) a CEP optimizer that chooses the best query execution plan with respect to low latency and/or reduced inter-cloud communication burden, and (d) a query analytics dashboard encompassing graph and map visualization tools to provide a holistic picture with respect to the detected complex events to final stakeholders. As a proof-of-concept, we apply FERARI to enable mobile fraud detection over real, properly anonymized, telecommunication data from T-Hrvatski Telekom network in Croatia.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:xtoqd-5pKcoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "System and method for determining the physical topology of a network having multiple subnets",
            "Publication year": 2009,
            "Publication url": "https://patents.google.com/patent/US7535911B2/en",
            "Abstract": "A system for, and method of, determining a physical topology of a network having multiple subnets. In one embodiment, the system includes:(1) a skeleton path initializer that uses addressing information from elements in the network to develop a collection of skeleton paths of direct physical connections between labeled ones of the elements, the skeleton paths traversing multiple of the subnets and (2) a skeleton path refiner, coupled to the skeleton path initializer, that refines the collection by inferring, from the direct physical connections and path constraints derived therefrom, other physical connections in the skeleton paths involving unlabeled ones of the elements.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:1yQoGdGgb4wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Indentification of altered MET network in oral cancer progression based on nonparametric network design",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6609565/",
            "Abstract": "Oral cancer is characterized by multiple genetic events such as alterations of a number of oncogenes and tumour suppressor genes. The aim of this study is to identify genes and their functional interactions that may play a crucial role on a specific disease-state, especially during oral cancer progression. We examine gene interaction networks on blood genomic data, obtained from twenty three oral cancer patients at four different time stages. We generate the gene-gene networks from sparse experimental temporal data using two methods, Partial Correlations and Kernel Density Estimation, in order to capture genetic interactions. The network study reveals an altered MET (hepatocyte growth factor receptor) network during oral cancer progression, which is further analyzed in relation to other studies.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:AvfA0Oy_GE0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Complex event recognition in the big data era: a survey",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-019-00557-w",
            "Abstract": "The concept of event processing is established as a generic computational paradigm in various application fields. Events report on state changes of a system and its environment. Complex event recognition (CER) refers to the identification of composite events of interest, which are collections of simple, derived events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. Examples include the recognition of anomalies in maritime surveillance, electronic fraud, cardiac arrhythmias and epidemic spread. This survey elaborates on the whole pipeline from the time CER queries are expressed in the most prominent languages, to algorithmic toolkits for scaling-out CER to clustered and geo-distributed architectural settings. We also highlight future research directions. ",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:Ehil0879vHcC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Querying and mining data streams: you only get one look a tutorial",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1145/564691.564794",
            "Abstract": "Traditional Database Management Systems (DBMS) software is built on the concept of persistent data sets, that are stored reliably in stable storage and queried/updated several times throughout their lifetime. For several emerging application domains, however, data arrives and needs to be processed on a continuous (24 x 7) basis, without the benefit of several passes over a static, persistent data image. Such continuous data streams arise naturally, for example, in the network installations of large Telecom and Internet service providers where detailed usage information (Call-Detail-Records (CDRs), SNMP/RMON packet-flow data, etc.) from different parts of the underlying network needs to be continuously collected and analyzed for interesting trends. Other applications that generate rapid, continuous and large volumes of stream data include transactions in retail chains, ATM and credit card operations in banks \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:UeHWp8X0CEIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sketch-based multi-query processing over data streams",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-24741-8_32",
            "Abstract": "Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.Randomized techniques, based on computing small \u201csketch\u201d synopses for each stream, have recently been shown to be a very effective tool for approximating the result of a single SQL query over streaming data tuples. In this paper, we investigate the problems arising when data-stream sketches are used to process multiple such queries concurrently. We demonstrate that, in the presence of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:qUcmZB5y_30C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "SPARTAN: using constrained models for guaranteed-error semantic compression",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/568574.568578",
            "Abstract": "While a variety of lossy compression schemes have been developed for certain forms of digital data (e.g., images, audio, video), the area of lossy compression techniques for arbitrary data tables has been left relatively unexplored. Nevertheless, such techniques are clearly motivated by the ever-increasing data collection rates of modern enterprises and the need for effective, guaranteed-quality approximate answers to queries over massive relational data sets.In this paper, we propose SPARTAN, a system that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive data tables. SPARTAN is based on the novel idea of exploiting predictive data correlations and prescribed error-tolerance constraints for individual attributes to construct concise and accurate Classification and Regression Tree (CaRT) models for entire columns of a table. More precisely, SPARTAN \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:D03iK_w7-QYC",
            "Publisher": "ACM"
        },
        {
            "Title": "Uncertainty-Aware Event Analytics over Distributed Settings",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3328905.3329763",
            "Abstract": "In complex event processing (CEP), simple derived event tuples are combined in pattern matching procedures to derive complex events (CEs) of interest. Big Data applications analyze event streams online and extract CEs to support decision making procedures. At massive scale, such applications operate over distributed networks of sites where efficient CEP requires reducing communication as much as possible. Besides, events often encompass various types of uncertainty. Therefore, massively distributed Big event Data applications in a world of uncertain events call for communication-efficient, uncertainty-aware CEP solutions, which is the focus of this work. As a proof-of-concept for the applicability of our techniques, we show how we bridge the gap between two recent CEP prototypes which use the same CEP engine and each extend it towards only one of the dimensions of distribution and uncertainty.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:P7Ujq4OLJYoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Proceedings of the 33rd International Conference on Very Large Data Bases (VLDB), University of Vienna, Austria, September 23-27, 2007",
            "Publication year": 2007,
            "Publication url": "https://madoc.bib.uni-mannheim.de/22073/",
            "Abstract": "Proceedings of the 33rd International Conference on Very Large Data Bases (VLDB), \nUniversity of Vienna, Austria, September 23-27, 2007 - MADOC Website der UB | \nImpressum | Datenschutzerkl\u00e4rung | Drucken | English Clear Cookie - decide language by \nbrowser settings MADOC Universit\u00e4t Mannheim Startseite St\u00f6bern Volltexte \nUniversit\u00e4tsbibliographie Statistik \u00dcber MADOC Hilfe Kontakt Login Erweiterte Suche \nProceedings of the 33rd International Conference on Very Large Data Bases (VLDB), \nUniversity of Vienna, Austria, September 23-27, 2007 Herausgeber: Koch, Christoph ; \nGehrke, Johannes ; Garofalakis, Minos N. ; Srivastava, Divesh ; Aberer, Karl ; Deshpande, \nAnand ; Chan, Chee Yong ; Ganti, Venkatesh ; Kanne, Carl-Christian ; Klas, Wolfgang ; \nNeuhold, Erich J. URL: http://www.informatik.uni-trier.de/~ley/db/conf/vl... Dokumenttyp: \nBuch Erscheinungsjahr: 2007 Titel einer Zeitschrift oder einer Reihe\u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:ClCfbGk0d_YC",
            "Publisher": "ACM"
        },
        {
            "Title": "Approximate Geometric Query Tracking over Distributed Streams.",
            "Publication year": 2015,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1057.2911&rep=rep1&type=pdf",
            "Abstract": "Effective Big Data analytics pose several difficult challenges for modern data management architectures. One key such challenge arises from the naturally streaming nature of big data, which mandates efficient algorithms for querying and analyzing massive, continuous data streams (that is, data that is seen only once and in a fixed order) with limited memory and CPU-time resources. Such streams arise naturally in emerging large-scale event monitoring applications; for instance, network-operations monitoring in large ISPs, where usage information from numerous sites needs to be continuously collected and analyzed for interesting trends. In addition to memory-and time-efficiency concerns, the inherently distributed nature of such applications also raises important communication-efficiency issues, making it critical to carefully optimize the use of the underlying network infrastructure. In this paper, we provide a brief introduction to the distributed data streaming model and the Geometric Method (GM), a generic technique for effectively tracking complex queries over massive distributed streams. We also discuss several recently-proposed extensions to the basic GM framework, such as the combination with streamsketching tools and local prediction models, as well as more recent developments leading to a more general theory of Safe Zones and interesting connections to convex Euclidean geometry. Finally, we outline various challenging directions for future research in this area.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:LI9QrySNdTsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "DATABASES SPECIAL SECTION ON MINING LARGE UNCERTAIN AND PROBABILISTIC Guest Editors' Introduction: Special Section on Mining Large Uncertain and Probabilistic Databases",
            "Publication year": 2010,
            "Publication url": "https://scholar.google.com/scholar?cluster=14351921900942905630&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:5awf1xo2G04C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Communication-efficient tracking of distributed triggers",
            "Publication year": 2006,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.218.3203",
            "Abstract": "There has been growing interest in large-scale distributed monitoring systems, such as Dynamic Denial of Service attack detectors and sensornet-based environmental monitors. Recent work has posited that these infrastructures lack a critical component, namely a distributed-triggering mechanism that fires when an aggregate of remote-site behavior exceeds some threshold. For several scenarios, the trigger conditions of interest are naturally cumulative, they continuously monitor the accumulation of threshold infractions (eg, resource overuse) over time. In this paper, we develop a novel framework and communicationefficient protocols to support distributed cumulative triggers. In sharp contrast to earlier work focusing on instantaneous violations, we introduce a general model of threshold conditions that enables us to track distributed cumulative violations over time windows of any size. In our system, a central coordinator efficiently tracks aggregate time-series data at remote sites by adaptively informing the sites how to locally filter their data and when to ship new information. Our proposed algorithmic framework allows us to:(1) provide guarantees on the coordinator\u2019s triggering accuracy;(2) flexibly tradeoff communication overhead versus accuracy; and,(3) develop an analytic solution for computing local filtering parameters. Our work is the first to solve the problem of communication-efficient monitoring for distributed cumulative trigger conditions using principled solutions with accuracy guarantees. We evaluate our system using time-series data generated from SNORT logs on PlanetLab nodes and demonstrate that our methods yield significant \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:pyW8ca7W8N0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Data management research at the technical university of crete",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1145/2590989.2590999",
            "Abstract": "The Technical University of Crete (TUC, www. tuc. gr) founded in 1977 in Chania, Crete is the youngest of the two technical universities in Greece (the other being the National Technical University of Athens). The purpose of this state institution is to provide high-quality undergraduate as well as graduate studies in modern engineering fields demanded by the Greek and international job market, to conduct research in cutting edge technologies as well as to develop links with the Greek and European industry. Today, the Technical University of Crete comprises five Engineering Schools (Electronic and Computer Engineering, Production Engineering and Management, Mineral Resources Engineering, Environmental Engineering, and Architecture). The School of Electronic and Computer Engineering (ECE) at TUC (www. ece. tuc. gr) has achieved an excellent reputation for its research and teaching internationally. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:epqYDVWIO7EC",
            "Publisher": "ACM"
        },
        {
            "Title": "Join sizes, frequency moments, and applications",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-28608-0_4",
            "Abstract": "We focus on a set of problems chiefly inspired by the problem of estimating the size of the (equi-)join between two relational data streams. This problem is at the heart of a wide variety of other problems, both in databases/data streams and beyond, including approximating range-query aggregates, quantiles, and heavy-hitter elements, and building approximate histograms and wavelet representations. Our discussion focuses on efficient, sketch-based streaming algorithms for join-size and self-join-size estimation problems, based on the influential papers of Alon, Matias, Gibbons, and Szegedy.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:HbR8gkJAVGIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "XSKETCH synopses for XML data graphs",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1166074.1166082",
            "Abstract": "Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows (1) path navigation and branching through the label structure of the XML data graph, and (2) predicates on the values of specific path/branch nodes, in order to reach the desired data elements. Clearly, optimizing such queries requires approximating the result cardinality of the referenced paths and hence hinges on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over the base XML data. In this article, we introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:mB3voiENLucC",
            "Publisher": "ACM"
        },
        {
            "Title": "What\u2019s Next in XML and Databases?",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-30192-9_31",
            "Abstract": "Since the time XML became a W3C standard for document representation and exchange over the Web, many efforts have been devoted to the development of standards, methodologies, and tools for handling, storing, retrieving, and protecting XML documents. The purpose of this panel, held during the international EDBT\u20192004 workshop on \u201cdatabase technologies for handling XML information on the Web\u201d [3], is to discuss the current status of the research in XML data management and to foresee new trends towards the XML-ization of database research.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:OU6Ihb5iCvQC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "System and/or method for processing events",
            "Publication year": 2011,
            "Publication url": "https://patents.google.com/patent/US7890494B2/en",
            "Abstract": "The subject matter disclosed herein relates to processing information regarding events. In one particular example, a stabbing query may be formulated in response to an event. One or more sets are associated with and/or mapped to nodes of a tree.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:wbdj-CoPYUoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Fast Approximation Scheme for Probabilistic Wavelet Synopses.",
            "Publication year": 2005,
            "Publication url": "http://cgi.di.uoa.gr/~adeli/tr4643.pdf",
            "Abstract": "Several studies have demonstrated the effectiveness of Haar wavelets in reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast, accurate approximate query answers. While Haar wavelets were originally designed for minimizing the overall root-mean-squared (ie, L2-norm) error in the data approximation, the recently-proposed idea of probabilistic wavelet synopses also enables their use in minimizing other error metrics, such as the relative error in individual datavalue reconstruction, which is arguably the most important for approximate query processing. Known construction algorithms for probabilistic wavelet synopses employ probabilistic schemes for coefficient thresholding that are based on optimal Dynamic-Programming (DP) formulations over the error-tree structure for Haar coefficients. Unfortunately, these (exact) schemes can scale quite poorly for large data-domain and synopsis sizes. To address this shortcoming, in this paper, we introduce a novel, fast approximation scheme for building probabilistic wavelet synopses over large data sets. Our algorithm\u2019s running time is near-linear in the size of the data-domain (even for very large synopsis sizes) and proportional to 1/\u03f5, where \u03f5 is the desired approximation guarantee. The key technical idea in our approximation scheme is to make exact DP formulations for probabilistic thresholding much \u201csparser\u201d, while ensuring a maximum relative degradation of \u03f5 on the quality of the approximate synopsis, ie, the desired approximation error metric. Extensive experimental results over synthetic and real-life data clearly demonstrate the benefits of our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:pqnbT2bcN3wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Data stream management: A brave new world",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-28608-0_1",
            "Abstract": "Traditional data-management systems software is built on the concept of persistent data sets that are stored reliably in stable storage and queried/updated several times throughout their lifetime. For several emerging application domains, however, data arrives and needs to be processed on a continuous basis, without the benefit of several passes over a static, persistent data image. Such continuous data streams arise naturally, for instance telecom and IP network monitoring. This volume focuses on the theory and practice of data stream management, and the difficult, novel challenges this emerging domain introduces for data-management systems. The collection of chapters (contributed by authorities in the field) offers a comprehensive introduction to both the algorithmic/theoretical foundations of data streams and the streaming systems/applications built in different domains. In the remainder of this \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:ruyezt5ZtCIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Approximate continuous querying over distributed streams",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1366102.1366106",
            "Abstract": "While traditional database systems optimize for performance on one-shot query processing, emerging large-scale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically distributed streams. Thus, effective solutions have to be simultaneously space/time efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. In a nutshell, our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:35N4QoGY0k4C",
            "Publisher": "ACM"
        },
        {
            "Title": "Public Health for the Internet \u03c6 Towards A New Grand Challenge for Information Management",
            "Publication year": 2007,
            "Publication url": "https://repository.upenn.edu/cis_papers/299/",
            "Abstract": "Business incentives have brought us within a small factor of achieving the database community's Grand Challenge set out in the Asilomar Report of 1998. This paper makes the case for a new, focused Grand Challenge: Public Health for the Internet. The goal of PHI (or \u03c6) is to enable collectives of hosts on the Internet to jointly monitor and promote network health by sharing information on network conditions in a peer-to-peer fashion. We argue that this will be a positive effort for the research community for a variety of reasons, both in terms of its technical reach and its societal impact.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:HoB7MX3m0LUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Correlating XML data streams using tree-edit distance embeddings",
            "Publication year": 2003,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/773153.773168",
            "Abstract": "We propose the first known solution to the problem of correlating, in small space, continuous streams of XML data through approximate (structure and content) matching, as defined by a general tree-edit distance metric. The key element of our solution is a novel algorithm for obliviously embedding tree-edit distance metrics into an L 1 vector space while guaranteeing an upper bound of O (log 2 n log* n) on the distance distortion between any data trees with at most n nodes. We demonstrate how our embedding algorithm can be applied in conjunction with known random sketching techniques to:(1) build a compact synopsis of a massive, streaming XML data tree that can be used as a concise surrogate for the full tree in approximate tree-edit distance computations; and,(2) approximate the result of tree-edit distance similarity joins over continuous XML document streams. To the best of our knowledge, these are the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:hFOr9nPyWt4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "System and methods for generating diversified vertical search listings",
            "Publication year": 2009,
            "Publication url": "https://patents.google.com/patent/US20090125502A1/en",
            "Abstract": "A method of generating a diversified vertical search results listing, including listing attribute values related to search criteria and their frequency of occurrence to create a plurality of listings; creating a plurality of interval bands based on the plurality of listings; generating a random diversity score for each listing over a substantially uniform distribution within each of the plurality of bands; and sorting a set of search results for diversified listing in response to a user searching for the search criteria according to the diversity score of each listing.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:ye4kPcJQO24C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spartan: A model-based semantic compression system for massive data tables",
            "Publication year": 2001,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/376284.375693",
            "Abstract": "While a variety of lossy compression schemes have been developed for certain forms of digital data (e.g., images, audio, video), the area of lossy compression techniques for arbitrary data tables has been left relatively unexplored. Nevertheless, such techniques are clearly motivated by the ever-increasing data collection rates of modern enterprises and the need for effective, guaranteed-quality approximate answers to queries over massive relational data sets. In this paper, we propose SPARTAN, a system that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive data tables. SPARTAN is based on the novel idea of exploiting predictive data correlations and prescribed error tolerances for individual attributes to construct concise and accurate Classification and Regression Tree (CaRT) models for entire columns of a table. More precisely, SPARTAN selects a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:KlAtU1dfN6UC",
            "Publisher": "ACM"
        },
        {
            "Title": "Synopses for massive data: Samples, histograms, wavelets, sketches",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1561/1900000004",
            "Abstract": "Methods for Approximate Query Processing (AQP) are essential for dealing with massive data. They are often the only means of providing interactive response times when exploring massive datasets, and are also needed to handle high speed data streams. These methods proceed by computing a lossy, compact synopsis of the data, and then executing the query of interest against the synopsis rather than the entire dataset. We describe basic principles and recent developments in AQP. We focus on four key synopses: random samples, histograms, wavelets, and sketches. We consider issues such as accuracy, space and time efficiency, optimality, practicality, range of applicability, error bounds on query answers, and incremental maintenance. We also discuss the trade-offs between the different synopsis types.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:j8SEvjWlNXcC",
            "Publisher": "Now Publishers Inc."
        },
        {
            "Title": "Automated document classifier tuning including training set adaptive to user browsing behavior",
            "Publication year": 2010,
            "Publication url": "https://patents.google.com/patent/US7797260B2/en",
            "Abstract": "Subject matter disclosed herein relates to document classification and/or automated document classifier tuning. In an example embodiment, a document received from a user computing platform in an online database stored in a memory of a server computing platform may be classified based, at least in part, on a training set. Also for an example embodiment, the training set may be modified based, at least in part, on statistics gathered from user browsing behavior.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:VL0QpB8kHFEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Distributed set-expression cardinality estimation",
            "Publication year": 2004,
            "Publication url": "http://www.vldb.org/conf/2004/RS8P3.PDF",
            "Abstract": "We consider the problem of estimating set-expression cardinality in a distributed streaming environment where rapid update streams originating at remote sites are continually transmitted to a central processing system. At the core of our algorithmic solutions for answering set-expression cardinality queries are two novel techniques for lowering data communication costs without sacrificing answer precision. Our first technique exploits global knowledge of the distribution of certain frequently occurring stream elements to significantly reduce the transmission of element state information to the central site. Our second technical contribution involves a novel way of capturing the semantics of the input set expression in a boolean logic formula, and using models (of the formula) to determine whether an element state change at a remote site can affect the set expression result. Results of our experimental study with real-life as well as synthetic data sets indicate that our distributed set-expression cardinality estimation algorithms achieve substantial reductions in message traffic compared to naive approaches that provide the same accuracy guarantees.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:4DMP91E08xMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scalable filtering of XML data for web services",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1167339/",
            "Abstract": "As the Web gains prevalence as an application-to-application communication medium, organizations are deploying more Web service applications to provide standardized, programmatic application functionality over the Internet. The paper considers how scalable content-based routing architectures for Web applications can handle the growing number of XML messages associated with Web services.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:r0BpntZqJG4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Tree-pattern similarity estimation for scalable content-based routing",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4221750/",
            "Abstract": "With the advent of XML as the de facto language for data publishing and exchange, scalable distribution of XML data to large, dynamic populations of consumers remains an important challenge. Content-based publish/subscribe systems offer a convenient design paradigm, as most of the complexity related to addressing and routing is encapsulated within the network infrastructure. To indicate the type of content that they are interested in, data consumers typically specify their subscriptions using a tree-pattern specification language (an important subset of XPath), while producers publish XML content without prior knowledge of any potential recipients. Discovering semantic communities of consumers with similar interests is an important requirement for scalable content-based systems: such \"semantic clusters\" of consumers play a critical role in the design of effective content-routing protocols and architectures. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:yD5IFk8b50cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Data Mining Meets Network Management: The NEMESIS Project.",
            "Publication year": 2001,
            "Publication url": "http://www.cs.cornell.edu/johannes/papers/dmkd2001-papers/p1_garofalakis.pdf",
            "Abstract": "Modern communication networks generate large amounts of operational data, including traffic and utilization statistics and alarm/fault data at various levels of detail. These massive collections of network-management data can grow in the order of several Terabytes per year, and typically hide \u201cknowledge\u201d that is crucial to some of the key tasks involved in effectively managing a communication network (eg, capacity planning and traffic engineering). In this short paper, we provide an overview of some of our recent and ongoing work in the context of the NEMESIS project at Bell Laboratories that aims to develop novel data warehousing and mining technology for the effective storage, exploration, and analysis of massive network-management data sets. We first give some highlights of our work on Model-Based Semantic Compression (MBSC), a novel data-compression framework that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive network-data tables. We discuss the architecture and some of the key algorithms underlying SPARTAN, a model-based semantic compression system that exploits predictive data correlations and prescribed error tolerances for individual attributes to construct concise and accurate Classification and Regression Tree (CaRT) models for entire columns of a table. We also summarize some of our ongoing work on warehousing and analyzing network-fault data and discuss our vision of how data-mining techniques can be employed to help automate and improve fault-management in modern communication networks. More specifically, we describe the two key \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:NMxIlDl6LWMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Efficient Strategies for Continuous Distributed Tracking Tasks.",
            "Publication year": 2005,
            "Publication url": "https://www.researchgate.net/profile/Raymond-Ng-9/publication/2630942_Data_Engineering/links/0046353398a9a03394000000/Data-Engineering.pdf#page=35",
            "Abstract": "While traditional databases have focused on single query evaluation in a centralized setting, emerging applications require continuous tracking of queries on data that is widely distributed and constantly updated. We describe such scenarios, and describe the challenges involved in designing communication-efficient protocols for the tracking tasks we define. We outline some solutions to these problems, by abstracting a model of the communication system, defining the tracking tasks of interest, and building query-tracking schemes based on three guiding principles of minimizing global information, using summaries to capture whole data streams, and seeking stability of the protocols.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:SeFeTyx0c_EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Method and apparatus for secure processing of XML-based documents",
            "Publication year": 2008,
            "Publication url": "https://patents.google.com/patent/US7433870B2/en",
            "Abstract": "Method for providing controlled access to an XML document includes defining at least one access control policy for a user of the XML document, deriving a security view of the XML document for the user based upon said access control policy and schema level processing of the XML document and translating a user query based on the security view of the XML document to an equivalent query based on the XML document. An apparatus for same includes means for defining an access control policy for a user of the XML document and means for deriving a security view of the XML document for the user based on said access control policy and schema level processing of the XML document. Also included are means for translating a user query based on the security view of the XML document to an equivalent query based on the XML document.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:olpn-zPbct0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Monitoring distributed fragmented skylines",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/article/10.1007/s10619-018-7223-7",
            "Abstract": "Distributed skyline computation is important for a wide range of domains, from distributed and web-based systems to ISP-network monitoring and distributed databases. The problem is particularly challenging in dynamic distributed settings, where the goal is to efficiently monitor a continuous skyline query over a collection of distributed streams. All existing work relies on the assumption of a single point of reference for object attributes/dimensions: objects may be vertically or horizontally partitioned, but the accurate value of each dimension for each object is always maintained by a single site. This assumption is unrealistic for several distributed applications, where object information is fragmented over a set of distributed streams (each monitored by a different site) and needs to be aggregated (e.g., averaged) across several sites. Furthermore, it is frequently useful to define skyline dimensions through \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:anf4URPfarAC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Proof sketches: Verifiable multi-party aggregation",
            "Publication year": 2006,
            "Publication url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-20.pdf",
            "Abstract": "Recent work on distributed aggregation has assumed a benign population of participants. In modern distributed systems, it is now necessary to account for adversarial behavior. In this paper we consider the problem of ensuring verifiable yet efficient results to typical aggregation queries in a distributed, multi-party setting. We describe a general framework for the problem, including the threat model for adversaries that we consider. We then present a mechanism called a proof sketch, which uses a compact combination of cryptographic signatures and Flajolet-Martin sketches to verify that a query answer is within acceptable error bounds with high probability. When verification fails, we provide efficient mechanisms to identify any participants responsible for the perturbed result. We derive proof sketches for count aggregates, and extend them to proof sketches for verifiable random samples, which, in turn, can be used to provide verifiable approximations for a broad class of data-analysis queries, including quantiles and heavy hitters. In addition to our specific proof sketches developed here, we sketch a general framework for developing new proof sketches. Finally, we examine the practical use of proof sketches, and observe that adversaries can often be reduced to much smaller violations in practice than our worst-case bounds suggest.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "Technical Report UCB/EECS-2006-20, EECS Department, University of California, Berkeley"
        },
        {
            "Title": "Network Data Mining and Analysis: The NEMESIS Project",
            "Publication year": 2002,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:KxtntwgDAa4C",
            "Publisher": "Springer Berlin/Heidelberg"
        },
        {
            "Title": "Distributed geometric query monitoring using prediction models",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2602137",
            "Abstract": "Many modern streaming applications, such as online analysis of financial, network, sensor, and other forms of data, are inherently distributed in nature. An important query type that is the focal point in such application scenarios regards actuation queries, where proper action is dictated based on a trigger condition placed upon the current value that a monitored function receives. Recent work [Sharfman et al. 2006, 2007b, 2008] studies the problem of (nonlinear) sophisticated function tracking in a distributive manner. The main concept behind the geometric monitoring approach proposed there is for each distributed site to perform the function monitoring over an appropriate subset of the input domain. In the current work, we examine whether the distributed monitoring mechanism can become more efficient, in terms of the number of communicated messages, by extending the geometric monitoring framework to utilize \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:BUYA1_V_uYcC",
            "Publisher": "ACM"
        },
        {
            "Title": "Toward sophisticated detection with distributed triggers",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1162678.1162684",
            "Abstract": "Recent research has proposed efficient protocols for distributed triggers, which can be used in monitoring infrastructures to maintain system-wide invariants and detect abnormal events with minimal communication overhead. To date, however, this work has been limited to simple thresholds on distributed aggregate functions like sums and counts. In this paper, we present our initial results that show how to use these simple threshold triggers to enable sophisticated anomaly detection in near-real time, with modest communication overheads. We design a distributed protocol to detect\" unusual traffic patterns\" buried in an Origin-Destination network flow matrix that: a) uses a Principal Components Analysis decomposition technique to detect anomalies via a threshold function on residual signals [10]; and b) efficiently tracks this threshold function in near-real time using a simple distributed protocol. In addition, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:TFP_iSt0sucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Method and apparatus for globally approximating quantiles in a distributed monitoring environment",
            "Publication year": 2010,
            "Publication url": "https://patents.google.com/patent/US7783647B2/en",
            "Abstract": "The invention comprises a method and apparatus for determining a rank of a query value. Specifically, the method comprises receiving a rank query request, determining, for each of the at least one remote monitor, a predicted lower-bound rank value and upper-bound rank value, wherein the predicted lower-bound rank value and upper-bound rank value are determined according to at least one respective prediction model used by each of the at least one remote monitor to compute the at least one local quantile summary, computing a predicted average rank value for each of the at least one remote monitor using the at least one predicted lower-bound rank value and the at least one predicted upper-bound rank value associated with the respective at least one remote monitor, and computing the rank of the query value using the at least one predicted average rank value associated with the respective at least one \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:eq2jaN3J8jMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Wavelet synopses with error guarantees",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/564691.564746",
            "Abstract": "Recent work has demonstrated the effectiveness of the wavelet decomposition in reducing large amounts of data to compact sets of wavelet coefficients (termed\" wavelet synopses\") that can be used to provide fast and reasonably accurate approximate answers to queries. A major criticism of such techniques is that unlike, for example, random sampling, conventional wavelet synopses do not provide informative error guarantees on the accuracy of individual approximate answers. In fact, as this paper demonstrates, errors can vary widely (without bound) and unpredictably, even for identical queries on nearly-identical values in distinct parts of the data. This lack of error guarantees severely limits the practicality of traditional wavelets as an approximate query-processing tool, because users have no idea of the quality of any particular approximate answer. In this paper, we introduce Probabilistic Wavelet Synopses, the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:Y0pCki6q_DkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Distributed set-expression cardinality estimation",
            "Publication year": 2011,
            "Publication url": "https://patents.google.com/patent/US7873689B2/en",
            "Abstract": "A method and system for answering set-expression cardinality queries while lowering data communication costs by utilizing a coordinator site to provide global knowledge of the distribution of certain frequently occurring stream elements to significantly reduce the transmission of element state information to the central site and, optionally, capturing the semantics of the input set expression in a Boolean logic formula and using models of the formula to determine whether an element state change at a remote site can affect the set expression result.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:LPZeul_q3PIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sketching probabilistic data streams",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1247480.1247513",
            "Abstract": "The management of uncertain, probabilistic data has recently emerged as a useful paradigm for dealing with the inherent unreliabilities of several real-world application domains, including data cleaning, information integration, and pervasive, multi-sensor computing. Unlike conventional data sets, a set of probabilistic tuples defines a probability distribution over an exponential number of possible worlds (ie,\" grounded\", deterministic databases). This\" possibleworlds\" interpretation allows for clean query semantics but also raises hard computational problems for probabilistic database query processors. To further complicate matters, in many scenarios (eg, large-scale process and environmental monitoring using multiple sensor modalities), probabilistic data tuples arrive and need to be processed in a streaming fashion; that is, using limited memory and CPU resources and without the benefit of multiple passes over a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:8k81kl-MbHgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Physical and service topology discovery in heterogeneous networks: the NetInventory system",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1341857/",
            "Abstract": "Knowledge of both, the up-to-date physical topology of an IP network and provisioned services in a core data or optical network, is crucial to a number of critical network management tasks, including reactive and proactive resource management, event correlation, and root-cause analysis. Given the dynamic nature of today's networks, keeping track of physical and service topology information manually is a daunting (if not impossible) task. Thus, effective algorithms for automatically discovering network topology are necessary. We present novel algorithms for (a) discovering physical topology in heterogeneous (i.e., multivendor) IP networks, and (b) discovering service topology in core data or optical networks. Our algorithms for physical topology of IP networks rely on standard SNMP MIB information that is widely supported by modern IP network elements. We have implemented the algorithms presented in This work \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:1sJd4Hv_s6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Declarative networking",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1592761.1592785",
            "Abstract": "Declarative Networking is a programming methodology that enables developers to concisely specify network protocols and services, which are directly compiled to a dataflow framework that executes the specifications. This paper provides an introduction to basic issues in declarative networking, including language design, optimization, and dataflow execution. We present the intuition behind declarative programming of networks, including roots in Datalog, extensions for networked environments, and the semantics of long-running queries over network state. We focus on a sublanguage we call Network Datalog (NDlog), including execution strategies that provide crisp eventual consistency semantics with significant flexibility in execution. We also describe a more general language called Overlog, which makes some compromises between expressive richness and semantic guarantees. We provide an overview of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:ZeXyd9-uunAC",
            "Publisher": "ACM"
        },
        {
            "Title": "Indexed Regular Expression Matching.",
            "Publication year": 2016,
            "Publication url": "https://scholar.google.com/scholar?cluster=14034342467544896689&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:MLfJN-KU85MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Wavelets on Streams.",
            "Publication year": 2009,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.418.3164&rep=rep1&type=pdf",
            "Abstract": "Unlike conventional database query-processing engines that require several passes over a static data image, streaming dataanalysis algorithms must often rely on building concise, approximate (but highly accurate) synopses of the input stream (s) in real-time (ie, in one pass over the streaming data). Such synopses typically require space that is significantly sublinear in the size of the data and can be used to provide approximate query answers. The collection of the top (ie, largest) coefficients in the wavelet transform (or, decomposition) of an input data vector is one example of such a key feature of the stream. Wavelets provide a mathematical tool for the hierarchical decomposition of functions, with a long history of successful applications in signal and image processing [10]. Applying the wavelet transform to a (one-or multi-dimensional) data vector and retaining a select small collection of the largest wavelet coefficient gives a very effective form of lossy data compression. Such wavelet summaries provide concise, general-purpose summaries of relational data, and can form the foundation for fast and accurate approximate query processing algorithms.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:SdhP9T11ey4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Adaptive cleaning for RFID data streams",
            "Publication year": 2006,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.324.1235&rep=rep1&type=pdf",
            "Abstract": "A major impediment to the widespread adoption of RFID technology is the unreliability of the data streams produced by RFID readers; a 30% drop rate is not uncommon for RFID deployments. To compensate, most RFID middleware systems provide a \u201csmoothing filter\u201d, a sliding-window aggregate that interpolates for lost readings. Typically, these middleware systems require the application to fix the size of the smoothing window in order to produce clean RFID data. Window-size selection, however, is a non-trivial problem: the window must be large enough to smooth lost readings but small enough to accurately capture tag movement. Furthermore, the ideal size may change over the course of the RFID deployment.In this paper, we propose SMURF, the first declarative, adaptive smoothing filter for RFID data cleaning. SMURF models the unreliability of RFID readings by viewing RFID streams as a statistical sample of tags in the physical world, and exploits techniques grounded in sampling theory to drive its cleaning processes. Through the use of tools such as binomial sampling and \u03c0-estimators, SMURF continuously adapts the smoothing window size in a principled manner to provide accurate RFID data to applications.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sketch-based querying of distributed sliding-window data streams",
            "Publication year": 2012,
            "Publication url": "https://arxiv.org/abs/1207.0139",
            "Abstract": "While traditional data-management systems focus on evaluating single, ad-hoc queries over static data sets in a centralized setting, several emerging applications require (possibly, continuous) answers to queries on dynamic data that is widely distributed and constantly updated. Furthermore, such query answers often need to discount data that is \"stale\", and operate solely on a sliding window of recent data arrivals (e.g., data updates occurring over the last 24 hours). Such distributed data streaming applications mandate novel algorithmic solutions that are both time- and space-efficient (to manage high-speed data streams), and also communication-efficient (to deal with physical data distribution). In this paper, we consider the problem of complex query answering over distributed, high-dimensional data streams in the sliding-window model. We introduce a novel sketching technique (termed ECM-sketch) that allows effective summarization of streaming data over both time-based and count-based sliding windows with probabilistic accuracy guarantees. Our sketch structure enables point as well as inner-product queries, and can be employed to address a broad range of problems, such as maintaining frequency statistics, finding heavy hitters, and computing quantiles in the sliding-window model. Focusing on distributed environments, we demonstrate how ECM-sketches of individual, local streams can be composed to generate a (low-error) ECM-sketch summary of the order-preserving aggregation of all streams; furthermore, we show how ECM-sketches can be exploited for continuous monitoring of sliding-window queries over distributed streams \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:V3AGJWp-ZtQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Program committee chairs' welcome",
            "Publication year": 2014,
            "Publication url": "https://nyuscholars.nyu.edu/en/publications/program-committee-chairs-welcome",
            "Abstract": "Program committee chairs' welcome \u2014 NYU Scholars Skip to main navigation Skip to \nsearch Skip to main content NYU Scholars Logo Help & FAQ Home Profiles Research Units \nResearch Output Search by expertise, name or affiliation Program committee chairs' \nwelcome Minos Garofalakis, Ian Soboroff, Torsten Suel, Min Wang Research output: \nContribution to journal \u203a Editorial \u203a peer-review Overview Fingerprint Original language \nEnglish (US) Pages (from-to) iv Journal CIKM 2014 - Proceedings of the 2014 ACM \nInternational Conference on Information and Knowledge Management State Published - \nNov 3 2014 Event 23rd ACM International Conference on Information and Knowledge \nManagement, CIKM 2014 - Shanghai, China Duration: Nov 3 2014 \u2192 Nov 7 2014 ASJC \nScopus subject areas Information Systems and Management Computer Science \nApplications Information Systems Access to Document Link to , \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:YohjEiUPhakC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Leveraging Reconfigurable Computing in Distributed Real-time Computation Systems.",
            "Publication year": 2016,
            "Publication url": "http://ceur-ws.org/Vol-1558/paper1.pdf",
            "Abstract": "The community of Big Data processing typically performs realtime computations on data streams with distributed systems such as the Apache Storm. Such systems offer substantial parallelism; however, the communication overhead among nodes for the distribution of the workload places an upper limit to the exploitable parallelism. The contribution of the present work is the integration of a reconfigurable platform with the Apache Storm, which is the main platform of the Big Data streaming processing community. By exploiting the internal bandwidth of FPGAs we show that the computational limits for stream processing are significantly increased vs. conventional distributed processing without compromising on the platform of choice or its seamless operation in a dynamic pipeline. The integration of a Maxeler MPC-C Series platform with the Apache Storm, as presented in detail, yields on the Hayashi-Yoshida correlation algorithm an impressive tenfold increase in real-time streaming input capacity, which corresponds to a hundred-fold computational load. Our methodology is sufficiently general to apply to any class of distributed systems or reconfigurable computers, and this work presents quantitative results of the expected I/O performance, depending on the means of network connection.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:-FonjvnnhkoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "2019 Index IEEE Technology and Society Magazine Vol. 38",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8936321/",
            "Abstract": "This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:u-coK7KVo8oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Omnibus outlier detection in sensor networks using windowed locality sensitive hashing",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167739X17319180",
            "Abstract": "Wireless Sensor Networks (WSNs) have become an integral part of cutting edge technological paradigms such as the Internet-of-Things (IoT) which incorporates a variety of smart application scenarios. WSNs include tiny sensors (motes), with constrained hardware capabilities and limited power supply that can collaboratively function in an unsupervised manner for a long period of time. Their purpose is to continuously monitor quantities of interest and provide answers to application queries. Sensor data streams are inherently spatiotemporal in nature, both because mote measurements form multidimensional time series and due to the spatial reference on the data based on the realm sensed by a mote. Motes are designed to be inexpensive, and thus sensory hardware is prone to temporary or permanent failures yielding faulty measurements. Such measurements may unpredictably forge a query answer, while \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:0izLItjtcgwC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Method and system for resource scheduling composite multimedia objects",
            "Publication year": 2003,
            "Publication url": "https://patents.google.com/patent/US6665732B1/en",
            "Abstract": "A system for the effective resource scheduling of composite multimedia objects involves a sequence packing formulation of the composite object scheduling problem and associated efficient algorithms using techniques from pattern matching and multiprocessor scheduling. An associated method of scheduling the provision of composite multimedia objects, each comprising one or more continuous media streams of audio data, video data and other data, where the continuous media streams are of varying bandwidth requirement and duration comprise the steps of; generating composite multimedia objects from the continuous media streams and determining a run-length compressed form for each of the generated composite multimedia objects.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:2P1L_qKh6hAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Issues in complex event processing: Status and prospects in the big data era",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121216300802",
            "Abstract": "Many Big Data technologies were built to enable the processing of human generated data, setting aside the enormous amount of data generated from Machine-to-Machine (M2M) interactions and Internet-of-Things (IoT) platforms. Such interactions create real-time data streams that are much more structured, often in the form of series of event occurrences. In this paper, we provide an overview on the main research issues confronted by existing Complex Event Processing (CEP) techniques, with an emphasis on query optimization aspects. Our study expands on both deterministic and probabilistic event models and spans from centralized to distributed network settings. In that, we cover a wide range of approaches in the CEP domain and review the current status of techniques that tackle efficient query processing. These techniques serve as a starting point for developing Big Data oriented CEP applications. Therefore \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:S16KYo8Pm5AC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Processing massive data streams",
            "Publication year": 2008,
            "Publication url": "http://users.softnet.tuc.gr/~minos/Talks/vldbSchool08.pdf",
            "Abstract": "10.1. 0.2 16.2. 3.7 12 20K http 18.6. 7.1 12.4. 0.3 16 24K http 13.9. 4.3 11.6. 8.2 15 20K http 15.2. 2.9 17.1. 2.1 19 40K http 12.4. 3.8 14.8. 7.4 26 58K http 10.5. 1.3 13.0. 0.1 27 100K ftp 11.1. 0.6 10.3. 4.5 32 300K ftp 19.7. 1.2 16.5. 5.8 18 80K ftp",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:p2g8aNsByqUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Probabilistic wavelet synopses for multiple measures",
            "Publication year": 2007,
            "Publication url": "https://patents.google.com/patent/US20070058871A1/en",
            "Abstract": "A technique for building probabilistic wavelet synopses for multi-measure data sets is provided. In the presence of multiple measures, it is demonstrated that the problem of exact probabilistic coefficient thresholding becomes significantly more complex. An algorithmic formulation for probabilistic multi-measure wavelet thresholding based on the idea of partial-order dynamic programming (PODP) is provided. A fast, greedy approximation algorithm for probabilistic multi-measure thresholding based on the idea of marginal error gains is provided. An empirical study with both synthetic and real-life data sets validated the approach, demonstrating that the algorithms outperform naive approaches based on optimizing individual measures independently and the greedy thresholding scheme provides near-optimal and, at the same time, fast and scalable solutions to the probabilistic wavelet synopsis construction problem.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:nb7KW1ujOQ8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Statistical synopses for graph-structured XML databases",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/564691.564733",
            "Abstract": "Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (eg, XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, We introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:eQOLeE2rZwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "XSketch synopses for XML",
            "Publication year": 2002,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.535.1044&rep=rep1&type=pdf",
            "Abstract": "All existing proposals for querying XML (eg, XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, we summarize our main results from our recent work on XS-KETCHes, a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data graph. To estimate the selectivities of complex path expressions over concise XSKETCH synopses, we develop an estimation framework that relies on appropriate statistical (uniformity and independence) assumptions to compensate for the lack of detailed distribution information. Given our estimation framework, We demonstrate that the problem of building an accuracy-optimal XSKETCH for a given amount of space is NP-hard, and propose an efficient heuristic algorithm based on greedy forward selection. Extensive experimental results with synthetic as well as real-life data sets verify the effectiveness of our approach. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured data and complex (branching) path expressions.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:08ZZubdj9fEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spartan: A model-based semantic compression system for massive data tables",
            "Publication year": 2001,
            "Publication url": "http://ilpubs.stanford.edu:8090/494/",
            "Abstract": "While a variety of lossy compression schemes have been developed for certain forms of digital data (e.g., images, audio, video),  the area of lossy compression techniques for arbitrary data tables has been left relatively unexplored. Nevertheless, such techniques  are clearly motivated  by the ever-increasing data collection rates of modern enterprises and the need for effective, guaranteed-quality approximate answers to queries over massive relational data sets.  In this paper, we propose SPARTAN, a system that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive data tables.  SPARTAN is based on the novel idea of exploiting predictive data correlations and  prescribed error tolerances for individual attributes to construct concise and accurate  Classification and Regression Tree models  for entire columns of a table. More precisely, SPARTAN selects a certain subset of attributes for which no values are explicitly stored in the compressed table; instead, concise CaRTs  that predict these values (within the prescribed error bounds) are maintained. To restrict the huge search space and construction cost of possible CaRT predictors, SPARTAN employs sophisticated learning techniques  and novel combinatorial optimization algorithms. Our experimentation with several real-life data sets offers convincing evidence of the effectiveness of SPARTAN's model-based approach --   SPARTAN is able to consistently yield substantially better compression ratios than existing semantic or syntactic compression tools (e.g., gzip)  while utilizing only small data samples for model inference.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:ZfRJV9d4-WMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scalable approximate query tracking over highly distributed data streams",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2882903.2915225",
            "Abstract": "The recently-proposed Geometric Monitoring (GM) method has provided a general tool for the distributed monitoring of arbitrary non-linear queries over streaming data observed by a collection of remote sites, with numerous practical applications. Unfortunately, GM-based techniques can suffer from serious scalability issues with increasing numbers of remote sites. In this paper, we propose novel techniques that effectively tackle the aforementioned scalability problems by exploiting a carefully designed sample of the remote sites for efficient approximate query tracking. Our novel sampling-based scheme utilizes a sample of cardinality proportional to\u221a N (compared to N for the original GM), where  is the number of sites in the network, to perform the monitoring process. Our experimental evaluation over a variety of real-life data streams demonstrates that our sampling-based techniques can significantly reduce the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:Dip1O2bNi0gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Physical topology discovery for large multisubnet networks",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1208686/",
            "Abstract": "Knowledge of the up-to-date physical (i.e., layer-2) topology of an Ethernet network is crucial to a number of critical network management tasks, including reactive and proactive resource management, event correlation, and root-cause analysis. Given the dynamic nature of today's IP networks, keeping track of topology information manually is a daunting (if not impossible) task. Thus, effective algorithms for automatically discovering physical network topology are necessary. In this paper, we propose the first complete algorithmic solution for discovering the physical topology of a large, heterogeneous Ethernet network comprising multiple subnets as well as (possibly) dumb or uncooperative network elements. Our algorithms rely on standard SNMP MIB information that is widely supported in modern IP networks and require no modifications to the operating system software running on elements or hosts. Furthermore \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:0EnyYjriUFMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Distributed Data Streams and the Power of Geometry.",
            "Publication year": 2016,
            "Publication url": "http://users.softnet.tuc.gr/~minos/Talks/minos-COMAD2016.pdf",
            "Abstract": "Distributed Data Streams and the Power of Geometry Page 1 Distributed Data Streams and the \nPower of Geometry Minos Garofalakis Technical University of Crete Software Technology and \nNetwork Applications Lab http://www.softnet.tuc.gr/~minos/ Work with: Haifa U, Technion, U \nNeuchatel, TU Dresden QualiMaster Page 2 Big Data is Big News (and Big Business\u2026) \u2022 \nMobile computing, sensornets, social networks, \u2026 \u2022 Data-driven science \u2022 How can we \ncost-effectively manage and analyze all this data\u2026? 2 Page 3 Big Data Challenges: The Four \nV\u2019s \u2013 and one D \u2022 Volume: Scaling from Terabytes to Exa/Zettabytes \u2022 Velocity: Processing \nmassive amounts of streaming data \u2022 Variety: Managing the complexity of multiple relational \nand non-relational data types and schemas \u2022 Veracity: Handling inherent uncertainty and noise \nin the data \u2022 Distribution: Dealing with massively distributed information \u2022 Our focus: Volume, , 3 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:_axFR9aDTf0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Discrete Wavelet Transform and Wavelet Synopses.",
            "Publication year": 2009,
            "Publication url": "http://www.softnet.tuc.gr/~minos/Papers/eds09wav.pdf",
            "Abstract": "Wavelets are a useful mathematical tool for hierarchically decomposing functions in ways that are both efficient and theoretically sound. Broadly speaking, the wavelet transform of a function consists of a coarse overall approximation together with detail coefficients that influence the function at various scales. The wavelet transform has a long history of successful applications in signal and image processing [11, 12]. Several recent studies have also demonstrated the effectiveness of the wavelet transform (and Haar wavelets, in particular) as a tool for approximate query processing over massive relational tables [2, 7, 8] and continuous data streams [3, 9]. Briefly, the idea is to apply wavelet transform to the input relation to obtain a compact data synopsis that comprises a select small collection of wavelet coefficients. The excellent energy compaction and de-correlation properties of the wavelet transform allow for concise and effective approximate representations that exploit the structure of the data. Furthermore, wavelet transforms can generally be computed in linear time, thus allowing for very efficient algorithms.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:kRWSkSYxWN8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "DTD Inference from XML Documents: The XTRACT Approach.",
            "Publication year": 2003,
            "Publication url": "https://www.researchgate.net/profile/Aristides_Gionis/publication/220283222_DTD_Inference_from_XML_Documents_The_XTRACT_Approach/links/5437e4950cf2590375c5626d/DTD-Inference-from-XML-Documents-The-XTRACT-Approach.pdf",
            "Abstract": "XML is rapidly emerging as the new standard for data representation and exchange on the Web. Document Type Descriptors (DTDs) contain valuable information on the structure of XML documents and thus have a crucial role in the efficient storage and querying of XML data. Despite their importance, however, DTDs are not mandatory, and it is quite possible for documents in XML databases to not have accompanying DTDs. In this paper, we present an overview of XTRACT, a novel system for inferring a DTD schema for a database of XML documents. Since the DTD syntax incorporates the full expressive power of regular expressions, naive approaches typically fail to produce concise and intuitive DTDs. Instead, the XTRACT inference algorithms employ a sequence of sophisticated steps that involve:(1) finding patterns in the input sequences and replacing them with regular expressions to generate \u201cgeneral\u201d candidate DTDs,(2) factoring candidate DTDs using adaptations of algorithms from the logic optimization literature, and (3) applying the Minimum Description Length (MDL) principle to find the best DTD among the candidates.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:a0OBvERweLwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Regular Expression Indexing.",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=10653697790113016292&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:fEOibwPWpKIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Streaming in a Connected World.",
            "Publication year": 2006,
            "Publication url": "https://scholar.google.com/scholar?cluster=5517961631692357812&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:ML0RJ9NH7IQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Method for performing information-preserving DTD schema embeddings",
            "Publication year": 2009,
            "Publication url": "https://patents.google.com/patent/US7496571B2/en",
            "Abstract": "Method for performing information-preserving DTD schema embeddings between a source schema when matching a source schema and a target schema. The preservation is realized by a matching process between the two schemas that finds a first string marking of the target schema, evaluates a legality of the first string marking, determines an estimated minimal cost of the first string marking and subsequently adjusts the estimated minimal cost based upon one to one mapping of source schema and target schema subcomponents.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:UxriW0iASnsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Foundations and Trends\u00ae in Databases",
            "Publication year": 2011,
            "Publication url": "https://scholar.google.com/scholar?cluster=2262218349340518781&hl=en&oi=scholarr",
            "Abstract": "Methods for Approximate Query Processing (AQP) are essential for dealing with massive data. They are often the only means of providing interactive response times when exploring massive datasets, and are also needed to handle high speed data streams. These methods proceed by computing a lossy, compact synopsis of the data, and then executing the query of interest against the synopsis rather than the entire dataset. We describe basic principles and recent developments in AQP. We focus on four key synopses: random samples, histograms, wavelets, and sketches. We consider issues such as accuracy, space and time efficiency, optimality, practicality, range of applicability, error bounds on query answers, and incremental maintenance. We also discuss the tradeoffs between the different synopsis types.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:XvxMoLDsR5gC",
            "Publisher": "Unknown"
        },
        {
            "Title": "for Massive Data Tables",
            "Publication year": 2001,
            "Publication url": "https://scholar.google.com/scholar?cluster=3404146662797063348&hl=en&oi=scholarr",
            "Abstract": "While a variety of lossy compression schemes have been developed for certain forms of digital data (eg, images, audio, video), the area of lossy compression techniques for arbitrary data tables has been left relatively unexplored. Nevertheless, such techniques are clearly motivated by the everincreasing data collection rates of modern enterprises and the need for effective, guaranteed-quality approximate answers to queries over massive relational data sets. In this paper, we propose SPARTAN, a system that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive data tables. SPARTAN is based on the novel idea of exploiting predictive data correlations and prescribed error tolerances for individual attributes to construct concise and accurate Classification and Regression Tree (CaRT) models for entire columns of a table. More precisely, SPARTAN selects a certain \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:fQNAKQ3IYiAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Secure XML querying with security views",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1007568.1007634",
            "Abstract": "The prevalent use of XML highlights the need for a generic, flexible access-control mechanism for XML documents that supports efficient and secure query access, without revealing sensitive information unauthorized users. This paper introduces a novel paradigm for specifying XML security constraints and investigates the enforcement of such constraints during XML query evaluation. Our approach is based on the novel concept of security views, which provide for each user group (a) an XML view consisting of all and only the information that the users are authorized to access, and (b) a view DTD that the XML view conforms to. Security views effectively protect sensitive data from access and potential inferences by unauthorized user, and provide authorized users with necessary schema information to facilitate effective query formulation and optimization. We propose an efficient algorithm for deriving security view \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Communication-efficient online detection of network-wide anomalies",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4215606/",
            "Abstract": "There has been growing interest in building large-scale distributed monitoring systems for sensor, enterprise, and ISP networks. Recent work has proposed using principal component analysis (PCA) over global traffic matrix statistics to effectively isolate network-wide anomalies. To allow such a PCA-based anomaly detection scheme to scale, we propose a novel approximation scheme that dramatically reduces the burden on the production network. Our scheme avoids the expensive step of centralizing all the data by performing intelligent filtering at the distributed monitors. This filtering reduces monitoring bandwidth overheads, but can result in the anomaly detector making incorrect decisions based on a perturbed view of the global data set. We employ stochastic matrix perturbation theory to bound such errors. Our algorithm selects the filtering parameters at local monitors such that the errors made by the detector \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:M3ejUd6NZC8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Complex event recognition in the big data era",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/3137765.3137829",
            "Abstract": "The concept of event processing is established as a generic computational paradigm in various application fields, ranging from data processing in Web environments, over maritime and transport, to finance and medicine. Events report on state changes of a system and its environment. Complex Event Recognition (CER) in turn, refers to the identification of complex/composite events of interest, which are collections of simple events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. Examples include the recognition of attacks in computer network nodes, human activities on video content, emerging stories and trends on the Social Web, traffic and transport incidents in smart cities, fraud in electronic marketplaces, cardiac arrhythmias, and epidemic spread. In each scenario, CER allows to make sense of Big event Data streams and react accordingly. The goal of this tutorial \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:bz8QjSJIRt4C",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Multi-Resource Parallel Query Scheduling and Optimization",
            "Publication year": 2014,
            "Publication url": "https://arxiv.org/abs/1403.7729",
            "Abstract": "Scheduling query execution plans is a particularly complex problem in shared-nothing parallel systems, where each site consists of a collection of local time-shared (e.g., CPU(s) or disk(s)) and space-shared (e.g., memory) resources and communicates with remote sites by message-passing. Earlier work on parallel query scheduling employs either (a) one-dimensional models of parallel task scheduling, effectively ignoring the potential benefits of resource sharing, or (b) models of globally accessible resource units, which are appropriate only for shared-memory architectures, since they cannot capture the affinity of system resources to sites. In this paper, we develop a general approach capturing the full complexity of scheduling distributed, multi-dimensional resource units for all forms of parallelism within and across queries and operators. We present a level-based list scheduling heuristic algorithm for independent query tasks (i.e., physical operator pipelines) that is provably near-optimal for given degrees of partitioned parallelism (with a worst-case performance ratio that depends on the number of time-shared and space-shared resources per site and the granularity of the clones). We also propose extensions to handle blocking constraints in logical operator (e.g., hash-join) pipelines and bushy query plans as well as on-line task arrivals (e.g., in a dynamic or multi-query execution environment). Experiments with our scheduling algorithms implemented on top of a detailed simulation model verify their effectiveness compared to existing approaches in a realistic setting. Based on our analytical and experimental results, we revisit the open problem \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:tkaPQYYpVKoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Structure and value synopses for XML data graphs",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/B9781558608696500482",
            "Abstract": "This chapter proposes a novel XSKETCH graph synopsis model for eXtensible Markup Language (XML) data graphs with raw data values. All existing proposals for querying XML rely on a pattern-specification language that allows path navigation and branching through the label structure of the XML data graph, and predicates on the values of specific path/branch nodes in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile time selectivity estimates for complex path expressions over graph-structured XML data. XML is rapidly emerging as the new standard for data representation and exchange on the Internet. The simple, self-describing nature of the XML standard promises to enable a broad suite of next-generation Internet applications, ranging from intelligent Web searching and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:LkGwnXOMwfcC",
            "Publisher": "Morgan Kaufmann"
        },
        {
            "Title": "Continuous fragmented skylines over distributed streams",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6816645/",
            "Abstract": "Distributed skyline computation is important for a wide range of application domains, from distributed and web-based systems to ISP-network monitoring and distributed databases. The problem is particularly challenging in dynamic distributed settings, where the goal is to efficiently monitor a continuous skyline query over a collection of distributed streams. All existing work relies on the assumption of a single point of reference for object attributes/dimensions, i.e., objects may be vertically or horizontally partitioned, but the accurate value of each dimension for each object is always maintained by a single site. This assumption is unrealistic for several distributed monitoring applications, where object information is fragmented over a set of distributed streams (each monitored by a different site) and needs to be aggregated (e.g., averaged) across several sites. Furthermore, it is frequently useful to define skyline \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:0KyAp5RtaNEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On Configuring BGP Route Reflectors",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4268087/",
            "Abstract": "The Border Gateway Protocol (BGP) is the standard protocol for exchanging routing information between border routers of Autonomous Systems (ASes) in today's Internet. Within an AS, border routers exchange externally-learned BGP route advertisements via Internal-BGP (I-BGP) peerings. Naive solutions for these I-BGP peering sessions (e.g., based on full-mesh topologies) simply cannot scale to the sizes of modern AS networks. Carefully designed route-reflector configurations can drastically reduce the total number and connection cost of the required I-BGP sessions. Nevertheless, no principled algorithmic approaches exist for designing such configurations, and current practice relies on manual reflector selection using simple, ad-hoc rules. In this paper, we address the novel and challenging optimization problems involved in designing effective BGP route-reflector configurations for AS networks. More \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:vRqMK49ujn8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Probabilistic Data Management for Pervasive Computing: The Data Furnace Project.",
            "Publication year": 2006,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.749&rep=rep1&type=pdf",
            "Abstract": "The wide deployment of wireless sensor and RFID (Radio Frequency IDentification) devices is one of the key enablers for next-generation pervasive computing applications, including large-scale environmental monitoring and control, context-aware computing, and \u201csmart digital homes\u201d. Sensory readings are inherently unreliable and typically exhibit strong temporal and spatial correlations (within and across different sensing devices); effective reasoning over such unreliable streams introduces a host of new data management challenges. The Data Furnace project at Intel Research and UC-Berkeley aims to build a probabilistic data management infrastructure for pervasive computing environments that handles the uncertain nature of such data as a first-class citizen through a principled framework grounded in probabilistic models and inference techniques.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:iH-uZ7U-co4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Prediction-based geometric monitoring over distributed data streams",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2213836.2213867",
            "Abstract": "Many modern streaming applications, such as online analysis of financial, network, sensor and other forms of data are inherently distributed in nature. An important query type that is the focal point in such application scenarios regards actuation queries, where proper action is dictated based on a trigger condition placed upon the current value that a monitored function receives. Recent work studies the problem of (non-linear) sophisticated function tracking in a distributed manner. The main concept behind the geometric monitoring approach proposed there, is for each distributed site to perform the function monitoring over an appropriate subset of the input domain. In the current work, we examine whether the distributed monitoring mechanism can become more efficient, in terms of the number of communicated messages, by extending the geometric monitoring framework to utilize prediction models. We initially \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:Y5dfb0dijaUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "INforE: Interactive Cross-platform Analytics for Everyone",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3340531.3417435",
            "Abstract": "We present INforE, a prototype supporting non-expert programmers in performing optimized, cross-platform, streaming analytics at scale. INforE offers: a) a new extension to the RapidMiner Studio for graphical design of Big streaming Data workflows,(b) a novel optimizer to instruct the execution of workflows across Big Data platforms and clusters,(c) a synopses data engine for interactivity at scale via the use of data summaries,(d) a distributed, online data mining and machine learning module. To our knowledge INforE is the first holistic approach in streaming settings. We demonstrate INforE in the fields of life science and financial data analysis.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:gKiMpY-AVTkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Approximate query processing using wavelets",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/article/10.1007/s007780100049",
            "Abstract": " Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:d1gkVwhDpl0C",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Reviewer profiling using sparse matrix regression",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5693432/",
            "Abstract": "Thousands of scientific conferences happen every year, and each involves a laborious scientific peer review process conducted by one or more busy scientists serving as Technical/Scientific Program Committee (TPC) chair(s). The chair(s) must match submitted papers to their reviewer pool in such a way that i) each paper is reviewed by experts in its subject matter, and ii) no reviewer is overloaded with reviews or under-utilized. Towards this end, seasoned TPC chairs know the value of reviewer and paper profiling: summarizing the expertise/interests of each reviewer and the subject matter of each paper using judiciously chosen domain-specific keywords. An automated profiling algorithm is proposed for this purpose, which starts from generic/noisy reviewer profiles extracted using Google Scholar and derives custom conference-centric reviewer and paper profiles. Each reviewer is expert on few sub-topics \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:abG-DnoFyZgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Declarative networking: language, execution and optimization",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1142473.1142485",
            "Abstract": "The networking and distributed systems communities have recently explored a variety of new network architectures, both for application-level overlay networks, and as prototypes for a next-generation Internet architecture. In this context, we have investigated declarative networking: the use of a distributed recursive query engine as a powerful vehicle for accelerating innovation in network architectures [23, 24, 33]. Declarative networking represents a significant new application area for database research on recursive query processing. In this paper, we address fundamental database issues in this domain. First, we motivate and formally define the Network Datalog (NDlog) language for declarative network specifications. Second, we introduce and prove correct relaxed versions of the traditional semi-na\u00efve query evaluation technique, to overcome fundamental problems of the traditional technique in an asynchronous \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "40_ AI Ethics in Predictive Policing: From Models of Threat to an Ethics of Care",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8733935/",
            "Abstract": "Table of Contents Page 1 2 IEEE TECHNOLOGY AND SOCIETY MAGAZINE \u2215 JUNE 2019 \nDigital Object Identifier 10.1109/MTS.2019.2913063 Date of publication: 30 May 2019 ON THE \nCOVER: WHATEVER THE PROMISE OF AUTOMATION AND ARTIFICIAL INTELLIGENCE, \nFUTURE TRANSPORTATION WILL REQUIRE AN INTEGRATED AND TRANSPARENT \nAPPROACH. SEE PP. 22\u201338 OF THIS ISSUE FOR MORE. IMAGE: ISTOCK/OWNGARDEN. \nVolume 38, Number 2, June 2019 www.technologyandsociety.org *Refereed articles. Fe a tu re \ns 40_ AI Ethics in Predictive Policing: From Models of Threat to an Ethics of Care Peter M. Asaro \n54_ Interactive Extreme-Scale Analytics: Towards Battling Cancer* Nikos Giatrakos, Nikos \nKatzouris, Antonios Deligiannakis, Alexander Artikis, Minos Garofalakis, George Paliouras, \nHolger Arndt, Raffaele Grasso, Ralf Klinkenberg, Miguel Ponce De Le\u00f3n, Gian Gaetano , , : \u2019\u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:Bg7qf7VwUHIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tracking set-expression cardinalities over continuous update streams",
            "Publication year": 2009,
            "Publication url": "https://patents.google.com/patent/US7596544B2/en",
            "Abstract": "A method of estimating set-expression cardinalities over data streams with guaranteed small maintenance time per data-element update. The method only examines each data element once and uses a limited amount of memory. The time-efficient stream synopsis extends 2-level hash-sketches by randomly, but uniformly, pre-hashing data-elements prior to logarithmically hashing them to a first-level hash-table. This generates a set of independent 2-level hash-sketches. The set-union cardinality can be estimated by determining the smallest hash-bucket index j at which only a predetermined fraction of the b hash-buckets has a non-empty union| A\u222a B|. Once a set-union cardinality is estimated, general set-expression cardinalities may be estimated by counting witness elements for the set-expression, ie, those first-level hash-buckets that are both a singleton for the set-expression and a set-union singleton. The set \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:8AbLer7MMksC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tracking Queries over Distributed Streams",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-28608-0_15",
            "Abstract": "Effective Big Data analytics pose several difficult challenges for modern data management architectures. One key such challenge arises from the naturally streaming nature of big data, which mandates efficient algorithms for querying and analyzing massive, continuous data streams (that is, data that is seen only once and in a fixed order) with limited memory and CPU-time resources. Such streams arise naturally in emerging large-scale event monitoring applications; for instance, network-operations monitoring in large ISPs, where usage information from numerous sites needs to be continuously collected and analyzed for interesting trends. In addition to memory- and time-efficiency concerns, the inherently distributed nature of such applications also raises important communication-efficiency issues, making it critical to carefully optimize the use of the underlying network infrastructure. In this chapter, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:foquWX3nUaYC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Complex event processing over streaming multi-cloud platforms: the FERARI approach",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2933267.2933289",
            "Abstract": "We present FERARI, a prototype for processing voluminous event streams over multi-cloud platforms. At its core, FERARI both exploits the potential for in-situ (intra-cloud) processing and orchestrates inter-cloud complex event detection in a communication-efficient way. At the application level, it includes a user-friendly query authoring tool and an analytics dashboard providing granular reports about detected events. In that, FERARI constitutes, to our knowledge, the first complete end-to-end solution of its kind. In this demo, we apply the FERARI approach on a real scenario from the telecommunication domain.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:i2xiXl-TujoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sketching streams through the net: Distributed approximate query tracking",
            "Publication year": 2005,
            "Publication url": "http://www.vldb.org/archives/website/2005/program/paper/tue/p13-cormode.pdf",
            "Abstract": "Emerging large-scale monitoring applications require continuous tracking of complex dataanalysis queries over collections of physicallydistributed streams. Effective solutions have to be simultaneously space/time efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. They rely on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication-and space/time-efficient solutions. The result is a powerful approximate query tracking framework that readily incorporates several complex analysis queries (including distributed join and multi-join aggregates, and approximate wavelet representations), thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:WF5omc3nYNoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Analyzing massive data streams: Past, present, and future",
            "Publication year": 2003,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/882082.882084",
            "Abstract": "Continuous data streams arise naturally, for example, in the installations of large telecom and Internet service providers where detailed usage information (Call-Detail-Records, SNMP-/RMON packet-flow data, etc.) from different parts of the underlying network needs to be continuously collected and analyzed for interesting trends. Such environments raise a critical need for effective stream-processing algorithms that can provide (typically, approximate) answers to data-analysis queries while utilizing only small space (to maintain concise stream synopses) and small processing time per stream item. In this talk, I will discuss the basic pseudo-random sketching mechanism for building stream synopses and our ongoing work that exploits sketch synopses to build an approximate SQL (multi) query processor. I will also describe our recent results on extending sketching to handle more complex forms of queries and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:dshw04ExmUIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Holistic query evaluation over information extraction pipelines",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/3149193.3149201",
            "Abstract": "We introduce holistic in-database query processing over information extraction pipelines. This requires considering the joint conditional distribution over generic Conditional Random Fields that uses factor graphs to encode extraction tasks. Our approach introduces Canopy Factor Graphs, a novel probabilistic model for effectively capturing the joint conditional distribution given a canopy clustering of the data, and special query operators for retrieving resolution information. Since inference on such models is intractable, we introduce an approximate technique for query processing and optimizations that cut across the integrated tasks for reducing the required processing time. Effectiveness and scalability are verified through an extensive experimental evaluation using real and synthetic data.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:GtLg2Ama23sC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Approximate decision making in large-scale distributed systems",
            "Publication year": 2007,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.439.1687&rep=rep1&type=pdf",
            "Abstract": "As the Internet has evolved into a valuable and critical service platform for business and daily life, the research community has enthusiastically applied data mining methods to improve application performance by analyzing and optimizing the behaviors of the underlying systems (eg, datacenter design, network resource provisioning, network security, etc.) These data mining procedures often use large-scale widelydistributed monitoring systems, which continuously generate numerous distributed data streams, and backhaul all of the data to a central location (eg, a Network Operation Center or NOC) for data analysis and decision making. This application scenario presents both new opportunities and challenges in efficient data analysis and online decision making, where a decision function depends on aggregating and analyzing continuous data streams from distributed monitors. The statistics and machine learning communities have performed extensive research into decision making methods [1], including outlier detection, clustering, classification, etc., with the results being algorithms that mainly assume all data have been collected at a central point, and focus on post-collection data analysis and problem diagnosis, with little consideration of the more general distributed, continuous data collection and analysis problem. We believe that the machine learning community should now focus on the design of algorithms that function well with limited data. We envision two open problems: efficiently performing online decision making with low communication overhead, and providing fine-grain control over the tradeoff between decision accuracy and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:738O_yMBCRsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Lightweight query authentication on streams",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2656336",
            "Abstract": "We consider a stream outsourcing setting, where a data owner delegates the management of a set of disjoint data streams to an untrusted server. The owner authenticates his streams via signatures. The server processes continuous queries on the union of the streams for clients trusted by the owner. Along with the results, the server sends proofs of result correctness derived from the owner's signatures, which are verifiable by the clients. We design novel constructions for a collection of fundamental problems over streams represented as linear algebraic queries. In particular, our basic schemes authenticate dynamic vector sums, matrix products, and dot products. These techniques can be adapted for authenticating a wide range of important operations in streaming environments, including group-by queries, joins, in-network aggregation, similarity matching, and event processing. We also present extensions to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:KUbvn5osdkgC",
            "Publisher": "ACM"
        },
        {
            "Title": "Network-wide complex event processing over geographically distributed data sources",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0306437919304946",
            "Abstract": "In this paper we focus on Complex Event Processing (CEP) applications where the data is generated by sites that are geographically dispersed across large regions. This geographic distribution, combined with the size of the collected data, imposes severe communication and computation challenges. To attack these challenges, we propose a novel approach for geographically distributed CEP, which combines algorithmic and systems contributions. At an algorithmic level, our work combines an in-network processing approach, which pushes parts of the processing (i.e., CEP operators) towards the sources of their input events, along with a push\u2013pull paradigm, in order to reduce the amount of communicated events. We present optimal (but computationally expensive) solutions which seek to minimize the maximum bandwidth consumption given input latency constraints for detecting events, as well as efficient \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:_OXeSy2IsFwC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Model-Based Semantic Compression for Network-Data Tables",
            "Publication year": 2001,
            "Publication url": "http://ilpubs.stanford.edu:8090/495/",
            "Abstract": "While a variety of lossy compression schemes have been developed for certain forms of digital data (e.g., images, audio, video),  the area of lossy compression techniques for arbitrary data tables has been left relatively unexplored. Nevertheless, such techniques  are clearly motivated  by the ever-increasing data collection rates of modern enterprises and the need for effective, guaranteed-quality approximate answers to queries over massive relational data sets. In this paper, we propose Model-Based Semantic Compression (MBSC), a novel data compression framework that takes  advantage of  attribute semantics and data-mining models to perform lossy compression of massive data tables. We describe the architecture and algorithms underlying SPARTAN, a model-based semantic compression  system  that exploits   predictive data correlations and  prescribed error tolerances for individual attributes to construct concise and accurate  Classification and Regression Tree (CaRT) models  for entire columns of a table. Our experimentation with several real-life data sets has offered convincing evidence of the effectiveness of SPARTAN's model-based approach --   SPARTAN is able to consistently yield substantially better compression ratios than existing semantic or syntactic compression tools (e.g., gzip)  while utilizing only small data samples for model inference. Several promising directions for future research and possible applications of MBSC in the context of network management are identified and discussed.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:4fKUyHm3Qg0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Continuous Distributed Stream Querying using Sketches",
            "Publication year": 2008,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.5897&rep=rep1&type=pdf",
            "Abstract": "While traditional database systems optimize for performance on one-shot query processing, emerging largescale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically-distributed streams. Thus, effective solutions have to be simultaneously space/time efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. In a nutshell, our algorithms rely on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication-and space/time-efficient solutions. The end result is a powerful approximate query tracking framework that readily incorporates several complex analysis queries (including distributed join and multi-join aggregates, and approximate wavelet representations), thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model. Experiments with real data validate our approach, revealing significant savings over naive solutions as well as our analytical worst-case guarantees.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:LjlpjdlvIbIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Method for distinct count estimation over joins of continuous update stream",
            "Publication year": 2010,
            "Publication url": "https://patents.google.com/patent/US7668856B2/en",
            "Abstract": "The invention provides methods and systems for summarizing multiple continuous update streams such that an approximate answer to a query over one or more of the continuous update streams (such as a Query requiring a join operation followed by a duplicate elimination step) may be rapidly provided. The systems and methods use multiple (parallel) Join Distinct (JD) Sketch data structures corresponding to hash buckets of at least one initial attribute.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:eJXPG6dFmWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Topology discovery in heterogeneous IP networks: the NetInventory system",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1306489/",
            "Abstract": "Knowledge of the up-to-date physical topology of an IP network is crucial to a number of critical network management tasks, including reactive and proactive resource management, event correlation, and root-cause analysis. Given the dynamic nature of today's IP networks, keeping track of topology information manually is a daunting (if not impossible) task. Thus, effective algorithms for automatically discovering physical network topology are necessary. Earlier work has typically concentrated on either 1) discovering logical (i.e., layer-3) topology, which implies that the connectivity of all layer-2 elements (e.g., switches and bridges) is ignored, or 2) proprietary solutions targeting specific product families. In this paper, we present novel algorithms for discovering physical topology in heterogeneous (i.e., multi-vendor) IP networks. Our algorithms rely on standard SNMP MIB information that is widely supported by modern \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:YsMSGLbcyi4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Tree pattern aggregation for scalable XML data dissemination",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/B9781558608696500780",
            "Abstract": "XML (Extensible Markup Language) has become the dominant standard for data encoding and exchange on the Internet, including e-business transactions in both business-to-business (B2B) and business-to-consumer (B2C) applications. Given the rapid growth of XML traffic on the Internet, the effective and efficient delivery of XML documents has become an important issue. Consequently, there is a growing interest in the area of XML content-based filtering and routing, which addresses the problem of effectively directing high volumes of XML-document traffic to interested consumers based on document contents. Effective support for scalable, content-based XML routing is crucial to enable efficient and timely delivery of relevant XML documents to a large, dynamic group of consumers.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:3fE2CSJIrl8C",
            "Publisher": "Morgan Kaufmann"
        },
        {
            "Title": "Processing set expressions over continuous update streams",
            "Publication year": 2003,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/872757.872790",
            "Abstract": "There is growing interest in algorithms for processing and querying continuous data streams (ie, data that is seen only once in a fixed order) with limited memory resources. In its most general form, a data stream is actually an update stream, ie, comprising data-item deletions as well as insertions. Such massive update streams arise naturally in several application domains (eg, monitoring of large IP network installations, or processing of retail-chain transactions). Estimating the cardinality of set expressions defined over several (perhaps, distributed) update streams is perhaps one of the most fundamental query classes of interest; as an example, such a query may ask\" what is the number of distinct IP source addresses seen in passing packets from both router R 1 and R 2 but not router R 3?\". Earlier work has only addressed very restricted forms of this problem, focusing solely on the special case of insert-only \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:MXK_kJrjxJIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "In-network PCA and anomaly detection",
            "Publication year": 2006,
            "Publication url": "https://www.researchgate.net/profile/Michael-Jordan-3/publication/228820124_In-Network_PCA_and_anomaly_detection/links/53d6746c0cf2a7fbb2eaa5f5/In-Network-PCA-and-anomaly-detection.pdf",
            "Abstract": "We consider the problem of network anomaly detection in large distributed systems. In this setting, Principal Component Analysis (PCA) has been proposed as a method for discovering anomalies by continuously tracking the projection of the data onto a residual subspace. This method was shown to work well empirically in highly aggregated networks, that is, those with a limited number of large nodes and at coarse time scales. This approach, however, has scalability limitations. To overcome these limitations, we develop a PCA-based anomaly detector in which adaptive local data filters send to a coordinator just enough data to enable accurate global detection. Our method is based on a stochastic matrix perturbation analysis that characterizes the tradeoff between the accuracy of anomaly detection and the amount of data communicated over the network.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sketch-based geometric monitoring of distributed stream queries",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/2536206.2536220",
            "Abstract": "Emerging large-scale monitoring applications rely on continuous tracking of complex data-analysis queries over collections of massive, physically-distributed data streams. Thus, in addition to the space- and time-efficiency requirements of conventional stream processing (at each remote monitor site), effective solutions also need to guarantee communication efficiency (over the underlying communication network). The complexity of the monitored query adds to the difficulty of the problem -- this is especially true for nonlinear queries (e.g., joins), where no obvious solutions exist for distributing the monitor condition across sites. The recently proposed geometric method offers a generic methodology for splitting an arbitrary (non-linear) global threshold-monitoring task into a collection of local site constraints; still, the approach relies on maintaining the complete stream(s) at each site, thus raising serious efficiency \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:nrtMV_XWKgEC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Methods and apparatus for representing probabilistic data using a probabilistic histogram",
            "Publication year": 2012,
            "Publication url": "https://patents.google.com/patent/US8145669B2/en",
            "Abstract": "Methods and apparatus for representing probabilistic data using a probabilistic histogram are disclosed. An example method comprises partitioning a plurality of ordered data items into a plurality of buckets, each of the data items capable of having a data value from a plurality of possible data values with a probability characterized by a respective individual probability distribution function (PDF), each bucket associated with a respective subset of the ordered data items bounded by a respective beginning data item and a respective ending data item, and determining a first representative PDF for a first bucket associated with a first subset of the ordered data items by partitioning the plurality of possible data values into a first plurality of representative data ranges and respective representative probabilities based on an error between the first representative PDF and a first plurality of individual PDFs characterizing the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:BrmTIyaxlBUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Display advertising inventory estimation",
            "Publication year": 2010,
            "Publication url": "https://patents.google.com/patent/US20100082428A1/en",
            "Abstract": "Example embodiments described herein may relate to estimating inventory for a display advertising system utilized, for example, in Web-based advertising.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:5ugPr518TE4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Uncertainty-Aware Event Analytics over Distributed Settings: Industry Paper",
            "Publication year": 2019,
            "Publication url": "http://www.softnet.tuc.gr/~adeli/papers/conferences/DEBS2019.pdf",
            "Abstract": "Complex event processing (CEP) refers to a generic computational paradigm where simple derived event (SDE) tuples are combined in pattern matching procedures so as to derive higher level, complex events (CE) of interest. Consider, for exhibition purposes, a mobile fraud detection application. A rule (pattern) of the form \u201cReport long (lasting more than Y minutes) calls to premium locations\u201d requires two SDEs to occur in sequence. The SDE of the initiation of a call to a premium location should be followed by the SDE of its duration surpassing a certain threshold Y. In case this sequence of SDEs occurs, a full pattern match exists. The corresponding output CE constitutes a high level representation of the business event, which in this particular occasion captures a mobile fraud incident. Modern Big Data applications analyze event streams in an online fashion and aim at extracting CEs in real-time so as to support critical decision making procedures. In our simple running example, streams refer to ongoing call records and CEs correspond to mobile fraud detection pattern matches. The decision that needs to be made in real-time involves the cut-off of an ongoing call in case it matches a fraud pattern, so that further monetary losses for the telecom provider are avoided.Big Data applications at massive scale usually operate over distributed, networked architectures. This is because data are not generated within corporate data centers, public or hybrid clouds but instead, event streams originate from a number of geo-distributed sites. Distributed architectures are ubiquitous in Big Data application scenarios ranging from Internet-of-Things (IoT) and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:GFxP56DSvIMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Configure, generate, run: Model-based development for big data processing",
            "Publication year": 2016,
            "Publication url": "https://scholar.google.com/scholar?cluster=9460456327368428107&hl=en&oi=scholarr",
            "Abstract": "The development of efficient and robust algorithms for Big Data processing is a demanding task, which has to cope with the characteristics of this type of data (3Vs). Putting such algorithms as processing elements into larger pipelines adds an extra level of complexity, which can be alleviated by relying on a model-based approach including code generation. This allows data analysts to compose such pipelines on a higher level of abstraction, reducing the development effort as well as the risk of errors. In this chapter, we outline a model-based and adaptive approach to the development of data processing pipelines in heterogeneous processing contexts. It relies on a flexible, tool-supported approach to configuration, which embraces three levels:(a) a heterogeneous processing infrastructure-including reconfigurable hardware,(b) the pipelines as well as (c) the stakeholder applications built upon the pipelines \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:Aul-kAQHnToC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Monitoring distributed streams using convex decompositions",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/2735479.2735487",
            "Abstract": "Emerging large-scale monitoring applications rely on continuous tracking of complex data-analysis queries over collections of massive, physically-distributed data streams. Thus, in addition to the space- and time-efficiency requirements of conventional stream processing (at each remote monitor site), effective solutions also need to guarantee communication efficiency (over the underlying communication network). The complexity of the monitored query adds to the difficulty of the problem --- this is especially true for non-linear queries (e.g., joins), where no obvious solutions exist for distributing the monitored condition across sites. The recently proposed geometric method, based on the notion of covering spheres, offers a generic methodology for splitting an arbitrary (non-linear) global condition into a collection of local site constraints, and has been applied to massive distributed stream-monitoring tasks, achieving \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:L7CI7m0gUJcC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Mining sequential patterns with regular expression constraints",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1000341/",
            "Abstract": "Discovering sequential patterns is an important problem in data mining with a host of application domains including medicine, telecommunications, and the World Wide Web. Conventional sequential pattern mining systems provide users with only a very restricted mechanism (based on minimum support) for specifying patterns of interest. As a consequence, the pattern mining process is typically characterized by lack of focus and users often end up paying inordinate computational costs just to be inundated with an overwhelming number of useless results. We propose the use of Regular Expressions (REs) as a flexible constraint specification tool that enables user-controlled focus to be incorporated into the pattern mining process. We develop a family of novel algorithms (termed SPIRIT-Sequential Pattern mining with Regular expression consTraints) for mining frequent sequential patterns that also satisfy user \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:u-x6o8ySG0sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Holistic aggregates in a networked world: Distributed tracking of approximate quantiles",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1066157.1066161",
            "Abstract": "While traditional database systems optimize for performance on one-shot queries, emerging large-scale monitoring applications require continuous tracking of complex aggregates and data-distribution summaries over collections of physically-distributed streams. Thus, effective solutions have to be simultaneously space efficient (at each remote site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality estimates. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking complex holistic aggregates in such a distributed-streams setting---our primary focus is on approximate quantile summaries, but our approach is more broadly applicable and can handle other holistic-aggregate functions (eg,\" heavy-hitters\" queries). We present the first known distributed-tracking schemes for maintaining accurate quantile estimates \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:W7OEmFMy1HYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Granularity conscious modeling for probabilistic databases",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4476714/",
            "Abstract": "The convergence of embedded sensor systems and stream query processing suggests an important role for database techniques, in managing data that only partially  and of- ten inaccurately  capture the state of the world. Reasoning about uncertainty as a first class citizen, inside a database system, becomes an increasingly important operation for processing non deterministic data. An essential step for such an approach lies in the choice of the appropriate un- certainty model, that captures the probabilistic information in the data, both accurately and at the right semantic de- tail level. This paper introduces Hierarchical First-Order Graphical Models (HFGMs), an intuitive and economical representation of the data correlations stored in a Proba- bilistic Data Management system, in a hierarchical setting. HFGM semantics allow for an efficient summarization of the probabilistic model that can be induced from a dataset \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:u9iWguZQMMsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Distributed sparse random projections for refinable approximation",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1236360.1236403",
            "Abstract": "Consider a large-scale wireless sensor network measuring compressible data, where n distributed data values can be well-approximated using only k \u00abn coefficients of some known transform. We address the problem of recovering an approximation of the n data values by querying any L sensors, so that the reconstruction error is comparable to the optimal k-term approximation. To solve this problem, we present a novel distributed algorithm based on sparse random projections, which requires no global coordination or knowledge. The key idea is that the sparsity of the random projections greatly reduces the communication cost of pre-processing the data. Our algorithm allows the collector to choose the number of sensors to query according to the desired approximation error. The reconstruction quality depends only on the number of sensors queried, enabling robust refinable approximation.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:YOwf2qJgpHMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "System and method for constraint based sequential pattern mining",
            "Publication year": 2002,
            "Publication url": "https://patents.google.com/patent/US6473757B1/en",
            "Abstract": "The present invention provides a method and system for sequential pattern mining with a given constraint. A Regular Expression (RE) is used for identifying the family of interesting frequent patterns. A family of methods that enforce the RE constraint to different degrees within the generating and pruning of candidate patterns during the mining process is utilized. This is accomplished by employing different relaxations of the RE constraint in the mining loop. Those sequences which satisfy the given constraint are thus identified most expeditiously.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:R3hNpaxXUhUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Interactive Extreme-Scale Analytics: Towards Battling Cancer",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8733936/",
            "Abstract": "A synergetic understanding of cancer evolution and the effect of combination drug therapies on the disease is the cornerstone for developing effective personalized treatments, which can radically improve patients' well-being and their quality of (work and social) life. By extension, improving the treatment of patients indirectly enhances the quality of life for families, friends, and careers. Moreover, personalizing effective therapeutic approaches reduces treatment duration, cutting down healthcare monetary costs, which can be redirected to other health and social services. Given that three out of four U.S. families will at some point experience a family member suffering from cancer (http://natamcancer.org/NAP_Native_American_Priorities.pdf), the potential impact of improved cancer treatment is of considerable socio-economic and organizational significance.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:F9fV5C73w3QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "XTRACT: A system for extracting document type descriptors from XML documents",
            "Publication year": 2000,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/342009.335409",
            "Abstract": "XML is rapidly emerging as the new standard for data representation and exchange on the Web. An XML document can be accompanied by a Document Type Descriptor (DTD) which plays the role of a schema for an XML data collection. DTDs contain valuable information on the structure of documents and thus have a crucial role in the efficient storage of XML data, as well as the effective formulation and optimization of XML queries. In this paper, we propose XTRACT, a novel system for inferring a DTD schema for a database of XML documents. Since the DTD syntax incorporates the full expressive power of regular expressions, naive approaches typically fail to produce concise and intuitive DTDs. Instead, the XTRACT inference algorithms employ a sequence of sophisticated steps that involve:(1) finding patterns in the input sequences and replacing them with regular expressions to generate \u201cgeneral\u201d candidate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:qjMakFHDy7sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "XTRACT: Learning document type descriptors from XML document collections",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/article/10.1023/A:1021560618289",
            "Abstract": "XML is rapidly emerging as the new standard for data representation and exchange on the Web. Unlike HTML, tags in XML documents describe the semantics of the data and not how it is to be displayed. In addition, an XML document can be accompanied by a Document Type Descriptor (DTD) which plays the role of a schema for an XML data collection. DTDs contain valuable information on the structure of documents and thus have a crucial role in the efficient storage of XML data, as well as the effective formulation and optimization of XML queries. Despite their importance, however, DTDs are not mandatory, and it is frequently possible that documents in XML databases will not have accompanying DTDs. In this paper, we propose XTRACT, a novel system for inferring a DTD schema for a database of XML documents. Since the DTD syntax incorporates the full expressive power of regular expressions \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Distributed query monitoring through convex analysis: Towards composable safe zones",
            "Publication year": 2017,
            "Publication url": "https://drops.dagstuhl.de/opus/volltexte/2017/7066/",
            "Abstract": "Continuous tracking of complex data analytics queries over high-speed distributed streams is becoming increasingly important. Query tracking can be reduced to continuous monitoring of a condition over the global stream. Communication-efficient monitoring relies on locally processing stream data at the sites where it is generated, by deriving site-local conditions which collectively guarantee the global condition. Recently proposed geometric techniques offer a generic approach for splitting an arbitrary global condition into local geometric monitoring constraints (known as\" Safe Zones\"); still, their application to various problem domains has so far been based on heuristics and lacking a principled, compositional methodology. In this paper, we present the first known formal results on the difficult problem of effective Safe Zone (SZ) design for complex query monitoring over distributed streams. Exploiting tools from convex analysis, our approach relies on an algebraic representation of SZs which allows us to:(1) Formally define the notion of a\" good\" SZ for distributed monitoring problems; and, most importantly,(2) Tackle and solve the important problem of systematically composing SZs for monitored conditions expressed as Boolean formulas over simpler conditions (for which SZs are known); furthermore, we prove that, under broad assumptions, the composed SZ is good if the component SZs are good. Our results are, therefore, a first step towards a principled compositional solution to SZ design for distributed query monitoring. Finally, we discuss a number of important applications for our SZ design algorithms, also demonstrating how earlier \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:PVjk1bu6vJQC",
            "Publisher": "Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik"
        },
        {
            "Title": "Histograms and wavelets on probabilistic data",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5453377/",
            "Abstract": "There is a growing realization that uncertain information is a first-class citizen in modern database management. As such, we need techniques to correctly and efficiently process uncertain data in database systems. In particular, data reduction techniques that can produce concise, accurate synopses of large probabilistic relations are crucial. Similar to their deterministic relation counterparts, such compact probabilistic data synopses can form the foundation for human understanding and interactive data exploration, probabilistic query planning and optimization, and fast approximate query processing in probabilistic database systems. In this paper, we introduce definitions and algorithms for building histogram- and Haar wavelet-based synopses on probabilistic data. The core problem is to choose a set of histogram bucket boundaries or wavelet coefficients to optimize the accuracy of the approximate representation of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:BqipwSGYUEgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Approximate Query Processing: Taming the TeraBytes.",
            "Publication year": 2001,
            "Publication url": "http://www.vldb.org/conf/2001/tut4.pdf",
            "Abstract": "+ Seeing entire data is very helpful (provably & in practice)(But must construct synopses for a family of queries)+ Often faster: better access patterns, small synopses can reside in memory or cache+ Middleware: Can use with any DBMS, no special index striding+ Also effective for remote or streaming data",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:kNdYIx-mwKoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Querying Big, Dynamic, Distributed Data.",
            "Publication year": 2014,
            "Publication url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/Querying-Big-Dynamic-Distributed-Data-Slides.pdf",
            "Abstract": "Querying Big, Dynamic, Distributed Data Page 1 1 Querying Big, Dynamic, Distributed Data \nMinos Garofalakis Technical University of Crete Software Technology and Network \nApplications Lab LIFT Cast: Antonios Deligiannakis, Vasilis Samoladas, Odysseas Papapetrou, \nNikos Giatrakos (TUC); Daniel Keren (Haifa U), Assaf Schuster, Tsachi Sharfman (Technion) \nPage 2 2 MSR BDA\u20192013 Big Data is Big News (and Big Business\u2026) Rapid growth due to \nseveral informationgenerating technologies, such as mobile computing, sensornets, and social \nnetworks How can we cost-effectively manage and analyze all this data\u2026? Page 3 3 MSR \nBDA\u20192013 Big Data Challenges: The Four V\u2019s (and one D)\u2026 Volume: Scaling from Terabytes to \nExa/Zettabytes Velocity: Processing massive amounts of streaming data Variety: Managing the \ncomplexity of multiple relational and nonrelational data types and schemas Veracity: the and : \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:EYYDruWGBe4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Document descriptor extraction method",
            "Publication year": 2006,
            "Publication url": "https://patents.google.com/patent/US7080314B1/en",
            "Abstract": "The present invention discloses a document descriptor extraction method and system. The document descriptor extraction method and system creates a document descriptor by generalizing input sequences within a document; factoring the input sequences and generalized input sequences; and selecting a document descriptor from the input sequences, generalized sequences, and factored sequences, preferably using minimum descriptor length (MDL) principles. Novel algorithms are employed to perform the generalizing, factoring, and selecting.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:CHSYGLWDkRkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Proof sketches: Verifiable in-network aggregation",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4221748/",
            "Abstract": "A work on distributed, in-network aggregation assumes a benign population of participants. Unfortunately, modern distributed systems are plagued by malicious participants. In this paper we present a first step towards verifiable yet efficient distributed, in-network aggregation in adversarial settings. We describe a general framework and threat model for the problem and then present proof sketches, a compact verification mechanism that combines cryptographic signatures and Flajolet-Martin sketches to guarantee acceptable aggregation error bounds with high probability. We derive proof sketches for count aggregates and extend them for random sampling, which can be used to provide verifiable approximations for a broad class of data-analysis queries, e.g., quantiles and heavy hitters. Finally, we evaluate the practical use of proof sketches, and observe that adversaries can often be reduced to much smaller \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:-f6ydRqryjwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Probabilistic histograms for probabilistic data",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/1687627.1687687",
            "Abstract": "There is a growing realization that modern database management systems (DBMSs) must be able to manage data that contains uncertainties that are represented in the form of probabilistic relations. Consequently, the design of each core DBMS component must be revisited in the presence of uncertain and probabilistic information. In this paper, we study how to build histogram synopses for probabilistic relations, for the purposes of enabling both DBMS-internal decisions (such as indexing and query planning), and (possibly, user-facing) approximate query processing tools. In contrast to initial work in this area, our probabilistic histograms retain the key possible-worlds semantics of probabilistic data, allowing for more accurate, yet concise, representation of the uncertainty characteristics of data and query results. We present a variety of techniques for building optimal probabilistic histograms, each one tuned to a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:rO6llkc54NcC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Approximate XML query answers",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1007568.1007599",
            "Abstract": "The rapid adoption of XML as the standard for data representation and exchange foreshadows a massive increase in the amounts of XML data collected, maintained, and queried over the Internet or in large corporate data-stores. Inevitably, this will result in the development of on-line decision support systems, where users and analysts interactively explore large XML data sets through a declarative query interface (eg, XQuery or XSLT). Given the importance of remaining interactive, such on-line systems can employ approximate query answers as an effective mechanism for reducing response time and providing users with early feedback. This approach has been successfully used in relational systems and it becomes even more compelling in the XML world, where the evaluation of complex queries over massive tree-structured data is inherently more expensive. In this paper, we initiate a study of approximate query \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:UebtZRa9Y70C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Guest Editors' Introduction: Special Section on Mining Large Uncertain and Probabilistic Databases",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5518330/",
            "Abstract": "The four papers in this special section were selected from 23 submissions and represent recent advances in the mining of uncertain databases. The works present new techniques for mining patterns, clustering, and ranking on uncertain data.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:XiSMed-E-HIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Biological interaction networks based on sparse temporal expansion of graphical models",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6399721/",
            "Abstract": "Biological networks are often described as probabilistic graphs in the context of gene and protein sequence analysis in molecular biology. Microarrays and proteomics technology allow the monitoring of expression levels over thousands of biological units over time. In experimental efforts we are interested in unveiling pairwise interactions. Many graphical models have been introduced in order to discover associations from the expression data analysis. However, the small size of samples compared to the number of observed genes/proteins makes the inference of the network structure quite challenging. In this study we generate gene-protein networks from sparse experimental data using two methods, partial correlations and Kernel Density Estimation, in order to capture genetic interactions. Dynamic Gaussian analysis is used to match special characteristics to genes and proteins at different time stages utilizing the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:NJ774b8OgUMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Practical private range search in depth",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3167971",
            "Abstract": "We consider a data owner that outsources its dataset to an untrusted server. The owner wishes to enable the server to answer range queries on a single attribute, without compromising the privacy of the data and the queries. There are several schemes on \u201cpractical\u201d private range search (mainly in database venues) that attempt to strike a trade-off between efficiency and security. Nevertheless, these methods either lack provable security guarantees or permit unacceptable privacy leakages. In this article, we take an interdisciplinary approach, which combines the rigor of security formulations and proofs with efficient data management techniques. We construct a wide set of novel schemes with realistic security/performance trade-offs, adopting the notion of Searchable Symmetric Encryption (SSE), primarily proposed for keyword search. We reduce range search to multi-keyword search using range-covering \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:8d8msizDQcsC",
            "Publisher": "ACM"
        },
        {
            "Title": "Lightweight authentication of linear algebraic queries on data streams",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2463676.2465281",
            "Abstract": "We consider a stream outsourcing setting, where a data owner delegates the management of a set of disjoint data streams to an untrusted server. The owner authenticates his streams via signatures. The server processes continuous queries on the union of the streams for clients trusted by the owner. Along with the results, the server sends proofs of result correctness derived from the owner's signatures, which are easily verifiable by the clients. We design novel constructions for a collection of fundamental problems over streams represented as linear algebraic queries. In particular, our basic schemes authenticate dynamic vector sums and dot products, as well as dynamic matrix products. These techniques can be adapted for authenticating a wide range of important operations in streaming environments, including group by queries, joins, in-network aggregation, similarity matching, and event processing. All our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:-_dYPAW6P2MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sketch-based multi-query processing over data streams",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-28608-0_12",
            "Abstract": "We consider the problem of approximately answering multiple general aggregate SQL queries over continuous data streams with limited memory. Our method extends the randomizing techniques of Alon et al. that compute small \u201csketch\u201d summaries of the streams that can then be used to provide approximate answers to aggregate queries with provable guarantees on the approximation error. By intelligently sharing the sketches among multiple queries, the memory required can be reduced. We provide necessary and sufficient conditions for the sketch sharing to result in correct estimation and address optimization problems that arise. We also demonstrate how existing statistical information on the base data (e.g., histograms) can be used in the proposed framework to improve the quality of the approximation provided by our algorithms. The key idea is to intelligently partition the domain of the underlying \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:Ug5p-4gJ2f0C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Sketching distributed sliding-window data streams",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-015-0380-7",
            "Abstract": "While traditional data management systems focus on evaluating single, ad hoc queries over static data sets in a centralized setting, several emerging applications require (possibly, continuous) answers to queries on dynamic data that is widely distributed and constantly updated. Furthermore, such query answers often need to discount data that is \u201cstale\u201d and operate solely on a sliding window of recent data arrivals (e.g., data updates occurring over the last 24 h). Such distributed data streaming applications mandate novel algorithmic solutions that are both time and space efficient (to manage high-speed data streams) and also communication efficient (to deal with physical data distribution). In this paper, we consider the problem of complex query answering over distributed, high-dimensional data streams in the sliding-window model. We introduce a novel sketching technique (termed ECM-sketch) that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:gsN89kCJA0AC",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "Wavelet synopses for general error metrics",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1114244.1114246",
            "Abstract": "Several studies have demonstrated the effectiveness of the wavelet decomposition as a tool for reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast, accurate approximate query answers. Conventional wavelet synopses that greedily minimize the overall root-mean-squared (i.e., L2-norm) error in the data approximation can suffer from important problems, including severe bias and wide variance in the quality of the data reconstruction, and lack of nontrivial guarantees for individual approximate answers. Thus, probabilistic thresholding schemes have been recently proposed as a means of building wavelet synopses that try to probabilistically control maximum approximation-error metrics (e.g., maximum relative error).A key open problem is whether it is possible to design efficient deterministic wavelet-thresholding algorithms for minimizing general, non-L2 error metrics \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "ACM"
        },
        {
            "Title": "Very Large Databases",
            "Publication year": 2007,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470050118.ecse443",
            "Abstract": "In this article, we provide an overview of date reduction and approximation methods for massive databases and discuss some of the issues that develop from different types of data, large data volumes, and applications\u2010specific requirements.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:_Ybze24A_UAC",
            "Publisher": "John Wiley & Sons, Inc."
        },
        {
            "Title": "Competitive on-line scheduling of continuous-media streams",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0022000001917994",
            "Abstract": "Multimedia applications require a guaranteed level of service for accessing continuous-media data. To obtain such guarantees, the database server where the data are residing must employ an admission control scheme to limit the number of clients that can be served concurrently. We investigate the problem of on-line admission control, where the decision of whether to accept or reject a request must be made without any knowledge about future requests. Employing competitive analysis techniques, we address the problem in its most general form with the following key contributions:(1) We prove a tight upper bound on the competitive ratio of the conventional Work-Conserving (W C) policy, showing that it is within a factor 1+ \u0394 1\u2212 \u03c1 of the optimal clairvoyant strategy, where \u0394 is the ratio of the maximum to minimum request length (ie, time duration), and \u03c1 is the maximum fraction of the server's bandwidth that a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:u_35RYKgDlwC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Efficient filtering of XML documents with XPath expressions",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-002-0077-6",
            "Abstract": " The publish/subscribe paradigm is a popular model for allowing publishers (i.e., data generators) to selectively disseminate data to a large number of widely dispersed subscribers (i.e., data consumers) who have registered their interest in specific information items. Early publish/subscribe systems have typically relied on simple subscription mechanisms, such as keyword or \u201dbag of words\u201d matching, or simple comparison predicates on attribute values. The emergence of XML as a standard for information exchange on the Internet has led to an increased interest in using more expressive subscription mechanisms (e.g., based on XPath expressions) that exploit both the structure and the content of published XML documents. Given the increased complexity of these new data-filtering mechanisms, the problem of effectively identifying the subscription profiles that match an incoming XML document poses a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:u5HHmVD_uO8C",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Topology discovery in heterogeneous IP networks",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/832196/",
            "Abstract": "Knowledge of the up-to-date physical topology of an IP network is crucial to a number of critical network management tasks, including reactive and proactive resource management, event correlation, and root-cause analysis. Given the dynamic nature of today's IP networks, keeping track of topology information manually is a daunting (if not impossible) task. Thus, effective algorithms for automatically discovering physical network topology are necessary. Earlier work has typically concentrated on either: (a) discovering logical (i.e., layer-3) topology, which implies that the connectivity of all layer-2 elements (e.g., switches and bridges) is ignored; or (b) proprietary solutions targeting specific product families. In this paper, we present novel algorithms for discovering physical topology in heterogeneous (i.e., multi-vendor) IP networks. Our algorithms rely on standard SNMP MIB information that is widely supported by \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:IjCSPb-OGe4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Analytics over probabilistic unmerged duplicates",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-11508-5_17",
            "Abstract": "This paper introduces probabilistic databases with unmerged duplicates (DB ud ), i.e., databases containing probabilistic information about instances found to describe the same real-world objects. We discuss the need for efficiently querying such databases and for supporting practical query scenarios that require analytical or summarized information. We also sketch possible methodologies and techniques that would allow performing efficient processing of queries over such probabilistic databases, and especially without the need to materialize the (potentially, huge) collection of all possible deduplication worlds.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:BwyfMAYsbu0C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "MashMaker: mashups for the masses",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1145/1247480.1247626",
            "Abstract": "MashMaker is an interactive tool for editing, querying, manipulating, and visualizing \u201clive\u201d semi-structured data. MashMaker borrows ideas from word processors, web browsers, and spreadsheets. Like a word processor, MashMaker allows ad-hoc, unstructured editing of data. Like a web browser, MashMaker encourages users to find information by exploring, rather than by writing queries. Like a spreadsheet, MashMaker allows users to mix computed values with their data, including editing \u201clive\u201d(ie, continuously-updated) data assembled through the web and/or user queries. MashMaker represents a novel paradigm for the ad-hoc exploration and management of diverse, heterogeneous collections of live data, that draws on the design principles of more \u201cnatural\u201d software tools (like web browsers and spreadsheets) and simple scripting languages, rather than formal database models, schemas, and queries.The goal \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:Se3iqnhoufwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Streaming analytics",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3168836.3168843",
            "Abstract": "Effective Big Data analytics need to rely on algorithms for querying and analyzing massive, continuous data streams (that is, data that is seen only once and in a fixed order) with limited memory and CPU-time resources. Such streams arise naturally in emerging large-scale event monitoring applications; for instance, network-operations monitoring in large ISPs, where usage information from numerous network devices needs to be continuously collected and analyzed for interesting trends and real-time reaction to different scenarios (eg, hotspots or DDoS attacks). In addition to memory-and time-efficiency concerns, the inherently distributed nature of such applications also raises important communication-efficiency issues, making it critical to carefully optimize the use of the underlying communication infrastructure. This course will provide an overview of some key algorithmic tools for supporting effective, real-time \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:FAceZFleit8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Re-tree: an efficient index structure for regular expressions",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-003-0094-0",
            "Abstract": "Due to their expressive power, regular expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multidimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:aqlVkmm33-oC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Intel mash maker: join the web",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1361348.1361355",
            "Abstract": "Intel\u00ae Mash Maker is an interactive tool that tracks what the user is doing and tries to infer what information and visualizations they might find useful for their current task. Mash Maker uses structured data from existing web sites to create new \"mashed up\" interfaces combining information from many sources.The Intel\u00ae Mash Maker client is currently implemented as an extension to the FireFox web browser. Mash Maker adds a toolbar to the browser that shows buttons representing enhancements that Mash Maker believes the user might want to apply to the current page. An enhancement might combine the data on the page with data from another source, or visualize data in a new way. Mash Maker is intended to be an integral part of the way the user browses information, rather than being a special tool that a user uses when they want to create mashups.In order to create mashups from normal websites, Mash Maker \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:hC7cP41nSMkC",
            "Publisher": "ACM"
        },
        {
            "Title": "Processing data-stream join aggregates using skimmed sketches",
            "Publication year": 2009,
            "Publication url": "https://patents.google.com/patent/US7483907B2/en",
            "Abstract": "A method of estimating an aggregate of a join over data-streams in real-time using skimmed sketches, that only examines each data element once and has a worst case space requirement of O (n 2/J), where J is the size of the join and n is the number of data elements. The skimmed sketch is an atomic sketch, formed as the inner product of the data-stream frequency vector and a random binary variable, from which the frequency values that exceed a predetermined threshold have been skimmed off and placed in a dense frequency vector. The join size is estimated as the sum of the sub-joins of skimmed sketches and dense frequency vectors. The atomic sketches may be arranged in a hash structure so that processing a data element only requires updating a single sketch per hash table. This keeps the per-element overhead logarithmic in the domain and stream sizes.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:K3LRdlH-MEoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Compact histograms for hierarchical identifiers",
            "Publication year": 2006,
            "Publication url": "https://www.researchgate.net/profile/Frederick-Reiss-2/publication/221309674_Compact_Histograms_for_Hierarchical_Identifiers/links/02e7e516d969a35d51000000/Compact-Histograms-for-Hierarchical-Identifiers.pdf",
            "Abstract": "Distributed monitoring applications often involve streams of unique identifiers (UIDs) such as IP addresses or RFID tag IDs. An important class of query for such applications involves partitioning the UIDs into groups using a large lookup table; the query then performs aggregation over the groups. We propose using histograms to reduce bandwidth utilization in such settings, using a histogram partitioning function as a compact representation of the lookup table. We investigate methods for constructing histogram partitioning functions for lookup tables over unique identifiers that form a hierarchy of contiguous groups, as is the case with network addresses and several other types of UID. Each bucket in our histograms corresponds to a subtree of the hierarchy. We develop three novel classes of partitioning functions for this domain, which vary in their structure, construction time, and estimation accuracy. Our approach provides several advantages over previous work. We show that optimal instances of our partitioning functions can be constructed efficiently from large lookup tables. The partitioning functions are also compact, with each partition represented by a single identifier. Finally, our algorithms support minimizing any error metric that can be expressed as a distributive aggregate; and they extend naturally to multiple hierarchical dimensions. In experiments on real-world network monitoring data, we show that our histograms provide significantly higher accuracy per bit than existing techniques.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:RYcK_YlVTxYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Data stream statistics over sliding windows: How to summarize 150 Million updates per second on a single node",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8892241/",
            "Abstract": "Traditional data management systems map information using centralized and static data structures. Modern applications need to process in real time datasets much larger than system memory. To achieve this, they use dynamic entities that are updated with streaming input data over a sliding window. For efficient and high performance processing, approximate sketch synopses of input streams have been proposed as effective means for the summarization of streaming data over large sliding windows with probabilistic accuracy guarantees. This work presents a system-level solution to accelerate the Exponential Count-Min (ECM) sketch algorithm on reconfigurable technology. Different reconfigurable architectures for the sketch structure that correspond to different cost and performance tradeoffs are presented. We map the proposed system-level ECM sketch architectures to a high-end modern HPC platform to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:8xutWZnSdmoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Declarative information extraction in a probabilistic database system",
            "Publication year": 2010,
            "Publication url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-120.pdf",
            "Abstract": "Full-text documents represent a large fraction of the world\u2019s data. Although not structured per se, they often contain snippets of structured information within them: eg, names, addresses, and document titles. Information Extraction (IE) techniques identify such structured information in text. In recent years, database research has pursued IE on two fronts: declarative languages and systems for managing IE tasks, and IE as an uncertain data source for Probabilistic Databases. It is natural to consider merging these two directions, but efforts to do so have had to compromise on the statistical robustness of IE algorithms in order to fit with early Probabilistic Database models.In this paper, we bridge the gap between these ideas by implementing a state-of-the-art statistical IE approach\u2013Conditional Random Fields (CRFs)\u2013in the setting of Probabilistic Databases that treat statistical models as first-class data objects. Using standard relational tables to capture CRF parameters, and inverted-file representations of text, we show that the Viterbi algorithm for CRF inference can be specified declaratively in recursive SQL, in a manner that can both choose likely segmentations, and provide detailed marginal distributions for label assignment. Given this implementation, we propose query processing optimizations that effectively combine probabilistic inference and relational operators such as selections and joins. In an experimental study with two data sets, we demonstrate the efficiency of our in-database Viterbi implementation in PostgreSQL relative to an open-source CRF library, and show the performance benefits of our optimizations.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:NhqRSupF_l8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Issues in complex event processing systems",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7345503/",
            "Abstract": "Many Big Data technologies were built to enable the processing of human generated data, setting aside the enormous amount of data generated from Machine-to-Machine (M2M) interactions. M2M interactions create real-time data streams that are much more structured, often in the form of series of event occurrences. In this paper, we provide an overview on the main research issues confronted by existing Complex Event Processing (CEP) techniques, as a starting point for Big Data applications that enable the monitoring of complex event occurrences in M2M interactions.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:lmc2jWPfTJgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Selectivity estimation for XML twigs",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1320003/",
            "Abstract": "Twig queries represent the building blocks of declarative query languages over XML data. A twig query describes a complex traversal of the document graph and generates a set of element tuples based on the intertwined evaluation (i.e., join) of multiple path expressions. Estimating the result cardinality of twig queries or, equivalently, the number of tuples in such a structural (path-based) join, is a fundamental problem that arises in the optimization of declarative queries over XML. It is crucial, therefore, to develop concise synopsis structures that summarize the document graph and enable such selectivity estimates within the time and space constraints of the optimizer. We propose novel summarization and estimation techniques for estimating the selectivity of twig queries with complex XPath expressions over tree-structured data. Our approach is based on the XSKETCH model, augmented with new types of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:mVmsd5A6BfQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "XML stream processing using tree-edit distance embeddings",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1061318.1061326",
            "Abstract": "We propose the first known solution to the problem of correlating, in small space, continuous streams of XML data through approximate (structure and content) matching, as defined by a general tree-edit distance metric. The key element of our solution is a novel algorithm for obliviously embedding tree-edit distance metrics into an L1 vector space while guaranteeing a (worst-case) upper bound of O(log2n log*n) on the distance distortion between any data trees with at most n nodes. We demonstrate how our embedding algorithm can be applied in conjunction with known random sketching techniques to (1) build a compact synopsis of a massive, streaming XML data tree that can be used as a concise surrogate for the full tree in approximate tree-edit distance computations; and (2) approximate the result of tree-edit-distance similarity joins over continuous XML document streams. Experimental results from an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:TQgYirikUcIC",
            "Publisher": "ACM"
        },
        {
            "Title": "Distributed Data Streams.",
            "Publication year": 2009,
            "Publication url": "http://www.softnet.tuc.gr/~minos/Papers/eds09dstreams.pdf",
            "Abstract": "A majority of today\u2019s data is constantly evolving and fundamentally distributed in nature. Data for almost any large-scale data-management task is continuously collected over a wide area, and at a much greater rate than ever before. Compared to traditional, centralized stream processing, querying such large-scale, evolving data collections poses new challenges, due mainly to the physical distribution of the streaming data and the communication constraints of the underlying network. Distributed stream processing algorithms should guarantee efficiency not only in terms of space and processing time (as conventional streaming techniques), but also in terms of the communication load imposed on the network infrastructure.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:EUQCXRtRnyEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deterministic wavelet thresholding for maximum-error metrics",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1055558.1055582",
            "Abstract": "Several studies have demonstrated the effectiveness of the wavelet, decomposition as a tool for reducing large amounts of data down to compact, wavelet synopses that can be used to obtain fast, accurate approximate answers to user queries. While conventional wavelet synopses are based on greedily minimizing the overall root-mean-squared (ie, L 2-norm) error in the data approximation, recent work has demonstrated that such synopses can suffer from important problems, including severe bias and wide variance in the quality of the data reconstruction, and lack of non-trivial guarantees for individual approximate answers. As a result, probabilistic thresholding schemes have been recently proposed as a means of building wavelet synopses that try to probabilistically control other approximation-error metrics, such as the maximum relative error in data-value reconstruction, which is arguably the most important for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:Zph67rFs4hoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Large-scale collective entity matching",
            "Publication year": 2011,
            "Publication url": "https://arxiv.org/abs/1103.2410",
            "Abstract": "There have been several recent advancements in Machine Learning community on the Entity Matching (EM) problem. However, their lack of scalability has prevented them from being applied in practical settings on large real-life datasets. Towards this end, we propose a principled framework to scale any generic EM algorithm. Our technique consists of running multiple instances of the EM algorithm on small neighborhoods of the data and passing messages across neighborhoods to construct a global solution. We prove formal properties of our framework and experimentally demonstrate the effectiveness of our approach in scaling EM algorithms.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:M05iB0D1s5AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Join-distinct aggregate estimation over update streams",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1065167.1065200",
            "Abstract": "There is growing interest in algorithms for processing and querying continuous data streams (ie, data that is seen only once in a fixed order) with limited memory resources. Providing (perhaps approximate) answers to queries over such streams is a crucial requirement for many application environments; examples include large IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:J_g5lzvAfSwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Privacy preserving medical data analytics using secure multi party computation. an end-to-end use case",
            "Publication year": 2018,
            "Publication url": "https://www.researchgate.net/profile/Dimitris-Mouris/publication/328382220_Privacy_Preserving_Medical_Data_Analytics_using_Secure_Multi_Party_Computation_An_End-To-End_Use_Case/links/5bc9928a299bf17a1c5f817b/Privacy-Preserving-Medical-Data-Analytics-using-Secure-Multi-Party-Computation-An-End-To-End-Use-Case.pdf",
            "Abstract": "The new era of big data demands high performance computing, since the amount of data published online is growing exponentially. Cloud computing has emerged as a result, providing strong computational power for both individuals and companies. Though cloud computing is the answer to many business models, there are many use-cases where cloud fails to meet the demands of information privacy. For instance, exposing financial and medical information to the cloud may violate the individuals\u2019 right to privacy. People are not comfortable sharing their sensitive data, and more importantly, they do not trust any cloud provider with this information; data that are uploaded in the cloud can be exposed to attacks from both the cloud provider and third parties.Nevertheless, there are many real world use cases that use information from different parties to jointly compute meaningful results, but due to the aforementioned limitations, some are avoided and others do not always respect data privacy. The solution to this is a technique called Secure Multi-Party Computation (SMPC or MPC), which leverages cryptographic primitives to carry out computations on confidential data, computing a function and learning nothing more than what the N parties would have if a separate trusted party had collected their inputs, computed the same function for them, and then return the result to all parties.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:2VqYfGB8ITEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Streaming algorithms for robust, real-time detection of ddos attacks",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4268161/",
            "Abstract": "Effective mechanisms for detecting and thwarting distributed denial-of-service (DDoS) attacks are becoming increasingly important to the success of today's Internet as a viable commercial and business tool. In this paper, we propose novel data-streaming algorithms for the robust, real-time detection of DDoS activity in large ISP networks. The key element of our solution is a new, hash-based synopsis data structure for network-data streams that allows us to efficiently track, in guaranteed small space and time, destination IP addresses in the underlying network that are \"large\" with respect to the number of distinct source IP addresses that have established potentially-malicious (e.g., \"half-open\") connections to them. Our work is the first to address the problem of efficiently tracking the top distinct-source frequencies over a general stream of updates (insertions and deletions) to the set of underlying network flows, thus \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:70eg2SAEIzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimal configuration of OSPF aggregates",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1194816/",
            "Abstract": "Open Shortest Path First (OSPF) is a popular protocol for routing within an autonomous system (AS) domain. In order to scale for large networks containing hundreds and thousands of subnets, OSPF supports a two-level hierarchical routing scheme through the use of OSPF areas. Subnet addresses within an area are aggregated, and this aggregation is a crucial requirement for scaling OSPF to large AS domains, as it results in significant reductions in routing table sizes, smaller link-state databases, and less network traffic to synchronize the router link-state databases. On the other hand, address aggregation also implies loss of information about the length of the shortest path to each subnet, which in turn, can lead to suboptimal routing. We address the important practical problem of configuring OSPF aggregates to minimize the error in OSPF shortest-path computations due to subnet aggregation. We first develop \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:HDshCWvjkbEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Allocation of internet advertising inventory",
            "Publication year": 2010,
            "Publication url": "https://patents.google.com/patent/US20100185515A1/en",
            "Abstract": "A method and system for allocating inventory in an Internet environment is provided. A method employed by the system may include generating an inventory pool that represents a number of impressions deliverable to all users, then determining, from multiple past orders for booking impressions, a hierarchy of parameters utilized to target users and a number of impressions deliverable to users characterized by the parameters. The inventory pool may then be partitioned into multiple inventory pools according to the hierarchy, where each inventory pool represents a number of impressions deliverable to users characterized by parameters associated with the inventory pool. The hierarchy of pools may then be stored to a database.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:dTyEYWd-f8wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Conclusions and looking forward",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-28608-0_25",
            "Abstract": "Today, data streaming is a part of the mainstream and several data steaming products are now publicly available. Data streaming algorithms are powering complex event processing, predictive analytics, and big data applications in the cloud. In this final chapter, we provide an overview of current data streaming products, and applications of data streaming to cloud computing, anomaly detection and predictive modeling. We also identify future research directions for mining and doing predictive analytics on data streams, especially in a distributed environment.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:q3CdL3IzO_QC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "best papers of VLDB 2007",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1007/s00778-009-0132-7",
            "Abstract": "This special issue of the VLDB Journal is dedicated to the best papers from the 33rd International Conference on Very Large Data Bases, which took place on 23\u201328 September 2007 at the University of Vienna in Austria. The conference received 668 submissions overall. The Core Database Technology Track received 263 submissions out of which 46 (17.5%) were accepted; the Infrastructure for Information Systems Track received 275 submissions out of which 45 (16.4%) were accepted; the Industrial, Applications, and Experience Track received 56 submissions out of which 17 (30.4%) were accepted; and the Demonstrations Track received 74 submissions out of which 29 (39.2%) were accepted.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:mvPsJ3kp5DgC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Distributed PCA and network anomaly detection",
            "Publication year": 2006,
            "Publication url": "https://people.eecs.berkeley.edu/~adj/publications/paper-files/EECS-2006-99.pdf",
            "Abstract": "We consider the problem of network anomaly detection given the data collected and processed over large distributed systems. Our algorithmic framework can be seen as an approximate, distributed version of the well-known Principal Component Analysis (PCA) method, which is concerned with continuously tracking the behavior of the data projected onto the residual subspace of the principal components within error bound guarantees. Our approach consists of a protocol for local processing at individual monitoring devices, and global decision-making and monitoring feedback at a coordinator. A key ingredient of our framework is an analytical method based on stochastic matrix perturbation theory for balancing the tradeoff between the accuracy of our approximate network anomaly detection, and the amount of data communication over the network.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:ns9cj8rnVeAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Processing data-stream join aggregates using skimmed sketches",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-24741-8_33",
            "Abstract": "There is a growing interest in on-line algorithms for analyzing and querying data streams, that examine each stream element only once and have at their disposal, only a limited amount of memory. Providing (perhaps approximate) answers to aggregate queries over such streams is a crucial requirement for many application environments; examples include large IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed. In this paper, we present the skimmed-sketch algorithm for estimating the join size of two streams. (Our techniques also readily extend to other join-aggregate queries.) To the best of our knowledge, our skimmed-sketch technique is the first comprehensive join-size estimation algorithm to provide tight error guarantees while: (1) achieving the lower bound on the space required by any join-size estimation method in a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:4JMBOYKVnBMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Querying distributed data streams",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-10933-6_1",
            "Abstract": "Effective Big Data analytics pose several difficult challenges for modern data management architectures. One key such challenge arises from the naturally streaming nature of big data, which mandates efficient algorithms for querying and analyzing massive, continuous data streams (that is, data that is seen only once and in a fixed order) with limited memory and CPU-time resources. Such streams arise naturally in emerging large-scale event monitoring applications; for instance, network-operations monitoring in large ISPs, where usage information from numerous sites needs to be continuously collected and analyzed for interesting trends. In addition to memory- and time-efficiency concerns, the inherently distributed nature of such applications also raises important communication-efficiency issues, making it critical to carefully optimize the use of the underlying network infrastructure. In this talk, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:4MWp96NkSFoC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Query analytics over probabilistic databases with unmerged duplicates",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7045501/",
            "Abstract": "Recent entity resolution approaches exhibit benefits when addressing the problem through unmerged duplicates: instances describing real-world objects are not merged based on apriori thresholds or human intervention, instead relevant resolution information is employed for evaluating resolution decisions during query processing using \u201cpossible worlds\u201d semantics. In this paper, we present the first known approach for efficiently handling complex analytical queries over probabilistic databases with unmerged duplicates. We propose the ENTITY-JOIN operator that allows expressing complex aggregation and iceberg/top-k queries over joins between tables with unmerged duplicates and other database tables. Our technical content includes a novel indexing structure for efficient access to the entity resolution information and novel techniques for the efficient evaluation of complex probabilistic queries that retrieve \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:TIZ-Mc8IlK0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "System and method for compressing a data table using models",
            "Publication year": 2006,
            "Publication url": "https://patents.google.com/patent/US7143046B2/en",
            "Abstract": "A system for, and method of compressing a data table using models and a database management system incorporating the system or the method. In one embodiment, the system includes:(1) a table modeller that discovers data mining models with guaranteed error bounds of at least one attribute in the data table in terms of other attributes in the data table and (2) a model selector, associated with the table modeller, that selects a subset of the at least one model to form a basis upon which to compress the data table.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:f2IySw72cVMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scalable data mining with model constraints",
            "Publication year": 2000,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1145/380995.381012",
            "Abstract": "Data mining can be abstractly defined as the process of extracting concise and interesting models (or, patterns) from large amounts of data. Unfortunately, conventional mining systems provide users with only very restricted mechanisms for specifying models of interest. As a consequence, the mining process is typically characterized by lack of focus and users often end up paying computational costs that are inordinately high compared to the specific models] patterns of interest. Exploiting user-defined model constraints during the mining process can help alleviate this problem and ensure system performance that is commensurate with the level of user focus. Attaining such performance goals, however, is not straightforward and, typically, requires the design of novel data mining algorithms that make effective use of the model constraints. In this paper, we provide an overview of our recent work on scalable, constraint \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:NaGl4SEjCO4C",
            "Publisher": "ACM"
        },
        {
            "Title": "Independence is good: Dependency-based histogram synopses for high-dimensional data",
            "Publication year": 2001,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:_FxGoFyzp5QC",
            "Publisher": "ACM"
        },
        {
            "Title": "Querying probabilistic information extraction",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/1920841.1920974",
            "Abstract": "Recently, there has been increasing interest in extending relational query processing to include data obtained from unstructured sources. A common approach is to use stand-alone Information Extraction (IE) techniques to identify and label entities within blocks of text; the resulting entities are then imported into a standard database and processed using relational queries. This two-part approach, however, suffers from two main drawbacks. First, IE is inherently probabilistic, but traditional query processing does not properly handle probabilistic data, resulting in reduced answer quality. Second, performance inefficiencies arise due to the separation of IE from query processing. In this paper, we address these two problems by building on an in-database implementation of a leading IE model---Conditional Random Fields using the Viterbi inference algorithm. We develop two different query approaches on top of this \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:g5m5HwL7SMYC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Bayesstore: managing large, uncertain data repositories with probabilistic graphical models",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/1453856.1453896",
            "Abstract": "Several real-world applications need to effectively manage and reason about large amounts of data that are inherently uncertain. For instance, pervasive computing applications must constantly reason about volumes of noisy sensory readings for a variety of reasons, including motion prediction and human behavior modeling. Such probabilistic data analyses require sophisticated machine-learning tools that can effectively model the complex spatio/temporal correlation patterns present in uncertain sensory data. Unfortunately, to date, most existing approaches to probabilistic database systems have relied on somewhat simplistic models of uncertainty that can be easily mapped onto existing relational architectures: Probabilistic information is typically associated with individual data tuples, with only limited or no support for effectively capturing and reasoning about complex data correlations. In this paper, we introduce \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:ULOm3_A8WrAC",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Sharing aggregate computation for distributed queries",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1247480.1247535",
            "Abstract": "An emerging challenge in modern distributed querying is to efficiently process multiple continuous aggregation queries simultaneously. Processing each query independently may be infeasible, so multi-query optimizations are critical for sharing work across queries. The challenge is to identify overlapping computations that may not be obvious in the queries themselves.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:RHpTSmoSYBkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Method For Generating Score-Optimal R-Trees",
            "Publication year": 2010,
            "Publication url": "https://patents.google.com/patent/US20100036865A1/en",
            "Abstract": "A method of constructing a score-optimal R-tree to support top-k stabbing queries over a set of scored intervals generates a constraint graph from the set, and determines over each node in the constraint graph that has no other nodes pointing to it the node with the smallest left endpoint; for each of these nodes, the associated interval is added to the tree and the node is removed from the constraint graph.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:t6usbXjVLHcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scalable approximate query tracking over highly distributed data streams with tunable accuracy guarantees",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0306437918300322",
            "Abstract": "The recently proposed Geometric Monitoring (GM) method has provided a general tool for the distributed monitoring of arbitrary non-linear queries over streaming data observed by a collection of remote sites, with numerous practical applications. Unfortunately, GM-based techniques can suffer from serious scalability issues with increasing numbers of remote sites. In this paper, we propose novel techniques that effectively tackle the aforementioned scalability problems by exploiting a carefully designed sample of the remote sites for efficient approximate query tracking. Our novel sampling-based scheme utilizes a sample of cardinality proportional to N (compared to N for the original GM and its variants), where N is the number of sites in the network, to perform the monitoring process. Our extensive experimental evaluation and comparative analysis over a variety of real-life data streams demonstrates that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:NXb4pA-qfm4C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Efficiently monitoring bandwidth and latency in IP networks",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/916285/",
            "Abstract": "Effective monitoring of network utilization and performance indicators is a key enabling technology for proactive and reactive resource management, flexible accounting, and intelligent planning in next-generation IP networks. In this paper, we address the challenging problem of efficiently monitoring bandwidth utilization and path latencies in an IP data network. Unlike earlier approaches, our measurement architecture assumes a single point-of-control in the network (corresponding to the network operations center) that is responsible for gathering bandwidth and latency information using widely-deployed management tools, like SNMP, RMON/NetFlow, and explicitly-routed IP probe packets. Our goal is to identify effective techniques for monitoring (a) bandwidth usage for a given set of links or packet flows, and (b) path latencies for a given set of paths, while minimizing the overhead imposed by the management \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Probabilistic wavelet synopses",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/974750.974753",
            "Abstract": "Recent work has demonstrated the effectiveness of the wavelet decomposition in reducing large amounts of data to compact sets of wavelet coefficients (termed \"wavelet synopses\") that can be used to provide fast and reasonably accurate approximate query answers. A major shortcoming of these existing wavelet techniques is that the quality of the approximate answers they provide varies widely, even for identical queries on nearly identical values in distinct parts of the data. As a result, users have no way of knowing whether a particular approximate answer is highly-accurate or off by many orders of magnitude. In this article, we introduce Probabilistic Wavelet Synopses, the first wavelet-based data reduction technique optimized for guaranteed accuracy of individual approximate answers. Whereas previous approaches rely on deterministic thresholding for selecting the wavelet coefficients to include in the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:_kc_bZDykSQC",
            "Publisher": "ACM"
        },
        {
            "Title": "Exploratory DSP-wavelet-based approximation techniques in database systems",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4015566/",
            "Abstract": "Several recent studies have demonstrated the effectiveness of the wavelet transform as a tool for approximate query processing over massive relational tables and continuous data streams. The idea is to apply wavelet transform to the input relation to obtain a compact data synopsis that comprises a select small collection of wavelet coefficients. The excellent energy compaction and decorrelation properties of the wavelet transform allow for concise and effective approximate representations that exploit the structure of the data. Furthermore, wavelet transforms can generally be computed in linear time, thus allowing for very efficient algorithms. This paper provides a brief overview of recent work and results on wavelet-based approximation techniques for relational database systems",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:Tiz5es2fbqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Techniques for information dissemination using tree pattern subscriptions and aggregation thereof",
            "Publication year": 2004,
            "Publication url": "https://patents.google.com/patent/US20040260683A1/en",
            "Abstract": "A set of subscriptions are provided, where one or more subscriptions each comprises a tree pattern, and a tree pattern comprises one or more interconnected nodes having a hierarchy and adapted to specify content and structure of information. The set of subscriptions is used to select information for dissemination to users. Generally, the one or more subscriptions having the tree pattern describe information the users are interested in receiving. Techniques are presented for determining an aggregation from the subscriptions, where the aggregation comprises a set of aggregate patterns. The set of subscriptions may comprise a number of tree patterns, and the aggregate patterns generally also comprise tree patterns comprising one or more interconnected nodes having a hierarchy and adapted to specify content and structure of information. The set of aggregation patterns is smaller than the set of subscriptions and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:lSLTfruPkqcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Data stream management: processing high-speed data streams",
            "Publication year": 2016,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=qiSpDAAAQBAJ&oi=fnd&pg=PR3&dq=info:zi2-SbSCfoMJ:scholar.google.com&ots=IQumC13P6-&sig=XGWsR2KtAFC6IR_Pke3bz5l4PqY",
            "Abstract": "This volume focuses on the theory and practice of data stream management, and the novel challenges this emerging domain poses for data-management algorithms, systems, and applications. The collection of chapters, contributed by authorities in the field, offers a comprehensive introduction to both the algorithmic/theoretical foundations of data streams, as well as the streaming systems and applications built in different domains. A short introductory chapter provides a brief summary of some basic data streaming concepts and models, and discusses the key elements of a generic stream query processing architecture. Subsequently, Part I focuses on basic streaming algorithms for some key analytics functions (eg, quantiles, norms, join aggregates, heavy hitters) over streaming data. Part II then examines important techniques for basic stream mining tasks (eg, clustering, classification, frequent itemsets). Part III discusses a number of advanced topics on stream processing algorithms, and Part IV focuses on system and language aspects of data stream processing with surveys of influential system prototypes and language designs. Part V then presents some representative applications of streaming techniques in different domains (eg, network management, financial analytics). Finally, the volume concludes with an overview of current data streaming products and new application domains (eg cloud computing, big data analytics, and complex event processing), and a discussion of future directions in this exciting field. The book provides a comprehensive overview of core concepts and technological foundations, as well as various systems and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:L8Ckcad2t8MC",
            "Publisher": "Springer"
        },
        {
            "Title": "XCluster synopses for structured XML content",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1617431/",
            "Abstract": "We tackle the difficult problem of summarizing the path/branching structure and value content of an XML database that comprises both numeric and textual values. We introduce a novel XML-summarization model, termed XCLUSTERs, that enables accurate selectivity estimates for the class of twig queries with numeric-range, substring, and textual IR predicates over the content of XML elements. In a nutshell, an XCLUSTER synopsis represents an effective clustering of XML elements based on both their structural and value-based characteristics. By leveraging techniques for summarizing XML-document structure as well as numeric and textual data distributions, our XCLUSTER model provides the first known unified framework for handling path/branching structure and different types of element values. We detail the XCLUSTER model, and develop a systematic framework for the construction of effective XCLUSTER \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:IWHjjKOFINEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Probabilistic declarative information extraction",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5447844/",
            "Abstract": "Unstructured text represents a large fraction of the world's data. It often contains snippets of structured information (e.g., people's names and zip codes). Information Extraction (IE) techniques identify such structured information in text. In recent years, database research has pursued IE on two fronts: declarative languages and systems for managing IE tasks, and probabilistic databases for querying the output of IE. In this paper, we make the first step to merge these two directions, without loss of statistical robustness, by implementing a state-of-the-art statistical IE model - Conditional Random Fields (CRF) - in the setting of a Probabilistic Database that treats statistical models as first-class data objects. We show that the Viterbi algorithm for CRF inference can be specified declaratively in recursive SQL. We also show the performance benefits relative to a standalone open-source Viterbi implementation. This work opens \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:fPk4N6BV_jEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Streaming in a connected world: querying and tracking distributed data streams",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1247480.1247649",
            "Abstract": "Today, a majority of data is fundamentally distributed in nature. Data for almost any task is collected over a broad area, and streams in at a much greater rate than ever before. In particular, advances in sensor technology and miniaturization have led to the concept of the sensor network: a (typically wireless) collection of sensing devices collecting detailed data about their surroundings. A fundamental question arises: how to query and monitor this rich new source of data? A similar scenario emerges within more traditional, wired networks: if data is collected over remote sites, either about observed external conditions or about the network itself (eg in IP network monitoring), how to process this data in order to answer certain queries? Additionally, other emerging models of distributed computation, such as peer-to-peer (P2P) networks and grid-based computing face the same problems of managing and interrogating \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:k_IJM867U9cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An adaptive RFID middleware for supporting metaphysical data independence",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-007-0084-8",
            "Abstract": "Sensor devices produce data that are unreliable, low-level, and seldom able to be used directly by applications. In this paper, we propose metaphysical data independence (MDI), a layer of independence that shields applications from the challenges that arise when interacting directly with sensor devices. The key philosophy behind MDI is that applications do not deal with any aspect of physical device data, but rather interface with a high-level reconstruction of the physical world created by a sensor infrastructure. As a concrete instantiation of MDI in such a sensor infrastructure, we detail MDI-SMURF, a Radio Frequency Identification (RFID) middleware system that alleviates issues associated with using RFID data through adaptive techniques based on a novel statistical framework.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Declarative networking with distributed recursive query processing",
            "Publication year": 2006,
            "Publication url": "https://dsf.berkeley.edu/jmh/tmp/dnsigmod06.pdf",
            "Abstract": "There have been recent proposals in the networking and distributed systems literature on declarative networking, where network protocols are declaratively specified using a recursive query language. This represents a significant new application area for recursive query processing technologies from databases. In this paper, we extend upon these recent proposals in the following ways. First, we motivate and formally define the NDlog language for declarative network specifications. We introduce the concept of link-restricted rules, which can be syntactically guaranteed to be executable via single-node derivations and message passing on an underlying network graph. Second, we introduce and prove correct relaxed versions of the traditional semi-naive execution technique that overcome fundamental problems of traditional semi-na\u0131ve evaluation in an asynchronous distributed setting. Third, we consider the dynamics of network state, and formalize the \u201ceventual consistency\u201d of our programs even when bursts of updates can arrive in the midst of query execution. Fourth, we present a number of query optimization opportunities that arise in the declarative networking context, including applications of traditional techniques and new optimizations. Last, we present evaluation results based on an implementation of the above ideas in the P2 declarative networking system, running on 100 machines over the Emulab network testbed.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:3s1wT3WcHBgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tracking set-expression cardinalities over continuous update streams",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/article/10.1007/s00778-004-0135-3",
            "Abstract": "There is growing interest in algorithms for processing and querying continuous data streams (i.e., data seen only once in a fixed order) with limited memory resources. In its most general form, a data stream is actually an update stream, i.e., comprising data-item deletions as well as insertions. Such massive update streams arise naturally in several application domains (e.g., monitoring of large IP network installations or processing of retail-chain transactions). Estimating the cardinality of set expressions defined over several (possibly distributed) update streams is perhaps one of the most fundamental query classes of interest; as an example, such a query may ask \u201cwhat is the number of distinct IP source addresses seen in passing packets from both router R 1 and R 2 but not router R 3?\u201d. Earlier work only addressed very restricted forms of this problem, focusing solely on the special \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:GnPB-g6toBAC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Method for distributed tracking of approximate join size and related summaries",
            "Publication year": 2010,
            "Publication url": "https://patents.google.com/patent/US7756805B2/en",
            "Abstract": "A method of distributed approximate query tracking relies on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication-efficient and space/time-efficient solutions. A powerful approximate query tracking framework readily incorporates several complex analysis queries, including distributed join and multi-join aggregates and approximate wavelet representations, thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:WbkHhVStYXYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fast approximate wavelet tracking on streams",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/11687238_4",
            "Abstract": "Recent years have seen growing interest in effective algorithms for summarizing and querying massive, high-speed data streams. Randomized sketch synopses provide accurate approximations for general-purpose summaries of the streaming data distribution (e.g., wavelets). The focus of existing work has typically been on minimizing space requirements of the maintained synopsis \u2014 however, to effectively support high-speed data-stream analysis, a crucial practical requirement is to also optimize: (1) the update time for incorporating a streaming data element in the sketch, and (2) the query time for producing an approximate summary (e.g., the top wavelet coefficients) from the sketch. Such time costs must be small enough to cope with rapid stream-arrival rates and the real-time querying requirements of typical streaming applications (e.g., ISP network monitoring). With cheap and plentiful memory, space \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Communication-efficient tracking of distributed cumulative triggers",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4268207/",
            "Abstract": "In recent work, we proposed D-Trigger, a framework for tracking a global condition over a large network that allows us to detect anomalies while only collecting a very limited amount of data from distributed monitors. In this paper, we expand our previous work by designing a new class of queries (conditions) that can be tracked for anomaly violations. We show how security violations can be detected over a time window of any size. This is important because security operators do not know in advance the window of time in which measurements should be made to detect anomalies. We also present an algorithm that determines how each machine should filter its time series measurements before back-hauling them to a central operations center. Our filters are computed analytically such that upper bounds on false positive and missed detection rates are guaranteed. In our evaluation, we show that botnet detection can \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:M3NEmzRMIkIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Practical private range search revisited",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2882903.2882911",
            "Abstract": "We consider a data owner that outsources its dataset to an untrusted server. The owner wishes to enable the server to answer range queries on a single attribute, without compromising the privacy of the data and the queries. There are several schemes on\" practical\" private range search (mainly in Databases venues) that attempt to strike a trade-off between efficiency and security. Nevertheless, these methods either lack provable security guarantees, or permit unacceptable privacy leakages. In this paper, we take an interdisciplinary approach, which combines the rigor of Security formulations and proofs with efficient Data Management techniques. We construct a wide set of novel schemes with realistic security/performance trade-offs, adopting the notion of Searchable Symmetric Encryption (SSE) primarily proposed for keyword search. We reduce range search to multi-keyword search using range covering \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:WZBGuue-350C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Composable XML integration grammars",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1031171.1031176",
            "Abstract": "The proliferation of XML as a standard for data representation and exchange in diverse, next-generation Web applications has created an emphatic need for effective XML data-integration tools. For several real-life scenarios, such XML data integration needs to beDTD-directed--in other words, the target, integrated XML database must conform to a prespecified, user-or application-defined DTD. In this paper, we propose a novel formalism,XML Integration Grammars (XIGs), for specifying DTD-directed integration of XML data. Abstractly, an XIG maps data from multiple XML sources to a target XML document that conforms to a predefined DTD. An XIG extracts source XML data via queries expressed in a fragment of XQuery, and controls target document generation with tree-valued attributes and the target DTD. The novelty of XIGs consists in not only their automatic support for DTD-conformance but also in their: an XIG \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Methods and apparatus to construct histogram and wavelet synopses for probabilistic data",
            "Publication year": 2013,
            "Publication url": "https://patents.google.com/patent/US8386412B2/en",
            "Abstract": "Example methods and apparatus to construct histogram and wavelet synopses for probabilistic data are disclosed. A disclosed example method involves receiving probabilistic data associated with probability measures and generating a plurality of histograms based on the probabilistic data. Each histogram is generated based on items represented by the probabilistic data. In addition, each histogram is generated using a different quantity of buckets containing different ones of the items. An error measure associated with each of the plurality of histograms is determined and one of the plurality of histograms is selected based on its associated error measure. The method also involves displaying parameter information associated with the one of the plurality of histograms to represent the data.",
            "Abstract entirety": 1,
            "Author pub id": "WemX9rAAAAAJ:yqoGN6RLRZoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Hybrid in-database inference for declarative information extraction",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1989323.1989378",
            "Abstract": "In the database community, work on information extraction (IE) has centered on two themes: how to effectively manage IE tasks, and how to manage the uncertainties that arise in the IE process in a scalable manner. Recent work has proposed a probabilistic database (PDB) based declarative IE system that supports a leading statistical IE model, and an associated inference algorithm to answer top-k-style queries over the probabilistic IE outcome. Still, the broader problem of effectively supporting general probabilistic inference inside a PDB-based declarative IE system remains open. In this paper, we explore the in-database implementations of a wide variety of inference algorithms suited to IE, including two Markov chain Monte Carlo algorithms, the Viterbi and the sum-product algorithms. We describe the rules for choosing appropriate inference algorithms based on the model, the query and the text, considering the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multi-query optimization for sketch-based estimation",
            "Publication year": 2009,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0306437908000598",
            "Abstract": "Randomized techniques, based on computing small \u201csketch\u201d synopses for each stream, have recently been shown to be a very effective tool for approximating the result of a single SQL query over streaming data tuples. In this paper, we investigate the problems arising when data-stream sketches are used to process multiple such queries concurrently. We demonstrate that, in the presence of multiple query expressions, intelligently sharing sketches among concurrent query evaluations can result in substantial improvements in the utilization of the available sketching space and the quality of the resulting approximation error guarantees. We provide necessary and sufficient conditions for multi-query sketch sharing that guarantee the correctness of the result-estimation process. We also investigate the difficult optimization problem of determining sketch-sharing configurations that are optimal (eg, under a certain error \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:dfsIfKJdRG4C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Nonparametric network design and analysis of disease genes in oral cancer progression",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6567891/",
            "Abstract": "Biological networks in living organisms can be seen as the ultimate means of understanding the underlying mechanisms in complex diseases, such as oral cancer. During the last decade, many algorithms based on high-throughput genomic data have been developed to unravel the complexity of gene network construction and their progression in time. However, the small size of samples compared to the number of observed genes makes the inference of the network structure quite challenging. In this study, we propose a framework for constructing and analyzing gene networks from sparse experimental temporal data and investigate its potential in oral cancer. We use two network models based on partial correlations and kernel density estimation, in order to capture the genetic interactions. Using this network construction framework on real clinical data of the tissue and blood at different time stages, we identified \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:yB1At4FlUx8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Biological interaction networks based on non-parametric estimation",
            "Publication year": 2013,
            "Publication url": "https://www.inderscienceonline.com/doi/abs/10.1504/IJBET.2013.058539",
            "Abstract": "Biological networks are often described as probabilistic graphs in the context of gene and protein sequence analysis in molecular biology. Microarrays and proteomics technologies facilitate the monitoring of expression levels over thousands of biological units over time. Several experimental efforts have appeared aiming to unveiling pairwise interactions, with many graphical models being introduced in order to discover associations from expression-data analysis. However, the small size of samples compared to the number of observed genes/proteins makes the inference of the network structure quite challenging. In this study, we generate gene\u2013protein networks from sparse experimental temporal data using two methods, partial correlations and Kernel Density Estimation (KDE), in an attempt to capture genetic interactions. Applying KDE method we model the genetic associations as Gaussians approximations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "WemX9rAAAAAJ:zLWjf1WUPmwC",
            "Publisher": "Inderscience Publishers Ltd"
        }
    ]
}]