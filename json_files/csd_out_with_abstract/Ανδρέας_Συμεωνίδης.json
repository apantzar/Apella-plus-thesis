[{
    "name": "\u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 \u03a3\u03c5\u03bc\u03b5\u03c9\u03bd\u03af\u03b4\u03b7\u03c2",
    "romanize name": "Andreas Symeonidis",
    "School-Department": "\u0397\u03bb\u03b5\u03ba\u03c4\u03c1\u03bf\u03bb\u03cc\u03b3\u03c9\u03bd \u039c\u03b7\u03c7 & \u039c\u03b7\u03c7 \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "auth",
    "Rank": "\u0391\u03bd\u03b1\u03c0\u03bb\u03b7\u03c1\u03c9\u03c4\u03ae\u03c2 \u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 19319,
    "Scholar name": "Andreas L. Symeonidis",
    "Scholar id": "339uVZQAAAAJ",
    "Affiliation": "Associate Professor, Dept. Electrical and Computer Engineering, Aristotle University of Thessaloniki",
    "Citedby": 1607,
    "Interests": [
        "Software Engineering Processes",
        "Agent-Oriented Software Engineering",
        "Data Mining for automated knowledge discovery",
        "Applied Data"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=339uVZQAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Measuring the reusability of software components using static analysis metrics and reuse rate information",
            "Publication year": 2019,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0164121219301979",
            "Abstract": "Nowadays, the continuously evolving open-source community and the increasing demands of end users are forming a new software development paradigm; developers rely more on reusing components from online sources to minimize the time and cost of software development. An important challenge in this context is to evaluate the degree to which a software component is suitable for reuse, i.e. its reusability. Contemporary approaches assess reusability using static analysis metrics by relying on the help of experts, who usually set metric thresholds or provide ground truth values so that estimation models are built. However, even when expert help is available, it may still be subjective or case-specific. In this work, we refrain from expert-based solutions and employ the actual reuse rate of source code components as ground truth for building a reusability estimation model. We initially build a benchmark dataset \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:PazO6pb-sMwC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Towards Scalable Bug Localization using the Edit Distance of Call Traces",
            "Publication year": 2013,
            "Publication url": "https://www.academia.edu/download/48275730/icsea_2013_2_30_10250.pdf",
            "Abstract": "Locating software bugs is a difficult task, especially if they do not lead to crashes. Current research on automating non-crashing bug detection dictates collecting function call traces and representing them as graphs, and reducing the graphs before applying a subgraph mining algorithm. A ranking of potentially buggy functions is derived using frequency statistics for each node (function) in the correct and incorrect set of traces. Although most existing techniques are effective, they do not achieve scalability. To address this issue, this paper suggests reducing the graph dataset in order to isolate the graphs that are significant in localizing bugs. To this end, we propose the use of tree edit distance algorithms to identify the traces that are closer to each other, while belonging to different sets. The scalability of two proposed algorithms, an exact and a faster approximate one, is evaluated using a dataset derived from a real-world application. Finally, although the main scope of this work lies in scalability, the results indicate that there is no compromise in effectiveness.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:SeFeTyx0c_EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Recommendation Systems in a Conversational Web.",
            "Publication year": 2018,
            "Publication url": "https://issel.ee.auth.gr/wp-content/uploads/2019/02/WEBIST_2018_29.pdf",
            "Abstract": "In this paper we redefine the concept of Conversation Web in the context of hyper-personalization. We argue that hyper-personalization in the WWW is only possible within a conversational web where websites and users continuously \u201cdiscuss\u201d(interact in any way). We present a modular system architecture for the conversational WWW, given that adapting to various user profiles and multivariate websites in terms of size and user traffic is necessary, especially in e-commerce. Obviously there cannot be a unique fit-to-all algorithm, but numerous complementary personalization algorithms and techniques are needed. In this context, we propose PRCW, a novel hybrid approach combining offline and online recommendations using RFMG, an extension of RFM modeling. We evaluate our approach against the results of a deep neural network in two datasets coming from different online retailers. Our evaluation indicates that a) the proposed approach outperforms current state-of-art methods in small-medium datasets and can improve performance in large datasets when combined with other methods, b) results can greatly vary in different datasets, depending on size and characteristics, thus locating the proper method for each dataset can be a rather complex task, and c) offline algorithms should be combined with online methods in order to get optimal results since offline algorithms tend to offer better performance but online algorithms are necessary for exploiting new users and trends that turn up.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:w9ZB08sdvuUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Genecity: A multi agent simulation environment for hereditary diseases",
            "Publication year": 2006,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.9282&rep=rep1&type=pdf",
            "Abstract": "Simulating the psycho-societal aspects of a human community is an issue always intriguing and challenging, aspiring us to help better understand, macroscopically, the way (s) humans behave. The mathematical models that have extensively been used for the analytical study of the various related phenomena prove inefficient, since they cannot conceive the notion of population heterogeneity, a parameter highly critical when it comes to community interactions. Following the more successful paradigm of artificial societies, coupled with multi-agent systems and other Artificial Intelligence primitives, and extending previous epidemiological research work, we have developed GeneCity: an extended agent community, where agents live and interact under the veil of a hereditary epidemic. The members of the community, which can be either healthy, carriers of a trait, or patients, exhibit a number of human-like social (and medical) characteristics: wealth, acceptance and influence, fear and knowledge, phenotype and reproduction ability. GeneCity provides a highly-configurable interface for simulating social environments and the way they are affected with the appearance of a hereditary disease, either Autosome or X-linked. This paper presents an analytical overview of the work conducted and examines a testhypothesis based on the spreading of Thalassaemia major.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:8k81kl-MbHgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "REMEDES for Alzheimer-R4Alz battery: Design and development of a new tool of cognitive control assessment for the diagnosis of minor and major neurocognitive disorders",
            "Publication year": 2019,
            "Publication url": "https://content.iospress.com/articles/journal-of-alzheimers-disease/jad190798",
            "Abstract": "Background: Subjective cognitive decline (SCD) and mild cognitive impairment (MCI) are acknowledged stages of the clinical spectrum of Alzheimer\u2019s disease (AD), and cognitive control seems to be among the first neuropsychological predictors of cognitive decline. Existing tests are usually affected by educational level, linguistic abilities, cultural differences, and social status, constituting them error-prone when differentiating between the aforementioned stages. Creating robust neuropsychological tests is therefore prominent.Objective: The design of a novel psychometric battery for the cognitive control and attention assessment, free of demographic effects, capable to discriminate cognitively healthy aging, SCD, MCI, and mild Dementia (mD). Methods: The battery initial hypothesis was tuned using iterations of administration on random sampling healthy older adults and people with SCD, MCI, and mD, from the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:Amrzk_ktLr0C",
            "Publisher": "IOS Press"
        },
        {
            "Title": "Continuous Implicit Authentication through Touch Traces Modelling",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9282760/",
            "Abstract": "Nowadays, the continuously increasing use of smart-phones as the primary way of dealing with day-to-day tasks raises several concerns mainly focusing on privacy and security. In this context and given the known limitations and deficiencies of traditional authentication mechanisms, a lot of research efforts are targeted towards continuous implicit authentication on the basis of behavioral biometrics. In this work, we propose a methodology towards continuous implicit authentication that refrains from the limitations imposed by small-scale and/or controlled environment experiments by employing a real-world application used widely by a large number of individuals. Upon constructing our models using Support Vector Machines, we introduce a confidence-based methodology, in order to strengthen the effectiveness and the efficiency of our approach. The evaluation of our methodology on a set of diverse scenarios \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:G887dSk7Sz8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Areas of Application & Future Directions",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/0-387-25757-8_10",
            "Abstract": "We have mentioned repeatedly in this book that the agent paradigm can be adopted by a great variety of application domains, since agents constitute the software building blocks of a new generation. Nevertheless, for a fruitful coupling of agent technology with data mining, either historical data (of any type) must be available, or the knowledge mechanisms of agents must allow self-organization. In Chapters 6 to 8 we have discussed in considerable detail three representative application domains. Here, we briefly outline some additional areas that exhibit the above characteristics and, thus, are good candidates for agent training.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:NaGl4SEjCO4C",
            "Publisher": "Springer US"
        },
        {
            "Title": "Mining Source Code for Component Reuse",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=4516268838218064179&hl=en&oi=scholarr",
            "Abstract": "Although the development of code search engines has brought forth syntax-aware capabilities when searching for reusable components, these engines do not fully exploit the given context and do not assess the retrieved source code. As a result, several test-driven reuse systems have been developed to offer context-aware component search and further assess the retrieved components using test cases. However, most of these systems employ strict matching criteria and do not offer information concerning the flow and the dependencies of the retrieved components. In this chapter, we present Mantissa, a system designed to overcome the aforementioned limitations. Mantissa allows code searching in growing repositories, such as GitHub. The user provides the input query as a code snippet and Mantissa employs a mechanism that uses Information Retrieval techniques to return functional software \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:-c_eYhz9dBkC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Syncing shared multimedia through audiovisual bimodal segmentation",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7106383/",
            "Abstract": "This work emanates from the particularities residing in contemporary social media storytelling, where multiple users and publishing channels capture and share public events, experiences, and places. Multichannel presentation and visualization mechanisms are pursued along with novel audiovisual mixing (such as time-delay-compensation enhancement, perceptual mixing, quality-based content selection, linking to context-aware metadata, and propagating multimedia semantics), thus promoting multimodal social media editing, processing, and authoring. While the exploitation of multiple time-based media (audio and video) describing the same event may lead to significant content enhancement, difficulties regarding detection and temporal synchronization of multimedia events have to be overcome. In many cases, one can identify events based only on audio features, thus performing an initial cost-effective \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:T_0gP6tLVL0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Data mining for agent reasoning: A synergy for training intelligent agents",
            "Publication year": 2007,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197607000176",
            "Abstract": "The task-oriented nature of data mining (DM) has already been dealt successfully with the employment of intelligent agent systems that distribute tasks, collaborate and synchronize in order to reach their ultimate goal, the extraction of knowledge. A number of sophisticated multi-agent systems (MAS) that perform DM have been developed, proving that agent technology can indeed be used in order to solve DM problems. Looking into the opposite direction though, knowledge extracted through DM has not yet been exploited on MASs. The inductive nature of DM imposes logic limitations and hinders the application of the extracted knowledge on such kind of deductive systems. This problem can be overcome, however, when certain conditions are satisfied a priori. In this paper, we present an approach that takes the relevant limitations and considerations into account and provides a gateway on the way DM techniques \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Towards the design of user friendly search engines for software projects",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-07983-7_22",
            "Abstract": "Current work proposes a linguistic approach for supporting the identification of User requirements and Software Specifications. We introduce an NLP-based tool, PYTHIA, that serves as a search engine capable of handling software engineering terminology, aiming to close the loop between the end-user and the software developer. It is an ontology-based question answering system that employs semantic analysis as well as external (both generic use and domain-specific) dictionaries in order to handle term disambiguation, as posed in user defined queries.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:cFHS6HbyZ2cC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "From requirements to source code: a Model-Driven Engineering approach for RESTful web services",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/article/10.1007/s10515-016-0206-x",
            "Abstract": "During the last few years, the REST architectural style has drastically changed the way web services are developed. Due to its transparent resource-oriented model, the RESTful paradigm has been incorporated into several development frameworks that allow rapid development and aspire to automate parts of the development process. However, most of the frameworks lack automation of essential web service functionality, such as authentication or database searching, while the end product is usually not fully compliant to REST. Furthermore, most frameworks rely heavily on domain specific modeling and require developers to be familiar with the employed modeling technologies. In this paper, we present a Model-Driven Engineering (MDE) engine that supports fast design and implementation of web services with advanced functionality. Our engine provides a front-end interface that allows developers to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:hrkNWuzUpWwC",
            "Publisher": "Springer US"
        },
        {
            "Title": "A simulation testbed for analyzing trust and reputation mechanisms in unreliable online markets",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1567422314000465",
            "Abstract": "Modern online markets are becoming extremely dynamic, indirectly dictating the need for (semi-) autonomous approaches for constant monitoring and immediate action in order to satisfy one\u2019s needs/preferences. In such open and versatile environments, software agents may be considered as a suitable metaphor for dealing with the increasing complexity of the problem. Additionally, trust and reputation have been recognized as key issues in online markets and many researchers have, in different perspectives, surveyed the related notions, mechanisms and models. Within the context of this work we present an adaptable, multivariate agent testbed for the simulation of open online markets and the study of various factors affecting the quality of the service consumed. This testbed, which we call Euphemus, is highly parameterized and can be easily customized to suit a particular application domain. It allows for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:D03iK_w7-QYC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "QualBoa: reusability-aware recommendations of source code components",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7832932/",
            "Abstract": "Contemporary software development processes involve finding reusable software components from online repositories and integrating them to the source code, both to reduce development time and to ensure that the final software project is of high quality. Although several systems have been designed to automate this procedure by recommending components that cover the desired functionality, the reusability of these components is usually not assessed by these systems. In this work, we present QualBoa, a recommendation system for source code components that covers both the functional and the quality aspects of software component reuse. Upon retrieving components, QualBoa provides a ranking that involves not only functional matching to the query, but also a reusability score based on configurable thresholds of source code metrics. The evaluation of QualBoa indicates that it can be effective for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:0Kh4an1R61UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "BioCrawler: An intelligent crawler for the semantic web",
            "Publication year": 2008,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0957417407002990",
            "Abstract": "Web crawling has become an important aspect of web search, as the WWW keeps getting bigger and search engines strive to index the most important and up to date content. Many experimental approaches exist, but few actually try to model the current behaviour of search engines, which is to crawl and refresh the sites they deem as important, much more frequently than others. BioCrawler mirrors this behaviour on the semantic web, by applying the learning strategies adopted in previous work on ecosystem simulation, called BioTope. BioCrawler employs the principles of BioTope\u2019s intelligent agents on the semantic web, learns which sites are rich in semantic content and which sites link to them and adjusts its crawling habits accordingly. In the end, it learns to behave much like the state of the art search engine crawlers do. However, BioCrawler reaches that behavior solely by exploiting on-page factors, rather \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:YsMSGLbcyi4C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "User-perceived reusability estimation based on analysis of software repositories",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8368459/",
            "Abstract": "The popularity of open-source software repositories has led to a new reuse paradigm, where online resources can be thoroughly analyzed to identify reusable software components. Obviously, assessing the quality and specifically the reusability potential of source code residing in open software repositories poses a major challenge for the research community. Although several systems have been designed towards this direction, most of them do not focus on reusability. In this paper, we define and formulate a reusability score by employing information from GitHub stars and forks, which indicate the extent to which software components are adopted/accepted by developers. Our methodology involves applying and assessing different state-of-the-practice machine learning algorithms, in order to construct models for reusability estimation at both class and package levels. Preliminary evaluation of our methodology \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:eLRq4zTgah0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A retraining methodology for enhancing agent intelligence",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1427118/",
            "Abstract": "Data mining has proven a successful gateway for discovering useful knowledge and for enhancing business intelligence in a range of application fields. Incorporating this knowledge into already deployed applications, though, is highly impractical, since it requires reconfigurable software architectures, as well as human expert consulting. In an attempt to overcome this deficiency, we have developed agent academy, an integrated development framework that supports both design and control of multiagent systems (MAS), as well as agent training. We define agent training as the automated incorporation of logic structures generated through data mining into the agents of the system. The increased flexibility and cooperation primitives of MAS, augmented with the training and retraining capabilities of agent academy, provide a powerful means for the dynamic exploitation of data mining extracted knowledge. In this \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:KlAtU1dfN6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Data-Driven Analytics towards Software Sustainability: The Case of Open-Source Multimedia Tools on Cultural Storytelling",
            "Publication year": 2021,
            "Publication url": "https://www.mdpi.com/968708",
            "Abstract": "The continuous evolution of modern software technologies combined with the deluge of available \u201cready-to-use\u201d data has triggered revolutionary breakthroughs in several domains, preservation of cultural heritage included. This breakthrough is more than obvious just by considering the numerous multimedia tools and frameworks that actually serve as a means of providing enhanced cultural storytelling experiences (eg, navigation in historical sites using VR, 3D modeling of artifacts, or even holograms), which are now readily available. In this context and inspired by the vital importance of sustainability as a concept that expresses the need to create the necessary conditions for future generations to use and evolve present artifacts, we target the software engineering domain and propose a systematic way towards measuring the extent to which a software artifact developed and applied in the cultural heritage domain is sustainable. To that end, we present a data-driven methodology that harnesses data residing in online software repositories and involves the analysis of various open-source multimedia tools and frameworks. View Full-Text",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:it4f3qIuXWYC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Mining Software Engineering Data for Software Reuse",
            "Publication year": 2020,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=swTaDwAAQBAJ&oi=fnd&pg=PP1&dq=info:LHzGOZ0RbI4J:scholar.google.com&ots=UQcAnEsdSk&sig=gANVh5VUsJaOZ02tJs0KuO_ZLWU",
            "Abstract": "This monograph discusses software reuse and how it can be applied at different stages of the software development process, on different types of data and at different levels of granularity. Several challenging hypotheses are analyzed and confronted using novel data-driven methodologies, in order to solve problems in requirements elicitation and specification extraction, software design and implementation, as well as software quality assurance. The book is accompanied by a number of tools, libraries and working prototypes in order to practically illustrate how the phases of the software engineering life cycle can benefit from unlocking the potential of data. Software engineering researchers, experts, and practitioners can benefit from the various methodologies presented and can better understand how knowledge extracted from software data residing in various repositories can be combined and used to enable effective decision making and save considerable time and effort through software reuse. Mining Software Engineering Data for Software Reuse can also prove handy for graduate-level students in software engineering.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:Weau3kkTRIMC",
            "Publisher": "Springer Nature"
        },
        {
            "Title": "Enhancing agent intelligence through evolving reservoir networks for predictions in power stock markets",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-27609-5_15",
            "Abstract": "In recent years, Time Series Prediction and clustering have been employed in hyperactive and evolving environments \u2013where temporal data play an important role\u2013 as a result of the need for reliable methods to estimate and predict the pattern or behavior of events and systems. Power Stock Markets are such highly dynamic and competitive auction environments, additionally perplexed by constrained power laws in the various stages, from production to transmission and consumption. As with all real-time auctioning environments, the limited time available for decision making provides an ideal testbed for autonomous agents to develop bidding strategies that exploit time series prediction. Within the context of this paper, we present Cassandra, a dynamic platform that fosters the development of Data-Mining enhanced Multi-agent systems. Special attention was given on the efficiency and reusability of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:j3f4tGmQtD8C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Cenote: a big data management and analytics infrastructure for the web of things",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3350546.3352531",
            "Abstract": "In the era of Big Data, Cloud Computing and Internet of Things, most of the existing, integrated solutions that attempt to solve their challenges are either proprietary, limit functionality to a predefined set of requirements, or hide the way data are stored and accessed. In this work we propose Cenote, an open source Big Data management and analytics infrastructure for the Web of Things that overcomes the above limitations. Cenote is built on component-based software engineering principles and provides an all-inclusive solution based on components that work well individually.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:djft3U1LymYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A framework for constructing multi-agent applications and training intelligent agents",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-24620-6_7",
            "Abstract": "As agent-oriented paradigm is reaching a significant level of acceptance by software developers, there is a lack of integrated high-level abstraction tools for the design and development of agent-based applications. In an effort to mitigate this deficiency, we introduce Agent Academy, an integrated development framework, implemented itself as a multi-agent system, that supports, in a single tool, the design of agent behaviours and reusable agent types, the definition of ontologies, and the instantiation of single agents or multi-agent communities. In addition to these characteristics, our framework goes deeper into agents, by implementing a mechanism for embedding rule-based reasoning into them. We call this procedure \u201cagent training\u201d and it is realized by the application of AI techniques for knowledge discovery on application-specific data, which may be available to the agent developer. In this respect, Agent \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:d1gkVwhDpl0C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "A multi-agent infrastructure for enhancing ERP system intelligence",
            "Publication year": 2007,
            "Publication url": "https://scpe.org/index.php/scpe/article/view/401/75",
            "Abstract": "\u041d\u041a \u0421\u0432\u0438\u0436\u0433 \u0439 \u0438 \u0433\u0432\u041a \u0432\u0438 \u0436\u0434\u0436 \u0437 \u042a \u0437\u0433\u0439\u0436 \u0428\u0430 \u0432\u0432 \u0432 \u0414 \u042a\u0428\u0415 \u0437\u043d\u0437\u0438 \u0431\u0437 \u0436 \u0439\u0437 \u0432 \u0437\u0437 \u0431 \u0432 \u0431 \u0432\u0438 \u0438\u0433\u0433\u0430\u0437 \u0438 \u0438 \u0439\u0419 \u0438\u0433\u0431 \u0438 \u0432 \u0432\u0438 \u0436 \u0438 \u0430\u0430 \u0433\u0431\u0434 \u0432\u043d \u0438\u0437\u0418 \u0432 \u0430\u0439 \u0432 \u0436 \u0430\u0419\u0438 \u0431 \u0434\u0430 \u0432\u0432 \u0432 \u0418 \u0431 \u0432\u0439 \u0438\u0439\u0436 \u0432 \u0418 \u0437 \u0430 \u0437\u0418 \u0432 \u0431 \u0436 \u0438 \u0432 \u041a \u042c \u0437 \u0434\u0436\u0433 \u0437\u0437 \u0437 \u0434\u0436\u0433 \u0439 \u0430 \u0436 \u0431\u0433\u0439\u0432\u0438\u0437 \u0433 \u0432\u0438 \u0436\u0434\u0436 \u0437 \u0438 \u0438 \u0438 \u0436 \u0418 \u0432 \u0438\u0439\u0436\u0432\u0418 \u0439\u0437 \u043d \u0431 \u0432 \u0436\u0437 \u0432 \u0431\u0434\u0430\u0433\u043d \u0437 \u0438\u0433 \u0432 \u0430 \u0430\u0430 \u0437\u0433\u0436\u0438\u0437 \u0433 \u0439\u0437 \u0432 \u0437\u0437 \u0438 \u0437 \u0437 \u0437\u0439 \u0437 \u0432\u043a \u0432\u0438\u0433\u0436\u043d \u0433\u0432\u0438\u0436\u0433\u0430\u0418 \u0433\u0436 \u0436 \u0438\u0436 \u0432 \u0418 \u0439\u0437\u0438\u0433\u0431 \u0436 \u0437 \u0436\u043a \u0418 \u0432 \u0432 \u0432 \u0432 \u0439\u0431 \u0432 \u0436 \u0437\u0433\u0439\u0436 \u0437 \u041d\u2104 \u041a \u0437\u0434 \u0438 \u0438 \u0437\u0439\u0434\u0434\u0433\u0436\u0438 \u0439\u0436\u0436 \u0432\u0438 \u042a\u0428 \u0437\u043d\u0437\u0438 \u0431\u0437 \u0434\u0436\u0433\u043a \u0433\u0432 \u0434\u0436\u0433 \u0437\u0437 \u0433\u0433\u0436 \u0432 \u0438 \u0433\u0432 \u0432 \u0438 \u0433\u0436 \u0432 \u043e \u0438 \u0433\u0432\u0418 \u0431\u0433\u0437\u0438 \u0433 \u0438 \u0431 \u0437\u0434 \u0430\u0430\u043d \u0430 \u043d \u0437\u043d\u0437\u0438 \u0431\u0437 \u0430 \u043a \u0432 \u0437 \u0433\u0432\u0419\u042b\u0439\u0434\u0434\u0433\u0436\u0438 \u0414 \u042b\u0415 \u0434 \u0430 \u0438 \u0437\u0418 \u0436 \u0437\u0439\u0430\u0438 \u0432 \u0438 \u0436 \u0433\u0436 \u0432 \u0436 \u0437 \u0433\u0431\u0434 \u0432\u043d \u0433\u0431\u0434 \u0438 \u0438 \u043a \u0432 \u0437\u0437\u041a \u0421\u0432 \u0438 \u0433\u0432\u0418 \u0436\u0433\u0431 \u0439\u0432 \u0438 \u0433\u0432 \u0430 \u0438\u043d \u0434 \u0436\u0437\u0434 \u0438 \u043a \u0418 \u0431\u0433\u0437\u0438 \u042a\u0428 \u0437\u043d\u0437\u0438 \u0431\u0437 \u0436 \u0430 \u0431 \u0438 \u0438\u0433 \u0431 \u0436 \u0438\u0436 \u0432\u0437 \u0438 \u0433\u0432 \u0430 \u0421\u042c \u0437\u043d\u0437\u0438 \u0431\u0437\u0418 \u0434 \u0430 \u0433 \u0435\u0439 \u0436 \u0432 \u0418 \u0434\u0436\u0433 \u0437\u0437 \u0432 \u0418 \u0432 \u0433\u0431\u0431\u0439\u0432 \u0438 \u0432 \u0436 \u043b \u0433\u0436 \u0439\u0432\u0437\u0433\u0434 \u0437\u0438 \u0438 \u0434\u0436\u0433 \u0437\u0437 \u0438 \u0433\u0432 \u0438 \u0433\u0431\u0434 \u0432\u043d\u0413\u0437 \u0434 \u0437\u0438 \u0432 \u0434\u0436 \u0437 \u0432\u0438 \u0437\u0439\u0434\u0434\u0430\u043d \u0432 \u0433\u0434 \u0436 \u0438 \u0433\u0432\u0437 \u041e\u2104 \u041a \u0421\u0432 \u0433\u0436 \u0436 \u0438\u0433 \u0433\u0434\u0438 \u0431 \u043e \u0439\u0437 \u0432 \u0437\u0437 \u0434\u0436\u0433 \u0437\u0437 \u0437 \u0432 \u0438 \u0438 \u0438 \u0430 \u0437\u0439\u0434\u0434\u0430\u043d \u0432 \u0431 \u0432 \u0431 \u0432\u0438 \u0430 \u043a \u0430\u0418 \u0438 \u0432 \u0433\u0436 \u0432 \u0430\u043d\u0438 \u0430 \u0421\u042c \u0437\u043d\u0437\u0438 \u0431\u0437 \u0438 \u0438 \u043b \u0430\u0430 \u043b\u0433\u0436 \u0432 \u0430\u0433\u0437 \u0433\u0433\u0434 \u0436 \u0438 \u0433\u0432 \u043b \u0438 \u0438 \u0430\u0436 \u043d \u0432\u0437\u0438 \u0430\u0430 \u042a\u0428 \u0437\u043d\u0437\u0438 \u0431\u0437 \u0437 \u0430\u0436 \u043d \u0432 \u0432\u0438 \u0418 \u0432 \u042b\u0419 \u0432 \u0430 \u0437\u043d\u0437\u0438 \u0431\u0437 \u0437\u0438 \u0432 \u0433\u0439\u0438 \u0437 \u0438 \u0431\u0433\u0437\u0438 \u0437\u0439 \u0437\u0437 \u0439\u0430 \u0438 \u043b \u043d \u0438\u0433\u043b \u0436 \u0437 \u0438 \u043a \u0430\u0433\u0434\u0431 \u0432\u0438 \u0433 \u0431\u0433\u0436 \u0432\u0438 \u0432 \u0431\u0433\u0436 \u0434\u0436\u0433 \u0438 \u0430 \u0437\u0433\u0430\u0439\u0438 \u0433\u0432\u0437\u041a \u0428\u0436\u0433 \u0432 \u043a \u0432 \u0439\u0436\u0438 \u0436\u0418 \u043a \u0432\u0434\u0433\u0436\u0438\u2104 \u0437\u0439 \u0437\u0438\u0437 \u0438 \u0438 \u0437 \u0433\u0432\u0419\u0431 \u0432 \u0434 \u0430 \u0438 \u0437 \u0437 \u0433\u0439\u0430 \u0438 \u0437 \u0432 \u043c\u0438 \u0432\u0437 \u0433\u0432 \u0433 \u0438 \u0439\u0431 \u0432 \u0430 \u0438\u043d \u0438\u0433 \u0434\u0436\u0433 \u0437\u0437 \u0432\u0433\u043b\u0430 \u0432 \u0434\u0436\u0433\u0434\u0433\u0437 \u0437 \u0438 \u0439\u0432 \u0438 \u0433\u0432 \u0433 \u0432\u0433\u043b\u0430 \u0431 \u0432 \u0431 \u0432\u0438 \u0437\u043d\u0437\u0438 \u0431\u0437 \u043b \u0438 \u0438 \u0430 \u0437\u0437 \u0430 \u0438\u0436 \u0432\u0437 \u0438 \u0433\u0432\u0419 \u0437 \u0437\u043d\u0437\u0438 \u0431\u0437\u0418 \u043b \u0430 \u0436\u0430\u0437\u0437\u0433\u0432 \u0432 \u042c\u0439\u0436 \u0432 \u041f\u2104 \u0430 \u0431 \u0438 \u0438 \u0438 \u0432\u0438 \u0436 \u0438 \u0433\u0432 \u0433 \u0437\u0431 \u0436\u0438 \u0419\u0433\u0432 \u0431\u0433 \u0439\u0430 \u0437 \u0438\u0433 \u0438 \u0430\u0436 \u043d \u0437\u0438 \u0430 \u0437 \u042a\u0428 \u0437\u043d\u0437\u0438 \u0431\u0437 \u0433\u0439\u0430 \u0431 \u0437\u0438 \u0432 \u0436 \u0437\u0433 \u0438\u043b \u0436 \u0431\u0433\u0436 \u0438 \u043a \u0432 \u0434\u0436\u0433 \u0439 \u0438 \u043a \u0433\u0436 \u0438 \u0432 \u0419\u0439\u0437 \u0436\u0437\u041a",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:hC7cP41nSMkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "CASSANDRA-A simulation-based, decision-support tool for energy market stakeholders",
            "Publication year": 2015,
            "Publication url": "http://eprints.eudl.eu/id/eprint/2221/",
            "Abstract": "Energy gives personal comfort to people, and is essential for the generation of commercial and societal wealth. Nevertheless, energy production and consumption place considerable pressures on the environment, such as the emission of green- house gases and air pollutants. They contribute to climate change, damage natural ecosystems and the man-made environment, and cause adverse effects to human health. Lately, novel market schemes emerge, such as the formation and operation of customer coalitions aiming to improve their market power through the pursuit of common benefits.  In this paper we present CASSANDRA, an open source 1 , expandable software platform for modelling the demand side of power systems, focusing on small scale consumers. The structural elements of the platform are a) the electrical installations (i.e. households, commercial stores, small industries etc.), b) the respective appliances installed, and c) the electrical consumption-related activities of the people residing in the installations.  CASSANDRA serves as a tool for simulation of real demand- side environments providing decision support for energy market stakeholders. The ultimate goal of the CASSANDRA simulation functionality is the identification of good practices that lead to energy efficiency, clustering electric energy consumers according to their consumption patterns, and the studying consumer change behaviour when presented with various demand response programs.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:3WNXLiBY60kC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9413056/",
            "Abstract": "In this work, we address the problem of audio-based near-duplicate video retrieval. We propose the Audio Similarity Learning (AuSiL) approach that effectively captures temporal patterns of audio similarity between video pairs. For the robust similarity calculation between two videos, we first extract representative audio-based video descriptors by leveraging transfer learning based on a Convolutional Neural Network (CNN) trained on a large scale dataset of audio events, and then we calculate the similarity matrix derived from the pairwise similarity of these descriptors. The similarity matrix is subsequently fed to a CNN network that captures the temporal structures existing within its content. We train our network following a triplet generation process and optimizing the triplet loss function. To evaluate the effectiveness of the proposed approach, we have manually annotated two publicly available video datasets based \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:O0MA3yP7Y3UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Competitive benchmarking: Lessons learned from the trading agent competition",
            "Publication year": 2012,
            "Publication url": "https://ojs.aaai.org/index.php/aimagazine/article/view/2396",
            "Abstract": "Over the years, competitions have been important catalysts for progress in artificial intelligence. We describe the goal of the overall Trading Agent Competition and highlight particular competitions. We discuss its significance in the context of today\u2019s global market economy as well as AI research, the ways in which it breaks away from limiting assumptions made in prior work, and some of the advances it has engendered over the past ten years. Since its introduction in 2000, TAC has attracted more than 350 entries and brought together researchers from AI and beyond.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:k_IJM867U9cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Providing Reusability-Aware Recommendations",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=7123951120925804993&hl=en&oi=scholarr",
            "Abstract": "As contemporary software development relies more on software reuse, several systems have been designed to automate the process of finding reusable software components from online sources and integrating them to one\u2019s source code. However, these systems focus on whether the proposed components cover the desired functionality, without assessing also their reusability. In this chapter, we present a recommendation system for source code components that covers both the functional and the quality aspects of component reuse. Our system, which is named QualBoa, retrieves components from online repositories and reports their functional matching to the query as well as their reusability score, which is based on configurable thresholds of source code metrics. Upon providing an example usage scenario and evaluating QualBoa, we conclude that it is effective for recommending reusable source code.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:jPVjDSAV6m0C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "A Robust Agent Design for Dynamic SCM",
            "Publication year": 2006,
            "Publication url": "https://scholar.google.com/scholar?cluster=5034572528380403841&hl=en&oi=scholarr",
            "Abstract": "The leap from decision support to autonomous systems has often raised a number of issues, namely system safety, soundness and security. Depending on the field of application, these issues can either be easily overcome or even hinder progress. In the case of Supply Chain Management (SCM), where system performance implies loss or profit, these issues are of high importance. SCM environments are often dy-namic markets providing incomplete information, therefore demanding intelligent solutions which can adhere to environment rules, perceive vari-ations, and act in order to achieve maximum revenue. Advancing on the way such autonomous solutions deal with the SCM process, we have built a robust, highly-adaptable and easily-configurable mechanism for efficiently dealing with all SCM facets, from material procurement and inventory management to goods production and shipment. Our agent has been crash-tested in one of the most challenging SCM environments, the trading agent competition SCM game and has proven capable of providing advanced SCM solutions on behalf of its owner. This paper introduces Mertacor and its main architectural primitives, provides an overview of the TAC SCM environment, and discusses Mertacor's performance.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:K-tzbvM8PMoC",
            "Publisher": "Springer"
        },
        {
            "Title": "Software requirements: A new domain for semantic parsers",
            "Publication year": 2014,
            "Publication url": "https://www.aclweb.org/anthology/W14-2410.pdf",
            "Abstract": "Software requirements are commonly written in natural language, making them prone to ambiguity, incompleteness and inconsistency. By converting requirements to formal semantic representations, emerging problems can be detected at an early stage of the development process, thus reducing the number of ensuing errors and the development costs. In this paper, we treat the mapping from requirements to formal representations as a semantic parsing task. We describe a novel data set for this task that involves two contributions: first, we establish an ontology for formally representing requirements; and second, we introduce an iterative annotation scheme, in which formal representations are derived through step-wise refinements.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:dfsIfKJdRG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards understanding how personality, motivation, and events trigger Web user activity",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5616449/",
            "Abstract": "Web 2.0 provided internet users with a dynamic medium, where information is updated continuously and anyone can participate. Though preliminary analysis exists, there is still little understanding on what exactly stimulates users to actively participate, create and share content in online communities. In this paper we present a methodology that aspires to identify and analyze those events that trigger web user activity, content creation and sharing in Web 2.0. Our approach is based on user personality and motivation, and on the occurrence of events with a personal or global impact. The proposed methodology was applied on data collected from Flickr and analysis was performed through the use of statistics and data mining techniques.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Mining Knowledge for Agent Communities",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/0-387-25757-8_8.pdf",
            "Abstract": "The third application level of knowledge diffusion differs substantially from the first two, because in this case there are no historical data to perform data mining on and extract knowledge models. Both the multiagent systems and the DM techniques are evolutionary. Interactions between the members of the system and the influence that some of them exert on others, play a significant role in the overall behavior of the system. We refer to such systems as agent communities, because the agents are faced with common problems (or goals), which they attempt to overcome (or reach) through either collaboration or competition. In either case, agent interaction is a pivotal element of the system. As already described in Chapter 5, key issues for agent communities are the proper modeling of the problem and the efficient representation of the agent knowledge model. As a typical example of this class of MAS, we have chosen to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:RGFaLdJalmkC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Extracting Source Code Snippets from Online Sources",
            "Publication year": 2018,
            "Publication url": "https://pdfs.semanticscholar.org/2805/f9d72b6b15702c30b5f0b8ce9d5a07547259.pdf",
            "Abstract": "CodeCatch - Extracting Source Code Snippets from Online Sources Page 1 CODECATCH \nExtracting Source Code Snippets from Online Sources May 21, 2018 Themistoklis \nDiamantopoulos Georgios Karagiannopoulos Andreas Symeonidis asymeon@eng.auth.gr \nElectrical & Computer Engineering Dept. Aristotle University of Thessaloniki Page 2 \nContents Introduction CodeCatch System Evaluation CONTENTS 1. Introduction 2. \nCodeCatch 3. System Evaluation 2 Page 3 INTRODUCTION Page 4 Contents Introduction \nCodeCatch System Evaluation RECOMMENDATION SYSTEMS The routine process of \nwriting new code involves using search engines to find snippets from websites like \nStackOverflow, blogs, documentation etc. This approach is time-consuming, distracting, and \nimplies a lot of personal effort by the developer. CodeCatch: A system that accelerates the \nprocess of searching and separating the for a . 4 -\u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:G7txJ4jiatkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Employing source code information to improve question-answering in Stack Overflow",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7180116/",
            "Abstract": "Nowadays, software development has been greatly influenced by question-answering communities, such as Stack Overflow. A new problem-solving paradigm has emerged, as developers post problems they encounter that are then answered by the community. In this paper, we propose a methodology that allows searching for solutions in Stack Overflow, using the main elements of a question post, including not only its title, tags, and body, but also its source code snippets. We describe a similarity scheme for these elements and demonstrate how structural information can be extracted from source code snippets and compared to further improve the retrieval of questions. The results of our evaluation indicate that our methodology is effective on recommending similar question posts allowing community members to search without fully forming a question.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:qYOp8iumCsAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Predicting hyperparameters from metafeatures in binary classification problems",
            "Publication year": 2018,
            "Publication url": "https://scholar.google.com/scholar?cluster=10147342874771195013&hl=en&oi=scholarr",
            "Abstract": "The presence of computationally demanding problems and the current inability to automatically transfer experience from the application of past experiments to new ones delays the evolution of knowledge itself. In this paper we present the Automated Data Scientist 1, a system that employs meta-learning for hyperparameter selection and builds a rich ensemble of models through forward model selection in order to automate binary classification tasks. Preliminary evaluation shows that the system is capable of coping with classification problems of medium complexity.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:fGk8ZXjgAbkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Response modeling of small-scale energy consumers for effective demand response applications",
            "Publication year": 2016,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0378779615003223",
            "Abstract": "The Smart Grid paradigm can be economically and socially sustainable by engaging potential consumers through understanding, trust and clear tangible benefits. Interested consumers may assume a more active role in the energy market by claiming new energy products/services on offer and changing their consumption behavior. To this end, suppliers, aggregators and Distribution System Operators can provide monetary incentives for customer behavioral change through demand response programs, which are variable pricing schemes aiming at consumption shifting and/or reduction. However, forecasting the effect of such programs on power demand requires accurate models that can efficiently describe and predict changes in consumer activities as a response to pricing alterations. Current work proposes such a detailed bottom-up response modeling methodology, as a first step towards understanding and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:CC3C2HR4nz8C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Employing contribution and quality metrics for quantifying the software development process",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3379597.3387490",
            "Abstract": "The full integration of online repositories in contemporary software development promotes remote work and collaboration. Apart from the apparent benefits, online repositories offer a deluge of data that can be utilized to monitor and improve the software development process. Towards this direction, we have designed and implemented a platform that analyzes data from GitHub in order to compute a series of metrics that quantify the contributions of project collaborators, both from a development as well as an operations (communication) perspective. We analyze contributions throughout the projects' lifecycle and track the number of coding violations, this way aspiring to identify cases of software development that need closer monitoring and (possibly) further actions to be taken. In this context, we have analyzed the 3000 most popular GitHub Java projects and provide the data to the community.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:rp474-M6Y4oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards Modeling the User-perceived Quality of Source Code using Static Analysis Metrics.",
            "Publication year": 2017,
            "Publication url": "https://pdfs.semanticscholar.org/9f1c/b2a6a56a292301ccd818e4c1222f4feeb274.pdf",
            "Abstract": "Nowadays, software has to be designed and developed as fast as possible, while maintaining quality standards. In this context, developers tend to adopt a component-based software engineering approach, reusing own implementations and/or resorting to third-party source code. This practice is in principle costeffective, however it may lead to low quality software products. Thus, measuring the quality of software components is of vital importance. Several approaches that use code metrics rely on the aid of experts for defining target quality scores and deriving metric thresholds, leading to results that are highly contextdependent and subjective. In this work, we build a mechanism that employs static analysis metrics extracted from GitHub projects and defines a target quality score based on repositories\u2019 stars and forks, which indicate their adoption/acceptance by the developers\u2019 community. Upon removing outliers with a one-class classifier, we employ Principal Feature Analysis and examine the semantics among metrics to provide an analysis on five axes for a source code component: complexity, coupling, size, degree of inheritance, and quality of documentation. Neural networks are used to estimate the final quality score given metrics from all of these axes. Preliminary evaluation indicates that our approach can effectively estimate software quality.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:HAmI6pRF5skC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A retraining methodology for enhancing agent intelligence",
            "Publication year": 2007,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950705106001687",
            "Abstract": "Data mining has proven a successful gateway for discovering useful knowledge and for enhancing business intelligence in a range of application fields. Incorporating this knowledge into already deployed applications, though, is highly impractical, since it requires reconfigurable software architectures, as well as human expert consulting. In an attempt to overcome this deficiency, we have developed Agent Academy, an integrated development framework that supports both design and control of multi-agent systems (MAS), as well as \u201cagent training\u201d. We define agent training as the automated incorporation of logic structures generated through data mining into the agents of the system. The increased flexibility and cooperation primitives of MAS, augmented with the training and retraining capabilities of Agent Academy, provide a powerful means for the dynamic exploitation of data mining extracted knowledge. In this \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Boosted seed oversampling for local community ranking",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0306457318308574",
            "Abstract": "Local community detection is an emerging topic in network analysis that aims to detect well-connected communities encompassing sets of priorly known seed nodes. In this work, we explore the similar problem of ranking network nodes based on their relevance to the communities characterized by seed nodes. However, seed nodes may not be central enough or sufficiently many to produce high quality ranks. To solve this problem, we introduce a methodology we call seed oversampling, which first runs a node ranking algorithm to discover more nodes that belong to the community and then reruns the same ranking algorithm for the new seed nodes. We formally discuss why this process improves the quality of calculated community ranks if the original set of seed nodes is small and introduce a boosting scheme that iteratively repeats seed oversampling to further improve rank quality when certain ranking algorithm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:SEKBqlyTJecC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Agents and Data Mining Interaction: 7th International Workshop, ADMI 2011, Taipei, Taiwan, May 2-6, 2011, Revised Selected Papers",
            "Publication year": 2011,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=CzuqCAAAQBAJ&oi=fnd&pg=PR2&dq=info:ZHGJdYx31SUJ:scholar.google.com&ots=fRquqRsFA9&sig=LE-9ZuFefIvACyepesTmvMLppUQ",
            "Abstract": "This book constitutes the thoroughly refereed post-workshop proceedings of the 7th International Workshop on Agents and Data Mining Interaction, ADMI 2011, held in Taipei, Taiwan, in May 2011 in conjunction with AAMAS 2011, the 10th International Joint Conference on Autonomous Agents and Multiagent Systems. The 11 revised full papers presented were carefully reviewed and selected from 24 submissions. The papers are organized in topical sections on agents for data mining; data mining for agents; and agent mining applications.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:r0BpntZqJG4C",
            "Publisher": "Springer"
        },
        {
            "Title": "Data Mining and Agent Technology: a fruitful symbiosis",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-0-387-69935-6_14",
            "Abstract": "Multi-agent systems (MAS) have grown quite popular in a wide spectrum of applications where argumentation, communication, scaling and adaptability are requested. And though the need for well-established engineering approaches for building and evaluating such intelligent systems has emerged, currently no widely accepted methodology exists, mainly due to lack of consensus on relevant definitions and scope of applicability. Even existing well-tested evaluation methodologies applied in traditional software engineering, prove inadequate to address the unpredictable emerging factors of the behavior of intelligent components. The following chapter aims to present such a unified and integrated methodology for a specific category of MAS. It takes all constraints and issues into account and denotes the way knowledge extracted with the use of Data mining (DM) techniques can be used for the formulation initially \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:ULOm3_A8WrAC",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "Ms Pacman and the Robotic Ghost: A Modern Cyber-Physical Remake of the Famous Pacman Game",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8939255/",
            "Abstract": "Robotics and Internet of Things (IoT) are two of the most blooming scientific areas during the last years. Robotics has gained a lot of attention in the last decades and includes several disciplines (mapping, localization, planning, control etc.), while IoT is a quite new and exciting area, where seamless data aggregation and resource utilization from heterogeneous physical objects (e.g. devices, sensor networks and robots) is defined via multi-layer architectures. Moreover, Cyber-Physical systems (CPS) share similar concepts and principles with the IoT, focused on interconnecting physical and computational resources via multi-layer architectures. The current paper joins the Robotics and CPS disciplines via an architecture where heterogeneous physical and computational elements exist (robots, web app, message broker etc.), so as to implement a cyber-physical port of the famous Pacman game, called RoboPacman.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:gI9wzKcniAoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Event identification in web social media through named entity recognition and topic modeling",
            "Publication year": 2013,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0169023X13000827",
            "Abstract": "The problem of identifying important online or real life events from large textual document streams that are freely available on the World Wide Web is increasingly gaining popularity, given the flourishing of the social web. An event triggers discussion and comments on the WWW, especially in the blogosphere and in microblogging services. Consequently, one should be able to identify the involved entities, topics, time, and location of events through the analysis of information publicly available on the web, create semantically rich representations of events, and then use this information to provide interesting results, or summarize news to users.In this paper, we define the concept of important event and propose an efficient methodology for performing event detection from large time-stamped web document streams. The methodology successfully integrates named entity recognition, dynamic topic map discovery, topic \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:M05iB0D1s5AC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Application of data mining and intelligent agent technologies to concurrent engineering",
            "Publication year": 2003,
            "Publication url": "https://www.academia.edu/download/33600174/ce2003application.pdf",
            "Abstract": "Software agent technology has matured enough to produce intelligent agents, which can be used to control a large number of concurrent engineering tasks. Multi-agent systems are communities of agents that exchange information and data in the form of messages. The agents\u2019 intelligence can range from rudimentary sensor monitoring and data reporting to more advanced forms of decision making and autonomous behavior. The behavior and intelligence of each agent in the community can be obtained by performing data mining on available application data and the respected knowledge domain. We have developed Agent Academy, a software platform for the design, creation, and deployment of multi-agent systems, which combines the power of knowledge discovery algorithms with the versatility of agents. Using this platform, we show how agents, equipped with a data-driven inference engine, can be dynamically and continuously trained. We also discuss a few prototype multi-agent systems developed with Agent Academy.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "AMNOS-mobile\u00ae: Exploiting handheld computers in efficient sheep recording",
            "Publication year": 2007,
            "Publication url": "https://www.research.ed.ac.uk/en/publications/amnos-mobile-exploiting-handheld-computers-in-efficient-sheep-rec",
            "Abstract": "AMNOS-mobile\u00ae: Exploiting handheld computers in efficient sheep recording \u2014 University of \nEdinburgh Research Explorer Skip to main navigation Skip to search Skip to main content \nUniversity of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles \nResearch Units Projects Datasets Prizes Activities Press / Media Equipment Search by \nexpertise, name or affiliation AMNOS-mobile\u00ae: Exploiting handheld computers in efficient sheep \nrecording Z. Abas, AL Symeonidis, A. Batzios, Z. Basdagianni, Georgios Banos, PA Mitkas, E. \nSinapis, A Pampoukidou Royal (Dick) School of Veterinary Studies Research output: Chapter \nin Book/Report/Conference proceeding \u203a Conference contribution Overview Original language \nEnglish Title of host publication Breeding, production recording, health and the evaluation \nof farm animals Subtitle of host publication Proceedings of the 35th biennial Session of , , , /\u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:IWHjjKOFINEC",
            "Publisher": "Wageningen, The Netherlands: Wageningen Academic Publishers"
        },
        {
            "Title": "Modeling Software Requirements",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=17767451236489579491&hl=en&oi=scholarr",
            "Abstract": "Enhancing requirements elicitation and specification extraction has always been of added value to software engineering, as it expedites the software development life cycle. In this context, the main challenge is to construct formal models that are capable of storing requirements from multimodal formats and can facilitate requirements reuse. In this chapter, we present an approach that captures the static and dynamic view of software projects and generates traceable system specifications. Our ontology-based approach can receive input in the form of functional requirements, UML use case and activity diagrams, and storyboards and allows for reasoning over the stored requirements for validation and reuse purposes.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:QI7uKX5mnFEC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Data mining and knowledge discovery: A brief overview",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/0-387-25757-8_2.pdf",
            "Abstract": "Data Mining has evolved into a mainstream technology because of two complementary, yet antagonistic phenomena: a) the data deluge, fueled by the maturing of database technology and the development of advanced automated data collection tools and, b) the starvation for knowledge, defined as the need to filter and interpret all these massive data volumes stored in databases, data warehouses and other information repositories. DM can be thought of as the logical succession to Information Technology (IT). Considering IT evolution over the past 50 years (Figure 2.1), the first radical step was taken in the 60's with the implementation of data collection, while in the 70's, the first Relational Database Management Systems (RDBMS) were developed. During the 80's, enhanced data access techniques began to emerge, the relational model was widely applied, and suitable programming languages were developed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Prior Signal Editing for Graph Filter Posterior Fairness Constraints",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2108.12397",
            "Abstract": "Graph filters are an emerging paradigm that systematizes information propagation in graphs as transformation of prior node values, called graph signals, to posterior scores. In this work, we study the problem of mitigating disparate impact, i.e. posterior score differences between a protected set of sensitive nodes and the rest, while minimally editing scores to preserve recommendation quality. To this end, we develop a scheme that respects propagation mechanisms by editing graph signal priors according to their posteriors and node sensitivity, where a small number of editing parameters can be tuned to constrain or eliminate disparate impact. We also theoretically explain that coarse prior editing can locally optimize posteriors objectives thanks to graph filter robustness. We experiment on a diverse collection of 12 graphs with varying number of nodes, where our approach performs equally well or better than previous ones in minimizing disparate impact and preserving posterior AUC under fairness constraints.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:GdZ7R06HQM4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards Extracting the Role and Behavior of Contributors in Open-source Projects.",
            "Publication year": 2019,
            "Publication url": "https://www.scitepress.org/Papers/2019/79665/79665.pdf",
            "Abstract": "Lately, the popular open source paradigm and the adoption of agile methodologies have changed the way software is developed. Effective collaboration within software teams has become crucial for building successful products. In this context, harnessing the data available in online code hosting facilities can help towards understanding how teams work and optimizing the development process. Although there are several approaches that mine contributions\u2019 data, they usually view contributors as a uniform body of engineers, and focus mainly on the aspect of productivity while neglecting the quality of the work performed. In this work, we design a methodology for identifying engineer roles in development teams and determine the behaviors that prevail for each role. Using a dataset of GitHub projects, we perform clustering against the DevOps axis, thus identifying three roles: developers that are mainly preoccupied with code commits, operations engineers that focus on task assignment and acceptance testing, and the lately popular role of DevOps engineers that are a mix of both. Our analysis further extracts behavioral patterns for each role, this way assisting team leaders in knowing their team and effectively directing responsibilities to achieve optimal workload balancing and task allocation.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:0urtJCGzaFQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Agents and Data Mining Interaction: 10th International Workshop, ADMI 2014, Paris, France, May 5-9, 2014, Revised Selected Papers",
            "Publication year": 2015,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=t_PLCQAAQBAJ&oi=fnd&pg=PR5&dq=info:j-8pePnVdhQJ:scholar.google.com&ots=eP1hNBvvgq&sig=axmd-PptMOSQbzlyzZ4W5UQ8-68",
            "Abstract": "This book constitutes the thoroughly refereed and revised selected papers from the 10th International Workshop on Agents and Data Mining Interactions, ADMI 2014, held in Paris, France, in May 2014 as satellite workshop of AAMAS 2014, the 13th International Conference on Autonomous Agents and Multiagent Systems. The 11 papers presented were carefully reviewed and selected from numerous submissions for inclusion in this volume. They present current research and engineering results, as well as potential challenges and prospects encountered in the respective communities and the coupling between agents and data mining.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:t9Ks5LMjN0QC",
            "Publisher": "Springer"
        },
        {
            "Title": "Mining Solutions for Extended Snippet Reuse",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=245146567918974606&hl=en&oi=scholarr",
            "Abstract": "The introduction of question\u2013answering services, such as Stack Overflow, has given rise to a new problem-solving paradigm in software development. Using these services, developers can post their programming questions online and get useful solutions by the community. In this chapter we propose a methodology that allows searching for solutions in Stack Overflow, using the main elements of a question post, including its title, tags, body, and source code snippets. We design a similarity scheme for these elements that can be used for finding similar question posts. Text elements are compared using Information Retrieval metrics, while snippet similarity is computed by first converting snippets into sequences using a representation that extracts structural information. The results of the evaluation of our methodology indicate that it can be effective for recommending similar question posts, and thus can be \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:ZpgFv6i7Z4gC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Of daemons and men: A file system approach towards intrusion detection",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1568494614004311",
            "Abstract": "We present FI2DS a file system, host based anomaly detection system that monitors Basic Security Module (BSM) audit records and determines whether a web server has been compromised by comparing monitored activity generated from the web server to a normal usage profile. Additionally, we propose a set of features extracted from file system specific BSM audit records, as well as an IDS that identifies attacks based on a decision engine that employs one-class classification using a moving window on incoming data. We have used two different machine learning algorithms, Support Vector Machines (SVMs) and Gaussian Mixture Models (GMMs) and our evaluation is performed on real-world datasets collected from three web servers and a honeynet. Results are very promising, since FI2DS detection rates range between 91% and 95.9% with corresponding false positive rates ranging between 8.1\u00d7 10\u22122 % and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:bkKuixW_xMkC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A framework for the implementation of large scale Demand Response",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6642380/",
            "Abstract": "The rationalization of electrical energy consumption is a constant goal driving research over the last decades. The pursuit of efficient solutions requires the involvement of electrical energy consumers through Demand Response programs. In this study, a framework is presented that can serve as a tool for designing and simulating Demand Response programs, aiming at energy efficiency through consumer behavioral change. It provides the capability to dynamically model groups of electrical energy consumers with respect to their consumption, as well as their behavior. This framework is currently under development within the scope of the EU funded FP7 project \u201cCASSANDRA - A multivariate platform for assessing the impact of strategic decisions in electrical power systems\u201d.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:g5m5HwL7SMYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Quantitative and qualitative evaluation of ROS-enabled local and global planners in 2D static environments",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s10846-019-01086-y",
            "Abstract": "Apart from perception, one of the most fundamental aspects of an autonomous mobile robot is the ability to adequately and safely traverse the environment it operates in. This ability is called Navigation and is performed in a two- or three-dimensional fashion, except for cases where the robot is neither a ground vehicle nor articulated (e.g. robotics arms). The planning part of navigation comprises a global planner, suitable for generating a path from an initial to a target pose, and a local planner tasked with traversing the aforementioned path while dealing with environmental, sensorial and motion uncertainties. However, the task of selecting the optimal global and/or local planner combination is quite hard since no research provides insight on which is best regarding the domain and planner limitations. In this context, current work performs a comparative analysis on qualitative and quantitative aspects of the most \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:RgMnzfD6kpIC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Monitoring Agent Communication in Soft Real-time Environments",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5616080/",
            "Abstract": "Real-time systems can be defined as systems operating under specific timing constraints, either hard or soft ones. In principle, agent systems are considered inappropriate for such kinds of systems, due to the asynchronous nature of their communication protocols, which directly influences their temporal behavior. Nevertheless, multi-agent systems could be successfully employed for solving problems where failure to meet a deadline does not have serious consequences, given the existence of a fail-safe system mechanism. Current work focuses on the analysis of multi-agent systems behavior under such soft real-time constraints. To this end, ERMIS has been developed: an integrated framework that provides the agent developer with the ability to benchmark his/her own architecture and identify its limitations and its optimal timing behavior, under specific hardware/software constraints. A variety of MAS configurations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:4DMP91E08xMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "SRCA-The Scalable Robotic Cloud Agents Architecture",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1804.04362",
            "Abstract": "In an effort to penetrate the market at an affordable cost, consumer robots tend to provide limited processing capabilities, just enough to serve the purpose they have been designed for. However, a robot, in principle, should be able to interact and process the constantly increasing information streams generated from sensors or other devices. This would require the implementation of algorithms and mathematical models for the accurate processing of data volumes and significant computational resources. It is clear that as the data deluge continues to grow exponentially, deploying such algorithms on consumer robots will not be easy. Current work presents a cloud-based architecture that aims to offload computational resources from robots to a remote infrastructure, by utilizing and implementing cloud technologies. This way robots are allowed to enjoy functionality offered by complex algorithms that are executed on the cloud. The proposed system architecture allows developers and engineers not specialised in robotic implementation environments to utilize generic robotic algorithms and services off-the-shelf.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:WTQy_8Ay2UsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Agent intelligence through data mining",
            "Publication year": 2005,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=YwGmFDO8uv4C&oi=fnd&pg=PR5&dq=info:jqkKoGsxZOkJ:scholar.google.com&ots=GIcPIv4X1K&sig=t7D6_uiXKxCTzJP0131gMlOn4HE",
            "Abstract": "AGENT INTELLIGENCE THROUGH DATA MINING offers a self-contained overview of a relatively young but important area of research: the intersection of agent technology and data mining. This intersection leads to considerable advancements in the area of information technologies, drawing the increasing attention of both research and industrial communities. It can take two forms: a) the more mundane use of intelligent agents for improved data mining and; b) the use of data mining for smarter, more efficient agents. The second approach is the main focus of this volume. Knowledge, routinely created and maintained by today\u2019s applications, is hidden in voluminous data repositories that can be extracted by data mining. The next step is to transform this discovered knowledge into the inference mechanisms or simply the behavior of agents and multi-agent systems. AGENT INTELLIGENCE THROUGH DATA MINING addresses this issue, as well as the arguable challenge of generating intelligence from data while transferring it to a separate, possibly autonomous, software entity. Following a brief review of data mining and agent technology fields, this book presents a methodology for developing multi-agent systems, describes available open-source tools to support this process, and demonstrates the application of the methodology on three different cases. AGENT INTELLIGENCE THROUGH DATA MINING is designed for a professional audience composed of researchers and practitioners in industry. This volume is also suitable for graduate-level students in computer science.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:u5HHmVD_uO8C",
            "Publisher": "Springer Science & Business Media"
        },
        {
            "Title": "Rapp: a robotic-oriented ecosystem for delivering smart user empowering applications for older people",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/article/10.1007/s12369-016-0361-z",
            "Abstract": "It is a general truth that increase of age is associated with a level of mental and physical decline but unfortunately the former are often accompanied by social exclusion leading to marginalization and eventually further acceleration of the aging process. A new approach in alleviating the social exclusion of older people involves the use of assistive robots. As robots rapidly invade everyday life, the need of new software paradigms in order to address the user\u2019s unique needs becomes critical. In this paper we present a novel architectural design, the RAPP [a software platform to deliver smart, user empowering robotic applications (RApps)] framework that attempts to address this issue. The proposed framework has been designed in a cloud-based approach, integrating robotic devices and their respective applications. We aim to facilitate seamless development of RApps compatible with a wide range of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:j6V8Syvup0UC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Symbiosis: Using predator-prey games as a test bed for studying competitive co-evolution",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4227534/",
            "Abstract": "The animal approach constitutes an intriguing attempt to study and comprehend the behavior of adaptive, learning entities in complex environments. Further inspired by the notions of co-evolution and evolutionary ''arms races\", we have developed Symbiosis, a virtual ecosystem that hosts two self-organizing, combating species - preys and predators. All animats live and evolve in this shared environment, they are self-maintaining and engage in a series of vital activities - nutrition, growth, communication - with the ultimate goals of survival and reproduction. The main objective of Symbiosis is to study the behavior of ecosystem members, especially in terms of the emergent learning mechanisms and the effect of co-evolution on the evolved behavioral strategies. In this direction, several indicators are used to assess individual behavior, with the overall effectiveness metric depending strongly on the animats net energy \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:ufrVoPGSRksC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Mining Agent Behaviors",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/0-387-25757-8_7.pdf",
            "Abstract": "The second level of dynamic knowledge infusion to MAS requires the existence of historical data describing the actions of agents within the MAS they reside. Such systems are mainly focused on the prediction of agent behaviors, and special attention is given on the way agent actions are modeled and represented. The core objective of agent action-based personalization is the discovery of a recommendation \u00abset, that will better predict the behavior of the agent\" in action\". In order for the reader to better comprehend the notions of agent behavior modeling and prediction, we have developed an agent-based framework (a recommendation engine for navigating the web), where agent actions are directly mapped to web user traversal decisions. The main issues related to the design and development of such a framework for predicting agent actions are discussed in this chapter, while the basic concessions made to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:ns9cj8rnVeAC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Towards Analyzing Contributions from Software Repositories to Optimize Issue Assignment",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9282784/",
            "Abstract": "Most software teams nowadays host their projects online and monitor software development in the form of issues/tasks. This process entails communicating through comments and reporting progress through commits and closing issues. In this context, assigning new issues, tasks or bugs to the most suitable contributor largely improves efficiency. Thus, several automated issue assignment approaches have been proposed, which however have major limitations. Most systems focus only on assigning bugs using textual data, are limited to projects explicitly using bug tracking systems, and may require manually tuning parameters per project. In this work, we build an automated issue assignment system for GitHub, taking into account the commits and issues of the repository under analysis. Our system aggregates feature probabilities using a neural network that adapts to each project, thus not requiring manual \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:JaLtl8ASYokC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Hermes: Seamless delivery of containerized bioinformatics workflows in hybrid cloud (HTC) environments",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S2352711017300304",
            "Abstract": "Hermes introduces a new \u201cdescribe once, run anywhere\u201d paradigm for the execution of bioinformatics workflows in hybrid cloud environments. It combines the traditional features of parallelization-enabled workflow management systems and of distributed computing platforms in a container-based approach. It offers seamless deployment, overcoming the burden of setting up and configuring the software and network requirements. Most importantly, Hermes fosters the reproducibility of scientific workflows by supporting standardization of the software execution environment, thus leading to consistent scientific workflow results and accelerating scientific output.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:cTy_cm4PQvYC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "RESTsec: a low-code platform for generating secure by design enterprise services",
            "Publication year": 2018,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/17517575.2018.1462403",
            "Abstract": "In the modern business world it is increasingly often that Enterprises opt to bring their business model online, in their effort to reach out to more end users and increase their customer base. While transitioning to the new model, enterprises consider securing their data of pivotal importance. In fact, many efforts have been introduced to automate this \u2018webification\u2019 process; however, they all fall short in some aspect: a) they either generate only the security infrastructure, assigning implementation to the developers, b) they embed mainstream, less powerful authorisation schemes, or c) they disregard the merits of the dominating REST architecture and adopt less suitable approaches. In this paper we present RESTsec, a Low-Code platform that supports rapid security requirements modelling for Enterprise Services, abiding by the state of the art ABAC authorisation scheme. RESTsec enables the developer to seamlessly \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:5p9vMBpPSXYC",
            "Publisher": "Taylor & Francis"
        },
        {
            "Title": "An agent structure for evaluating micro-level MAS performance",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1660877.1660910",
            "Abstract": "Although the need for well-established engineering approaches in Intelligent Systems (IS) performance evaluation is urging, currently no widely accepted methodology exists, mainly due to lack of consensus on relevant definitions and scope of applicability, multi-disciplinary issues and immaturity of the field of IS. Even existing well-tested evaluation methodologies applied in other domains, such as (traditional) software engineering, prove inadequate to address the unpredictable emerging factors of the behavior of intelligent components. In this paper, we present a generic methodology and associated tools for evaluating the performance of IS, by exploiting the software agent paradigm as a representative modeling concept for intelligent systems. Based on the assessment of observable behavior of agents or multi-agent systems, the proposed methodology provides a concise set of guidelines and representation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:HDshCWvjkbEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Source Code Indexing for Component Reuse",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=12990899292606610058&hl=en&oi=scholarr",
            "Abstract": "The momentum of the open-source community has been constantly increasing, thus leading to numerous tools for writing, maintaining, and sharing source code. Several code search engines have been developed to support development tasks and facilitate reuse either directly or by functioning as information sources for code recommenders. In this chapter, we present AGORA, a code search engine that facilitates reuse in component level, snippet level, and project level. Through its Elasticsearch index, AGORA fosters advanced queries (syntax-aware, regular expressions), while the engine also integrates with popular code hosting repositories and offers a well-designed API. We provide representative examples and a usage scenario to illustrate the functionality of AGORA, and perform a comparative analysis in a code reuse context, which indicates that AGORA provides an efficient alternative to current \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:1YatL4jblGcC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Open source supply chains.",
            "Publication year": 2003,
            "Publication url": "https://www.academia.edu/download/48665676/Open_source_supply_chains20160908-31657-t8mumh.pdf",
            "Abstract": "Enterprise Resource Planning (ERP) systems tend to deploy Supply Chains, in order to successfully integrate customers, suppliers, manufacturers and warehouses, and therefore minimize system-wide costs, while satisfying service level requirements. Although efficient, these systems are neither versatile nor adaptive, since newly discovered customer trends cannot be easily integrated. Furthermore, the development of such systems is subject to strict licensing, since the exploitation of such kind of software is usually proprietary. This leads to a monolithic approach and to sub-utilization of efforts from all sides. Introducing a completely new paradigm of how primitive Supply Chain Management (SCM) rules apply on ERP systems, we have developed a framework as an Open Source Multi-Agent System that introduces adaptive intelligence as a powerful add-on for ERP software customization. In this paper the SCM system developed is described, whereas the expected benefits of the open source initiative employed are illustrated.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:eQOLeE2rZwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A software agent framework for exploiting demand-side consumer social networks in power systems",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6040749/",
            "Abstract": "This work introduces Energy City, a multi-agent framework designed and developed in order to simulate the power system and explore the potential of Consumer Social Networks (CSNs) as a means to promote demand-side response and raise social awareness towards energy consumption. The power system with all its involved actors (Consumers, Producers, Electricity Suppliers, Transmission and Distribution Operators) and their requirements are modeled. The semantic infrastructure for the formation and analysis of electricity CSNs is discussed, and the basic consumer attributes and CSN functionality are identified. Authors argue that the formation of such CSNs is expected to increase the electricity consumer market power by enabling them to act in a collective way.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:RHpTSmoSYBkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Association studies on cervical cancer facilitated by inference and semantic technologies: the ASSIST approach",
            "Publication year": 2008,
            "Publication url": "https://person.hst.aau.dk/ska/MIE2008/ParalleSessions/PapersForDownloads/03.DS&KM/SHTI136-0241.pdf",
            "Abstract": "Cervical cancer (CxCa) is currently the second leading cause of cancerrelated deaths, for women between 20 and 39 years old. As infection by the human papillomavirus (HPV) is considered as the central risk factor for CxCa, current research focuses on the role of specific genetic and environmental factors in determining HPV persistence and subsequent progression of the disease. ASSIST is an EU-funded research project that aims to facilitate the design and execution of genetic association studies on CxCa in a systematic way by adopting inference and semantic technologies. Toward this goal, ASSIST provides the means for seamless integration and virtual unification of distributed and heterogeneous CxCa data repositories, and the underlying mechanisms to undertake the entire process of expressing and statistically evaluating medical hypotheses based on the collected data in order to generate medically important associations. The ultimate goal for ASSIST is to foster the biomedical research community by providing an open, integrated and collaborative framework to facilitate genetic association studies.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:YOwf2qJgpHMC",
            "Publisher": "IOS Press; 1999"
        },
        {
            "Title": "Merging robotics and aal ontologies: The rapp methodology",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-15847-1_28",
            "Abstract": "Cloud robotics is becoming a trend in the modern robotics field, as it became evident that true artificial intelligence can be achieved only by sharing collective knowledge. In the ICT area, the most common way to formulate knowledge is via the ontology concept, where different meanings connect semantically. Additionally, a considerable effort to merge robotics with assisted living concepts exists, as the modern societies suffer from lack of caregivers for the persons in need. In the current work, an attempt is made to merge a robotic and an AAL ontology, as well as utilize it in the RAPP Project (EU-FP7).",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:qhW0HyKmSusC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Banner Personalization for e-Commerce",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-19823-7_53",
            "Abstract": "Real-time website personalization is a concept that is being discussed for more than a decade, but has only recently been applied in practice, according to new marketing trends. These trends emphasize on delivering user-specific content based on behavior and preferences. In this context, banner recommendation in the form of personalized ads is an approach that has attracted a lot of attention. Nevertheless, banner recommendation in terms of e-commerce main page sliders and static banners is even today an underestimated problem, as traditionally only large e-commerce stores deal with it. In this paper we propose an integrated framework for banner personalization in e-commerce that can be applied in small-medium e-retailers. Our approach combines topic-models and a neural network, in order to recommend and optimally rank available banners of an e-commerce store to each user separately. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:JhbybO29vGQC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Agents and Data Mining Interaction: 8th International Workshop, ADMI 2012, Valencia, Spain, June 4-5, 2012, Revised Selected Papers",
            "Publication year": 2013,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=FCO5BQAAQBAJ&oi=fnd&pg=PP2&dq=info:vxClS0K5o2oJ:scholar.google.com&ots=dV_BsVUn5Q&sig=vR2rEBtKroVFivL4xYrJJTdeoME",
            "Abstract": "This book constitutes the thoroughly refereed post-workshop proceedings of the 8th International Workshop on Agents and Data Mining Interaction, ADMI 2012, held in Valencia, Spain, in June 2012. The 16 revised full papers were carefully reviewed and selected from numerous submissions. The papers are organized in topical sections on agents for data mining, data mining for agents, and agent mining applications.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:E2bRg1zSkIsC",
            "Publisher": "Springer"
        },
        {
            "Title": "Of daemons and men: reducing false positive rate in intrusion detection systems with file system footprint analysis",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/article/10.1007/s00521-018-3550-x",
            "Abstract": "In this work, we propose a methodology for reducing false alarms in file system intrusion detection systems, by taking into account the daemon\u2019s file system footprint. More specifically, we experimentally show that sequences of outliers can serve as a distinguishing characteristic between true and false positives, and we show how analysing sequences of outliers can lead to lower false positive rates, while maintaining high detection rates. Based on this analysis, we developed an anomaly detection filter that learns outlier sequences using k-nearest neighbours with normalised longest common subsequence. Outlier sequences are then used as a filter to reduce false positives on the  file system intrusion detection system. This filter is evaluated on both overlapping and non-overlapping sequences of outliers. In both cases, experiments performed on three real-world web servers and a honeynet show that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:QYmifXMdJWgC",
            "Publisher": "Springer London"
        },
        {
            "Title": "Defining behaviorizeable relations to enable inference in semi-automatic program synthesis",
            "Publication year": 2021,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S2352220821000778",
            "Abstract": "Developing and using automatic program synthesis mechanisms within Domain Specific Languages (DSLs) requires transcribing empirical knowledge and specifications to formal models. Since the related expertise is time-consuming to build, software engineers are discouraged from adopting automated synthesis as a development paradigm. New approaches promise a significant reduction of human training effort by synthesizing programs from lax specifications, such as unstructured or semi-structured natural language. However, they do not introduce semantics of logical inference that would help users interact with the synthesis mechanism, for example to treat misinterpreted specifications. To cover this shortcoming, in this work we introduce the concept of behaviorizeable relations, which can be less rigorous than behavioral equivalence \u2013and therefore allow laxer specifications\u2013 but still perform inference under \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:nbnL2fqDbzcC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Intelligent Agents and Multi-Agent Systems",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/0-387-25757-8_3.pdf",
            "Abstract": "This definition can be applied to software agents, which are instantiated and act instead of a user or a software program that controls them. Thus, one of the most characteristic agent features is its agency. In the rest of this book, the term agent is synonymous to a software agent. The difficulty in defining an agent arises from the fact that the various aspects of agency are weighted differently, with respect to the application domain at hand. Although, for example, agent learning is considered of pivotal importance in certain applications, for others it may be considered not only trivial, but even undesirable. Consequently, a number of definitions could be provided with respect to agent objectives. Wooldridge & Jennings have succeeded in combining general agent features into the following generic, nevertheless abstract, definition:",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:O3NaXMp0MMsC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Fingerprinting Localization of RFID tags with Real-Time Performance-Assessment, using a Moving Robot",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8739486/",
            "Abstract": "This work is focused on unmanned inventorying and localization, by deploying an RFID-equipped autonomous robot. The robot is able to perform Simultaneous Localization and Mapping (SLAM), thanks to its optical sensors. As the robot moves inside the target area, it continuously interrogates all RFID tags within range. Passive RFID tags, placed at known locations, are used for the estimation of the locations of the target tags, by properly manipulating the measured backscattered power. The proposed method does not depend on the location of the reader, but only on the locations of the reference tags. Hence, positioning-errors related to SLAM are not accumulated. Mobility of the robot ensures rich collection of measurements. We propose a method for dynamic, real-time configuration of the parameters of the fingerprinting algorithm and real-time evaluation of the localization error of the unknown tags. This is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:xm1hsP5ya-EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "D2. 4 Mining models for SE-related associations",
            "Publication year": 2015,
            "Publication url": "http://s-case.github.io/publications/eis2017/S-CASE_D2.4.pdf",
            "Abstract": "The objectives of WP2 include the design and development of a Model-Driven Engineering mechanism as well as the research and development of mining techniques for software artefacts. As part of task T2. 4, this deliverable comprises novel methodologies for mining models for Software Engineering related associations.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:LNjCCq68lIgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Performance Evaluation of Agents and Multi-agent Systems Using Formal Specifications in Z Notation",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-20230-3_6",
            "Abstract": "Despite the plethora of frameworks and tools for developing agent systems, there is a remarkable lack of generalised methodologies for assessing their performance. Existing methods in the field of software engineering do not adequately address the unpredictable and complex nature of intelligent agents. We introduce a generic methodology for evaluating agent performance; the Agent Performance Evaluation (APE) methodology consists of representation tools, guidelines and techniques for organizing, categorizing and using metrics, measurements and aggregated characterizations of agent performance. The core of APE is the Metrics Representation Tree, a generic structure that enables efficient manipulation of evaluation-specific information. This paper provides a formal specification of the proposed methodology in Z notation and demonstrates how to apply it on an existing multi-agent system.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "An integrated infrastructure for monitoring and evaluating agent-based systems",
            "Publication year": 2009,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0957417408006738",
            "Abstract": "Driven by the urging need to thoroughly identify and accentuate the merits of agent technology, we present in this paper, MEANDER, an integrated framework for evaluating the performance of agent-based systems. The proposed framework is based on the Agent Performance Evaluation (APE) methodology, which provides guidelines and representation tools for performance metrics, measurement collection and aggregation of measurements. MEANDER comprises a series of integrated software components that implement and automate various parts of the methodology and assist evaluators in their tasks. The main objective of MEANDER is to integrate performance evaluation processes into the entire development lifecycle, while clearly separating any evaluation-specific code from the application code at hand. In this paper, we describe in detail the architecture and functionality of the MEANDER components and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:L8Ckcad2t8MC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Eikonomia-an integrated semantically aware tool for description and retrieval of byzantine art information",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4410392/",
            "Abstract": "Semantic annotation and querying is currently applied on a number of versatile disciplines, providing the added-value of such an approach and, consequently the need for more elaborate - either case-specific or generic - tools. In this context, we have developed Eikonomia: an integrated semantically-aware tool for the description and retrieval of Byzantine artwork Information. Following the needs of the ORMYLIA Art Diagnosis Center for adding semantics to their legacy data, an ontology describing Byzantine artwork based on CIDOC-CRM, along with the interfaces for synchronization to and from the existing RDBMS have been implemented. This ontology has been linked to a reasoning tool, while a dynamic interface for the automated creation of semantic queries in SPARQL was developed. Finally, all the appropriate interfaces were instantiated, in order to allow easy ontology manipulation, query results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Designing Pricing Mechanisms for Autonomous Agents Based on Bid\u2010Forecasting",
            "Publication year": 2005,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/10196780500035340",
            "Abstract": "Autonomous agents that participate in the electronic market environment introduce an advanced paradigm for realizing automated deliberations over offered prices of auctioned goods. These agents represent humans and their assets, therefore it is critical for them not only to act rationally but also efficiently. By enabling agents to deploy bidding strategies and to compete with each other in a marketplace, a valuable amount of historical data is produced. An optimal dynamic forecasting of the maximum offered bid would enable more gainful behaviours by agents. In this respect, this paper presents a methodology that takes advantage of price offers generated in e\u2010auctions, in order to provide an adequate short\u2010term forecasting schema based on time\u2010series analysis. The forecast is incorporated into the reasoning mechanism of a group of autonomous e\u2010auction agents to improve their bidding behaviour. In order to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:Y0pCki6q_DkC",
            "Publisher": "Taylor & Francis Group"
        },
        {
            "Title": "Assessing the user-perceived quality of source code components using static analysis metrics",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-93641-3_1",
            "Abstract": "Nowadays, developers tend to adopt a component-based software engineering approach, reusing own implementations and/or resorting to third-party source code. This practice is in principle cost-effective, however it may also lead to low quality software products, if the components to be reused exhibit low quality. Thus, several approaches have been developed to measure the quality of software components. Most of them, however, rely on the aid of experts for defining target quality scores and deriving metric thresholds, leading to results that are context-dependent and subjective. In this work, we build a mechanism that employs static analysis metrics extracted from GitHub projects and defines a target quality score based on repositories\u2019 stars and forks, which indicate their adoption/acceptance by developers. Upon removing outliers with a one-class classifier, we employ Principal Feature Analysis and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:nj26e0utjpAC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "An agent framework for dynamic agent retraining: Agent academy",
            "Publication year": 2004,
            "Publication url": "https://arxiv.org/abs/cs/0407025",
            "Abstract": "Agent Academy (AA) aims to develop a multi-agent society that can train new agents for specific or general tasks, while constantly retraining existing agents in a recursive mode. The system is based on collecting information both from the environment and the behaviors of the acting agents and their related successes/failures to generate a body of data, stored in the Agent Use Repository, which is mined by the Data Miner module, in order to generate useful knowledge about the application domain. Knowledge extracted by the Data Miner is used by the Agent Training Module as to train new agents or to enhance the behavior of agents already running. In this paper the Agent Academy framework is introduced, and its overall architecture and functionality are presented. Training issues as well as agent ontologies are discussed. Finally, a scenario, which aims to provide environmental alerts to both individuals and public authorities, is described an AA-based use case.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:qjMakFHDy7sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "QATCH-An adaptive framework for software product quality assessment",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0957417417303883",
            "Abstract": "The subjectivity that underlies the notion of quality does not allow the design and development of a universally accepted mechanism for software quality assessment. This is why contemporary research is now focused on seeking mechanisms able to produce software quality models that can be easily adjusted to custom user needs. In this context, we introduce QATCH, an integrated framework that applies static analysis to benchmark repositories in order to generate software quality models tailored to stakeholder specifications. Fuzzy multi-criteria decision-making is employed in order to model the uncertainty imposed by experts\u2019 judgments. These judgments can be expressed into linguistic values, which makes the process more intuitive. Furthermore, a robust software quality model, the base model, is generated by the system, which is used in the experiments for QATCH system verification. The paper provides an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:yNlG6JgpFqoC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "An Adaptive Proportional Value-per-Click Agent for Bidding in Ad Auctions",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-34889-1_2",
            "Abstract": "Sponsored search auctions constitutes the most important source of revenue for search engine companies, offering new opportunities for advertisers. The Trading Agent Competition (TAC) Ad Auctions tournament is one of the first attempts to study the competition among advertisers for their placement in sponsored positions along with organic search engine results. In this paper, we describe agent Mertacor, a simulation-based game theoretic agent coupled with on-line learning techniques to optimize its behavior that successfully competed in the 2010 tournament. In addition, we evaluate different facets of our agent to draw conclusions about certain aspects of its strategy.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:isC4tDSrTZIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Codecatch: extracting source code snippets from online sources",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3194104.3194107",
            "Abstract": "Nowadays, developers rely on online sources to find example snippets that address the programming problems they are trying to solve. However, contemporary API usage mining methods are not suitable for locating easily reusable snippets, as they provide usage examples for specific APIs, thus requiring the developer to know which library to use beforehand. On the other hand, the approaches that retrieve snippets from online sources usually output a list of examples, without aiding the developer to distinguish among different implementations and without offering any insight on the quality and the reusability of the proposed snippets. In this work, we present CodeCatch, a system that receives queries in natural language and extracts snippets from multiple online sources. The snippets are assessed both for their quality and for their usefulness/preference by the developers, while they are also clustered according to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:XUAslYVNQLQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Assessing the Reusability of Source Code Components",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=7939034254814894080&hl=en&oi=scholarr",
            "Abstract": "In the context of reusing components from online repositories, assessing the quality and specifically the reusability of source code before reusing it poses a major challenge for the research community. Although several quality assessment systems have been proposed, most of them do not focus on reusability. In this chapter, we design a reusability score using as ground truth information from GitHub stars and forks, which indicate the extent to which software components are adopted/preferred by developers. Our methodology includes applying different machine learning algorithms in order to produce reusability estimation models at both class and package levels. Finally, evaluating our methodology indicates that it can be effective for assessing reusability as perceived by developers.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:zGWyAL6qfKUC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Designing robust strategies for continuous trading in contemporary power markets",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-40864-9_3",
            "Abstract": "In contemporary energy markets participants interact with each other via brokers that are responsible for the proper energy flow to and from their clients (usually in the form of long-term or short-term contracts). Power TAC is a realistic simulation of a real-life energy market, aiming towards providing a better understanding and modeling of modern energy markets, while boosting research on innovative trading strategies. Power TAC models brokers as software agents, competing against each other in Double Auction environments, in order to increase their client base and market share. Current work discusses such a broker agent architecture, striving to maximize his own profit. Within the context of our analysis, Double Auction markets are treated as microeconomic systems and, based on state-of-the-art price formation strategies, the following policies are designed: an adaptive price formation policy, a policy for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:iH-uZ7U-co4C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "A Data-driven Methodology towards Interpreting Readability against Software Properties.",
            "Publication year": 2020,
            "Publication url": "https://www.researchgate.net/profile/Michail-Papamichail/publication/342964896_A_Data-driven_Methodology_towards_Interpreting_Readability_against_Software_Properties/links/5fa2d587299bf10f73229bdd/A-Data-driven-Methodology-towards-Interpreting-Readability-against-Software-Properties.pdf",
            "Abstract": "In the context of collaborative, agile software development, where effective and efficient software maintenance is of utmost importance, the need to produce readable source code is evident. Towards this direction, several approaches aspire to assess the extent to which a software component is readable. Most of them rely on experts who are responsible for determining the ground truth and/or set custom evaluation criteria, leading to results that are context-dependent and subjective. In this work, we employ a large set of static analysis metrics along with various coding violations towards interpreting readability as perceived by developers. In an effort to provide a fully automated and extendible methodology, we refrain from using experts; rather we harness data residing in online code hosting facilities towards constructing a dataset that includes more than one million methods that cover diverse development scenarios. After performing clustering based on source code size, we employ Support Vector Regression in order to interpret the extent to which a software component is readable on three axes: complexity, coupling, and documentation. Preliminary evaluation on several axes indicates that our approach effectively interprets readability as perceived by developers against the aforementioned three primary source code properties.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:pbQCAAeKvsAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Versatile internet of things for agriculture: an eXplainable AI approach",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-49186-4_16",
            "Abstract": "The increase of the adoption of IoT devices and the contemporary problem of food production have given rise to numerous applications of IoT in agriculture. These applications typically comprise a set of sensors that are installed in open fields and measure metrics, such as temperature or humidity, which are used for irrigation control systems. Though useful, most contemporary systems have high installation and maintenance costs, and they do not offer automated control or, if they do, they are usually not interpretable, and thus cannot be trusted for such critical applications. In this work, we design Vital, a system that incorporates a set of low-cost sensors, a robust data store, and most importantly an explainable AI decision support system. Our system outputs a fuzzy rule-base, which is interpretable and allows fully automating the irrigation of the fields. Upon evaluating Vital in two pilot cases, we conclude that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:OlbiQ0ttILcC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Sketching a methodology for efficient Supply Chain Management agents enhanced through Data Mining",
            "Publication year": 2008,
            "Publication url": "https://www.inderscienceonline.com/doi/abs/10.1504/IJIIDS.2008.017244",
            "Abstract": "Supply Chain Management (SCM) environments demand intelligent solutions, which can perceive variations and achieve maximum revenue. This highlights the importance of a commonly accepted design methodology, since most current implementations are application-specific. In this work, we present a methodology for building an intelligent trading agent and evaluating its performance at the Trading Agent Competition (TAC) SCM game. We justify architectural choices made, ranging from the adoption of specific Data Mining (DM) techniques, to the selection of the appropriate metrics for agent performance evaluation. Results indicate that our agent has proven capable of providing advanced SCM solutions in demanding SCM environments.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:roLk4NBRz8UC",
            "Publisher": "Inderscience Publishers"
        },
        {
            "Title": "Redefining the market power of small-scale electricity consumers through consumer social networks",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6686237/",
            "Abstract": "Energy markets have undergone important changes at the conceptual level over the last years. Decentralized supply, small-scale production and smart grid optimization and control are the new building blocks. These changes offer substantial opportunities for all energy market stakeholders, some of which however, remain largely unexploited. Small-scale consumers, as a whole, account for significant amount of energy in current markets (up to 40%), as individuals though their consumption is trivial, and their market power practically non-existent. Thus, it is necessary to assist small-scale energy market stakeholders combine their market power. Within the context of this work we propose Consumer Social Networks (CSNs) as a means for achieve the objective. We present a simulation environment for the creation of CSNs and provide a proof of concept on how CSNs can be formulated based on various criteria. Each \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:ZHo1McVdvXMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Relieving Robots from Their Burdens: The Cloud Agent Concept (Short Paper)",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7776599/",
            "Abstract": "The consumer robotics concept has already invaded our everyday lives, however two major drawbacks have become apparent both for the roboticists and the consumers. The first is that these robots are pre-programmed to perform specific tasks and usually their software is proprietary, thus not open to \"interventions\". The second is that even if their software is open source, low-cost robots usually lack sufficient resources such as CPU power or memory capabilities, thus forbidding advanced algorithms to be executed in-robot. Within the context of RAPP (Robotic Applications for Delivering Smart User Empowering Applications) we treat robots as platforms, where applications can be downloaded and automatically deployed. Furthermore, we propose and implement a novel multi-agent architecture, empowering robots to offload computations in entities denoted as Cloud Agents. This paper discusses the respective \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:puFLaqDw8dcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "pygrank: A Python Package for Graph Node Ranking",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2110.09274",
            "Abstract": "We introduce pygrank, an open source Python package to define, run and evaluate node ranking algorithms. We provide object-oriented and extensively unit-tested algorithm components, such as graph filters, post-processors, measures, benchmarks and online tuning. Computations can be delegated to numpy, tensorflow or pytorch backends and fit in back-propagation pipelines. Classes can be combined to define interoperable complex algorithms. Within the context of this paper we compare the package with related alternatives and demonstrate its flexibility and ease of use with code examples.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:rCzfLUpcSPoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An automatic speech detection architecture for social robot oral interaction",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2814895.2814931",
            "Abstract": "Social robotics have become a trend in contemporary robotics research, since they can be successfully used in a wide range of applications. One of the most fundamental communication skills a robot must have is the oral interaction with a human, in order to provide feedback or accept commands. And, although text-to-speech is an almost solved problem, this isn't the case for speech detection, since it includes a large number of different conditions, many of which are literally unpredictable. There are quite a few well established ASR (Automatic Speech Recognition) tools, however without providing efficient results, especially in less popular languages. The current paper investigates different speech detection strategies via the utilization of the Sphinx-4 open-source library. The first is a way to incorporate languages for which no acoustic or language model exists (Greek in our case), following the grapheme-to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:Kv9jytqXTosC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A robust agent design for dynamic SCM environments",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/11752912_15",
            "Abstract": "The leap from decision support to autonomous systems has often raised a number of issues, namely system safety, soundness and security. Depending on the field of application, these issues can either be easily overcome or even hinder progress. In the case of Supply Chain Management (SCM), where system performance implies loss or profit, these issues are of high importance. SCM environments are often dynamic markets providing incomplete information, therefore demanding intelligent solutions which can adhere to environment rules, perceive variations, and act in order to achieve maximum revenue. Advancing on the way such autonomous solutions deal with the SCM process, we have built a robust, highly-adaptable and easily-configurable mechanism for efficiently dealing with all SCM facets, from material procurement and inventory management to goods production and shipment. Our agent \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:9yKSN-GCB0IC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Exploiting Data Mining on Mas",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/0-387-25757-8_4.pdf",
            "Abstract": "The synergy of two rather diverse technologies, such as DM and AT, has been attempted at many levels. A great number of researchers with varied scientific backgrounds have been involved in the development of systems that use agents to automate data collection while satisfying the imperative need for the interpretation and exploitation of massive data volumes. In fact, each research community defines the basic concepts of AT and DM technologies in a way that increases comprehension and familiarity among its members. This the reason for having summarized the basic DM and AT concepts through the prism of the current work. As already mentioned in Chapter 1, the main objective of this book is the presentation of a unified methodology for building MAS that have the ability of dynamically incorporating DM-extracted knowledge. Two are the main issues that hinder the coupling of Data Mining with Intelligent \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:70eg2SAEIzsC",
            "Publisher": "Springer US"
        },
        {
            "Title": "npm Packages as Ingredients: A Recipe-based Approach.",
            "Publication year": 2019,
            "Publication url": "https://www.scitepress.org/Papers/2019/79668/79668.pdf",
            "Abstract": "The sharing and growth of open source software packages in the npm JavaScript (JS) ecosystem has been exponential, not only in numbers but also in terms of interconnectivity, to the extend that often the size of dependencies has become more than the size of the written code. This reuse-oriented paradigm, often attributed to the lack of a standard library in node and/or in the micropackaging culture of the ecosystem, yields interesting insights on the way developers build their packages. In this work we view the dependency network of the npm ecosystem from a \u201cculinary\u201d perspective. We assume that dependencies are the ingredients in a recipe, which corresponds to the produced software package. We employ network analysis and information retrieval techniques in order to capture the dependencies that tend to co-occur in the development of npm packages and identify the communities that have been evolved as the main drivers for npm\u2019s exponential growth.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:vkz5F8TaVKkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Message from the Workshop Chairs",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9060373/",
            "Abstract": "Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:mo9XK3BEATIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Quality control of national genetic evaluation results using data-mining techniques; a progress report",
            "Publication year": 2003,
            "Publication url": "https://journal.interbull.org/index.php/ib/article/view/790/781",
            "Abstract": "BackgroundData quality constitutes one of the most critical issues in genetic evaluations both at national and international level. International genetic evaluations computed by Interbull are based on the analysis of national genetic evaluation results. Therefore, the validity of international comparisons depends on the quality of the output of the various national genetic evaluation systems. The current method for data quality assurance is mainly determined by the consistency of consecutive evaluation results and is based on thorough statistical examination (Klei et al., 2002). In a separate project, national genetic evaluation programs are being tested on simulated datasets with known properties (T\u00e4ubert et al., 2002).Data-mining (DM) provides a different perspective on data quality control. DM is an algorithm-based, data-driven approach in the knowledge discovery process. It is defined as the extraction of interesting \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:WF5omc3nYNoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Phase ReLock-localization of RFID tags by a moving robot",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8739423/",
            "Abstract": "In this work, we propose a prototype method for the localization of RFID tags, by deploying RFID equipment on a robotic platform. The constructed robot is capable to perform Simultaneous Localization (of its own position) and Mapping of the environment and then locate the RFID tags around its path. The proposed method is based on properly treating the measured phase of the backscattered signal by each tag at the reader's antenna, located on top of the robot. More specifically, the measured phase samples are reconstructed, such that the 2\u03c0 discontinuities are eliminated. This allows for the formation of an optimization problem, which can be solved rapidly by standard methods. The proposed method is experimentally compared against the most accurate reported method in prior-art and the same accuracy is preserved. However, the problem is solved more than one order of magnitude faster, allowing for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:UY3hNwcQ290C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A mechanism for automatically summarizing software functionality from source code",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8854713/",
            "Abstract": "When developers search online to find software components to reuse, they usually first need to understand the container projects/libraries, and subsequently identify the required functionality. Several approaches identify and summarize the offerings of projects from their source code, however they often require that the developer has knowledge of the underlying topic modeling techniques; they do not provide a mechanism for tuning the number of topics, and they offer no control over the top terms for each topic. In this work, we use a vectorizer to extract information from variable/method names and comments, and apply Latent Dirichlet Allocation to cluster the source code files of a project into different semantic topics. The number of topics is optimized based on their purity with respect to project packages, while topic categories are constructed to provide further intuition and Stack Exchange tags are used to express \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:mS4qin7VKjkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Localizing Software Bugs using the Edit Distance of Call Traces",
            "Publication year": 2014,
            "Publication url": "https://issel.ee.auth.gr/wp-content/uploads/2017/01/Localizing-Software-Bugs-using-the-Edit-Distance-of-Call-Traces.pdf",
            "Abstract": "Automating the localization of software bugs that do not lead to crashes is a difficult task that has drawn the attention of several researchers. Several popular methods follow the same approach; function call traces are collected and represented as graphs, which are subsequently mined using subgraph mining algorithms in order to provide a ranking of potentially buggy functions-nodes. Recent work has indicated that the scalability of state-of-the-art methods can be improved by reducing the graph dataset using tree edit distance algorithms. The call traces that are closer to each other, but belong to different sets, are the ones that are most significant in localizing bugs. In this work, we further explore the task of selecting the most significant traces, by proposing different call trace selection techniques, based on the Stable Marriage problem, and testing their effectiveness against current solutions. Upon evaluating our methods on a real-world dataset, we prove that our methodology is scalable and effective enough to be applied on dynamic bug detection scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:p-HGrieyzrAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Variable structure robot control systems: The RAPP approach",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0921889016306248",
            "Abstract": "This paper presents a method of designing variable structure control systems for robots. As the on-board robot computational resources are limited, but in some cases the demands imposed on the robot by the user are virtually limitless, the solution is to produce a variable structure system. The task dependent part has to be exchanged, however the task governs the activities of the robot. Thus not only exchange of some task-dependent modules is required, but also supervisory responsibilities have to be switched. Such control systems are necessary in the case of robot companions, where the owner of the robot may demand from it to provide many services.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:43B52WW2E64C",
            "Publisher": "North-Holland"
        },
        {
            "Title": "A methodology for predicting agent behavior by the use of data mining techniques",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11492870_13",
            "Abstract": "One of the most interesting issues in agent technology has always been the modeling and enhancement of agent behavior. Numerous approaches exist, attempting to optimally reflect both the inner states, as well as the perceived environment of an agent, in order to provide it either with reactivity or proactivity. Within the context of this paper, an alternative methodology for enhancing agent behavior is presented. The core feature of this methodology is that it exploits knowledge extracted by the use of data mining techniques on historical data, data that describe the actions of agents within the MAS they reside. The main issues related to the design, development, and evaluation of such a methodology for predicting agent actions are discussed, while the basic concessions made to enable agent cooperation are outlined. We also present \u03ba-Profile, a new data mining mechanism for discovering action profiles \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:Se3iqnhoufwC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Evaluating knowledge intensive multi-agent systems",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-72839-9_6",
            "Abstract": "As modern applications tend to stretch between large, ever-growing datasets and increasing demand for meaningful content at the user end, more elaborate and sophisticated knowledge extraction technologies are needed. Towards this direction, the inherently contradicting technologies of deductive software agents and inductive data mining have been integrated, in order to address knowledge intensive problems. However, there exists no generalized evaluation methodology for assessing the efficiency of such applications. On the one hand, existing data mining evaluation methods focus only on algorithmic precision, ignoring overall system performance issues. On the other hand, existing systems evaluation techniques are insufficient, as the emergent intelligent behavior of agents introduce unpredictable factors of performance. In this paper, we present a generalized methodology for performance \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:3fE2CSJIrl8C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Information agents cooperating with heterogenous data sources for customer-order management",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/967900.967915",
            "Abstract": "As multi-agent systems and information agents obtain an increasing acceptance by application developers, existing legacy Enterprise Resource Planning (ERP) systems still provide the main source of data used in customer, supplier and inventory resource management. In this paper we present a multi-agent system, comprised of information agents, which cooperates with a legacy ERP in order to carry out orders posted by customers in an enterprise environment. Our system is enriched by the capability of producing recommendations to the interested customer through agent cooperation. At first, we address the problem of information workload in an enterprise environment and explore the opportunity of a plausible solution. Secondly we present the architecture of our system and the types of agents involved in it. Finally, we show how it manipulates retrieved information for efficient and facile customer-order \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:W7OEmFMy1HYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Policy search through adaptive function approximation for bidding in tac scm",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-40864-9_2",
            "Abstract": "Agent autonomy is strongly related to learning and adaptation. Machine learning models generated through the use of historical data or current environmental signals, provide agents with the necessary decision-making and generalization capabilities in competitive, dynamic, partially observable and stochastic environments. In this work, we discuss learning and adaptation in the context of the TAC SCM game. We apply a variety of machine learning and computational intelligence methods for generating the most efficient sales component of the agent, dealing with customer orders and production throughput. Along with utility maximization and bid acceptance probability estimation methods, we evaluate regression trees, particle swarm optimization, heuristic control and policy search via adaptive function approximation in order to build an efficient, near-real time, bidding mechanism. Results indicate that a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:bEWYMUwI8FkC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Dealing with Trust and Reputation in Unreliable Multi-agent Trading Environments",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-34889-1_7",
            "Abstract": "In shared competitive environments, where information comes from various sources, agents may interact with each other in a competitive manner in order to achieve their individual goals. Numerous research efforts exist, attempting to define protocols, rules and interfaces for agents to abide by and ensure trustworthy exchange of information. Auction environments and e-commerce platforms are such paradigms, where trust and reputation are vital factors determining agent strategy. And though the process is always secured with a number of safeguards, there is always the issue of unreliability. In this context, the Agent Reputation and Trust (ART) testbed has provided researchers with the ability to test different trust and reputation strategies, in various types of trust/reputation environments. Current work attempts to identify the most viable trust and reputation models stated in the literature, while it further \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:NMxIlDl6LWMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Improving agent bidding in power stock markets through a data mining enhanced agent platform",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-03603-3_9",
            "Abstract": "Like in any other auctioning environment, entities participating in Power Stock Markets have to compete against other in order to maximize own revenue. Towards the satisfaction of their goal, these entities (agents - human or software ones) may adopt different types of strategies - from na?ve to extremely complex ones - in order to identify the most profitable goods compilation, the appropriate price to buy or sell etc, always under time pressure and auction environment constraints. Decisions become even more difficult to make in case one takes the vast volumes of historical data available into account: goods\u2019 prices, market fluctuations, bidding habits and buying opportunities. Within the context of this paper we present Cassandra, a multi-agent platform that exploits data mining, in order to extract efficient models for predicting Power Settlement prices and Power Load values in typical Day-ahead Power \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:4JMBOYKVnBMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Constructing optimal fuzzy metric trees for agent performance evaluation",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4740645/",
            "Abstract": "The field of multi-agent systems has reached a significant degree of maturity with respect to frameworks, standards and infrastructures. Focus is now shifted to performance evaluation of real-world applications, in order to quantify the practical benefits and drawbacks of agent systems. Our approach extends current work on generic evaluation methodologies for agents by employing fuzzy weighted trees for organizing evaluation-specific concepts/metrics and linguistic terms to intuitively represent and aggregate measurement information.Furthermore, we introduce meta-metrics that measure the validity and complexity of the contribution of each metric in the overall performance evaluation. These are all incorporated for selecting optimal subsets of metrics and designing the evaluation process incompliance with the demands/restrictions of various evaluation setups, thus minimizing intervention by domain experts. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:ZeXyd9-uunAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Mining Source Code for Snippet Reuse",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=6614601494247874263&hl=en&oi=scholarr",
            "Abstract": "As developers rely more and more on reusing components from online sources, an important challenge is that of finding snippets in order to integrate these components and/or to address common programming problems. Thus, several snippet mining systems have been developed, which however have important limitations. API usage mining systems require the developer to know which library to use beforehand, while more generic snippet mining systems usually output a list of examples, without distinguishing among different implementations and without assessing the quality and the reusability of the proposed snippets. In this chapter, we present CodeCatch, a system that receives queries in natural language and assesses the retrieved snippets both for their quality and for their preference by the developers. Furthermore, our system clusters the snippets according to their API calls, thus allowing the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:h-xndbdg2koC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Robotic Applications Towards an Interactive Alerting System for Medical Purposes",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8104258/",
            "Abstract": "Social consumer robots are slowly but strongly invading our everyday lives as their prices are becoming lower and lower, constituting them affordable for a wide range of civilians. There has been a lot of research concerning the potential applications of social robots, some of which may implement companionship or proxying technology-related tasks and assisting in everyday household endeavors, among others. In the current work, the RAPP framework is being used towards easily creating robotic applications suitable for utilization as a socially interactive alerting system with the employment of the NAO robot. The developed application stores events in an on-line calendar, directly via the robot or indirectly via a web environment, and asynchronously informs an end-user of imminent events.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:MnogvFdIBdwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multilevel Readability Interpretation Against Software Properties: A Data-Centric Approach",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-83007-6_10",
            "Abstract": "Given the wide adoption of the agile software development paradigm, where efficient collaboration as well as effective maintenance are of utmost importance, the need to produce readable source code is evident. To that end, several research efforts aspire to assess the extent to which a software component is readable. Several metrics and evaluation criteria have been proposed; however, they are mostly empirical or rely on experts who are responsible for determining the ground truth and/or set custom thresholds, leading to results that are context-dependent and subjective. In this work, we employ a large set of static analysis metrics along with various coding violations towards interpreting readability as perceived by developers. Unlike already existing approaches, we refrain from using experts and we provide a fully automated and extendible methodology built upon data residing in online code hosting \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:lOG7zRu2uA8C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Autonomous Robots, Drones and Repeaters for Fast, Reliable, Low-Cost RFID Inventorying & Localization",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9566425/",
            "Abstract": "This paper presents our latest results of our prototype robots and drones, aiming continuous inventorying and accurate real-time 3D localization of RFID tagged items. We have designed and constructed two ground robots, capable of autonomous inventorying in unknown regions, exploiting state-of-the-art methods from the field of robotics and RF-localization. Furthermore, we present our prototype drone, also capable of 3D inventorying and localization. In addition, we demonstrate the performance of our prototype RFID repeater, which also boosts the read-range of traditional RFID technology. The results of measurements campaigns conducted in different environments for the above prototypes are presented herein.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:Q_E8KsG3g9MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Creating an extrovert robotic assistant via IoT networking devices",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1804.04361",
            "Abstract": "The communication and collaboration of Cyber-Physical Systems, including machines and robots, among themselves and with humans, is expected to attract researchers' interest for the years to come. A key element of the new revolution is the Internet of Things (IoT). IoT infrastructures enable communication between different connected devices using internet protocols. The integration of robots in an IoT platform can improve robot capabilities by providing access to other devices and resources. In this paper we present an IoT-enabled application including a NAO robot which can communicate through an IoT platform with a reflex measurement system and a hardware node that provides robotics-oriented services in the form of RESTful web services. An activity reminder application is also included, illustrating the extension capabilities of the system.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:jMZTt8odoasC",
            "Publisher": "Unknown"
        },
        {
            "Title": "CERTH at MediaEval Placing Task 2013.",
            "Publication year": 2013,
            "Publication url": "https://www.researchgate.net/profile/Eleftherios-Spyromitros-Xioufis/publication/260449946_CERTH_at_MediaEval_placing_task_2013/links/0a85e531597d265ba1000000/CERTH-at-MediaEval-placing-task-2013.pdf",
            "Abstract": "We describe the participation of the CERTH team in the Placing task of MediaEval 2013. We submitted 5 runs on the full test set, two of which are based on tag information, two on visual content, and one uses both tag and visual information. Our best performance (median error 650km) was achieved with the use of tag features.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:pqnbT2bcN3wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards an MDA Mechanism for RESTful Services Development.",
            "Publication year": 2015,
            "Publication url": "http://ceur-ws.org/Vol-1563/paper6.pdf",
            "Abstract": "Automated software engineering research aspires to lead to more consistent software, faster delivery and lower production costs. Meanwhile, RESTful design is rapidly gaining momentum towards becoming the primal software engineering paradigm for the web, due to its simplicity and reusability. This paper attempts to couple the two perspectives and take the first step towards applying the MDE paradigm to RESTful service development at the PIM zone. A UML profile is introduced, which performs PIM meta-modeling of RESTful web services abiding by the third level of Richardson\u2019s maturity model. The profile embeds a slight variation of the MVC design pattern to capture the core REST qualities of a resource. The proposed profile is followed by an indicative example that demonstrates how to apply the concepts presented, in order to automate PIM production of a system according to MOF stack. Next steps include the introduction of the corresponding CIM, PSM and code production.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:i8eIfGGcn98C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Expanding a robot's life: Low power object recognition via FPGA-based DCNN deployment",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8376612/",
            "Abstract": "FPGAs are commonly used to accelerate domain-specific algorithmic implementations, as they can achieve impressive performance boosts, are reprogrammable and exhibit minimal power consumption. In this work, the SqueezeNet DCNN is accelerated using an SoC FPGA in order for the offered object recognition resource to be employed in a robotic application. Experiments are conducted to investigate the performance and power consumption of the implementation in comparison to deployment on other widely-used computational systems.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:HoDPlbN_d1QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A generic methodology for early identification of non-maintainable source code components through analysis of software releases",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0950584919302289",
            "Abstract": "Contemporary development approaches consider that time-to-market is of utmost importance and assume that software projects are constantly evolving, driven by the continuously changing requirements of end-users. This practically requires an iterative process where software is changing by introducing new or updating existing software/user features, while at the same time continuing to support the stable ones. In order to ensure efficient software evolution, the need to produce maintainable software is evident.In this work, we argue that non-maintainable software is not the outcome of a single change, but the consequence of a series of changes throughout the development lifecycle. To that end, we define a maintainability evaluation methodology across releases and employ various information residing in software repositories, so as to decide on the maintainability of software.Upon using \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:beqBT5984LEC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Data Mining on the Application Level of a Mas",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/0-387-25757-8_6.pdf",
            "Abstract": "In the first case of dynamic knowledge diffusion to MAS, knowledge is extracted by the application of DM techniques on historical application data. In such systems, an adequately large dataset is required (the bigger the better!), in order to produce a valid knowledge model, which reflects the trends in the data. A wide range of application domains may produce and maintain such large data volumes, including environmental systems, transaction systems, data warehouses, web logs etc. Special attention should be drawn, however, to Enterprise Resource Planning (ERP) systems, which are employed by all kinds of companies and for diverse purposes. Managing enterprise resources is an inherently distributed problem, that requires intelligent solutions. In addition, ERP systems generate and maintain large data volumes and DM techniques have already been exploited for improving several of the processes that are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:35N4QoGY0k4C",
            "Publisher": "Springer US"
        },
        {
            "Title": "Dp-core: A design pattern detection tool for code reuse",
            "Publication year": 2016,
            "Publication url": "https://www.scitepress.org/Papers/2016/62233/62233.pdf",
            "Abstract": "In order to maintain, extend or reuse software projects one has to primarily understand what a system does and how well it does it. And, while in some cases information on system functionality exists, information covering the non-functional aspects is usually unavailable. Thus, one has to infer such knowledge by extracting design patterns directly from the source code. Several tools have been developed to identify design patterns, however most of them are limited to compilable and in most cases executable code, they rely on complex representations, and do not offer the developer any control over the detected patterns. In this paper we present DP-CORE, a design pattern detection tool that defines a highly descriptive representation to detect known and define custom patterns. DP-CORE is flexible, identifying exact and approximate pattern versions even in non-compilable code. Our analysis indicates that DP-CORE provides an efficient alternative to existing design pattern detection tools.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:oE_QS-WwsdAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ERGASIOGNOMON-a Model System of Advanced Digital Services Designed and Developed to Support the Job Marketplace",
            "Publication year": 2003,
            "Publication url": "https://issel.ee.auth.gr/wp-content/uploads/2016/02/Ergasiognomon-A-Model-System-of-Advanced-Digital-Services-Designed-and-Developed-to-Support-the-Job-Marketplace.pdf",
            "Abstract": "The continuous expansion of Internet has enabled the development of a wide range of advanced digital services. Real-time data diffusion has eliminated processing bottlenecks and has led to fast, easy and no-cost communications. This primitive has been widely exploited in the process of job searching. Numerous systems have been developed offering job candidates with the opportunity to browse for vacancies, submit resumes, and even contact the most appealing of the employers. Although effective, most of these systems are characterized by their simplicity, acting more like an enhanced bulletin board, rather than an integrated, fully functional system. Even for the more advanced of these systems user interaction is obligatory, in order to couple job seekers with job providers, thus continuous supervising of the process is unavoidable. Advancing on the way primitive job recruitment techniques apply on Internet-based systems, and dealing with their lack of efficiency and interactivity, we have developed a robust software system that employs intelligent techniques for coupling candidates and jobs, according to the formers\u2019 skills and the latter\u2019s requirements. A thorough analysis of the system specifications has been conducted, and all issues concerning information retrieval and data filtering, coupling intelligence, storage, security, user interaction and ease-of-use have been integrated into one web-based job portal.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:Zph67rFs4hoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards interpretable defect-prone component analysis using genetic fuzzy systems",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7168329/",
            "Abstract": "The problem of Software Reliability Prediction is attracting the attention of several researchers during the last few years. Various classification techniques are proposed in current literature which involve the use of metrics drawn from version control systems in order to classify software components as defect-prone or defect-free. In this paper, we create a novel genetic fuzzy rule-based system to efficiently model the defect-proneness of each component. The system uses a Mamdani-Assilian inference engine and models the problem as a one-class classification task. System rules are constructed using a genetic algorithm, where each chromosome represents a rule base (Pittsburgh approach). The parameters of our fuzzy system and the operators of the genetic algorithm are designed with regard to producing interpretable output. Thus, the output offers not only effective classification, but also a comprehensive set of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:WGv8Og3F3KgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Security and privacy for smart meters: a data-driven mapping study",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8905611/",
            "Abstract": "Smart metering systems have been gaining popularity as a vital part of the general smart grid paradigm. Naturally, as new technologies arise to cover this emerging field, so do security and privacy related issues regarding the energy consumer's personal data. These challenges impose the need for the development of new methods through a better understanding of the state-of-the-art. This paper aims at identifying the main categories of security and privacy techniques utilized in smart metering systems from a three-point perspective: i) a field research survey, ii) EU initiatives and findings towards the same direction and iii) a data-driven analysis of the state-of-the-art and the identification of its main topics (or themes) using topic modeling techniques. Detailed quantitative results of this analysis, such as semantic interpretation of the identified topics and a graph representation of the topic trends over time, are presented.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:yaBp1wUtcLsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "D2. 1.1 A transformation mechanism of 3 rd party web services",
            "Publication year": 2014,
            "Publication url": "https://scholar.google.com/scholar?cluster=7878895944978507854&hl=en&oi=scholarr",
            "Abstract": "The deliverable presents the Web Service Annotation Tool (WSAT) for importing 3rd party web services into the S-CASE realm. The tool imports REST and WS-*(SOAP) services through the use of their descriptor files in WADL and WSDL formats, respectively. Having imported the web service, the user can annotate the most significant parts using natural language and keyword tagging. WSAT provides a full-fledged REST API for other S-CASE or 3rd party clients to access its repository and a web client for 3rd party service providers to manage and annotate their services. Additionally, it allows external users to query and retrieve related services.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:vkuYBMKU6wEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A multi-agent simulation framework for spiders traversing the semantic web",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4061462/",
            "Abstract": "Although search engines traditionally use spiders for traversing and indexing the Web, there has not yet been any methodological attempt to model, deploy and test learning spiders. The flourishing of the semantic Web provides understandable information that may improve the accuracy of search engines. In this paper, we introduce BioSpider, an agent-based simulation framework for developing and testing autonomous, intelligent, semantically-focused Web spiders. BioSpider assumes a direct analogy of the problem at hand with a multi-variate ecosystem, where each member is self-maintaining. The population of the ecosystem comprises cooperative spiders incorporating communication, mobility and learning skills, striving to improve efficiency. Genetic algorithms and classifier rules have been employed for spider adaptation and learning. A set of experiments has been performed in order to qualitatively test the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:M3ejUd6NZC8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Supporting Agent-Oriented Software Engineering for Data Mining Enhanced Agent Development",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-36288-0_3",
            "Abstract": "The emergence of Multi-Agent systems as a software paradigm that most suitably fits all types of problems and architectures is already experiencing significant revisions. A more consistent approach on agent programming, and the adoption of Software Engineering standards has indicated the pros and cons of Agent Technology and has limited the scope of the, once considered, programming \u2018panacea\u2019. Nowadays, the most active area of agent development is by far that of intelligent agent systems, where learning, adaptation, and knowledge extraction are at the core of the related research effort. Discussing knowledge extraction, data mining, once infamous for its application on bank processing and intelligence agencies, has become an unmatched enabling technology for intelligent systems. Naturally enough, a fruitful synergy of the aforementioned technologies has already been proposed that would \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:hMod-77fHWUC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "The hackathon model to spur innovation around global mHealth",
            "Publication year": 2016,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/03091902.2016.1213903",
            "Abstract": "The challenge of providing quality healthcare to underserved populations in low- and middle-income countries (LMICs) has attracted increasing attention from information and communication technology (ICT) professionals interested in providing societal impact through their work. Sana is an organisation hosted at the Institute for Medical Engineering and Science at the Massachusetts Institute of Technology that was established out of this interest. Over the past several years, Sana has developed a model of organising mobile health bootcamp and hackathon events in LMICs with the goal of encouraging increased collaboration between ICT and medical professionals and leveraging the growing prevalence of cellphones to provide health solutions in resource limited settings. Most recently, these events have been based in Colombia, Uganda, Greece and Mexico. The lessons learned from these events can provide a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:JavbeY_VQWIC",
            "Publisher": "Taylor & Francis"
        },
        {
            "Title": "Data-mining-enhanced agents in dynamic supply-chain-management environments",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4983382/",
            "Abstract": "In modern supply chains, stakeholders with varying degrees of autonomy and intelligence compete against each other in a constant effort to establish beneficiary contracts and maximize their own revenue. In such competitive environments, entities-software agents being a typical programming paradigm-interact in a dynamic and versatile manner, so each action can cause ripple reactions and affect the overall result. In this article, the authors argue that the utilization of data mining primitives could prove beneficial in order to analyze the supply-chain model and identify pivotal factors. They elaborate on the benefits of data mining analysis on a well-established agent supply-chain management network, both at a macro and micro level. They also analyze the results and discuss specific design choices in the context of agent performance improvement.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:4TOpqqG69KYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "BrainRun: A behavioral biometrics dataset towards continuous implicit authentication",
            "Publication year": 2019,
            "Publication url": "https://www.mdpi.com/455030",
            "Abstract": "The widespread use of smartphones has dictated a new paradigm, where mobile applications are the primary channel for dealing with day-to-day tasks. This paradigm is full of sensitive information, making security of utmost importance. To that end, and given the traditional authentication techniques (passwords and/or unlock patterns) which have become ineffective, several research efforts are targeted towards biometrics security, while more advanced techniques are considering continuous implicit authentication on the basis of behavioral biometrics. However, most studies in this direction are performed \u201cin vitro\u201d resulting in small-scale experimentation. In this context, and in an effort to create a solid information basis upon which continuous authentication models can be built, we employ the real-world application \u201cBrainRun\u201d, a brain-training game aiming at boosting cognitive skills of individuals. BrainRun embeds a gestures capturing tool, so that the different types of gestures that describe the swiping behavior of users are recorded and thus can be modeled. Upon releasing the application at both the \u201cGoogle Play Store\u201d and \u201cApple App Store\u201d, we construct a dataset containing gestures and sensors data for more than 2000 different users and devices. The dataset is distributed under the CC0 license and can be found at the EU Zenodo repository. View Full-Text",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:CKf5m1HYVjMC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Towards a generic methodology for evaluating MAS performance",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4227544/",
            "Abstract": "As agent technology (AT) becomes a well-established engineering field of computing, the need for generalized, standardized methodologies for agent evaluation is imperative. Despite the plethora of available development tools and theories that researchers in agent computing have access to, there is a remarkable lack of general metrics, tools, benchmarks and experimental methods for formal validation and comparison of existing or newly developed systems. It is argued that AT has reached a certain degree of maturity, and it is therefore feasible to move from ad-hoc, domain-specific evaluation methods to standardized, repeatable and easily verifiable procedures. In this paper, we outline a first attempt towards a generic evaluation methodology for MAS performance. Instead of following the research path towards defining more powerful mathematical description tools for determining intelligence and performance \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:kNdYIx-mwKoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Towards an integrated robotics architecture for social inclusion\u2013The RAPP paradigm",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1389041716300535",
            "Abstract": "Scientific breakthroughs have led to an increase in life expectancy, to the point where senior citizens comprise an ever increasing percentage of the general population. In this direction, the EU funded RAPP project \u201cRobotic Applications for Delivering Smart User Empowering Applications\u201d introduces socially interactive robots that will not only physically assist, but also serve as a companion to senior citizens. The proposed RAPP framework has been designed aiming towards a cloud-based integrated approach that enables robotic devices to seamlessly deploy robotic applications, relieving the actual robots from computational burdens. The Robotic Applications (RApps) developed according to the RAPP paradigm will empower consumer social robots, allowing them to adapt to versatile situations and materialize complex behaviors and scenarios. The RAPP pilot cases involve the development of RApps for the NAO \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:WgvcDLhf7hwC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Biotope: an integrated framework for simulating distributed multiagent computational systems",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1420670/",
            "Abstract": "The study of distributed computational systems issues, such as heterogeneity, concurrency, control, and coordination, has yielded a number of models and architectures, which aspire to provide satisfying solutions to each of the above problems. One of the most intriguing and complex classes of distributed systems are computational ecosystems, which add an \"ecological\" perspective to these issues and introduce the characteristic of self-organization. Extending previous research work on self-organizing communities, we have developed Biotope, which is an agent simulation framework, where each one of its members is dynamic and self-maintaining. The system provides a highly configurable interface for modeling various environments as well as the \"living\" or computational entities that reside in them, while it introduces a series of tools for monitoring system evolution. Classifier systems and genetic algorithms \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Agents and Data Mining Interaction 7th International Workshop on Agents and Data Mining Interation, ADMI 2011, Taipei, Taiwan, May 2-6, 2011, Revised Selected Papers",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/978-3-642-27608-8",
            "Abstract": "This book constitutes the thoroughly refereed post-workshop proceedings of the 7th International Workshop on Agents and Data Mining Interaction, ADMI 2011, held in Taipei, Taiwan, in May 2011 in conjunction with AAMAS 2011, the 10th International Joint Conference on Autonomous Agents and Multiagent Systems. The 11 revised full papers presented were carefully reviewed and selected from 24 submissions. The papers are organized in topical sections on agents for data mining; data mining for agents; and agent mining applications.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:RdUpO4xyVKMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Mining Software Requirements",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=6351516155819253869&hl=en&oi=scholarr",
            "Abstract": "Requirements identification is one of the most important phases in software engineering, as incomplete or badly specified requirements are the most common cause of project failure. In this chapter, we design a methodology to facilitate requirements identification based on software reuse. Our methodology employs our ontology-based model and is applied to functional requirements and UML diagrams. Concerning functional requirements, we apply association rule mining and heuristics to detect incomplete or missing requirements, while for UML use case and activity diagrams, we employ model matching techniques to find similar diagrams and thus allow the engineer to improve the description of the functionality and the data flow of the project.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:qkm5LKljiV4C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "An alarm firing system for national genetic evaluation quality control",
            "Publication year": 2004,
            "Publication url": "https://journal.interbull.org/index.php/ib/article/view/858/849",
            "Abstract": "National genetic evaluation results form the basis of Interbull services. The current method for quality assurance is mainly determined by the consistency of consecutive evaluation results and is based on thorough statistical examination (Klei et al., 2002). In a separate project, national genetic evaluation programs are being tested on simulated data sets with known properties (T\u00e4ubert et al., 2002). Datamining (DM) offers an alternative way to examine data and extract valuable information (Han and Kamber, 2000), potentially leading to inference on data quality. In a recent progress report, the development of a DM algorithm for the analysis of national genetic evaluation results was presented (Banos et al., 2003). Data quality was assessed by subjective inspection of DM results. The present study introduces a method to evaluate DM application results with objective criteria leading, when necessary, to the automatic \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:hqOjcs7Dif8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving the predictions of soil properties from VNIR\u2013SWIR spectra in an unlabeled region using semi-supervised and active learning",
            "Publication year": 2021,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0016706120325854",
            "Abstract": "Monitoring the status of the soil ecosystem to identify the spatio-temporal extent of the pressures exerted and mitigate the effects of climate change and land degradation necessitates the need for reliable and cost-effective solutions. To address this need, soil spectroscopy in the visible, near- and shortwave-infrared (VNIR\u2013SWIR) has emerged as a viable alternative to traditional analytical approaches. To this end, large-scale soil spectral libraries coupled with advanced machine learning tools have been developed to infer the soil properties from the hyperspectral signatures. However, models developed from one region may exhibit diminished performance when applied to a new, unseen by the model, region due to the large and inherent soil variability (e.g. pedogenetical differences, diverse soil types etc.). Given an existing spectral library with labeled data and a new unlabeled region (i.e. where no soil samples \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:Wqn_fehR_TUC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Analysing behaviours for intrusion detection",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7247578/",
            "Abstract": "In this work, a Behaviour-based Intrusion Detection Model is suggested. The proposed model can be employed from a single host configuration to a distributed mixture of host-based and network-based Intrusion Detection Systems (IDSs). Unlike most state-of-the-art IDSs that rely on analysing lower-level, raw-data representations, our proposed architecture suggests to use higher-level notions -behaviours- instead; this way, the IDS is able to identify more sophisticated attacks. To assess our premise, a Behaviour-based IDS (BIDS) prototype has been designed and developed that scans file system data to identify attacks. BIDS achieves high detection rates with low corresponding false positive rates, superseding other state-of-the-art file system IDSs.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:9o6PfxSMcEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Data Mining-Driven Analysis and Decomposition in Agent Supply Chain Management Networks",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4740842/",
            "Abstract": "In complex and dynamic environments where interdependencies cannot monotonously determine causality, data mining techniques may be employed in order to analyze the problem, extract key features and identify pivotal factors. Typical cases of such complexity and dynamicity are supply chain networks, where a number of involved stakeholders struggle towards their own benefit. These stakeholders may be agents with varying degrees of autonomy and intelligence, in a constant effort to establish beneficiary contracts and maximize own revenue. In this paper, we illustrate the benefits of data mining analysis on a well-established agent supply chain management network. We apply data mining techniques, both at a macro and micro level, analyze the results and discuss them in the context of agent performance improvement.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:-f6ydRqryjwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Npm-miner: An infrastructure for measuring the quality of the npm registry",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8595175/",
            "Abstract": "As the popularity of the JavaScript language is constantly increasing, one of the most important challenges today is to assess the quality of JavaScript packages. Developers often employ tools for code linting and for the extraction of static analysis metrics in order to assess and/or improve their code. In this context, we have developed npn-miner, a platform that crawls the npm registry and analyzes the packages using static analysis tools in order to extract detailed quality metrics as well as high-level quality attributes, such as maintainability and security. Our infrastructure includes an index that is accessible through a web interface, while we have also constructed a dataset with the results of a detailed analysis for 2000 popular npm packages.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:-BKJ5vZJwzMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A Comparative Analysis of Pattern Matching Techniques Towards OGM Evaluation",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s10846-019-01053-7",
            "Abstract": "The alignment of two occupancy grid maps generated by SLAM algorithms is a quite researched problem, being an obligatory step either for unsupervised map merging techniques or for evaluation of OGMs (Occupancy Grid Maps) against a blueprint of the environment. This paper provides an overview of the existing automatic alignment techniques of two occupancy grid maps that employ pattern matching. Additionally, an alignment pipeline using local features and image descriptors is implemented, as well as a method to eliminate erroneous correspondences, aiming at producing the correct transformation between the two maps. Finally, map quality metrics are proposed and utilized, in order to quantify the produced map\u2019s correctness. A comparative analysis was performed over a number of image processing and OGM-oriented detectors and descriptors, in order to identify the best combinations for the map \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:V63Ir2N1OTwC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Mining patterns and rules for improving agent intelligence through an integrated multi-agent platform",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=10748839465580445152&hl=en&oi=scholarr",
            "Abstract": "Andreas L. Symeonidis Pericles A. Mitkas Dionisis D. Kechagias Tel.+ 30-310-99-6349 Tel.+ 30-310-99-6390 Tel.+ 30-310-99-6349 Fax+ 30-310-99-6398 Fax+ 30-310-99-6396 Fax+ 30-310-99-6398 email: asymeon@ ee. auth. gr email: mitkas@ eng. auth. gr email: diok@ iti. gr",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bottom-up modeling of small-scale energy consumers for effective demand response applications",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197614001377",
            "Abstract": "In contemporary power systems, small-scale consumers account for up to 50% of a country\u05f3s total electrical energy consumption. Nevertheless, not much has been achieved towards eliminating the problems caused by their inelastic consumption habits, namely the peaks in their daily power demand and the inability of energy suppliers to perform short-term forecasting and/or long-term portfolio management. Typical approaches applied in large-scale consumers, like providing targeted incentives for behavioral change, cannot be employed in this case due to the lack of models for everyday habits, activities and consumption patterns, as well as the inability to model consumer response based on personal comfort. Current work aspires to tackle these issues; it introduces a set of small-scale consumer models that provide statistical descriptions of electrical consumption patterns, parameterized from the analysis of real \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:pyW8ca7W8N0C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "A Natural Language Driven Approach for Automated Web API Development: Gherkin2OAS",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3184558.3191654",
            "Abstract": "Speeding up the development process of Web Services, while adhering to high quality software standards is a typical requirement in the software industry. This is why industry specialists usually suggest\" driven by\" development approaches to tackle this problem. In this paper, we propose such a methodology that employs Specification Driven Development and Behavior Driven Development in order to facilitate the phases of Web Service requirements elicitation and specification. Furthermore, we introduce gherkin2OAS, a software tool that aspires to bridge the aforementioned development approaches. Through the suggested methodology and tool, one may design and build RESTful services fast, while ensuring proper functionality.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:w-fwKiQwpQAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Coupling Data Mining with Intelligent Agents",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/0-387-25757-8_5.pdf",
            "Abstract": "1. The Unified Methodology 1.1 Formal Model Let O be the ontology of the MAS. Let A={A\\, A2,..., An} be the set of attributes described in O and defined on V, the application data domain. Let D C V be a set of data, where each dataset tuple is a vector t={ti,\u00a3 2,\u2022\u2022\u2022 5^ n} 5 and\u00a3 ij i= 1... n, is a value for the corresponding attribute A {. Missing values are allowed within t. Let us now consider a MAS with k different agent types Qi, i\u2014\\,..., k. For each type, qi agent instances Qi (j) may exist with 1< Qi< m-Thus, the total number of agents in the MAS is equal to: Y! fi= iQi'Fr these agents, three are the training cases, with respect to the knowledge types extracted:",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:RYcK_YlVTxYC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Agent-based small-scale energy consumer models for energy portfolio management",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6690776/",
            "Abstract": "In contemporary power systems, residential consumers may account for up to 50% of a country's total electrical energy consumption. Even though they constitute a significant portion of the energy market, not much has been achieved towards eliminating the inability for energy suppliers to perform long-term portfolio management, thus maximizing their revenue. The root cause of these problems is the difficulty in modeling consumers' behavior, based on their everyday activities and personal comfort. If one were able to provide targeted incentives based on consumer profiles, the expected impact and market benefits would be significant. This paper introduces a formal residential consumer modeling methodology, that allows (i) the decomposition of the observed electrical load curves into consumer activities and, (ii) the evaluation of the impact of behavioral changes on the household's aggregate load curve. Analyzing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:3s1wT3WcHBgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improving Multilingual Interaction for Consumer Robots through Signal Enhancement in Multichannel Speech",
            "Publication year": 2016,
            "Publication url": "http://www.aes.org/e-lib/online/browse.cfm?elib=18337",
            "Abstract": "In order for social robots to be truly successful, they need the ability to orally communicate with humans, providing feedback and accepting commands. Social robots need automatic speech recognition (ASR) tools that function with different users, using different languages, voice pitches, pronunciations, and speech speeds over a wide range of sound and noise levels. This paper describes different methodologies for voice activity detection and noise elimination when used with ASR-based oral interaction within an affordable budget robot. Acoustically quasi-stationary environments are assumed, which in conjunction with the high background noise of the robot\u2019s microphones makes the ASR challenging. This work has been performed in the context of project RAPP, which attempts to deliver a cloud repository of applications and services that can be utilized by heterogeneous robots, aiming at assisting people with a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:d6JCS5z0ckYC",
            "Publisher": "Audio Engineering Society"
        },
        {
            "Title": "Pose Selection and Feedback Methods in Tandem Combinations of Particle Filters with Scan-Matching for 2D Mobile Robot Localisation",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s10846-020-01253-6",
            "Abstract": "Robot localisation is predominantly resolved via parametric or non-parametric probabilistic methods. The particle filter, the most common non-parametric approach, is a Monte Carlo Localisation (MCL) method that is extensively used in robot localisation, as it can represent arbitrary probabilistic distributions, in contrast to Kalman filters, which is the standard parametric representation. In particle filters, a weight is internally assigned to each particle, and this weight serves as an indicator of a particle\u2019s estimation certainty. Their output, the tracked object\u2019s pose estimate, is implicitly assumed to be the weighted average pose of all particles; however, we argue that disregarding low-weight particles from this averaging process may yield an increase in accuracy. Furthermore, we argue that scan-matching, treated as a prosthesis of (or, put differently, fit in tandem with) a particle filter, can also lead to better accuracy \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:ef2wPL15CskC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Software requirements as an application domain for natural language processing",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/article/10.1007/s10579-017-9381-z",
            "Abstract": "Mapping functional requirements first to specifications and then to code is one of the most challenging tasks in software development. Since requirements are commonly written in natural language, they can be prone to ambiguity, incompleteness and inconsistency. Structured semantic representations allow requirements to be translated to formal models, which can be used to detect problems at an early stage of the development process through validation. Storing and querying such models can also facilitate software reuse. Several approaches constrain the input format of requirements to produce specifications, however they usually require considerable human effort in order to adopt domain-specific heuristics and/or controlled languages. We propose a mechanism that automates the mapping of requirements to formal representations using semantic role labeling. We describe the first publicly available \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:-LHtoeeytlUC",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Exploiting data mining techniques for improving the efficiency of a supply chain management agent",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4053196/",
            "Abstract": "Supply chain management (SCM) environments are often dynamic markets providing a plethora of information, either complete or incomplete. It is, therefore, evident that such environments demand intelligent solutions, which can perceive variations and act in order to achieve maximum revenue. To do so, they must also provide some sophisticated mechanism for exploiting the full potential of the environments they inhabit. Advancing on the way autonomous solutions usually deal with the SCM process, we have built a robust and highly-adaptable mechanism for efficiently dealing with all SCM facets, while at the same time incorporating a module that exploits data mining technology in order to forecast the price of the winning bid in a given order and, thus, adjust its bidding strategy. The paper presents our agent, Mertacor, and focuses on the forecasting mechanism it incorporates, aiming to optimal agent efficiency",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:0EnyYjriUFMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An Intelligent Recommendation Framework for ERP Systems.",
            "Publication year": 2005,
            "Publication url": "https://www.academia.edu/download/48665699/An_Intelligent_Recommendation_Framework_20160908-11424-1vg5w2l.pdf",
            "Abstract": "Enterprise Resource Planning systems efficiently administer all tasks concerning real-time planning and manufacturing, material procurement and inventory monitoring, customer and supplier management. Nevertheless, the incorporation of domain knowledge and the application of adaptive decision making into such systems require extreme customization with a cost that becomes unaffordable, especially in the case of SMEs. We present an alternative approach for incorporating adaptive business intelligence into the company\u2019s backbone. We have designed and developed a highly reconfigurable, adaptive, cost efficient multi-agent framework that acts as an add-on to ERP software, employing Data Mining and Soft Computing techniques in order to provide intelligent recommendations on customer, supplier and inventory management. In this paper, we present the architectural details of the developed framework.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:_FxGoFyzp5QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Software reusability dataset based on static analysis metrics and reuse rate information",
            "Publication year": 2019,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S235234091931042X",
            "Abstract": "The widely adopted component-based development paradigm considers the reuse of proper software components as a primary criterion for successful software development. As a result, various research efforts are directed towards evaluating the extent to which a software component is reusable. Prior efforts follow expert-based approaches, however the continuously increasing open-source software initiative allows the introduction of data-driven alternatives. In this context we have generated a dataset that harnesses information residing in online code hosting facilities and introduces the actual reuse rate of software components as a measure of their reusability. To do so, we have analyzed the most popular projects included in the maven registry and have computed a large number of static analysis metrics at both class and package levels using SourceMeter tool [2] that quantify six major source code properties \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:-ZoC36zw86wC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "D3. 1.2 Module for extracting software artefacts from text",
            "Publication year": 2014,
            "Publication url": "http://s-case.github.io/publications/eis2017/S-CASE_D3.1.pdf",
            "Abstract": "The module for extracting software artefacts from text converts informal requirements expressed in natural language into a formal language that maps to the S-CASE ontology. This is achieved by using natural language processing techniques to automatically parse and \u201cunderstand\u201d the unstructured textual input.The semantic parser is developed on the basis of supervised optimization methods from machine learning and hence relies on annotated data as training material. The concept ontology is integrated as background knowledge and provides guidance for automatically detecting and classifying instances of concepts and relations in previously unseen text.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:DwWRdx-KAo4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "An integrated framework for enhancing the semantic transformation, editing and querying of relational databases",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S095741741001002X",
            "Abstract": "The transition from the traditional to the Semantic Web has proven much more difficult than initially expected. The volume, complexity and versatility of data of various domains, the computational limitations imposed on semantic querying and inferencing have drastically reduced the thrust semantic technologies had when initially introduced. In order for the semantic web to actually \u2018work\u2019 efficient tools are needed, allowing the semantic transformation of legacy data, which can then be queried, processed and reasoned upon. And, though remarkable efforts exist towards tackling part of the required functionality, no system integrates all tasks in a user-friendly, easy-to-adapt manner. In this context, we introduce Iconomy, an integrated framework for enhancing the Semantic Transformation, Editing and Querying of Relational Databases. Aiming to ease-of-use, scalability and extensibility, Iconomy supports state-of-the-art \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:TQgYirikUcIC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Agents in dynamic Supply Chain Management environments: Data mining-driven design choices",
            "Publication year": 2009,
            "Publication url": "https://www.researchgate.net/profile/Andreas-Symeonidis/publication/268404366_Agents_in_dynamic_Supply_Chain_Management_environments_Data_mining-driven_design_choices/links/55310b8a0cf27acb0de9297d/Agents-in-dynamic-Supply-Chain-Management-environments-Data-mining-driven-design-choices.pdf",
            "Abstract": "In modern supply chains, stakeholders with varying degrees of autonomy and intelligence compete against each other, in a constant effort to establish beneficiary contracts and maximize their own revenue. In such competitive environments, entities-software agents being a typical programming paradigminteract in a dynamic and versatile manner, and each action may cause ripple reactions and affect the overall result. In this paper, we argue that the utilization of data mining primitives may prove beneficial in order to analyze the supply chain model and identify pivotal factors. We elaborate on the benefits of data mining analysis on a well-established agent supply chain management network, both at a macro and micro level. We analyze the results and discuss specific design choices in the context of agent performance improvement.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:NKlx0PmyA3cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Theoretical Background and State-of-the-Art",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=6311175421885040871&hl=en&oi=scholarr",
            "Abstract": "This chapter provides an overview of the background knowledge that is relevant to the main areas of application of this book. The areas of software engineering, software reuse, and software quality are discussed in the context of taking advantage of useful data in order to improve the software development process. Upon providing the relevant definitions and outlining the data and metrics provided as part of software development, we discuss how data mining techniques can be applied to software engineering data and illustrate the reuse potential that is provided in an integrated manner.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:tfDI-GPdlUQC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Application of Data Mining and intelligent agent technologies to Concurrent Engineering",
            "Publication year": 2007,
            "Publication url": "https://www.inderscienceonline.com/doi/abs/10.1504/IJPLM.2007.014278",
            "Abstract": "Software agent technology has matured enough to produce intelligent agents, which can be used to control a large number of Concurrent Engineering tasks. Multi-Agent Systems (MAS) are communities of agents that exchange information and data in the form of messages. The agents' intelligence can range from rudimentary sensor monitoring and data reporting, to more advanced forms of decision-making and autonomous behaviour. The behaviour and intelligence of each agent in the community can be obtained by performing Data Mining on available application data and the respected knowledge domain. We have developed Agent Academy (AA), a software platform for the design, creation, and deployment of MAS, which combines the power of knowledge discovery algorithms with the versatility of agents. Using this platform, we illustrate how agents, equipped with a data-driven inference engine, can be \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:MXK_kJrjxJIC",
            "Publisher": "Inderscience Publishers"
        },
        {
            "Title": "User-perceived source code quality estimation based on static analysis metrics",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7589790/",
            "Abstract": "The popularity of open source software repositories and the highly adopted paradigm of software reuse have led to the development of several tools that aspire to assess the quality of source code. However, most software quality estimation tools, even the ones using adaptable models, depend on fixed metric thresholds for defining the ground truth. In this work we argue that the popularity of software components, as perceived by developers, can be considered as an indicator of software quality. We present a generic methodology that relates quality with source code metrics and estimates the quality of software components residing in popular GitHub repositories. Our methodology employs two models: a one-class classifier, used to rule out low quality code, and a neural network, that computes a quality score for each software component. Preliminary evaluation indicates that our approach can be effective for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:iWL_APfBKHwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Intelligent policy recommendations on enterprise resource planning by the use of agent technology and data mining techniques",
            "Publication year": 2003,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S095741740300099X",
            "Abstract": "Enterprise Resource Planning systems tend to deploy Supply Chain Management and/or Customer Relationship Management techniques, in order to successfully fuse information to customers, suppliers, manufacturers and warehouses, and therefore minimize system-wide costs while satisfying service level requirements. Although efficient, these systems are neither versatile nor adaptive, since newly discovered customer trends cannot be easily integrated with existing knowledge. Advancing on the way the above mentioned techniques apply on ERP systems, we have developed a multi-agent system that introduces adaptive intelligence as a powerful add-on for ERP software customization. The system can be thought of as a recommendation engine, which takes advantage of knowledge gained through the use of data mining techniques, and incorporates it into the resulting company selling policy. The intelligent \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:u-x6o8ySG0sC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "E-commerce Personalization with Elasticsearch",
            "Publication year": 2019,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1877050919306271",
            "Abstract": "Personalization techniques are constantly gaining traction among e-commerce retailers, since major advancements have been made at research level and the benefits are clear and pertinent. However, effectively applying personalization in real life is a challenging task, since the proper mixture of technology, data and content is complex and differs between organizations. In fact, personalization applications such as personalized search remain largely unfulfilled, especially by small and medium sized retailers, due to time and space limitations. In this paper we propose a novel approach for near real-time personalized e-commerce search that provides improved personalized results within the limited accepted time frames required for online browsing. We propose combining features such as product popularity, user interests, and query-product relevance with collaborative filtering, and implement our solution in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:3pYxbvHKFu8C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Embedding Rasa in edge Devices: Capabilities and Limitations",
            "Publication year": 2021,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S187705092101499X",
            "Abstract": "Over the past few years, there has been a boost in the use of commercial virtual assistants. Obviously, these proprietary tools are well-performing, however the functionality they offer is limited, users are \u201dvendor-locked\u201d, while possible user privacy issues rise. In this paper we argue that low-cost, open hardware solutions may also perform well, given the proper setup. Specifically, we perform an initial assessment of a low-cost virtual agent employing the Rasa framework integrated into a Raspberry Pi 4. We set up three different architectures, discuss their capabilities and limitations and evaluate the dialogue system against three axes: assistant comprehension, task success and assistant usability. Our experiments show that our low-cost virtual assistant performs in a satisfactory manner, even when a small-sized training dataset is used.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:FwTEoIZreccC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Extracting Semantics from Question-Answering Services for Snippet Reuse",
            "Publication year": 2020,
            "Publication url": "https://library.oapen.org/bitstream/handle/20.500.12657/37725/2020_Book_FundamentalApproachesToSoftwar.pdf?sequence=1#page=130",
            "Abstract": "Nowadays, software developers typically search online for reusable solutions to common programming problems. However, forming the question appropriately, and locating and integrating the best solution back to the code can be tricky and time consuming. As a result, several mining systems have been proposed to aid developers in the task of locating reusable snippets and integrating them into their source code. Most of these systems, however, do not model the semantics of the snippets in the context of source code provided. In this work, we propose a snippet mining system, named StackSearch, that extracts semantic information from Stack Overlow posts and recommends useful and in-context snippets to the developer. Using a hybrid language model that combines Tf-Idf and fastText, our system effectively understands the meaning of the given query and retrieves semantically similar posts. Moreover, the results are accompanied with useful metadata using a named entity recognition technique. Upon evaluating our system in a set of common programming queries, in a dataset based on post links, and against a similar tool, we argue that our approach can be useful for recommending ready-to-use snippets to the developer.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:AkvegQHRDQ8C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Agents and data mining interaction",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/978-3-642-27609-5.pdf",
            "Abstract": "\u00a9 Springer-Verlag Berlin Heidelberg 2012",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:anDooRL1HQEC",
            "Publisher": "Springer-Verlag Heidelberg"
        },
        {
            "Title": "RoboCup Rescue 2015-Robot League Team PANDORA (Greece)",
            "Publication year": 2015,
            "Publication url": "http://archive.robocup.info/TODO/Rescue/Robot/TDPs/RoboCup/2015/RoboCup_Symposium_2015_submission_102.pdf",
            "Abstract": "Within the context of the 2015 RoboCup-Rescue competition (www. robocup. org) the PANDORA Robotics Team of the Aristotle University of Thessaloniki has developed an experimental robotic platform dedicated to exploration and victim identification. Our robot is able to autonomously navigate through unknown space (eg building ruins after an earthquake), avoid obstacles, search for signs of life and identify victims. We are going to use a 4-wheel drive robotic platform aiming at identifying victims residing in the yellow arena and the radio drop-off zone.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:8rLWgkbgOXQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Semantic analysis of web documents for the generation of optimal content",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197614001304",
            "Abstract": "The Web has been under major evolution over the last decade and search engines have been trying to incorporate the changes of the web and provide the user with improved \u2013 in terms of quality \u2013 content. In order to evaluate the quality of a document there has been a plethora of attempts, some of which have considered the use of semantic analysis for extracting conclusions upon documents around the web. In turn, Search Engine Optimization (SEO) has been under development in order to cope with the changes of search engines and the web. SEO\u05f3s aim has been the creation of effective strategies for optimal ranking of websites and webpages in search engines. Current work probes on semantic analysis of web content. We further elaborate on LDArank, a mechanism that employs Latent Dirichlet Allocation (LDA) for the semantic analysis of web content and the generation of optimal content for given queries \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:yD5IFk8b50cC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Enhancing requirements reusability through semantic modeling and data mining techniques",
            "Publication year": 2018,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/17517575.2017.1416177",
            "Abstract": "Enhancing the requirements elicitation process has always been of added value to software engineers, since it expedites the software lifecycle and reduces errors in the conceptualization phase of software products. The challenge posed to the research community is to construct formal models that are capable of storing requirements from multimodal formats (text and UML diagrams) and promote easy requirements reuse, while at the same time being traceable to allow full control of the system design, as well as comprehensible to software engineers and end users. In this work, we present an approach that enhances requirements reuse while capturing the static (functional requirements, use case diagrams) and dynamic (activity diagrams) view of software projects. Our ontology-based approach allows for reasoning over the stored requirements, while the mining methodologies employed detect incomplete or \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:VBDT71xRUdcC",
            "Publisher": "Taylor & Francis"
        },
        {
            "Title": "Towards improving multiagent simulation in safety management and hazards control environments",
            "Publication year": 2002,
            "Publication url": "https://issel.ee.auth.gr/wp-content/uploads/2017/01/TOWARDS-IMPROVING-MULTI-AGENT-SIMULATION-IN-SAFETY-MANAGEMENT-AND-HAZARD-CONTROL-ENVIRONMENTS.pdf",
            "Abstract": "This paper introduces the capabilities of Agent Academy in the area of Safety Management and Hazard Control Systems. Agent Academy is a framework under development, which uses data mining techniques for training intelligent agents. This framework generates software agents with an initial degree of intelligence and trains them to manipulate complex tasks. The agents, are further integrated into a simulation multi-agent environment capable of managing issues in a hazardous environment, as well as regulating the parameters of the safety management strategy to be deployed in order to control the hazards. The initially created agents take part in long agentto-agent transactions and their activities are formed into behavioural data, which are stored in a database. As soon as the amount of collected data increases sufficiently, a data mining process is initiated, in order to extract specific trends adapted by agents and improve their intelligence. The result of the overall procedure aims to improve the simulation environment of safety management. The communication of agents as well as the architectural characteristics of the simulation environment adheres to the set of specifications imposed by the Foundation for Intelligent Physical Agents (FIPA).",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:LkGwnXOMwfcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Personalization and the Conversational Web",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-35330-8_4",
            "Abstract": " Hyper-personalization intends to maximize the opportunities a marketer has to tailor content that fits each and every customer\u2019s wants and needs. Naturally, gathering and analyzing more data is the key to those opportunities. This is were the \u201cConversation Web\u201d comes in, which in the near future is expected to transform to so much more than just conversational interfaces (chat-bots). In a truly Conversation Web, websites and users implicitly \u201cdiscuss\u201d in the form of clicks, mouse scrolls and movements, as well as page views and product purchases. Websites use this information for decoding user interests and profile and provide customized one-to-one services. In this work we proposed an integrated architecture for the conversational Web; consequently we propose a novel hybrid approach for recommendations using offline and online analysis, as well as we propose a novel personalized search strategy that takes \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:tL5YfqkXb3gC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Association studies on cervical cancer facilitated by inference and semantic technologies",
            "Publication year": 2008,
            "Publication url": "http://ikee.lib.auth.gr/record/224444?ln=fr",
            "Abstract": "Cervical cancer (CxCa) is currently the second leading cause of cancer-related deaths, for women between 20 and 39 years old. As infection by the human papillomavirus (HPV) is considered as the central risk factor for CxCa, current research focuses on the role of specific genetic and environmental factors in determining HPV persistence and subsequent progression of the disease. ASSIST is an EU-funded research project that aims to facilitate the design and execution of genetic association studies on CxCa in a systematic way by adopting inference and semantic technologies. Toward this goal, ASSIST provides the means for seamless integration and virtual unification of distributed and heterogeneous CxCa data repositories, and the underlying mechanisms to undertake the entire process of expressing and statistically evaluating medical hypotheses based on the collected data in order to generate medically \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:TY5xIG7f_2sC",
            "Publisher": "Aristotle University of Thessaloniki"
        },
        {
            "Title": "Identifying valid search engine ranking factors in a Web 2.0 and Web 3.0 context for building efficient SEO mechanisms",
            "Publication year": 2015,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197615000329",
            "Abstract": "It is common knowledge that the web has been continuously evolving, from a read medium to a read/write scheme and, lately, to a read/write/infer corpus. To follow the evolution, Search Engines have been undergoing continuous updates in order to provide the user with a well-targeted, personalized and improved experience of the web. Along with this focus on content quality and user preferences, search engines have also been striving to integrate Semantic Web primitives, in order to enhance their intelligence. Current work discusses the evolution of search engine ranking factors in a Web 2.0 and Web 3.0 context. A benchmark crawler LSHrank, has been developed, which employs known search engine APIs and evaluates results against various already established metrics, in different domains and types of web content. The ultimate LSHrank objective is the development of a Search Engine Optimization (SEO \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:J6OZcwVsj5AC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "A decision-tree-based alarming system for the validation of national genetic evaluations",
            "Publication year": 2006,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0168169906000135",
            "Abstract": "The aim of this work was to explore possibilities to build an alarming system based on the results of the application of data mining (DM) techniques in genetic evaluations of dairy cattle, in order to assess and assure data quality. The technique used combined data mining using classification and decision-tree algorithms, Gaussian binned fitting functions, and hypothesis tests. Data were quarterly national genetic evaluations, computed between February 1999 and February 2003 in nine countries. Each evaluation run included 73,000\u201390,000 bull records complete with their genetic values and evaluation information. Milk production traits were considered. Data mining algorithms were applied separately for each country and evaluation run to search for associations across several dimensions, including bull origin, type of proof, age of bull, and number of daughters. Then, data in each node were fitted to the Gaussian \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:_kc_bZDykSQC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Agent Retraining and Dynamical Improvement of Agent Intelligence",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/0-387-25757-8_9.pdf",
            "Abstract": "A major advantage of the methodology presented in this book is agent retraining, the process of revising the knowledge model (s) of agent (s) by re-applying DM techniques. Retraining aspires to improve agent intelligence/performance and can be applied periodically or on a needbased mode. Since there is no point in repeating the DM process on the same dataset, retraining requires either the existence of new application data (Case 1-Chapter 6)) the existence of new behavior data (Case 2-Chapter 7), or the application of an evolutionary DM technique (Case 3-Chapter 8).It is through retraining that we intent to show that certain DM techniques can be used to augment agent intelligence and therefore improve MAS overall performance. In this chapter we provide the formal retraining model for the four DM techniques presented and we appose a number of experimental results.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:J_g5lzvAfSwC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Real-time 3D localization of RFID-tagged products by ground robots and drones with commercial off-the-shelf RFID equipment: Challenges and solutions",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9244904/",
            "Abstract": "In this paper we investigate the problem of localizing passive RFID tags by ground robots and drones. We focus on autonomous robots, capable of entering a previously unknown environment, creating a 3D map of it, navigating safely in it, localizing themselves while moving, then localizing all RFID tagged objects and pinpointing their locations in the 3D map with cm accuracy. To the best of our knowledge, this is the first paper that presents the complex joint problem, including challenges from the field of robotics - i) sensors utilization, ii) local and global path planners, iii) navigation, iv) simultaneous localization of the robot and mapping - and from the field of RFIDs - vi) localization of the tags. We restrict our analysis to solutions, involving commercial UHF EPC Gen2 RFID tags, commercial off-the-self RFID readers and 3D real-time-only methods for tag-localization. We briefly present a new method, suitable for real \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:MSzX15-gZgkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Agent-Mediated Electronic Commerce. Designing Trading Strategies and Mechanisms for Electronic Markets: AMEC and TADA 2012, Valencia, Spain, June 4th, 2012, Revised Selected Papers",
            "Publication year": 2013,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=Wkq6BQAAQBAJ&oi=fnd&pg=PP5&dq=info:1wy_dfTvvKIJ:scholar.google.com&ots=u7MHaKFXQa&sig=F6s872Ivz7T-pax0_oMKBJP6tro",
            "Abstract": "This volume contains 11 thoroughly refereed and revised papers detailing recent advances in research on designing trading agents and mechanisms for agent-mediated e-commerce. They were originally presented at the Joint Workshop on Trading Agent Design and Analysis (TADA 2012) and Agent-Mediated Electronic Commerce (AMEC 2012) co-located with AAMAS 2012 in Valencia, Spain, in June 2012. The increasing reliance on software agents has created a range of pressing new research challenges, including the design of appropriate agent decision algorithms, approaches for predicting the complex behaviors and interactions of multiple agents, including the computation of equilibria, and the engineering of protocols and mechanisms that ensure electronic markets behave in a stable manner or fulfill other desirable criteria. Drawing upon a diverse range of scientific disciplines, including computer science, economics, artificial intelligence, operations research and game theory, the papers collected in this volume represent a cross-section of recent research and cover topics such as strategies for individual trading agents, the design of markets and interaction protocols between agents, and a variety of applications.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:BqipwSGYUEgC",
            "Publisher": "Springer"
        },
        {
            "Title": "Agent Mertacor: A robust design for dealing with uncertainty and variation in SCM environments",
            "Publication year": 2008,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0957417407002710",
            "Abstract": "Supply Chain Management (SCM) has recently entered a new era, where the old-fashioned static, long-term relationships between involved actors are being replaced by new, dynamic negotiating schemas, established over virtual organizations and trading marketplaces. SCM environments now operate under strict policies that all interested parties (suppliers, manufacturers, customers) have to abide by, in order to participate. And, though such dynamic markets provide greater profit potential, they also conceal greater risks, since competition is tougher and request and demand may vary significantly in the quest for maximum benefit. The need for efficient SCM actors is thus implied, actors that may handle the deluge of (either complete or incomplete) information generated, perceive variations and exploit the full potential of the environments they inhabit. In this context, we introduce Mertacor, an agent that employs \u2026",
            "Abstract entirety": 0,
            "Author pub id": "339uVZQAAAAJ:UeHWp8X0CEIC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Agent Academy: An integrated tool for developing multi-agent systems and embedding decision structures into agents",
            "Publication year": 2002,
            "Publication url": "https://www.researchgate.net/profile/Andreas-Symeonidis/publication/228933259_Agent_Academy_An_integrated_tool_for_developing_multi-agent_systems_and_embedding_decision_structures_into_agents/links/0c96051dfe57c35060000000/Agent-Academy-An-integrated-tool-for-developing-multi-agent-systems-and-embedding-decision-structures-into-agents.pdf",
            "Abstract": "In this paper we present Agent Academy, a framework that enables software developers to quickly develop multi-agent applications, when prior historical data relevant to a desired rule-based behaviour are available. Agent Academy is implemented itself as a multi-agent system, that supports, in a single tool, the design of agent behaviours and reusable agent types, the definition of ontologies, and the instantiation of single agents or multi-agent communities. Once an agent has been designed within the framework, the agent developer can create a specific ontology that describes the historical data. In this way, agents become capable of having embedded rule-based reasoning. We call this procedure \u201cagent training\u201d and it is realized by the application of data mining and knowledge discovery techniques on the application-specific historical data. From this point of view, Agent Academy provides a tool for both creating multi-agent systems and embedding rule-based decision structures into one or more of the participating agents.",
            "Abstract entirety": 1,
            "Author pub id": "339uVZQAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Unknown"
        }
    ]
}]