[{
    "name": "Andreas Moshovos",
    "romanize name": "Andreas Moshovos",
    "School-Department": " Electrical and Computer Engineering",
    "University": "University of Toronto",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 7924,
    "Scholar name": "Andreas Moshovos",
    "Scholar id": "D2VLt-8AAAAJ",
    "Affiliation": "Professor, Electrical and Computer Engineering, University of Toronto",
    "Citedby": 7515,
    "Interests": [
        "Computer Architecture"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=D2VLt-8AAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Addict: Advanced instruction chasing for transactions",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.14778/2733085.2733095",
            "Abstract": "Recent studies highlight that traditional transaction processing systems utilize the micro-architectural features of modern processors very poorly. L1 instruction cache and long-latency data misses dominate execution time. As a result, more than half of the execution cycles are wasted on memory stalls. Previous works on reducing stall time aim at improving locality through either hardware or software techniques. However, exploiting hardware resources based on the hints given by the software-side has not been widely studied for data management systems.In this paper, we observe that, independently of their high-level functionality, transactions running in parallel on a multicore system execute actions chosen from a limited sub-set of predefined database operations. Therefore, we initially perform a memory characterization study of modern transaction processing systems using standardized benchmarks. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:nb7KW1ujOQ8C",
            "Publisher": "VLDB Endowment"
        },
        {
            "Title": "Memory requirements for convolutional neural network hardware accelerators",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8573527/",
            "Abstract": "The rapid pace and successful application of machine learning research and development has seen widespread deployment of deep convolutional neural networks (CNNs). Alongside these algorithmic efforts, the compute- and memory-intensive nature of CNNs has stimulated a large amount of work in the field of hardware acceleration for these networks. In this paper, we profile the memory requirements of CNNs in terms of both on-chip memory size and off-chip memory bandwidth, in order to understand the impact of the memory system on accelerator design. We show that there are fundamental tradeoffs between performance, bandwidth, and on-chip memory. Further, this paper explores how the wide variety of CNNs for different application domains each have fundamentally different characteristics. We show that bandwidth and memory requirements for different networks, and occasionally for different layers \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:t6usbXjVLHcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A Building Block for Coarse-Grain Optimizations in the On-Chip Memory Hierarchy",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4359919/",
            "Abstract": "Current on-chip block-centric memory hierarchies exploit access patterns at the fine-grain scale of small blocks. Several recently proposed memory hierarchy enhancements for coherence traffic reduction and prefetching suggest that additional useful patterns emerge with a macroscopic, coarse-grain view. This paper presents RegionTracker, a dual-grain, on-chip cache design that exposes coarse-grain behavior while maintaining block-level communication. RegionTracker eliminates the extraneous, often imprecise coarse-grain tracking structures of previous proposals. It can be used as the building block for coarse-grain optimizations, reducing their overall cost and easing their adoption. Using full-system simulation of a quad-core chip multiprocessor and commercial workloads, we demonstrate that RegionTracker overcomes the inefficiencies of previous coarse-grain cache designs. We also demonstrate how \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:iH-uZ7U-co4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Asymmetric-Frequency Clustering: A Power-Aware Back-End for High-Performance Processors",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=16830591178374448097&hl=en&oi=scholarr",
            "Abstract": "We introduce asymmetric frequency clustering (AFC), a micro-architectural technique that reduces the dynamic power dis-sipated by a processor's back-end while maintaining high perfor-mance. We present a dual-cluster, dual-frequency machine comprising a performance oriented cluster and a power-aware one.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:yB1At4FlUx8C",
            "Publisher": "Association for Computing Machinery"
        },
        {
            "Title": "Prediction-based superpage-friendly TLB designs",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7056034/",
            "Abstract": "This work demonstrates that a set of commercial and scale-out applications exhibit significant use of superpages and thus suffer from the fixed and small superpage TLB structures of some modern core designs. Other processors better cope with superpages at the expense of using power-hungry and slow fully-associative TLBs. We consider alternate designs that allow all pages to freely share a single, power-efficient and fast set-associative TLB. We propose a prediction-guided multi-grain TLB design that uses a superpage prediction mechanism to avoid multiple lookups in the common case. In addition, we evaluate the previously proposed skewed TLB [1] which builds on principles similar to those used in skewed associative caches [2]. We enhance the original skewed TLB design by using page size prediction to increase its effective associativity. Our prediction-based multi-grain TLB design delivers more hits \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:p2g8aNsByqUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Branch predictor prediction: A power-aware branch predictor for high-performance processors",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1106813/",
            "Abstract": "We introduce branch predictor prediction (BPP) as a power-aware branch prediction technique for high performance processors. Our predictor reduces branch prediction power dissipation by selectively turning on and off two of the three tables used in the combined branch predictor BPP relies on a small buffer that stores the addresses and the sub-predictors used by the most recent branches executed. Later we refer to this buffer to decide if any of the sub-predictors and the selector could be gated without harming performance. In this paper we study power and performance trade-offs for a subset of SPEC 2k benchmarks. We show that on the average and for an 8-way processor, BPP can reduce branch prediction power dissipation by 28% and 14% compared to non-banked and banked 32k predictors respectively. This comes with a negligible impact on performance (1% max). We show that BPP always reduces \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:Se3iqnhoufwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Value-based deep-learning acceleration",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8259428/",
            "Abstract": "This article summarizes our recent work on value-based hardware accelerators for image classification using Deep Convolutional Neural Networks (CNNs). The presented designs exploit runtime value properties that are difficult or impossible to discern in advance. These include values that are zero or near zero and that prove ineffectual, have reduced yet variable precision needs, or have ineffectual bits. The designs offer a spectrum of choices in terms of area cost, energy efficiency, and relative performance when embedded in server class installations. More importantly, the accelerators reward advances in CNN design that increase the aforementioned properties.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:V3AGJWp-ZtQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Cnvlutin2: Ineffectual-activation-and-weight-free deep neural network computing",
            "Publication year": 2017,
            "Publication url": "https://arxiv.org/abs/1705.00125",
            "Abstract": "We discuss several modifications and extensions over the previous proposed Cnvlutin (CNV) accelerator for convolutional and fully-connected layers of Deep Learning Network. We first describe different encodings of the activations that are deemed ineffectual. The encodings have different memory overhead and energy characteristics. We propose using a level of indirection when accessing activations from memory to reduce their memory footprint by storing only the effectual activations. We also present a modified organization that detects the activations that are deemed as ineffectual while fetching them from memory. This is different than the original design that instead detected them at the output of the preceding layer. Finally, we present an extended CNV that can also skip ineffectual weights.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:LPZeul_q3PIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Behavior and performance of interactive multi-player game servers",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/article/10.1023/A:1025718026938",
            "Abstract": "With the recent explosion in deployment of services to large numbers of customers over the Internet and in global services in general, issues related to the architecture of scalable servers are becoming increasingly important. However, our understanding of these types of applications is currently limited, especially on how well they scale to support large numbers of users. One such, novel, commercial class of applications, are interactive, multi-player game servers. Multi-player games are both an important class of commercial applications (in the entertainment industry) and they can be valuable in understanding the architectural requirements of scalable services. They impose requirements on system performance, scalability, and availability, stressing multiple aspects of the system architecture (e.g., compute cycles and network I/O). Recently there has been a lot of interest on client side issues with respect to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:eQOLeE2rZwMC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Fpraker: A processing element for accelerating neural network training",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3466752.3480106",
            "Abstract": "We present FPRaker, a processing element for composing training accelerators. FPRaker processes several floating-point multiply-accumulation operations concurrently and accumulates their result into a higher precision accumulator. FPRaker boosts performance and energy efficiency during training by taking advantage of the values that naturally appear during training. It processes the significand of the operands of each multiply-accumulate as a series of signed powers of two. The conversion to this form is done on-the-fly. This exposes ineffectual work that can be skipped: values when encoded have few terms and some of them can be discarded as they would fall outside the range of the accumulator given the limited precision of floating-point. FPRaker also takes advantage of spatial correlation in values across channels and uses delta-encoding off-chip to reduce memory footprint and bandwidth. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:ZfRJV9d4-WMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "BranchTap: Reducing Branch Misprediction Penalty through Speculation Control",
            "Publication year": 2006,
            "Publication url": "http://www.eecg.toronto.edu/~moshovos/research/branchtap-taco.pdf",
            "Abstract": "This work proposes BranchTap, a novel checkpoint-aware speculation strategy that temporarily throttles control flow speculation to reduce recovery cost while allowing speculation to proceed when it is likely to boost performance. BranchTap targets high-performance architectures with limited checkpoint resources. This work differs from previous proposals for control flow speculation control primarily in that it accounts for the cost of mispeculation recovery, and in that it proposes dynamic adaptation. BranchTap is orthogonal to the recently proposed checkpoint prediction and intelligent management techniques. For example, this work demonstrates that for a 1K-entry window processor with a First-In-First-Out (FIFO) buffer of just four checkpoints and a confidence-based checkpoint predictor with 1K entries, BranchTap achieves performance that is within 2.53% of that possible with an infinite number of checkpoints. This represents an improvement of 35.6% over using just prediction-based checkpoint allocation.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:hMod-77fHWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Late Breaking Results: Building an On-Chip Deep Learning Memory Hierarchy Brick by Brick",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9218728/",
            "Abstract": "Data accesses between on- and off-chip memories account for a large fraction of overall energy consumption during inference with deep learning networks. We present Boveda, a lossless on-chip memory compression technique for neural networks operating on fixed-point values. Boveda reduces the datawidth used per block of values to be only as long as necessary: since most values are of small magnitude Boveda drastically reduces their footprint. Boveda can be used to increase the effective on-chip capacity, to reduce off-chip traffic, or to reduce the on-chip memory capacity needed to achieve a performance/energy target. Boveda reduces total model footprint to 53%.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:tKAzc9rXhukC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Demystifying GPU microarchitecture through microbenchmarking",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5452013/",
            "Abstract": "Graphics processors (GPU) offer the promise of more than an order of magnitude speedup over conventional processors for certain non-graphics computations. Because the GPU is often presented as a C-like abstraction (e.g., Nvidia's CUDA), little is known about the characteristics of the GPU's architecture beyond what the manufacturer has documented. This work develops a microbechmark suite and measures the CUDA-visible architectural characteristics of the Nvidia GT200 (GTX280) GPU. Various undisclosed characteristics of the processing elements and the memory hierarchies are measured. This analysis exposes undocumented features that impact program performance and correctness. These measurements can be useful for improving performance optimization, analysis, and modeling on this architecture and offer additional insight on the decisions made in developing this GPU.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sprex: A soft processor with runahead execution",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6416786/",
            "Abstract": "There is a growing demand for high-performance computation cores in embedded devices built over reconfigurable hardware. As a result, various soft core architecture techniques have been proposed, each targeting different application classes. This work presents SPREX, an FPGA-friendly Runahead soft processor architecture that targets applications with unstructured instruction level parallelism. The architecture of choice for such applications has traditionally relied on a mix of superscalar, out-of-order, and speculative execution. Unfortunately, the implementation of these techniques does not map well on reconfigurable hardware. This work shows that by exploiting the key characteristics of reconfigurable fabrics, and by tuning the architecture for the embedded environment, a fast and practical Runahead soft processor is viable. Runahead has been shown to offer many of the benefits of conventional \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:70eg2SAEIzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A physical-level study of the compacted matrix instruction scheduler for dynamically-scheduled superscalar processors",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5289240/",
            "Abstract": "This work studies physical-level characteristics of the recently proposed compacted matrix instruction scheduler for dynamically-scheduled, superscalar processors. Previous work focused on the matrix scheduler's architecture and argued in support of its speed and scalability advantages. However, no physical-level implementation or models were reported for it. Using full-custom layouts in a commercial 90 nm fabrication technology, this work investigates the latency and energy variations of the compacted matrix and its accompanying logic as a function of the issue width, the window size, and the number of global recovery checkpoints. This work also proposes an energy optimization that throttles unnecessary pre-charges and evaluations. This optimization reduces energy by 10% and 18% depending on the scheduler size.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:NaGl4SEjCO4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Stripes: Bit-serial deep neural network computing",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7783722/",
            "Abstract": "Motivated by the variance in the numerical precision requirements of Deep Neural Networks (DNNs) [1], [2], Stripes (STR), a hardware accelerator is presented whose execution time scales almost proportionally with the length of the numerical representation used. STR relies on bit-serial compute units and on the parallelism that is naturally present within DNNs to improve performance and energy with no accuracy loss. In addition, STR provides a new degree of adaptivity enabling on-the-fly trade-offs among accuracy, performance, and energy. Experimental measurements over a set of DNNs for image classification show that STR improves performance over a state-of-the-art accelerator [3] from 1.30x to 4.51x and by 1.92x on average with no accuracy loss. STR is 57% more energy efficient than the baseline at a cost of 32% additional area. Additionally, by enabling configurable, per-layer and per-bit precision \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:4fKUyHm3Qg0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "MemAlign: A Memory Structure to Accelerate Gene Sequencing",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8941703/",
            "Abstract": "Since 2003, when the human reference genome was discovered, several applications found gene sequencing a promising mechanism to help improve their results. These include studying hereditary diseases, prenatal monitoring and others. Gene sequencing in its typical configuration consists of several elaborate processing stages, each performed by a separate software package. The intermediate results are transferred via large files between gene sequencing different steps, hence making gene sequencing a processing and I/O demanding task. Taking advantage of advances memory speed and capacity and with the ultimate goal of pipelining the gene sequencing steps and avoiding utilizing file storage to communicate intermediate results, in this paper we present MemAlign, a novel pre-sorted memory structure to pipeline the first and second processing steps of gene sequencing; Alignment and Sort. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:1yQoGdGgb4wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "What limits the operating frequency of a soft processor design",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7032565/",
            "Abstract": "This work systematically explores what limits the operation frequency in a typical general purpose, soft processor design on a modern FPGA. The analysis mirrors a typical design cycle: It starts from a base implementation of a 5-stage pipelined core where correctness, modularity, and speed of development are the primary considerations. The analysis then proceeds in a series of identify-and-then-revise steps. At each step, the analysis identifies the critical path and then \"removes \" it. The result is a list of components and mechanisms that restrict the frequency of operation. A designer would have to cleverly redesign over these paths in order to improve the processor's operating clock frequency. Using the results of this analysis, this work proposes various optimizations to improve the efficiency of some of these components. The optimizations increase the processor clock frequency from 145MHz to 281MHz on Stratix \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:uWQEDVKXjbEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Shapeshifter: Enabling fine-grain data width adaptation in deep learning",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3352460.3358295",
            "Abstract": "We show that selecting a data width for all values in Deep Neural Networks, quantized or not and even if that width is different per layer, amounts to worst-case design. Much shorter data widths can be used if we target the common case by adjusting the data type width at a much finer granularity. We propose ShapeShifter, where we group weights and activations and encode them using a width specific to each group and where typical group sizes vary from 16 to 256 values. The per group widths are selected statically for the weights and dynamically by hardware for the activations. We present two applications of ShapeShifter. In the first, that is applicable to any system, ShapeShifter reduces off-and on-chip storage and communication. This ShapeShifter-based memory compression is simple and low cost yet reduces off-chip traffic to 33% and 36% for 8-bit and 16-bit models respectively. This makes it possible to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:ye4kPcJQO24C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A physical level study and optimization of CAM-based checkpointed register alias table",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5529049/",
            "Abstract": "Using full-custom layouts in 130 nm technology, this work studies how the latency and energy of a checkpointed, CAM-based Register Alias Table (cRAT) vary as a function of the window size, the issue width, and the number of embedded global checkpoints (GCs). These results are compared to those of the SRAM-based RAT (sRAT). Understanding these variations is useful during the early stages of architectural exploration where physical level information is not yet available. It is found that compared to sRAT, cRAT is more sensitive to the number of physical registers and issue width, however, it is less sensitive to the number of GCs. In addition, beyond a certain number of GCs, cRAT becomes faster than its equivalent sRAT. For instance, this is true when a RAT for 64 architectural and 128 physical registers has at least 20 GCs. This work also proposes an energy optimization for the cRAT; this optimization \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:R3hNpaxXUhUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "CHIMAERA: A high-performance architecture with a tightly-coupled reconfigurable functional unit",
            "Publication year": 2000,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/342001.339687",
            "Abstract": "Reconfigurable hardware has the potential for significant performance improvements by providing support for application-specific operations. We report our experience with Chimaera, a prototype system that integrates a small and fast reconfigurable functional unit (RFU) into the pipeline of an aggressive, dynamically-scheduled superscalar processor. Chimaera is capable of performing 9-input/1-output operations on integer data. We discuss the Chimaera C compiler that automatically maps computations for execution in the RFU. Chimaera is capable of: (1) collapsing a set of instructions into RFU operations, (2) converting control-flow into RFU operations, and (3) supporting a more powerful fine-grain data-parallel model than that supported by current multimedia extension instruction sets (for integer operations). Using a set of multimedia and communication applications we show that even with simple optimizations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:d1gkVwhDpl0C",
            "Publisher": "ACM"
        },
        {
            "Title": "A dual grain hit-miss detector for large die-stacked DRAM caches",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6513478/",
            "Abstract": "Die-Stacked DRAM caches offer the promise of improved performance and reduced energy by capturing a larger fraction of an application's working set than on-die SRAM caches. However, given that their latency is only 50% lower than that of main memory, DRAM caches considerably increase latency for misses. They also incur a significant energy overhead for remote lookups in snoop-based multi-socket systems. Ideally, it would be possible to detect in advance that a request will miss in the DRAM cache and thus selectively bypass it. This work proposes a \u201cdual grain filter\u201d which successfully predicts whether an access is a hit or a miss in most cases. Experimental results with commercial and scientific workloads show that a 158KB dual-grain filter can correctly predict data block residency for 85% of all accesses to a 256MB DRAM cache. As a result, average off-die latency with our filter is within 8% of that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:pqnbT2bcN3wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Accurate and complexity-effective spatial pattern prediction",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1410084/",
            "Abstract": "Recent research suggests that there are large variations in a cache's spatial usage, both within and across programs. Unfortunately, conventional caches typically employ fixed cache line sizes to balance the exploitation of spatial and temporal locality, and to avoid prohibitive cache fill bandwidth demands. The resulting inability of conventional caches to exploit spatial variations leads to suboptimal performance and unnecessary cache power dissipation. We describe the spatial pattern predictor (SPP), a cost-effective hardware mechanism that accurately predicts reference patterns within a spatial group (i.e., a contiguous region of data in memory) at runtime. The key observation enabling an accurate, yet low-cost, SPP design is that spatial patterns correlate well with instruction addresses and data reference offsets within a cache line. We require only a small amount of predictor memory to store the predicted \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Evaluating the memory system behavior of smartphone workloads",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6893198/",
            "Abstract": "Modern smartphones comprise several processing and input/output units that communicate mostly through main memory. As a result, memory represents a critical performance bottleneck for smartphones. This work 1  introduces a set of emerging workloads for smartphones and characterizes the performance of several memory controller policies and address-mapping schemes for those workloads. The workloads include high-resolution video conferencing, computer vision algorithms such as upper-body detection and feature extraction, computational photography techniques such as high dynamic range imaging, and web browsing. This work also considers combinations of these workloads that represent possible use cases of future smartphones such as detecting and focusing on people or other objects in live video. While some of these workloads have been characterized before, this is the first work that studies \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:xtRiw3GOFMkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9251854/",
            "Abstract": "Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:_Ybze24A_UAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Practical off-chip meta-data for temporal memory streaming",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4798239/",
            "Abstract": "Prior research demonstrates that temporal memory streaming and related address-correlating prefetchers improve performance of commercial server workloads though increased memory level parallelism. Unfortunately, these prefetchers require large on-chip meta-data storage, making previously-proposed designs impractical. Hence, to improve practicality, researchers have sought ways to enable timely prefetch while locating meta-data entirely off-chip. Unfortunately, current solutions for off-chip meta-data increase memory traffic by over a factor of three. We observe three requirements to store meta-data off chip: minimal off-chip lookup latency, bandwidth-efficient meta-data updates, and off-chip lookup amortized over many prefetches. In this work, we show: (1) minimal off-chip meta-data lookup latency can be achieved through a hardware-managed main memory hash table, (2) bandwidth-efficient updates can \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:QIV2ME_5wuYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Spatial memory streaming",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1150019.1136508",
            "Abstract": "Prior research indicates that there is much spatial variation in applications' memory access patterns. Modern memory systems, however, use small fixed-size cache blocks and as such cannot exploit the variation. Increasing the block size would not only prohibitively increase pin and interconnect bandwidth demands, but also increase the likelihood of false sharing in shared-memory multiprocessors. In this paper, we show that memory accesses in commercial workloads often exhibit repetitive layouts that span large memory regions (e.g., several kB), and these accesses recur in patterns that are predictable through codebased correlation. We propose Spatial Memory Streaming, a practical on-chip hardware technique that identifies codecorrelated spatial access patterns and streams predicted blocks to the primary cache ahead of demand misses. Using cycle-accurate full-system multiprocessor simulation of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:Y0pCki6q_DkC",
            "Publisher": "ACM"
        },
        {
            "Title": "JETTY: Filtering snoops for reduced energy consumption in SMP servers",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/903254/",
            "Abstract": "We propose methods for reducing the energy consumed by snoop requests in snoopy bus-based symmetric multiprocessor (SMP) systems. Observing that a large fraction of snoops do not find copies in many of the other caches, we introduce JETTY, a small, cache-like structure. A JETTY is introduced in-between the bus and the L2 backside of each processor. There it filters the vast majority of snoops that would not find a locally cached copy. Energy is reduced as accesses to the much more energy demanding L2 tag arrays are decreased. No changes in the existing coherence protocol are required and no performance loss is experienced. We evaluate our method on a 4-way SMP server using a set of shared-memory applications. We demonstrate that a very small JETTY filters 74% (average) of all snoop-induced tag accesses that would miss. This results in an average energy reduction of 29% (range: 12% to 40 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:qjMakFHDy7sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Bit-tactical: Exploiting ineffectual computations in convolutional neural networks: Which, why, and how",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1803.03688",
            "Abstract": "We show that, during inference with Convolutional Neural Networks (CNNs), more than 2x to $8x ineffectual work can be exposed if instead of targeting those weights and activations that are zero, we target different combinations of value stream properties. We demonstrate a practical application with Bit-Tactical (TCL), a hardware accelerator which exploits weight sparsity, per layer precision variability and dynamic fine-grain precision reduction for activations, and optionally the naturally occurring sparse effectual bit content of activations to improve performance and energy efficiency. TCL benefits both sparse and dense CNNs, natively supports both convolutional and fully-connected layers, and exploits properties of all activations to reduce storage, communication, and computation demands. While TCL does not require changes to the CNN to deliver benefits, it does reward any technique that would amplify any of the aforementioned weight and activation value properties. Compared to an equivalent data-parallel accelerator for dense CNNs, TCLp, a variant of TCL improves performance by 5.05x and is 2.98x more energy efficient while requiring 22% more area.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:1qzjygNMrQYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Self-contained, accurate precomputation prefetching",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2830772.2830816",
            "Abstract": "This work revisits precomputation prefetching targeting long access latency loads with access patterns that are hard to predict. It presents Ekivolos, a precomputation prefetcher system that automatically builds prefetching slices that contain enough control flow instructions to faithfully and autonomously recreate the program's access behavior without inducing monitoring and execution overhead on the main thread. Ekivolos departs from the traditional notion of creating optimized short slices. In contrast, it shows that even longer slices can run ahead of the main thread and perform useful prefetches as long as they are sufficiently accurate. Ekivolos operates on arbitrary application binaries and takes advantage of the observed execution paths in creating its slices. On a set of emerging workloads Ekivolos outperforms three state-of-the-art hardware prefetchers and previously proposed precomputation-based prefetchers.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:K3LRdlH-MEoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dynamic stripes: Exploiting the dynamic precision requirements of activation values in neural networks",
            "Publication year": 2017,
            "Publication url": "https://arxiv.org/abs/1706.00504",
            "Abstract": "Stripes is a Deep Neural Network (DNN) accelerator that uses bit-serial computation to offer performance that is proportional to the fixed-point precision of the activation values. The fixed-point precisions are determined a priori using profiling and are selected at a per layer granularity. This paper presents Dynamic Stripes, an extension to Stripes that detects precision variance at runtime and at a finer granularity. This extra level of precision reduction increases performance by 41% over Stripes.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:eJXPG6dFmWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Laconic deep learning computing",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1805.04513",
            "Abstract": "We motivate a method for transparently identifying ineffectual computations in unmodified Deep Learning models and without affecting accuracy. Specifically, we show that if we decompose multiplications down to the bit level the amount of work performed during inference for image classification models can be consistently reduced by two orders of magnitude. In the best case studied of a sparse variant of AlexNet, this approach can ideally reduce computation work by more than 500x. We present Laconic a hardware accelerator that implements this approach to improve execution time, and energy efficiency for inference with Deep Learning Networks. Laconic judiciously gives up some of the work reduction potential to yield a low-cost, simple, and energy efficient design that outperforms other state-of-the-art accelerators. For example, a Laconic configuration that uses a weight memory interface with just 128 wires outperforms a conventional accelerator with a 2K-wire weight memory interface by 2.3x on average while being 2.13x more energy efficient on average. A Laconic configuration that uses a 1K-wire weight memory interface, outperforms the 2K-wire conventional accelerator by 15.4x and is 1.95x more energy efficient. Laconic does not require but rewards advances in model design such as a reduction in precision, the use of alternate numeric representations that reduce the number of bits that are \"1\", or an increase in weight or activation sparsity.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:5ugPr518TE4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference",
            "Publication year": 2020,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2020arXiv200503842H/abstract",
            "Abstract": "Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:W5xh706n7nkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reducing OLTP instruction misses with thread migration",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2236584.2236586",
            "Abstract": "During an instruction miss a processor is unable to fetch instructions. The more frequent instruction misses are the less able a modern processor is to find useful work to do and thus performance suffers. Online transaction processing (OLTP) suffers from high instruction miss rates since the instruction footprint of OLTP transactions does not fit in today's L1-I caches. However, modern many-core chips have ample aggregate L1 cache capacity across multiple cores. Looking at the code paths concurrently executing transactions follow, we observe a high degree of repetition both within and across transactions. This work presents TMi a technique that uses thread migration to reduce instruction misses by spreading the footprint of a transaction over multiple L1 caches. TMi is a software-transparent, hardware technique; TMi requires no code instrumentation, and efficiently utilizes available cache capacity. This work \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:RGFaLdJalmkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Proteus: Exploiting numerical precision variability in deep neural networks",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2925426.2926294",
            "Abstract": "This work exploits the tolerance of Deep Neural Networks (DNNs) to reduced precision numerical representations and specifically, their recently demonstrated ability to tolerate representations of different precision per layer while maintaining accuracy. This flexibility enables improvements over conventional DNN implementations that use a single, uniform representation. This work proposes Proteus, which reduces the data traffic and storage footprint needed by DNNs, resulting in reduced energy and improved area efficiency for DNN implementations. Proteus uses a different representation per layer for both the data (neurons) and the weights (synapses) processed by DNNs. Proteus is a layered extension over existing DNN implementations that converts between the numerical representation used by the DNN execution engines and the shorter, layer-specific fixed-point representation used when reading and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:08ZZubdj9fEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Power-aware register renaming",
            "Publication year": 2002,
            "Publication url": "https://www.ece.uvic.ca/~amiralib/courses/ELEC669/register_renaming.pdf",
            "Abstract": "We propose power optimizations for the register renaming unit. Our optimizations reduce power dissipation in two ways. First, they reduce the number of read and write ports that are needed at the register alias table. Second, they reduce the number of internal checkpoints that are required to allow highly-aggressive control speculation and rapid recovery from control flow miss-speculations. To reduce the number of read and write ports we exploit the fact that most instructions do not use the maximum number of source and destination register operands. We also use intra-block dependence detection logic to avoid accessing the register alias table for those operands that have a RAW or a WAW dependence with a preceding, simultaneously decoded instructions. To reduce the number of internal checkpoints we propose out-of-order control flow resolution as an alternative to the conventional method of in-order resolution. We study our optimizations for a set of integer applications from the SPEC2000 and for a multimedia application from mediabench and for an aggressive 8-way superscalar. We find that it is possible to reduce the number of read ports from 24 to 12 and the number of write ports from 8 to 6 with a minor slowdown of 0.5%(worst case slowdown of 2.2%). Furthermore, we find that out-of-order control flow resolution achieves performance that is within 0.6% of that possible with an infinite number of checkpoints (worst case slowdown of 0.8%). We model the maximum power dissipation of the rename unit for a 0.18 um process and demonstrate that our methods can reduce overall rename power by 42%.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:M3ejUd6NZC8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Electronic processor providing direct data transfer between linked data consuming instructions",
            "Publication year": 2003,
            "Publication url": "https://patents.google.com/patent/US6658554B1/en",
            "Abstract": "A data dependence prediction technique is used to establish linkage between two instructions using data so that accessing the data from memory may be bypassed. Instead, the data retrieved in the first data using instruction is temporarily stored in a local register to be used by the second data using instruction. Parallel processing techniques of squashing are used in the event that the prediction is erroneous.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:_FxGoFyzp5QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Temporal instruction fetch streaming",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4771774/",
            "Abstract": "L1 instruction-cache misses pose a critical performance bottleneck in commercial server workloads. Cache access latency constraints preclude L1 instruction caches large enough to capture the application, library, and OS instruction working sets of these workloads. To cope with capacity constraints, researchers have proposed instruction prefetchers that use branch predictors to explore future control flow. However, such prefetchers suffer from several fundamental flaws: their lookahead is limited by branch prediction bandwidth, their accuracy suffers from geometrically-compounding branch misprediction probability, and they are ignorant of the cache contents, frequently predicting blocks already present in L1. Hence, L1 instruction misses remain a bottleneck. We propose temporal instruction fetch streaming (TIFS)-a mechanism for prefetching temporally-correlated instruction streams from lower-level caches \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:4TOpqqG69KYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Noema: Hardware-Efficient Template Matching for Neural Population Pattern Detection",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3466752.3480121",
            "Abstract": "Repeating patterns of activity across neurons is thought to be key to understanding how the brain represents, reacts, and learns. Advances in imaging and electrophysiology allow us to observe activities of groups of neurons in real-time, with ever increasing detail. Detecting patterns over these activity streams is an effective means to explore the brain, and to detect memories, decisions, and perceptions in real-time while driving effectors such as robotic arms, or augmenting and repairing brain function. Template matching is a popular algorithm for detecting recurring patterns in neural populations and has primarily been implemented on commodity systems. Unfortunately, template matching is memory intensive and computationally expensive. This has prevented its use in portable applications, such as neuroprosthetics, which are constrained by latency, form-factor, and energy. We present Noema a dedicated \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:0KyAp5RtaNEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "RegionScout: Exploiting coarse grain sharing in snoop-based coherence",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1431560/",
            "Abstract": "It has been shown that many requests miss in all remote nodes in shared memory multiprocessors. We are motivated by the observation that this behavior extends to much coarser grain areas of memory. We define a region to be a continuous, aligned memory area whose size is a power of two and observe that many requests find that no other node caches a block in the same region even for regions as large as 16K bytes. We propose RegionScout, a family of simple filter mechanisms that dynamically detect most non-shared regions. A node with a RegionScout filter can determine in advance that a request will miss in all remote nodes. RegionScout filters are implemented as a layered extension over existing snoop-based coherence systems. They require no changes to existing coherence protocols or caches and impose no constraints on what can be cached simultaneously. Their operation is completely \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:UeHWp8X0CEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Phantom-btb: a virtualized branch target buffer design",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1508284.1508281",
            "Abstract": "Modern processors use branch target buffers (BTBs) to predict the target address of branches such that they can fetch ahead in the instruction stream increasing concurrency and performance. Ideally, BTBs would be sufficiently large to capture the entire working set of the application and sufficiently small for fast access and practical on-chip dedicated storage. Depending on the application, these requirements are at odds.This work introduces a BTB design that accommodates large instruction footprints without dedicating expensive onchip resources. In the proposed Phantom-BTB (PBTB) design, a conventional BTB is augmented with a virtual table that collects branch target information as the application runs. The virtual table does not have fixed dedicated storage. Instead, it is transparently allocated, on demand, in the on-chip caches, at cache line granularity. The entries in the virtual table are proactively \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:7PzlFSSx8tAC",
            "Publisher": "ACM"
        },
        {
            "Title": "Making address-correlated prefetching practical",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5430739/",
            "Abstract": "Despite a decade of research demonstrating its efficacy, address-correlated prefetching has never been implemented in a shipping processor because it requires megabytes of metadata - too large to store practically on chip. New storage-, latency-, and bandwidth-efficient mechanisms for storing metadata off chip yield a practical design that achieves 90 percent of the performance potential of idealized on-chip metadata storage.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:TQgYirikUcIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Instruction distribution heuristics for quad-cluster, dynamically-scheduled, superscalar processors",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/898083/",
            "Abstract": "We investigate instruction distribution methods for quad-cluster, dynamically-scheduled superscalar processors. We study a variety of methods with different cost, performance and complexity characteristics. We investigate both Pion-adaptive and adaptive methods and their sensitivity both to inter-cluster communication latencies and pipeline depth. Furthermore, we develop a set of models that allow us to identify how well each method attacks issue-bandwidth and inter-cluster communication restrictions. We find that a relatively simple method that changes clusters every other three instructions offers only a 17% performance slowdown compared to a non-clustered configuration operating at the same frequency Moreover; we show that by utilizing adaptive methods it is possible to further reduce this gap down to about 14%. Furthermore, performance appears to be more sensitive to inter-cluster communication \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:IjCSPb-OGe4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Training CNNs faster with Dynamic Input and Kernel Downsampling",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1910.06548",
            "Abstract": "We reduce training time in convolutional networks (CNNs) with a method that, for some of the mini-batches: a) scales down the resolution of input images via downsampling, and b) reduces the forward pass operations via pooling on the convolution filters. Training is performed in an interleaved fashion; some batches undergo the regular forward and backpropagation passes with original network parameters, whereas others undergo a forward pass with pooled filters and downsampled inputs. Since pooling is differentiable, the gradients of the pooled filters propagate to the original network parameters for a standard parameter update. The latter phase requires fewer floating point operations and less storage due to the reduced spatial dimensions in feature maps and filters. The key idea is that this phase leads to smaller and approximate updates and thus slower learning, but at significantly reduced cost, followed by passes that use the original network parameters as a refinement stage. Deciding how often and for which batches the downsmapling occurs can be done either stochastically or deterministically, and can be defined as a training hyperparameter itself. Experiments on residual architectures show that we can achieve up to 23% reduction in training time with minimal loss in validation accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:VL0QpB8kHFEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Wormhole branch prediction using multidimensional histories",
            "Publication year": 2014,
            "Publication url": "https://www.jilp.org/cbp2014/slides/JorgeAlbericio.pdf",
            "Abstract": "Wormhole branch prediction using multi-dimensional histories Page 1 Wormhole branch \nprediction using multi-dimensional histories Jorge Albericio, Joshua San Miguel, Natalie Enright \nJerger, and Andreas Moshovos Page 2 2 Page 3 3 foreach frame: foreach object: if \ndistance(object, p) < threshold: { /* do something */ } Motivation Page 4 3 foreach frame: foreach \nobject: if distance(object, p) < threshold: { /* do something */ } Motivation Page 5 3 foreach frame: \nforeach object: if distance(object, p) < threshold: { /* do something */ } Motivation Page 6 3 \nforeach frame: foreach object: if distance(object, p) < threshold: { /* do something */ } Motivation \nPage 7 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 Multidimensional histories 4 foreach frame: foreach object: \nif distance(object, p) < threshold: { /* do something */ } Page 8 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 \nMultidimensional histories 4 foreach frame: foreach object: if distance(object, p) < threshold: { /\u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:CHSYGLWDkRkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Coarse-grain coherence tracking: RegionScout and region coherence arrays",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1603499/",
            "Abstract": "Cache-coherent shared-memory multiprocessors have wide-ranging applications, from commercial transaction processing and database services to large-scale scientific computing. Coarse-grain coherence tracking (CGCT) is a new technique that extends a conventional coherence mechanism and optimizes coherence enforcement. It monitors the coherence status of large regions of memory and uses that information to avoid unnecessary broadcasts and filter unnecessary cache tag lookups, thus improving system performance and power consumption. This article presents two CGCT implementations, RegionScout and Region Coherence Arrays, and provides simulation results for a broadcast-based multiprocessor system running commercial, scientific, and multiprogrammed workloads",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:8k81kl-MbHgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Characterizing sources of ineffectual computations in deep learning networks",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8695654/",
            "Abstract": "Hardware accelerators for inference with neural networks can take advantage of the properties of data they process. Performance gains and reduced memory bandwidth during inference have been demonstrated by using narrower data types [1] [2] and by exploiting the ability to skip and compress values that are zero [3]-[6]. Similarly useful properties have been identified at a lower-level such as varying precision requirements [7] and bit-level sparsity [8] [9]. To date, the analysis of these potential sources of superfluous computation and communication has been constrained to a small number of older Convolutional Neural Networks (CNNs) used for image classification. It is an open question as to whether they exist more broadly. This paper aims to determine whether these properties persist in: (1) more recent and thus more accurate and better performing image classification networks, (2) models for image \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:WA5NYHcadZ8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "RECAP: A region-based cure for the common cold (cache)",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6522309/",
            "Abstract": "Virtualization has become a magic bullet to increase utilization, improve security, lower costs, and reduce management overheads. In many scenarios, the number of virtual machines consolidated onto a single processor has grown even faster than the number of hardware threads. This results in multiprogrammed virtualization where many virtual machines time-share a single processor core. Such fine-grain sharing comes at a cost; each time a virtual machine gets scheduled by the hypervisor, it effectively begins with a \u201ccold\u201d cache, since any cache blocks it accessed in the past have likely been evicted by other virtual machines. Recently, cache restoration prefetchers have been shown to reduce the cold cache effects caused by multiprogrammed virtualization. However, these prefetchers waste large amounts of bandwidth prefetching many useless blocks and writing and reading metadata to and from memory \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:RYcK_YlVTxYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An Architectural Approach to Characterizing and Eliminating Sources of Inefficiency in a Soft Processor Design",
            "Publication year": 2014,
            "Publication url": "https://www.computer.org/csdl/proceedings-article/fccm/2014/5111a169/12OmNwKGAqy",
            "Abstract": "This work presents a computer architectural approach at identifying sources of inefficiency in a typical 5-stage pipelined, general purpose soft processor implementation on a modern FPGA. The analysis starts with a naive implementation of the processor which focuses on correctness, modularity, and speed of development. It then extracts a list of components and mechanisms in the processor pipeline as sources of inefficiency. A designer would have to cleverly redesign these components in order to improve the processor's operating clock frequency. Using the results of this analysis, this work proposes various optimizations to improve the efficiency of such components. The optimizations increase the processor clock frequency from 145MHz to 281MHz on Stratix III devices, while overall instruction processing throughput increases by 80%.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:P5F9QuxV20EC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "Slice-processors: an implementation of operation-based prediction",
            "Publication year": 2001,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/377792.377856",
            "Abstract": "We describe the Slice Processor micro-architecture that implements a generalized operation-based prefetching mechanism. Operation-based prefetchers predict the series of operations, or the computation slice that can be used to calculate forthcoming memory references. This is in contrast to outcome-based predictors that exploit regularities in the (address) outcome stream. Slice processors are a generalization of existing operation-based prefetching mechanisms such as stream buffers where the operation itself is fixed in the design (eg, address+ stride). A slice processor dynamically identifies frequently missing loads and extracts on-the-fly the relevant address computation slices. Such slices are then executed in-parallel with the main sequential thread prefetching memory data. We describe the various support structures and emphasize the design of the slice detection mechanism. We demonstrate that a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:zYLM7Y9cAGgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "DPRed: Making Typical Activation and Weight Values Matter In Deep Learning Computing",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1804.06732",
            "Abstract": "We show that selecting a single data type (precision) for all values in Deep Neural Networks, even if that data type is different per layer, amounts to worst case design. Much shorter data types can be used if we target the common case by adjusting the precision at a much finer granularity. We propose Dynamic Precision Reduction (DPRed), where we group weights and activations and encode them using a precision specific to each group. The per group precisions are selected statically for the weights and dynamically by hardware for the activations. We exploit these precisions to reduce: 1) off-chip storage and off- and on-chip communication, and 2) execution time. DPRed compression reduces off-chip traffic to nearly 35% and 33% on average compared to no compression respectively for 16b and 8b models. This makes it possible to sustain higher performance for a given off-chip memory interface while also boosting energy efficiency. We also demonstrate designs where the time required to process each group of activations and/or weights scales proportionally to the precision they use for convolutional and fully-connected layers. This improves execution time and energy efficiency for both dense and sparse networks. We show the techniques work with 8-bit networks, where 1.82x and 2.81x speedups are achieved for two different hardware variants that take advantage of dynamic precision variability.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:J-pR_7NvFogC",
            "Publisher": "Unknown"
        },
        {
            "Title": "IDEAL: Image denoising accelerator",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8686525/",
            "Abstract": "Computational imaging pipelines (CIPs) convert the raw output of imaging sensors into the high-quality images that are used for further processing. This work studies how Block-Matching and 3D filtering (BM3D), a state-of-the-art denoising algorithm can be implemented to meet the demands of user-interactive (UI) applications. Denoising is the most computationally demanding stage of a CIP taking more than 95% of time on a highly-optimized software implementation [29]. We analyze the performance and energy consumption of optimized software implementations on three commodity platforms and find that their performance is inadequate. Accordingly, we consider two alternatives: a dedicated accelerator, and running recently proposed Neural Network (NN) based approximations of BM3D [9, 27] on an NN accelerator. We develop Image DEnoising AcceLerator(IDEAL), a hardware BM3D accelerator which \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:q3oQSFYPqjQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "SEPAS: A highly accurate energy-efficient branch predictor",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1013235.1013250",
            "Abstract": "Designers have invested much effort in developing accurate branch predictors with short learning periods. Such techniques rely on exploiting complex and relatively large structures. Although exploiting such structures is necessary to achieve high accuracy and fast learning, once the short learning phase is over, a simple structure can efficiently predict the branch outcome for the majority of branches. Moreover, for a large number of branches, once the branch reaches the steady state phase, updating the branch predictor unit is unnecessary since there is already enough information available to the predictor to predict the branch outcome accurately. Therefore, aggressive usage of complex large branch predictors appears to be inefficient since it results in unnecessary energy consumption. In this work we introduce Selective Predictor Access (SEPAS) to exploit this design inefficiency. SEPAS uses a simple power \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:0EnyYjriUFMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Mechanisms for store-wait-free multiprocessors",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1250662.1250696",
            "Abstract": "Store misses cause significant delays in shared-memory multiprocessors because of limited store buffering and ordering constraints required for proper synchronization. Today, programmers must choose from a spectrum of memory consistency models that reduce store stalls at the cost of increased programming complexity. Prior research suggests that the performance gap among consistency models can be closed through speculation--enforcing order only when dynamically necessary. Unfortunately, past designs either provide insufficient buffering, replace all stores with read-modify-write operations, and/or recover from ordering violations via impractical fine-grained rollback mechanisms.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:WF5omc3nYNoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Optimizing memory translation emulation in full system emulators",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2686034",
            "Abstract": "The emulation speed of a full system emulator (FSE) determines its usefulness. This work quantitatively measures where time is spent in QEMU [Bellard 2005], an industrial-strength FSE. The analysis finds that memory emulation is one of the most heavily exercised emulator components. For workloads studied, 38.1% of the emulation time is spent in memory emulation on average, even though QEMU implements a software translation lookaside buffer (STLB) to accelerate dynamic address translation. Despite the amount of time spent in memory emulation, there has been no study on how to further improve its speed. This work analyzes where time is spent in memory emulation and studies the performance impact of a number of STLB optimizations. Although there are several performance optimization techniques for hardware TLBs, this work finds that the trade-offs with an STLB are quite different compared to those \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:SP6oXDckpogC",
            "Publisher": "ACM"
        },
        {
            "Title": "Tensordash: Exploiting sparsity to accelerate deep neural network training",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9251995/",
            "Abstract": "TensorDash is a hardware-based technique that enables data-parallel MAC units to take advantage of sparsity in their input operand streams. When used to compose a hardware accelerator for deep learning, TensorDash can speedup the training process while also increasing energy efficiency. TensorDash combines a low-cost sparse input operand interconnect with an area-efficient hardware scheduler. The scheduler can effectively extract sparsity in the activations, the weights, and the gradients. Over a wide set of state-of-the-art models covering various applications, TensorDash accelerates the training process by 1.95\u00d7 while being 1.5\u00d7 more energy efficient when incorporated on top of a Tensorcore-based accelerator at less than 5% area overhead. TensorDash is datatype agnostic and we demonstrate it with IEEE standard mixed-precision floating-point units and a popular optimized for machine learning \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:_Re3VWB3Y0AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Pointy: a hybrid pointer prefetcher for managed runtime systems",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7842922/",
            "Abstract": "This work proposes Pointy, a software assisted hardware pointer prefetcher for Java applications. Pointy exploits the strengths of both software and hardware. Its runtime software component communicates points-to relationships between objects to the underlying hardware. This points-to information is maintained and tracked in any managed runtime that implements automatic garbage collection. Pointy stores the object connectivity information in a separate hardware structure and uses it to generate timely pointer prefetches. To achieve a low-cost hardware implementation, Pointy spills the object metadata to the conventional memory hierarchy and retrieves it only when needed. Taking advantage of its hybrid design, Pointy can selectively communicate points-to metadata to the hardware based on class profiling that is readily available at the runtime level, while impractical to extract at the hardware level \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:YFjsv_pBGBYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Asymmetric-frequency clustering: a power-aware back-end for high-performance processors",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/566408.566474",
            "Abstract": "We introduce asymmetric frequency clustering (AFC), a micro-architectural technique that reduces the dynamic power dissipated by a processor's back-end while maintaining high performance. We present a dual-cluster, dual-frequency machine comprising a performance oriented cluster and a power-aware one. The power-aware cluster operates at half the frequency of the performance oriented cluster and uses a lower voltage supply. We show that this organization significantly reduces back-end power dissipation by executing non-performance-critical instructions in the power-aware cluster. AFC localizes the two frequency/voltage domains. Consequently, it mitigates many of the complexities associated with maintaining multiple supply voltage and frequency domains on the same chip. Key to the success of this technique are methods that assign as many instructions as possible to the slower/lower power cluster \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:aqlVkmm33-oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Checkpointing alternatives for high-performance, power-aware processors",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1231886/",
            "Abstract": "High performance processors use checkpointing to rapidly recover from branch mispredictions and possibly other exceptions. We demonstrate that conventional checkpointing becomes unattractive in terms of resource and power requirements for future generation processors. We propose out-of-order checkpoint release and checkpoint prediction, two alternatives that require significantly less resources and power while maintaining high-performance. We demonstrate their utility at the register alias table (RAT). Our methods reduce the number of RAT checkpoints to 1/3 (from 48 down to 16) for an aggressive, 8-way superscalar processor with a 256-entry instruction window. Using a 0.18 /spl mu/m process model we estimate that RAT power is reduced by 24%.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:hqOjcs7Dif8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Tartan: Accelerating Fully-Connected and Convolutional Layers in Deep Learning Networks by Exploiting Numerical Precision Variability",
            "Publication year": 2016,
            "Publication url": "https://openreview.net/forum?id=Hy-lMNqex",
            "Abstract": "Tartan {TRT} a hardware accelerator for inference with Deep Neural Networks (DNNs) is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied, TRT outperforms a state-of-the-art bit-parallel accelerator by 1.90 x without any loss in accuracy while it is 1.17 x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2.04 x faster and 1.25 x more energy efficient than the bit-parallel accelerator. This revision includes post-layout results and a better configuration that processes 2bits at time resulting in better efficiency and lower area overhead.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:fQNAKQ3IYiAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An efficient non-blocking data cache for soft processors",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5695275/",
            "Abstract": "Soft processors often use data caches to reduce the gap between processor and main memory speeds. To achieve high efficiency, simple, blocking caches are used. Such caches are not appropriate for processor designs such as run ahead and out-of-order execution that require non-blocking caches to tolerate main memory latencies. Conventional non-blocking caches are expensive and slow on FPGAs as they use content-addressable memories (CAMs). This work exploits key properties of run ahead execution and demonstrates an FPGA-friendly non-blocking cache design that does not require CAMs. A non-blocking 4KB cache operates at 329MHz on Stratix III FPGAs while it uses only 270 logic elements. A 32KB non-blocking cache operates at 278Mhz and uses 269 logic elements.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:_Qo2XoVZTnwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Accelerator for deep neural networks",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US20200125931A1/en",
            "Abstract": "A system for bit-serial computation in a neural network is described. The system may be embodied on an integrated circuit and include one or more bit-serial tiles for performing bit-serial computations in which each bit-serial tile receives input neurons and synapses, and communicates output neurons. Also included is an activation memory for storing the neurons and a dispatcher and a reducer. The dispatcher reads neurons and synapses from memory and communicates either the neurons or the synapses bit-serially to the one or more bit-serial tiles. The other of the neurons or the synapses are communicated bit-parallelly to the one or more bit-serial tiles, or according to a further embodiment, may also be communicated bit-serially to the one or more bit-serial tiles. The reducer receives the output neurons from the one or more tiles, and communicates the output neurons to the activation memory.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:Fu2w8maKXqMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Gene sequencing: where time goes",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8573474/",
            "Abstract": "Next Generation Sequencing (NGS) processes raw DNA data samples to extract differences between a genome under investigation and a reference genome [1]. This study profiles the execution time of a state-of-the-art NGS software pipeline. This analysis can inform software-or hardware-based NGS acceleration efforts. We find that two steps dominate execution time, Read Alignment (RA) and Variant Calling (VC). We further analyzed the runtime behavior of these two steps at the microarchitecture level. We find that I/O is not a major bottleneck and that parallelization can effectively accelerate only RA. Further analysis shows which functions within each step dominate execution time. Follow up acceleration work may thus focus on further analyzing these steps.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:Y5dfb0dijaUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Instruction flow-based front-end throttling for power-aware high-performance processors",
            "Publication year": 2001,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1145/383082.383088",
            "Abstract": "We present a number of power-aware instruction front-end (fetch/decode) throttling methods for high-performance dynamically-scheduled superscalar processors. Our methods reduce power dissipation by selectively turning on and off instruction fetch and decode. Moreover, they have a negligible impact on performance as they deliver instructions just in time for exploiting the available parallelism. Previously proposed front-end throttling methods rely on branch prediction confidence estimation. We introduce a new class of methods that exploit information about instruction flow (rate of instructions passing through stages). We show that our methods can boost power savings over previously proposed methods. In particular, for an 8-way processor a combined method reduces traffic by 14%, 20%, 6% and 6% for the fetch, decode, issue and complete stages respectively while performance remains mostly unaffected \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:W7OEmFMy1HYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Predictor virtualization",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1353535.1346301",
            "Abstract": "Many hardware optimizations rely on collecting information about program behavior at runtime. This information is stored in lookup tables. To be accurate and effective, these optimizations usually require large dedicated on-chip tables. Although technology advances offer an increased amount of on-chip resources, these resources are allocated to increase the size of on-chip conventional cache hierarchies.This work proposes Predictor Virtualization, a technique that uses the existing memory hierarchy to emulate large predictor tables. We demonstrate the benefits of this technique by virtualizing a state-of-the-art data prefetcher. Full-system, cycle-accurate simulations demonstrate that the virtualized prefetcher preserves the performance benefits of the original design, while reducing the on-chip storage dedicated to the predictor table from 60KB down to less than one kilobyte.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:MXK_kJrjxJIC",
            "Publisher": "ACM"
        },
        {
            "Title": "Towards a viable out-of-order soft core: Copy-free, checkpointed register renaming",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5272544/",
            "Abstract": "As a step torward a viable, single-issue out-of-order soft core, this work presents copy-free checkpointing (CFC), an FPGA-friendly register renaming design. CFC supports speculative execution by implementing checkpoint recovery. Compared against the best conventional register renaming implementation CFC requires 7.5x to 6.4x fewer LUTs and is at least 10% faster.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:4DMP91E08xMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Branchtap: Improving performance with very few checkpoints through adaptive speculation control",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1183401.1183409",
            "Abstract": "Checkpoint prediction and intelligent management have been recently proposed for reducing the number of coarse-grain checkpoints needed to achieve high performance through speculative execution. In this work, we take a closer look at various checkpoint prediction and management alternatives, comparing their performance and requirements as the scheduler window size increases. We also study a few additional design choices. The key contribution of this work is BranchTap, a novel checkpoint-aware speculation strategy that temporarily throttles speculation to reduce recovery cost while allowing speculation to proceed when it is likely to boost performance. BranchTap dynamically adapts to application behavior. We demonstrate that for a 1K-entry window processor with a FIFO of just four checkpoints, our adaptive speculation control mechanism leads to an average performance degradation of just 1.49 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:_kc_bZDykSQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Image Signal Processors on FPGAs",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6861620/",
            "Abstract": "An Image Signal Processor (ISP) converts raw imaging sensor data into a format appropriate for further processing and human inspection. This work explores FPGA-based ISP designs considering specialized and programmable implementations and proposes an ISP using a programmable generic processing unit with comparable performance versus the dedicated implementations.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:1sJd4Hv_s6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Proteus: Exploiting precision variability in deep neural networks",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167819117300741",
            "Abstract": "This work investigates how using reduced precision data in Deep Neural Networks (DNNs) affects network accuracy during classification. We observe that the tolerance of DNNs to reduced precision data not only varies across networks, but also within networks. We study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. To exploit these low precision configurations, this work proposes Proteus, which reduces the data traffic and storage footprint needed by DNNs, resulting in reduced energy for DNN implementations. Proteus uses a different representation per layer for both the data (neuron activations) and the weights (synapses) processed by DNNs. Proteus is a layered extension over existing DNN implementations that maintains the native precision of the compute engine by converting to and from a fixed-point \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:VOx2b1Wkg3QC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "QTrace: An interface for customizable full system instrumentation",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6557159/",
            "Abstract": "This work presents QTrace, an open-source instrumentation extension API developed on top of QEMU. QTrace instruments unmodified applications and OS binaries for uni- and multi-processor systems, enabling custom, full-system instrumentation tools for the x86 guest architecture. Computer architects can use QTrace to study whole program execution including system-level code. This paper motivates the need for QTrace, illustrates what QTrace can do, and discusses how QEMU was modified to implement QTrace.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:fPk4N6BV_jEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Neural network processing element",
            "Publication year": 2021,
            "Publication url": "https://patents.google.com/patent/US20210125046A1/en",
            "Abstract": "Described is a neural network accelerator tile. It includes an activation memory interface for interfacing with an activation memory to receive a set of activation representations and a weight memory interface for interfacing with a weight memory to receive a set of weight representations, and a processing element. The processing element is configured to implement a one-hot encoder, a histogrammer, an aligner, a reducer, and an accumulation sub-element which process the set of activation representations and the set of weight representations to produce a set of output representations.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:t7zJ5fGR-2UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tartan: Accelerating fully-connected and convolutional layers in deep learning networks by exploiting numerical precision variability",
            "Publication year": 2017,
            "Publication url": "https://arxiv.org/abs/1707.09068",
            "Abstract": "Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks (DNNs), is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied, TRT outperforms a state-of-the-art bit-parallel accelerator by 1:90x without any loss in accuracy while it is 1:17x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2:04x faster and 1:25x more energy efficient than a conventional bit-parallel accelerator. A Tartan configuration that processes 2-bits at time, requires less area than the 1-bit configuration, improves efficiency to 1:24x over the bit-parallel baseline while being 73% faster for convolutional layers and 60% faster for fully-connected layers is also presented.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:BrmTIyaxlBUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A case for asymmetric-cell cache memories",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1498843/",
            "Abstract": "In this paper, we make the case for building high-performance asymmetric-cell caches (ACCs) that employ recently-proposed asymmetric SRAMs to reduce leakage proportionally to the number of resident zero bits. Because ACCs target memory value content (independent of cell activity and access patterns), they complement prior proposals for reducing cache leakage that target memory access characteristics. Through detailed simulation and leakage estimation using a commercial 0.13-/spl mu/m CMOS process model, we show that: 1) on average 75% of resident data cache bits and 64% of resident instruction cache bits are zero; 2) while prior research carefully evaluated the fraction of accessed zero bytes, we show that a high fraction of accessed zero bytes is neither a necessary nor a sufficient condition for a high fraction of resident zero bits; 3) the zero-bit program behavior persists even when we restrict our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:KlAtU1dfN6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Memory dependence speculation tradeoffs in centralized, continuous-window superscalar processors",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/824359/",
            "Abstract": "We consider a variety of dynamic, hardware-based methods for exploiting load/store parallelism, including mechanisms that use memory dependence speculation. While previous work has also investigated such methods, this has been done primarily for split, distributed window processor models. We focus on centralized continuous-window processor models (the common configuration today). We confirm that exploiting load/store parallelism can greatly improve performance. Moreover, we show that much of this performance potential can be captured if addresses of the memory locations accessed by both loads and stores can be used to schedule loads. However, using addresses to schedule load execution may not always be an option due to complexity, latency, and cost considerations. For this reason, we also consider configurations that use just memory dependence speculation to guide load execution. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:YOwf2qJgpHMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "BarTLB: Barren page resistant TLB for managed runtime languages",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6974692/",
            "Abstract": "This work observes that many translation lookaside buffer (TLB) misses in Java workloads originate from barren pages. That is, pages that contain mostly dead objects sprinkled with only a few live objects. Barren pages experience only a few accesses every time they are touched thrashing a conventional TLB. This work characterizes the barren page phenomenon and proposes (1) a low-cost barren page identification technique, and (2) two simple, low-cost techniques for improving TLB performance: (a) The Barren Page First (BPF) replacement policy extends an existing TLB replacement policy to prefer barren pages on evictions. (b) Selective In-Cache Translation Caching (SICTC) avoids installing barren pages in the TLB by augmenting one way of a virtually-indexed, physically-tagged L1 data cache with virtual tags. For all workloads considered BPF and SICTC not only prove robust but also improve \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:dshw04ExmUIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A framework for coarse-grain optimizations in the on-chip memory hierarchy",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4408265/",
            "Abstract": "Current on-chip block-centric memory hierarchies exploit access patterns at the fine-grain scale of small blocks. Several recently proposed techniques for coherence traffic reduction and prefetching suggest that further useful patterns emerge with a macroscopic, coarse-grain view. To exploit coarse- grain behavior, previous work extended conventional caches with additional coarse-grain tracking and management structures considerably increasing overall cost and complexity. This paper demonstrates that as multi-megabyte caches have become commonplace, coarse-grain tracking and management no longer needs to be an afterthought. This functionality comes \"for free\" via RegionTracker. RegionTracker is a dual-grain cache design that maintains block-level communication while directly supporting coarse-grain tracking and management. Compared to a block-centric conventional cache of the same data \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:Zph67rFs4hoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Accelerator for deep neural networks",
            "Publication year": 2019,
            "Publication url": "https://patents.google.com/patent/US20190205740A1/en",
            "Abstract": "Described is a system, integrated circuit and method for reducing ineffectual computations in the processing of layers in a neural network. One or more tiles perform computations where each tile receives input neurons, offsets and synapses, and where each input neuron has an associated offset. Each tile generates output neurons, and there is also an activation memory for storing neurons in communication with the tiles via a dispatcher and an encoder. The dispatcher reads neurons from the activation memory and communicates the neurons to the tiles and reads synapses from a memory and communicates the synapses to the tiles. The encoder receives the output neurons from the tiles, encodes them and communicates the output neurons to the activation memory. The offsets are processed by the tiles in order to perform computations only on non-zero neurons. Optionally, synapses may be similarly processed to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:WqliGbK-hY8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "SW+: On Accelerating Smith-Waterman Execution of GATK HaplotypeCaller",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-63061-4_13",
            "Abstract": "Next Generation sequencing is widely used today in several applications such as in studying hereditary diseases or in prenatal genetic testing. Genome Analysis ToolKit (GATK) workflow is currently the best practice flow in use in industry and academia. Variant Calling, the last step in the GATK pipeline, is performed by GATK HaplotypeCaller. It is one of the most time consuming steps in the whole pipeline. In this paper, we investigated the Smith-Waterman implementation of HaplotyeCaller and achieved an up to 40% reduction in Smith-Waterman execution of the HaplotypeCaller by proposing a new optimization where when possible we conclude the Smith-Waterman results by running a simpler linear comparison function. The optimization reduces the HaplotypeCaller run time by up to 10%.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:-_dYPAW6P2MC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Toward virtualizing branch direction prediction",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6176514/",
            "Abstract": "This work introduces a new branch predictor design that increases the perceived predictor capacity without increasing its delay by using a large virtual second-level table allocated in the second-level caches. Virtualization is applied to a state-of-the-art multi-table branch predictor. We evaluate the design using instruction count as proxy for timing on a set of commercial workloads. For a predictor whose size is determined by access delay constraints, accuracy can be improved by 8.7%. Alternatively, the design can be used to achieve the same accuracy as a non-virtualized design while using 25% less dedicated storage.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:TFP_iSt0sucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Asymmetric-cell caches: Exploiting bit value biases to reduce leakage power in deep-submicron, high-performance caches",
            "Publication year": 2002,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.9810&rep=rep1&type=pdf",
            "Abstract": "We propose a novel approach that leverages circuit-and architecture-level techniques to drastically reduce leakage power dissipation in high-performance caches even when most of the cache cells are actively used. We observe that the cache resident memory values of ordinary programs exhibit a strong bias towards zero or one at the bit level. We introduce a family of high-speed dual-Vt SRAM cell designs that exploit this bit-level bias to reduce leakage power while maintaining low access latency. The key characteristic of this cell family is asymmetry: Leakage power dissipation depends on the actual bit value stored. In the preferred state, the leakage power is smaller by as much as approximately 10x and comparable to that of an high-Vt cell. Asymmetry is also key to maintaining high performance reads (the main disadvantage of an high-Vt cell). We propose two asymmetric-cell cache (ACC) designs. The first is statically biased towards the zero bit value. The other uses run-time selective inversion to increase the number of zero-holding bits. We evaluate our designs using the SPEC2000 benchmarks and for a commercial 0.13\u00b5, 1.2 V CMOS technology. We find that for most programs the majority of memory bits are zero with the actual fraction varying from 52% to 88% for the level one data cache. We also find that this bias is less evident in the instruction stream (around 60% on the average). Using selective inversion, it is possible to further increase the fraction of zeroholding bits by another 6% and 11% for the level one data and instruction caches respectively. Overall, for one cell design, leakage power is reduced by 96% and 94% for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:Wp0gIr-vW9MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bitpruning: Learning bitlengths for aggressive and accurate quantization",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2002.03090",
            "Abstract": "Neural networks have demonstrably achieved state-of-the art accuracy using low-bitlength integer quantization, yielding both execution time and energy benefits on existing hardware designs that support short bitlengths. However, the question of finding the minimum bitlength for a desired accuracy remains open. We introduce a training method for minimizing inference bitlength at any granularity while maintaining accuracy. Namely, we propose a regularizer that penalizes large bitlength representations throughout the architecture and show how it can be modified to minimize other quantifiable criteria, such as number of operations or memory footprint. We demonstrate that our method learns thrifty representations while maintaining accuracy. With ImageNet, the method produces an average per layer bitlength of 4.13, 3.76 and 4.36 bits on AlexNet, ResNet18 and MobileNet V2 respectively, remaining within 2.0%, 0.5% and 0.5% of the base TOP-1 accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:UHK10RUVsp4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cost-effective, highperformance giga-scale checkpoint/restore",
            "Publication year": 2004,
            "Publication url": "http://www.eecg.utoronto.ca/~moshovos/research/gigacr.pdf",
            "Abstract": "This work proposes a novel checkpoint store compression method for giga-scale, coarse-grain checkpoint/restore. This mechanism can be useful for debugging, post-mortem analysis and error-recovery. The effectiveness of our compression method lies in exploiting value locality in the memory data and address streams. Previously proposed dictionary-based hardware compressors exploit the same properties however they are expensive and relatively slow. We observe that because program behavior is typically not very random much simpler mechanisms can offer most of the compression benefits. We propose three compressors that exploit value locality using very small direct mapped structures. Our compressors require few resources, can be easily pipelined and can process one full block per processor cycle. We study two uses of our compressors for post-mortem analysis:(1) Using them alone, and (2) using them in-series with a dictionary-based compressor. When used alone their offer relatively competitive compression rates in most cases. We demonstrate that when combined with previously proposed hardware-based compression methods, our compressors improve overall compression rates while significantly reducing on-chip buffer requirements. Specifically, with an on-chip buffer of just 1K bytes a combination of our compressor with a dictionary-based compressor results into an overall performance slowdown of just 1.6% on the average for storing checkpoints to main memory. Moreover, this combination reduces checkpoints to 34% of their original size. The dictionary-based compressor alone even when used with a 64Kbyte on-chip \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:hC7cP41nSMkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exploiting coarse grain non-shared regions in snoopy coherent multiprocessors",
            "Publication year": 2003,
            "Publication url": "https://www.academia.edu/download/30659776/region-TR.pdf",
            "Abstract": "It has been shown that many requests miss in all remote nodes in shared memory multiprocessors. We are motivated by the observation that this behavior extends to much coarser grain areas of memory. We define a region to be a continuous, aligned memory area whose size is a power of two and observe that many requests find that no other node caches a block in the same region even for regions as large as 16K bytes (it has already been known that this phenomenon applies to the special cases of a block or a page). We propose RegionScout, a family of simple filter mechanisms that dynamically detect most non-shared regions. A node with a RegionScout filter can determine in advance that a request will miss in all remote nodes. RegionScout filters are implemented as a layered extension over existing snoop-based coherence systems. They require no changes to existing coherence protocols or caches and impose no constraints on what can be cached simultaneously. Their operation is completely transparent to software and the operating system. RegionScout filters require little additional storage and a single additional global signal. These characteristics are made possible by utilizing imprecise information about the regions cached in each node. Since they rely on dynamically collected information RegionScout filters can adapt to changing sharing patterns. We present two applications of RegionScout: In the first RegionScout is used to avoid broadcasts for non-shared regions thus reducing bandwidth. In the second RegionScout is used to avoid snoop induced tag lookups thus reducing energy.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:L8Ckcad2t8MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "QTrace: a framework for customizable full system instrumentation",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7095810/",
            "Abstract": "This work presents QTrace, an open-source instrumentation extension API for QEMU (1) that can instrument unmodified applications and OS binaries for uni- and multi-processor systems. QTrace facilitates the development of custom, full-system instrumentation tools for the X86 guest architecture enabling statistics collection and program execution studies including system-level code. This paper: illustrates QTrace's API through instrumentation examples, discusses how QEMU was modified to implement QTrace, explains the validation testing procedures, shows QTrace's usefulness in comparison to a user-level binary instrumentation tool in workloads that spend significant time in the kernel, and illustrates that QTrace does not impose a significant performance penalty. Experiments show that for an instruction count plug-in, QTrace is 12.2X slower than PIN [2], a user-level only instrumentation tool, and 4.1X faster \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:u9iWguZQMMsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Memory controller design under cloud workloads",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7581279/",
            "Abstract": "This work studies the behavior of state-of-the-art memory controller designs when executing scale-out workloads. It considers memory scheduling techniques, memory page management policies, the number of memory channels, and the address mapping scheme used. Experimental measurements demonstrate: 1) Several recently proposed memory scheduling policies are not a good match for these scale-out workloads. 2) The relatively simple First-Ready-First-Come- First-Served (FR-FCFS) policy performs consistently better, and 3) for most of the studied workloads, the even simpler First-Come-First-Served scheduling policy is within 1% of FRFCFS. 4) Increasing the number of memory channels offers negligible performance benefits, e.g., performance improves by 1.7% on average for 4-channels vs. 1-channel. 5) 77%- 90% of DRAM rows activations are accessed only once before closure. These observation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:geHnlv5EZngC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Laconic deep learning inference acceleration",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8980351/",
            "Abstract": "We present a method for transparently identifying ineffectual computations during inference with Deep Learning models. Specifically, by decomposing multiplications down to the bit level, the amount of work needed by multiplications during inference can be potentially reduced by at least 40x across a wide selection of neural networks (8b and 16b). This method produces numerically identical results and does not affect overall accuracy. We present Laconic, a hardware accelerator that implements this approach to boost energy efficiency for inference with Deep Learning Networks. Laconic judiciously gives up some of the work reduction potential to yield a low-cost, simple, and energy efficient design that outperforms other state-of-the-art accelerators: an optimized DaDianNao-like design [13], Eyeriss [15], SCNN [71], Pragmatic [3], and BitFusion [83]. We study 16b, 8b, and 1b/2b fixed-point quantized models.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:5awf1xo2G04C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Design space exploration of instruction schedulers for out-of-order soft processors",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5681442/",
            "Abstract": "This work explores instruction scheduler designs for single-issue, out-of-order soft processors targeting irregular workloads. It shows the effect of scheduler size, scheduling policy and back-to-back scheduling on performance, area, and frequency. It is shown that for a modern, high-end FPGA (Altera Stratix III) the best performance is achieved by a small, 4-entry instruction scheduler with an age-based instruction selection policy and back-to-back scheduling. A combined scheduler and register renamer is shown to operate at 303 MHz.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:NMxIlDl6LWMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Identifying and Exploiting Ineffectual Computations to Enable Hardware Acceleration of Deep Learning",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8585656/",
            "Abstract": "This article summarizes somde of our work on hardware accelerators for inference with Deep Learning Neural Networks (DNNs). Early success in hardware acceleration for DNNs exploited the computation structure and the significant reuse in their access stream. Our approach to further boost benefits has been to first identify properties in the value stream of DNNs which we can exploit at the hardware level to improve execution time, reduce off- and on-chip communication and storage, resulting in higher energy efficiency and execution time reduction. We have been focusing on properties that are difficult or impossible to discern in advance. These properties include values that are zero or near zero and that prove ineffectual, values that have reduced precision needs, or even the bit-level content of values that lead to ineffectual computations. The presented designs cover a spectrum of choices in terms of area cost \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:tkaPQYYpVKoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multi-grain coherence directories",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7847639/",
            "Abstract": "Conventional directory coherence operates at the finest granularity possible, that of a cache block. While simple, this organization fails to exploit frequent application behavior: at any given point in time, large, continuous chunks of memory are often accessed only by a single core. We take advantage of this behavior and investigate reducing the coherence directory size by tracking coherence at multiple different granularities. We show that such a Multi-grain Directory (MGD) can significantly reduce the required number of directory entries across a variety of different workloads. Our analysis shows a simple dual-grain directory (DGD) obtains the majority of the benefit while tracking individual cache blocks and coarse-grain regions of 1KB to 8KB. We propose a practical DGD design that is transparent to software, requires no changes to the coherence protocol, and has no unnecessary bandwidth overhead. This design \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:D03iK_w7-QYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Regiontracker: A case for dual-grain tracking in the memory system",
            "Publication year": 2006,
            "Publication url": "http://www.eecg.toronto.edu/~moshovos/research/regiontracker_TR.pdf",
            "Abstract": "This work proposes area, power and latency efficient implementations of memory hierarchy lookup structures aimed primarily at higher-level, relatively large on-chip caches. The mechanisms proposed provide location information for a large fraction of cache references eliminating the corresponding accesses to a much larger, slower and power demanding conventional tag array. The key contribution of this work is dual-grain tracking where a two-level, two-grain approach is used to dynamically focus a set of few tracking resources on high-payoff memory areas. At the first level, a coarse-grain structure tracks which large regions of memory have blocks currently cached and uses this information to detect newly accessed regions. A second-level fine-grain mechanism tracks the location of all the blocks within a few newly accessed regions as these are identified by the first level. We demonstrate that our dual-grain tracking significantly outperforms the conventional, demandbased allocation of tracking resources and propose RegionTracker, an implementation of dual-grain tracking. RegionTracker is simple, can be easily partitioned for optimizing its power and latency, does not use cascaded lookups and does not impose any restrictions on cache placement. RegionTracker can capture a large fraction of memory references for various unified L2 caches. For example, we show that a RegionTracker that uses just 5% of the storage used by a conventional tag array and that can track just 128 8Kbyte regions, is able to capture 56% of all memory references in a 4Mbyte L2 cache and reduce L2 lookup power by 38% on the average. We also demonstrate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:-f6ydRqryjwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exploiting Typical Values to Accelerate Deep Learning",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8364645/",
            "Abstract": "To deliver the hardware computation power advances needed to support deep learning innovations, identifying deep learning properties that designers could potentially exploit is invaluable. This article articulates our strategy and overviews several value properties of deep learning models that we identified and some of our hardware designs that exploit them to reduce computation, and on- and off-chip storage and communication.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:XiVPGOgt02cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Microarchitectural innovations: Boosting microprocessor performance beyond semiconductor technology scaling",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/964438/",
            "Abstract": "Semiconductor technology scaling provides faster and more plentiful transistors to build microprocessors, and applications continue to drive the demand for more powerful microprocessors. Weaving the \"raw\" semiconductor material into a microprocessor that offers the performance needed by modern and future applications is the role of computer architecture. This paper overviews some of the microarchitectural techniques that empower modem high-performance microprocessors. The techniques are classified into: 1) techniques meant to increase the concurrency in instruction processing, while maintaining the appearance of sequential processing and 2) techniques that exploit program behavior. The first category includes pipelining, superscalar execution, out-of-order execution, register renaming, and techniques to overlap memory-accessing instructions. The second category includes memory hierarchies \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "L-CBF: a low-power, fast counting Bloom filter architecture",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4509488/",
            "Abstract": "An increasing number of architectural techniques have relied on hardware counting bloom filters (CBFs) to improve upon the energy, delay, and complexity of various processor structures. CBFs improve the energy and speed of membership tests by maintaining an imprecise and compact representation of a large set to be searched. This paper studies the energy, delay, and area characteristics of two implementations for CBFs using full custom layouts in a commercial 0.13-mum fabrication technology. One implementation, S-CBF, uses an SRAM array of counts and a shared up/down counter. Our proposed implementation, L-CBF, utilizes an array of up/down linear feedback shift registers and local zero detectors. Circuit simulations show that for a 1 K-entry CBF with a 15-bit count per entry, L-CBF compared to S-CBF is 3.7times or 1.6times faster and requires 2.3times or 1.4times less energy depending on the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:ULOm3_A8WrAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Memory dependence prediction in multimedia applications",
            "Publication year": 2000,
            "Publication url": "http://www.jilp.org/vol2/v2paper9.pdf",
            "Abstract": "We identify that a set of multimedia applications exhibit highly regular read-after-read (RAR) and read-after-write (RAW) memory dependence streams. We exploit this regularity to predict both RAW and RAR memory dependences. We also study how two previously proposed memory dependence prediction-based memory latency reduction techniques perform for this multimedia workload. In the first technique, a load can obtain a value by simply naming a preceding load (or store) with which a RAR (or RAW) dependence is predicted. The second technique speculatively converts a series of LOAD1-USE1,..., LOADN-USEN (or DEF-STORE-LOAD-USE) chains into a singleLOAD1-USE1... USEN (or DEF-USE) producer/consumer graph. We show that via memory dependence prediction it is possible to correctly predict 33.3% of all loads on the average. Moreover, the two memory dependence prediction based techniques result on average performance improvements of 2.6% over a highly-aggressive, out-of-order, superscalar processor. The actual range of performance improvements is 0% to 8.5%. When cache latency is increased from 2 to 3 cycles, performance improves by 3.75% on average, with the range being 0% to 16.35%.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:IWHjjKOFINEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cnvlutin: Ineffectual-neuron-free deep neural network computing",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3007787.3001138",
            "Abstract": "This work observes that a large fraction of the computations performed by Deep Neural Networks (DNNs) are intrinsically ineffectual as they involve a multiplication where one of the inputs is zero. This observation motivates Cnvlutin (CNV), a value-based approach to hardware acceleration that eliminates most of these ineffectual operations, improving performance and energy over a state-of-the-art accelerator with no accuracy loss. CNV uses hierarchical data-parallel units, allowing groups of lanes to proceed mostly independently enabling them to skip over the ineffectual computations. A co-designed data storage format encodes the computation elimination decisions taking them off the critical path while avoiding control divergence in the data parallel units. Combined, the units and the data storage format result in a data-parallel architecture that maintains wide, aligned accesses to its memory hierarchy and that \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:tS2w5q8j5-wC",
            "Publisher": "ACM"
        },
        {
            "Title": "Low leakage asymmetric SRAM cell devices",
            "Publication year": 2007,
            "Publication url": "https://patents.google.com/patent/US7307905B2/en",
            "Abstract": "Asymmetric SRAM cell designs exploiting data storage patterns found in ordinary software programs wherein most of the bits stored are zeroes for data and instruction streams. The asymmetric SRAM cell designs offer lower leakage power with little impact on latency. In asymmetric SRAM cells, selected transistors are \u201cweakened\u201d to reduce leakage current when the cell is storing a zero. Transistor weakening may be achieved by using higher voltage threshold transistors, by varying transistor geometries, or other means. In addition, a novel sense amplifier design is provided that leverages the asymmetric nature of the asymmetric SRAM cells to offer cell read times that are comparable with conventional symmetric SRAM cells. Lastly, cache memory designs are provided that are based on asymmetric SRAM cells offering leakage power reduction while maintaining high performance, comparable noise margins, and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:qUcmZB5y_30C",
            "Publisher": "Unknown"
        },
        {
            "Title": "NCOR: An FPGA-friendly nonblocking data cache for soft processors with runahead execution",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1155/2012/915178",
            "Abstract": "Soft processors often use data caches to reduce the gap between processor and main memory speeds. To achieve high efficiency, simple, blocking caches are used.  Such caches are not appropriate for processor designs such as Runahead and out-of-order execution that require nonblocking caches to tolerate main memory latencies.  Instead, these processors use non-blocking caches to extract memory level parallelism and improve performance. However, conventional non-blocking cache designs are expensive and slow on FPGAs as they use content-addressable memories (CAMs). This work proposes NCOR, an FPGA-friendly non-blocking cache that exploits the key properties of Runahead execution. NCOR does not require CAMs and utilizes smart cache controllers. A 4\u2009KB NCOR operates at 329\u2009MHz on Stratix III FPGAs while it uses only 270 logic elements. A 32\u2009KB NCOR operates at 278\u2009Mhz and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:O3NaXMp0MMsC",
            "Publisher": "Hindawi"
        },
        {
            "Title": "ReCast: boosting tag line buffer coverage in low-power high-level caches\" for free\"",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1524214/",
            "Abstract": "We revisit the idea of using small line buffers in-front of caches. We propose ReCast, a tiny tag set cache that filters a significant number of tag probes to the L2 tag array thus reducing power. The key contribution in ReCast is S-Shift, a simple indexing function (no logic involved just wires) that greatly improves the utility of line buffers with no additional hardware cost. S-Shift can be viewed as a technique for emulating larger cache blocks and hence exploiting more spatial locality but without paying the penalties of actually using a larger L2 cache block. Using several SPEC CPU2000 applications and a model of an aggressive, dynamically-scheduled, superscalar processor we demonstrate that a practical ReCast organization can significantly reduce power in the L2. Specifically, a 64-entry ReCast comprising eight sub-banks of eight entries each can filter about 50% of all tag probes for a 1 Mbyte L2 cache. A \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:hFOr9nPyWt4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Boveda: Building an On-Chip Deep Learning Memory Hierarchy Brick by Brick",
            "Publication year": 2021,
            "Publication url": "https://proceedings.mlsys.org/papers/2021/138",
            "Abstract": "Data access between on-and off-chip memories account for a large fraction of overall energy consumption during inference with deep learning networks. On-chip memory compression can greatly reduce this energy cost as long as it balances the simplicity and low cost of the compression/decompression implementation and its effectiveness in data size reduction. We present Boveda, a simple and effective on-chip lossless memory compression technique for fixed-point precision networks. It reduces data widths by exploiting the value distribution deep learning applications naturally exhibit. Boveda can increase the effective on-chip capacity, reduce off-chip traffic, and/or achieve a desired performance/energy target while using smaller on-chip memories. Boveda can be placed after any memory block in the on-chip memory hierarchy and can work with\\textul {any} data-parallel processing units such as the vector-like or the tensorcore units of modern graphics processors, systolic arrays such as that used in the Tensor Processing Unit, and units that process sparse tensors such as those used in the SCNN accelerator. To demonstrate the potential of Boveda, we implement it over (i) SCNN, a state-of-the-art accelerator for sparse networks,(ii) a Tensorcore-like architecture, and (iii) TPU. Boveda reduces memory footprint by 34\\% for SCNN and sparse models on top of zero compression. For dense models, Boveda improves compression by 47\\%. We also present a prototype FPGA implementation.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:z_wVstp3MssC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Low-cost, high-performance branch predictors for soft processors",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6645536/",
            "Abstract": "This work studies branch predictor implementations for general purpose, pipelined, single core soft processors. It shows that the existing designs do not map well onto reconfigurable hardware since they were optimized for custom logic implementation. This work then proposes an accurate and fast branch predictor that uses few resources on FPGAs. The proposed predictor uses: (1) an FPGA-friendly pattern based direction predictor, (2) a return address stack, (3) in-fetch target address calculation instead of a branch target buffer, and (4) instruction pre-decoding. Experimental measurements using a subset of the SPECCPU2006 workloads show that the presented FPGA-friendly branch predictor delivers high performance while operating at approximately 259 MHz using only 147 ALUTs and one BRAM on an Altera Stratix IV FPGA.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:a0OBvERweLwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "SEPAS: A Highly Accurate Energy-Efficient Branch Predictor",
            "Publication year": 2004,
            "Publication url": "https://scholar.google.com/scholar?cluster=17827787253014795767&hl=en&oi=scholarr",
            "Abstract": "Designers have invested much effort in developing accurate branch predictors with short learning periods. Such techniques rely on exploiting complex and relatively large structures. Although exploiting such structures is necessary to achieve high accuracy and fast learning, once the short learning phase is over, a simple structure can efficiently predict the branch outcome for the majority of branches. Moreover, for a large number of branches, once the branch reaches the steady state phase, updating the branch predictor unit is unnecessary since there is already enough information available to the predictor to predict the branch outcome accurately. Therefore, aggressive usage of complex large branch predictors appears to be inefficient since it results in unnecessary energy consumption.In this work we introduce Selective Predictor Access (SEPAS) to exploit this design inefficiency. SEPAS uses a simple power efficient structure to identify well behaved branch instructions that are in their steady state phase. Once such branches are identified, the predictor is no longer accessed to predict their outcome or to update the associated data. We show that it is possible to reduce the number of predictor accesses and energy consumption considerably with a negligible performance loss (worst case 0.25%).",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:uJ-U7cs_P_0C",
            "Publisher": "Institute of Electrical & Electronics Engineers (IEEE)"
        },
        {
            "Title": "On the latency and energy of checkpointed superscalar register alias tables",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5169866/",
            "Abstract": "This paper investigates how the latency and energy of register alias tables (RATs) vary as a function of the number of global checkpoints (GCs), processor issue width, and window size. It improves upon previous RAT checkpointing work that ignored the actual latency and energy tradeoffs and focused solely on evaluating performance in terms of instructions per cycle (IPC). This work utilizes measurements from the full-custom checkpointed RAT implementations developed in a commercial 130-nm fabrication technology. Using physical- and architectural-level evaluations together, this paper demonstrates the tradeoffs among the aggressiveness of the RAT checkpointing, performance, and energy. This paper also shows that, as expected, focusing on IPC alone incorrectly predicts performance. The results of this study justify checkpointing techniques that use very few GCs (e.g., four). Additionally, based on full \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:HDshCWvjkbEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Deep Learning Language Modeling Workloads: Where Time Goes on Graphics Processors",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9041972/",
            "Abstract": "Language Modeling is at the core of many natural language processing tasks. We analyze two such recent models: a Gated Convolutional Network (GCN) with five layers on the Wikitext-2 dataset and a Transformer network with 24 layers on the Google Billion Word dataset. We find that when executed on modern graphics processors, 30% - 40% of the execution time is due to the final adaptive softmax layer. Analytical modeling of the computation and memory demands of the GCN shows that this behavior will persist even if the hidden state is increased - which could be needed to improve accuracy or to support a wider vocabulary. We present variations of the adaptive softmax layer that reduce execution time for the layer by 40% and that scale better with the hidden state.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:uLbwQdceFCQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Tensordash: Exploiting sparsity to accelerate deep neural network training and inference",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2009.00748",
            "Abstract": "TensorDash is a hardware level technique for enabling data-parallel MAC units to take advantage of sparsity in their input operand streams. When used to compose a hardware accelerator for deep learning, TensorDash can speedup the training process while also increasing energy efficiency. TensorDash combines a low-cost, sparse input operand interconnect comprising an 8-input multiplexer per multiplier input, with an area-efficient hardware scheduler. While the interconnect allows a very limited set of movements per operand, the scheduler can effectively extract sparsity when it is present in the activations, weights or gradients of neural networks. Over a wide set of models covering various applications, TensorDash accelerates the training process by  while being  more energy-efficient,  more energy efficient when taking on-chip and off-chip memory accesses into account. While TensorDash works with any datatype, we demonstrate it with both single-precision floating-point units and bfloat16.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:7T2F9Uy0os0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Advanced branch predictors for soft processors",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7032495/",
            "Abstract": "This work studies implementations of the Perceptron [1] and TAGE [2] branch predictors for general purpose, in-order pipelined single core soft processors. It proposes FPGA-friendly optimizations whose goal is to achieve high operating frequency. This work discusses the design tradeoffs and proposes a highly accurate and fast branch predictor variant based on TAGE, O-TAGE-SC. It operates at 270MHz, the maximum frequency of Altera's highest performing soft-processor Nios II-f. Using a representative subset of the SPECCPU2006 benchmarks, this work shows that O-TAGE-SC delivers 5.2% better instruction throughput versus the previously proposed gRselect predictor [3].",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:OU6Ihb5iCvQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "PACT: Power Aware Compilation and Architectural Techniques",
            "Publication year": 2003,
            "Publication url": "https://apps.dtic.mil/sti/citations/ADA418780",
            "Abstract": "The goal of this project was to take DoD applications written in C and generate power and performance efficient code for systems utilizing the architectural power-aware techniques developed. The PACT project consisted of 3 research tasks 1 Power-aware architectural approaches, 2 Power-aware compilation strategies, and 3 Power-aware CAD tools for power estimation and synthesis. As part of the power aware architecture research, we developed power aware techniques for on-chip buses, power aware memory hierarchies, and a framework to evaluate heterogeneous embedded systems for performance and energy consumption. As part of the power aware compiler research, we have developed a compiler that takes general C programs and generates power aware codes for three targets 1 General purpose embedded processor such as the StrongARM, 2 General purpose field-programmable gate arrays FPGAs, and 3 General purpose application specific integrated circuits ASICs. We have developed improved strategies for power optimization and management, and improved design methodologies and design philosophies for better estimation and optimization.Descriptors:",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:ns9cj8rnVeAC",
            "Publisher": "CALIFORNIA UNIV LOS ANGELES DEPT OF COMPUTER SCIENCE"
        },
        {
            "Title": "Accelerator for deep neural networks",
            "Publication year": 2019,
            "Publication url": "https://patents.google.com/patent/US10387771B2/en",
            "Abstract": "A system for bit-serial computation in a neural network is described. The system may be embodied on an integrated circuit and include one or more bit-serial tiles for performing bit-serial computations in which each bit-serial tile receives input neurons and synapses, and communicates output neurons. Also included is an activation memory for storing the neurons and a dispatcher and a reducer. The dispatcher reads neurons and synapses from memory and communicates either the neurons or the synapses bit-serially to the one or more bit-serial tiles. The other of the neurons or the synapses are communicated bit-parallelly to the one or more bit-serial tiles, or according to a further embodiment, may also be communicated bit-serially to the one or more bit-serial tiles. The reducer receives the output neurons from the one or more tiles, and communicates the output neurons to the activation memory.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:mvPsJ3kp5DgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reducing memory latency via read-after-read memory dependence prediction",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/990129/",
            "Abstract": "We observe that typical programs exhibit highly regular read-after-read (RAR) memory dependence streams. To exploit this regularity, we introduce read-after-read memory dependence prediction. This technique predicts whether: 1) a load will access a memory location that a preceding load accesses and 2) exactly which this preceding load is. This prediction is done without actual knowledge of the corresponding memory addresses. We also present two techniques that utilize RAR memory dependence prediction to reduce memory latency. In the first technique, a load may obtain a value by naming a preceding load with which an RAR dependence is predicted. The second technique speculatively converts a series of LOAD/sub 1/-USE/sub 1/,...,LOAD/sub N/-USE/sub N/ chains into a single LOAD/sub 1/-USE/sub 1/...USE/sub N/ producer/consumer graph. Our techniques can be implemented as small extensions to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:dhFuZR0502QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "MEMORY IN DEEP NEURAL NETS",
            "Publication year": 2015,
            "Publication url": "https://www.researchgate.net/profile/Raquel-Urtasun/publication/284219754_Reduced-Precision_Strategies_for_Bounded_Memory_in_Deep_Neural_Nets/links/565b3a7508aefe619b242f56/Reduced-Precision-Strategies-for-Bounded-Memory-in-Deep-Neural-Nets.pdf",
            "Abstract": "This work shows how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. Unlike similar studies, this study considers networks where each layer may use different precision data. The key result of this study is that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. The trends of error tolerance across layers are studied and a method for finding a low precision configuration for a network while maintaining high accuracy is presented. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floatingpoint representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:hkOj_22Ku90C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Doppelg\u00e4nger: a cache for approximate computing",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2830772.2830790",
            "Abstract": "Modern processors contain large last level caches (LLCs) that consume substantial energy and area yet are imperative for high performance. Cache designs have improved dramatically by considering reference locality. Data values are also a source of optimization. Compression and deduplication exploit data values to use cache storage more efficiently resulting in smaller caches without sacrificing performance. In multi-megabyte LLCs, many identical or similar values may be cached across multiple blocks simultaneously. This redundancy effectively wastes cache capacity. We observe that a large fraction of cache values exhibit approximate similarity. More specifically, values across cache blocks are not identical but are similar. Coupled with approximate computing which observes that some applications can tolerate error or inexactness, we leverage approximate similarity to design a novel LLC architecture: the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:WbkHhVStYXYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bit-tactical: A software/hardware approach to exploiting value and bit sparsity in neural networks",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3297858.3304041",
            "Abstract": "Weight and activation sparsity can be leveraged in hardware to boost the performance and energy efficiency of Deep Neural Networks during inference. Fully capitalizing on sparsity requires re-scheduling and mapping the execution stream to deliver non-zero weight/activation pairs to multiplier units for maximal utilization and reuse. However, permitting arbitrary value re-scheduling in memory space and in time places a considerable burden on hardware to perform dynamic at-runtime routing and matching of values, and incurs significant energy inefficiencies. Bit-Tactical (TCL) is a neural network accelerator where the responsibility for exploiting weight sparsity is shared between a novel static scheduling middleware, and a co-designed hardware front-end with a lightweight sparse shuffling network comprising two (2-to 8-input) multiplexers per activation input. We empirically motivate two back-end designs chosen \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:9vf0nzSNQJEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The Predictability of Computations that Produce Unpredictable Outcomes",
            "Publication year": 2001,
            "Publication url": "https://people.ece.ubc.ca/aamodt/publications/papers/slice-locality.pdf",
            "Abstract": "We study the dynamic stream of slices (ie, slice traces) that lead to branches that foil an existing branch predictor and to loads that miss and measure whether these slices exhibit locality (ie, repetition). We argue that this regularity can be used to dynamically extract slices for an operation-based predictor that speculatively pre-computes a load address or branch target (ie, an outcome) rather than directly predicting the outcome based upon the history of outcomes. We study programs from the SPEC2000 suite and find they exhibit good slice locality for these problem loads and branches. Moreover, we study the performance of an idealized operation-based predictor (it can execute slices instantaneously). We find that it interacts favorably with an existing sophisticated outcome-based branch predictor, and that slice locality provides good insight into the fraction of all branch mispredictions it can potentially eliminate. Similar observations hold for operation-based prefetching of loads that miss. On average slice locality for branches and loads was found to be above 70% and 76% respectively when recording the 4 most recent unique slices per branch or load over a window of 64 committed instructions, and close to 68% and 71% for branches and loads respectively when we look at slices over a window of up to 128 committed instructions. The idealized operation predictor was found to correct approximately 74% of branch mispredictions or prefetch about 67% of loads that miss respectively (slices detected over a window of 64 instructions). At the same time, on average, the branch operation predictor mispredicts less than 0.8% of all branches that are \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:mVmsd5A6BfQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "STREX: boosting instruction cache reuse in OLTP workloads through stratified transaction execution",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2485922.2485946",
            "Abstract": "Online transaction processing (OLTP) workload performance suffers from instruction stalls; the instruction footprint of a typical transaction exceeds by far the capacity of an L1 cache, leading to ongoing cache thrashing. Several proposed techniques remove some instruction stalls in exchange for error-prone instrumentation to the code base, or a sharp increase in the L1-I cache unit area and power. Others reduce instruction miss latency by better utilizing a shared L2 cache. SLICC [2], a recently proposed thread migration technique that exploits transaction instruction locality, is promising for high core counts but performs sub-optimally or may hurt performance when running on few cores.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:HoB7MX3m0LUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Two-stage, pipelined register renaming",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5599894/",
            "Abstract": "Register renaming is a performance-critical component of modern, dynamically-scheduled processors. Register renaming latency increases as a function of several architectural parameters (e.g., processor issue width, processor window size, and processor checkpoint count). Pipelining of the register renaming logic can help avoid restricting the processor clock frequency. This work presents a full-custom, two-stage register renaming implementation in a 130-nm fabrication technology. The latency of non-pipelined and two-stage, pipelined renaming is compared, and the underlying performance and complexity tradeoffs are discussed. The two-stage pipelined design reduces the renaming logic depth from 23 fan-out-of-four (FO4) down to 9.5 FO4.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:r0BpntZqJG4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the latency, energy and area of checkpointed, superscalar register alias tables",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5514293/",
            "Abstract": "We present two full-custom implementations of the Register Alias Table (RAT) for a 4-way superscalar dynamically-scheduled processor in a commercial 130nm CMOS technology. The implementations differ in the way they organize the embedded global checkpoints (GCs) which support speculative execution. In the first implementation, representative of early designs, the GCs are organized as shift registers. In the second implementation, representative of more recent proposals, the GCs are organized as random access buffers. We measure the impact of increasing the number of GCs on the latency, energy, and area of the RAT. The results support the importance of recent techniques that reduce the number of GCs while maintaining performance.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:9ZlFYXVOiuMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "RegionTracker: Using dual-grain tracking for energy efficient cache lookup",
            "Publication year": 2006,
            "Publication url": "http://www.eecg.utoronto.ca/~moshovos/research/regiontracker-wced.pdf",
            "Abstract": "This work proposes energy efficient memory hierarchy lookup structures aimed primarily at relatively large, higher-level on-chip caches. The mechanisms proposed provide location information for a large fraction of cache references and eliminate the corresponding accesses to a larger, slower and less energy efficient tag array. A key contribution of this work is the concept of dual-grain tracking where a two-level, two-grain approach is used to dynamically focus a set of few tracking resources on high-payoff memory areas. A coarse-grain tracking structure uses imprecise information to identify accesses to new regions of memory and then directs the allocation of a precise, fine-grain tracking structure. We propose RegionTracker, a simple implementation of dual-grain tracking which can be easily partitioned for optimizing its power and latency, and which does not use cascaded lookups or impose any restrictions on cache placement. We demonstrate that RegionTracker can significantly reduce lookup energy for various L2 caches. For example, we show that a RegionTracker that uses just 6.9% of the storage used by a conventional tag array and that can track just 128 8Kbyte regions, is able to reduce L2 lookup energy by 35% on the average for a 4MB L2 cache. We also demonstrate that RegionTracker can complement conventional, demand-driven tag set buffers and that it provides better energy savings.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:BqipwSGYUEgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Value-Based Deep Learning Hardware Acceleration",
            "Publication year": 2018,
            "Publication url": "https://www.computer.org/csdl/proceedings-article/nocarc/2018/08541207/17D45WaTkcA",
            "Abstract": "I will be reviewing our efforts in identifying value properties of Deep Learning models that hardware accelerators can use to improve execution time performance and energy efficiency. Our goal it to not sacrifice accuracy and to not require any changes to the model. I will be presenting our accelerator family which includes designs that exploit these properties. Our accelerators exploit ineffectual activations and weights, their variable precision requirements, or even their value content at the bit level. We have demonstrated performance benefits of 50% to up 27x over a highly optimized execution engine for neural networks. Further, our accelerators also enable on-the-fly trading off accuracy for further performance and energy efficiency improvements. I will emphasize our latest designs, Diffy, Tactical and Laconic. Tactical targets sparse models whereas Laconic achieves the highest performance when configured for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:AXPGKjj_ei8C",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "JETTY: Reducing Snoop-Induced Power Consumption in Small-Scale, Bus-Based SMP Systems",
            "Publication year": 2000,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.8328&rep=rep1&type=pdf",
            "Abstract": "We propose methods for reducing the power required for handling snoop requests in small-scale, snoop-coherence, bus-based SMP systems. Observing that a large fraction of snoops do not find copies on all other caches, we introduce JETTY, which is a small, cache-like structure. Ajetty is placed in-between the bus and the L2 backside of each processor where it acts as a filter for snoop requests. In particular, it filters the vast majority of snoops that would not find a locally cached copy. We propose a number of alternative JETTY methods that operate by identifying either a subset of non-locally-cached blocks or a superset of locally cached blocks. We demonstrate that for a set of parallel applications and a 4-way SMP system, relatively small JETTY structures can filter up to 77% of all snoops that would miss on the average. This resulted in average power reduction of 41% measured as a fraction of the power required for all snoop-induced tag-array accesses.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:isC4tDSrTZIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exploiting load/store parallelism via memory dependence prediction",
            "Publication year": 2005,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=aokeGc8vBIQC&oi=fnd&pg=PA355&dq=info:-AEAZ2C2zc4J:scholar.google.com&ots=YnSZiGJoSy&sig=ectW8L8q9Q0YdsnumpOJgHjnDBk",
            "Abstract": "Since memory reads or loads are very frequent, memory latency, that is the time it takes for memory to respond to requests, can impact performance significantly. Today, reading data from main memory requires more than 100 processor cycles while in \u201ctypical\u201d programs about one in five instructions reads from memory. A naively built multi-GHz processor that executes instructions sequentially would thus have to spend most of its time simply waiting for memory to respond. The overall performance of such a processor would not be noticeably better than that of a processor that operated with a much slower clock (in the order of a few hundred MHz). Clearly, increasing processor speeds alone without at the same time finding a way to make memories respond faster makes no sense. Ideally, the memory latency problem would be attacked directly. Unfortunately, it is practically impossible to build a large, fast and cost effective memory. While it is presently impossible to make memory respond fast to all requests, it is possible to make memory respond faster to some requests. The more requests it can process faster, the higher the overall performance. This is the goal of traditional memory hierarchies where a collection of faster but smaller memory devices (commonly referred to as caches) is used to provide faster access to a dynamically changing subset of memory data. Given the limited size of caches and imperfections in the caching policies, memory hierarchies provide only a partial solution to",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:e5wmG9Sq2KIC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "Teaching old caches new tricks: Regiontracker and predictor virtualization",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5291238/",
            "Abstract": "On-chip last-level caches are increasing to tens of megabytes to accommodate applications with large memory footprints and to compensate for high memory latencies and limited off-chip bandwidth. This paper reviews two on-going research efforts that exploit such large caches: coarse-grain cache management, and predictor virtualization. Coarse-grain cache management collects and stores cache information at a large memory region granularity (e.g., 1 KB to 8 KB). This coarse view of memory access behaviour enables optimizations that were not previously possible with conventional caches. Predictor virtualization is motivated by the observation that on-chip storage has become sufficiently large to accommodate allocating, on demand, a small percentage of its capacity for purposes other than storing program data and instructions. Predictor virtualization uses conventional caches to store program metadata, i.e \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:J_g5lzvAfSwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Temporal streams in commercial server applications",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4636095/",
            "Abstract": "Commercial server applications remain memory bound on modern multiprocessor systems because of their large data footprints, frequent sharing, complex non-strided access patterns, and long chains of dependant misses. To improve memory system performance despite these challenging access patterns, researchers have proposed prefetchers that exploit temporal streams-recurring sequences of memory accesses. Although prior studies show substantial performance improvement from such schemes, they fail to explain why temporal streams arise; that is, they treat commercial applications as a black box and do not identify the specific behaviors that lead to recurring miss sequences. In this paper, we perform an information-theoretic analysis of miss traces from single-chip and multi-chip multiprocessors to identify recurring temporal streams in web serving, online transaction processing, and decision support \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:qxL8FJ1GzNcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Low-leakage asymmetric-cell SRAM",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1229876/",
            "Abstract": "We introduce a novel family of asymmetric dual-V/sub t/ static random access memory cell designs that reduce leakage power in caches while maintaining low access latency. Our designs exploit the strong bias toward zero at the bit level exhibited by the memory value stream of ordinary programs. Compared to conventional symmetric high-performance cells, our cells offer significant leakage reduction in the zero state and, in some cases, also in the one state, albeit to a lesser extent. A novel sense amplifier, in combination with dummy bitlines, allows for read times to be on par with conventional symmetric cells. With one cell design, leakage is reduced by 7/spl times/ (in the zero state) with no performance degradation, but with a stability degradation of 6%. Another cell design reduces leakage by 2/spl times/ (in the zero state) with no performance or stability loss. An alternative cell design reduces leakage by 58/spl \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:2osOgNQ5qMEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Loom: Exploiting weight and activation precisions to accelerate convolutional neural networks",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8465915/",
            "Abstract": "Loom (LM), a hardware inference accelerator for Convolutional Neural Networks (CNNs) is presented. In LM every bit of data precision that can be saved translates to proportional performance gains. For both weights and activations LM exploits profile-derived per layer precisions. However, at runtime LM further trims activation precisions at a much smaller than a layer granularity. On average, across several image classification CNNs and for a configuration that can perform the equivalent of 12816b \u00d7 16b multiply-accumulate operations per cycle LM outperforms a state-of-the-art bit-parallel accelerator [3] by 3.19\u00d7 without any loss in accuracy while being 2.59\u00d7 more energy efficient. LM can trade-off accuracy for additional improvements in execution performance and energy efficiency and compares favorably to an accelerator that targeted only activation precisions.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:eflP2zaiRacC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Slicc: Self-assembly of instruction cache collectives for oltp workloads",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6493619/",
            "Abstract": "Online transaction processing (OLTP) is at the core of many data center applications. OLTP workloads are known to have large instruction footprints that foil existing L1 instruction caches resulting in poor overall performance. Prefetching can reduce the impact of such instruction cache miss stalls, however, state-of-the-art solutions require large dedicated hardware tables on the order of 40KB in size. SLICC is a programmer transparent, low cost technique to minimize instruction cache misses when executing OLTP workloads. SLICC migrates threads, spreading their instruction footprint over several L1 caches. It exploits repetition within and across transactions, where a transaction's first iteration prefetches the instructions for subsequent iterations or similar subsequent transactions. SLICC reduces instruction misses by 58% on average for TPC-C and TPCE, thereby improving performance by 68%. When compared \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:lSLTfruPkqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Neural network accelerator",
            "Publication year": 2021,
            "Publication url": "https://patents.google.com/patent/US20210004668A1/en",
            "Abstract": "Described is a neural network accelerator tile for exploiting input sparsity. The tile includes a weight memory to supply each weight lane with a weight and a weight selection metadata, an activation selection unit to receive a set of input activation values and rearrange the set of input activation values to supply each activation lane with a set of rearranged activation values, a set of multiplexers including at least one multiplexer per pair of activation and weight lanes, where each multiplexer is configured to select a combination activation value for the activation lane from the activation lane set of rearranged activation values based on the weight lane weight selection metadata, and a set of combination units including at least one combination unit per multiplexer, where each combination unit is configured to combine the activation lane combination value with the weight lane weight to output a weight lane product.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:VLnqNzywnoUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "V. CONCLUSION",
            "Publication year": 2011,
            "Publication url": "https://scholar.google.com/scholar?cluster=15077709733770286005&hl=en&oi=scholarr",
            "Abstract": "Register renaming is a performance-critical component of modern, dynamically-scheduled processors. Register renaming latency increases as a function of several architectural parameters (eg, processor issue width, processor window size, and processor checkpoint count). Pipelining of the register renaming logic can help avoid restricting the processor clock frequency. This work presents a full-custom, two-stage register renaming implementation in a 130-nm fabrication technology. The latency of non-pipelined and two-stage, pipelined renaming is compared, and the underlying performance and complexity tradeoffs are discussed. The two-stage pipelined design reduces the renaming logic depth from 23 fan-out-of-four (FO4) down to 9.5 FO4.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:fEOibwPWpKIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Accelerating Image-Sensor-Based Deep Learning Applications",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8781863/",
            "Abstract": "We review two inference accelerators that exploit value properties in deep neural networks: 1) Diffy that targets spatially correlated activations in computational imaging DNNs, and 2) Tactical that targets sparse neural networks using a low-overhead hardware/software weight-skipping front-end. Then we combine both into Di-Tactical to boost benefits for both scene understanding workloads and computational imaging tasks.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:LjlpjdlvIbIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Method and device with convolution neural network processing",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US20200065646A1/en",
            "Abstract": "A processor-implemented method implementing a convolution neural network includes: determining a plurality of differential groups by grouping a plurality of raw windows of an input feature map into the plurality of differential groups; determining differential windows by performing, for each respective differential group of the differential groups, a differential operation between the raw windows of the respective differential group; determining a reference element of an output feature map corresponding to a reference raw window among the raw windows by performing a convolution operation between a kernel and the reference raw window; and determining remaining elements of the output feature map by performing a reference element summation operation based on the reference element and each of a plurality of convolution operation results determined by performing respective convolution operations between \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:JQOojiI6XY0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Wormhole: Wisely predicting multidimensional branches",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7011413/",
            "Abstract": "Improving branch prediction accuracy is essential in enabling high-performance processors to find more concurrency and to improve energy efficiency by reducing wrong path instruction execution, a paramount concern in today's power-constrained computing landscape. Branch prediction traditionally considers past branch outcomes as a linear, continuous bit stream through which it searches for patterns and correlations. The state-of-the-art TAGE predictor and its variants follow this approach while varying the length of the global history fragments they consider. This work identifies a construct, inherent to several applications that challenges existing, linear history based branch prediction strategies. It finds that applications have branches that exhibit multi-dimensional correlations. These are branches with the following two attributes: 1) they are enclosed within nested loops, and 2) they exhibit correlation across \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:KxtntwgDAa4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimization of JETTY Snoop Filters based on Memory Access Locality",
            "Publication year": 2003,
            "Publication url": "https://scholar.google.com/scholar?cluster=11036555618944213944&hl=en&oi=scholarr",
            "Abstract": "In symmetric multiprocessing systems using snoop-based coherence protocols tag lookups caused by snooped memory events contribute significantly to power consumption. The JETTY mechanism was proposed to reduce this effect by filtering snoops that can be determined to be cache misses without checking the L2 tag array.The JETTY snoop filters proposed to date do not consider patterns of locality in data access beyond assuming that such locality exists. This paper examines data access patterns for a wide range of test loads and introduces new JETTY filter designs that take advantage of the locality trends found. The resulting filters offer significantly better filter rates (43.9% of snoops filtered on average compared to 39.1% for nonoptimized filters for filters using 10-bit table indices and a maximum of 4 counter tables).",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:vV6vV6tmYwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bit-pragmatic deep neural network computing",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3123939.3123982",
            "Abstract": "Deep Neural Networks expose a high degree of parallelism, making them amenable to highly data parallel architectures. However, data-parallel architectures often accept inefficiency in individual computations for the sake of overall efficiency. We show that on average, activation values of convolutional layers during inference in modern Deep Convolutional Neural Networks (CNNs) contain 92% zero bits. Processing these zero bits entails ineffectual computations that could be skipped. We propose Pragmatic (PRA), a massively data-parallel architecture that eliminates most of the ineffectual computations on-the-fly, improving performance and energy efficiency compared to state-of-the-art high-performance accelerators [5]. The idea behind PRA is deceptively simple: use serial-parallel shift-and-add multiplication while skipping the zero bits of the serial input. However, a straightforward implementation based on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:8AbLer7MMksC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reduced-precision strategies for bounded memory in deep neural nets",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1511.05236",
            "Abstract": "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. More specifically, this study considers networks where each layer may use different precision data. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floating-point representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:738O_yMBCRsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Characterizing the performance benefits of fused CPU/GPU systems using FusionSim",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6513594/",
            "Abstract": "We use FusionSim to characterize the performance of the Rodinia benchmarks on fused and discrete systems. We demonstrate that the speed-up due to fusion is highly correlated with the input data size. We demonstrate that for benchmarks that benefit most from fusion, a 9.72x speed up is possible for small problem sizes. This speedup reduces to 1.84x with medium or large problem sizes. We study a simple, software-managed coherence solution for the fused system. We find that it imposes a minor performance overhead of 2% for most benchmarks and as high as 5% for some. Finally, we develop an analytical model for the performance benefit that is to be expected from fusion for applications with a simple communication and computation pattern and show that FusionSim follows the predicted performance trend.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:SeFeTyx0c_EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Memory state compressors for giga-scale checkpoint/restore",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1515602/",
            "Abstract": "We propose a checkpoint store compression method for coarse-grain giga-scale checkpoint/restore. This mechanism can be useful for debugging, post-mortem analysis and error recovery. Our compression method exploits value locality in the memory data and address streams. Our compressors require few resources, can be easily pipelined and can process a full cache block per processor cycle. We study two applications of our compressors for post-mortem analysis: (1) using them alone, and (2) using them in-series with a dictionary-based compressor. When used alone they offer competitive compression rates in most cases. When combined with dictionary compressors, they significantly reduce on-chip buffer requirements.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:4JMBOYKVnBMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A tagless coherence directory",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1669112.1669166",
            "Abstract": "A key challenge in architecting a CMP with many cores is maintaining cache coherence in an efficient manner. Directory-based protocols avoid the bandwidth overhead of snoop-based protocols, and therefore scale to a large number of cores. Unfortunately, conventional directory structures incur significant area overheads in larger CMPs.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:LkGwnXOMwfcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "CHIMAERA: Integrating a Reconfigurable Unit into a High-Performance, Dynamically-Scheduled Superscalar Processor",
            "Publication year": 2000,
            "Publication url": "http://www2.ee.washington.edu/faculty/hauck/publications/YeTVLSI.pdf",
            "Abstract": "We report our experience with Chimaera, a prototype system that integrates a small and fast reconfigurable functional unit (RFU) into the pipeline of an aggressive, dynamically-scheduled superscalar processor. We discuss the Chimaera C compiler that automatically maps computations for execution in the RFU. Using a set of multimedia and communication applications we show that even with simple optimizations, the Chimaera C compiler is able to map 22% of all instructions to the RFU on the average. Timing experiments demonstrate that for a 4-way out-of-order superscalar processor Chimaera results in average performance improvements of 21%. This assuming a very aggressive core processor design (most pessimistic RFU latency model) and communication overheads from and to the RFU.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:RHpTSmoSYBkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Turbo-rob: A low cost checkpoint/restore accelerator",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-77560-7_18",
            "Abstract": "Modern processors use speculative execution to improve performance. However, speculative execution requires a checkpoint/restore mechanism to repair the machine\u2019s state whenever speculation fails. Existing checkpoint/restore mechanisms do not scale well for processors with relatively large windows (i.e., 128 or more). This work presents Turbo-ROB, a checkpoint/restore recovery accelerator that can complement or replace existing checkpoint/restore mechanisms. We show that the Turbo-ROB improves performance and reduces resource requirements compared to a conventional Re-order Buffer mechanism. For example, on the average, a 64-entry TROB matches the performance of a 512-entry ROB, while a 128- and a 512-entry TROB outperform the 512-entry ROB by 6.8% and 9.1% respectively. We also demonstrate that the TROB improves performance with register alias table checkpoints \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:mB3voiENLucC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Diffy: A D\u00e9j\u00e0 vu-free differential deep neural network accelerator",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8574537/",
            "Abstract": "We show that Deep Convolutional Neural Network (CNN) implementations of computational imaging tasks exhibit spatially correlated values. We exploit this correlation to reduce the amount of computation, communication, and storage needed to execute such CNNs by introducing Diffy, a hardware accelerator that performs Differential Convolution. Diffy stores, communicates, and processes the bulk of the activation values as deltas. Experiments show that, over five state-of-the-art CNN models and for HD resolution inputs, Diffy boosts the average performance by 7.1\u00d7 over a baseline value-agnostic accelerator [1] and by 1.41\u00d7 over a state-of-the-art accelerator that processes only the effectual content of the raw activation values [2]. Further, Diffy is respectively 1.83\u00d7 and 1.36\u00d7 more energy efficient when considering only the on-chip energy. However, Diffy requires 55% less on-chip storage and 2.5\u00d7 less off-chip \u2026",
            "Abstract entirety": 0,
            "Author pub id": "D2VLt-8AAAAJ:HE397vMXCloC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Bwa-mem performance: Suffix array storage size",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8834510/",
            "Abstract": "Gene Sequencing is the procedure of investigating a genome to discover its differences from the human reference genome. Short Read Alignment, the first processing step in the gene sequencing pipeline, finds the location of short reads in the human reference genome. In general, Burrows Wheeler Transform based alignment tools are widely popular in gene sequencing pipelines. We investigate how storage arrangements of the Suffix Array (SA) impact the overall performance of BWA-MEM, a popular BWT based aligner. We show that partial storage of SA imposes a considerable processing burden to BWA-MEM. We study the memory-compute trade off of different SA storage arrangements and demonstrate that the performance of BWA-MEM can be improved by up to 19 percent by simply storing the whole SA in memory.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:N5tVd3kTz84C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Building an on-chip deep learning memory hierarchy brick by brick: late breaking results",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.5555/3437539.3437787",
            "Abstract": "Data accesses between on-and off-chip memories account for a large fraction of overall energy consumption during inference with deep learning networks. We present Boveda, a lossless on-chip memory compression technique for neural networks operating on fixed-point values. Boveda reduces the datawidth used per block of values to be only as long as necessary: since most values are of small magnitude Boveda drastically reduces their footprint. Boveda can be used to increase the effective on-chip capacity, to reduce off-chip traffic, or to reduce the on-chip memory capacity needed to achieve a performance/energy target. Boveda reduces total model footprint to 53%.",
            "Abstract entirety": 1,
            "Author pub id": "D2VLt-8AAAAJ:35r97b3x0nAC",
            "Publisher": "Unknown"
        }
    ]
}]