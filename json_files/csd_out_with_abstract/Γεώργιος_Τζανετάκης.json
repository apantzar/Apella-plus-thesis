[{
    "name": "\u0393\u03b5\u03ce\u03c1\u03b3\u03b9\u03bf\u03c2 \u03a4\u03b6\u03b1\u03bd\u03b5\u03c4\u03ac\u03ba\u03b7\u03c2",
    "romanize name": "Georgios Tzanetakis",
    "School-Department": " Computer Science",
    "University": "University of Victoria, Canada",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 384,
    "Scholar name": "George Tzanetakis",
    "Scholar id": "yPgxxpwAAAAJ",
    "Affiliation": "Professor of Computer Science, Faculty of Engineering, University of Victoria",
    "Citedby": 10608,
    "Interests": [
        "music information retrieval",
        "audio signal processing",
        "machine learning",
        "human-computer interaction"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=yPgxxpwAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Estimation of the direction of strokes and arpegios",
            "Publication year": 2014,
            "Publication url": "https://riuma.uma.es/xmlui/handle/10630/8370",
            "Abstract": "Whenever a chord is played in a musical instrument, the notes  are not commonly played at the same time. Actually, in some instruments, it is impossible to trigger multiple notes simultaneously. In others, the player can consciously select the order of the sequence of notes to play to create a chord. In either case, the notes in the chord can be played very fast, and they can be played from the lowest to the highest pitch note (upstroke) or from the highest to the lowest pitch note (downstroke).   In this paper, we describe a system to automatically estimate the direction of strokes and arpeggios from audio recordings. The proposed system is based on the analysis of the spectrogram to identify meaningful changes. In addition to the estimation of the up or down stroke direction, the proposed method provides information about the number of notes that constitute the chord, as well as the chord playing speed. The system has been tested with four different instruments: guitar, piano, autoharp and organ.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:SFOYbPikdlgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "MUSESCAPE: An interactive content-aware music browser",
            "Publication year": 2003,
            "Publication url": "https://www.dafx.de/paper-archive/2003/pdfs/dafx83.pdf",
            "Abstract": "Advances in hardware performance, network bandwidth and audio compression have made possible the creation of large personal digital music collections. Although, there is a significant body of work in image and video browsing, there has been little work that directly addresses the problem of audio and especially music browsing. In this paper, Musescape, a prototype music browsing system is described and evaluated. The main characteristics of the system are automatic configuration based on Computer Audition techniques and the use of continuous audio-music feedback while browsing and interacting with the system. The described ideas and techniques take advantage of the unique characteristics of music signals. A pilot user study was conducted to explore and evaluate the proposed user interface. The results indicate that the use of automatically extracted tempo information reduces browsing time and that continuous interactive audio feedback is appropriate for this particular domain.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:L8Ckcad2t8MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Music information retrieval",
            "Publication year": 2014,
            "Publication url": "http://marsyas.cs.uvic.ca/mirBook/mirBook_jan05_2015.pdf",
            "Abstract": "Music Information Retrieval (MIR) is a field that has been rapidly evolving since 2000. It encompases a wide variety of ideas, algorithms, tools, and systems that have been proposed to handle the increasingly large and varied amounts of musical data available digitally. Researchers in this emerging field come from many different backgrounds. These include Computer Science, Electrical Engineering, Library and Information Science, Music, and Psychology. They all share the common vision of designing algorithms and building tools that can help us organize, understand and search large collections of music in digital form. However, they differ in the way they approach the problem as each discipline has its own terminology, existing knowledge, and value system. The resulting complexity, characteristic of interdisciplinary research, is one of the main challenges facing students and researchers entering this new field. At the same time, it is exactly this interdisciplinary complexity that makes MIR such a fascinating topic.The goal of this book is to provide a comprehensive overview of past and current research in Music Information Retrieval. A considerable challenge has been to make the book accessible, and hopefully interesting, to readers coming from the wide variety of backgrounds present in MIR research. Intuition and high-level understanding are stressed while at the same time providing sufficient technical information to support implementation of the described algorithms. The fundamental ideas and concepts that are needed in order to understand the published literature are also introduced. A thorough bibliography to the relevant literature is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:rQcm2j6_ZE8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A new method for classification of events in noisy hydrophone data",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6032972/",
            "Abstract": "In this paper a new method for classifying events in noisy hydrophone data is developed. The method takes an image processing approach to the 1D hydrophone data by first converting it into a log-frequency spectrogram image (cepstrum). This image is then filtered by reconstructing it based on mutual information (MI) criteria of the dominant orientation map. The features of the reconstructed cepstrum are then enhanced using a combination of edge-tracking and noise smoothing. Feature classification on the processed cepstrum is performed using a least-squares support vector machine (LS-SVM). The method showed event detection sensitivity in excess of 99% for rare events such as whale calls from noisy hydrophone recordings from the NEPTUNE Canada project, with in excess of 97% specificity and 98% overall accuracy. With relatively low computational cost and high accuracy, the proposed method is useful \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:WA5NYHcadZ8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Beyond timbral statistics: Improving music classification using percussive patterns and bass lines",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5565447/",
            "Abstract": "This paper discusses a new approach for clustering sequences of bar-long percussive and bass-line patterns in audio music collections and its application to genre classification. Many musical genres and styles are characterized by two kinds of distinct representative patterns, i.e., percussive patterns and bass-line patterns. So far, in most automatic genre classification systems, rhythmic and bass melody information has not been effectively used. In order to extract bar-long unit rhythmic patterns for a music collection, we propose a clustering method based on one-pass dynamic programming and  k -means clustering. For clustering bass-line patterns, a method based on  k  -means clustering capable of handling pitch-shifting is proposed. After extracting these two fundamental kinds of patterns for each style/genre, feature vectors which are suitable for representing information about the patterns are proposed for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:738O_yMBCRsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Visualyzic: A live-input visualizer",
            "Publication year": 2018,
            "Publication url": "http://dspace.library.uvic.ca/handle/1828/9247",
            "Abstract": "Audio visualization is the process of generation of images based on the audio data. This audio data is extracted from the music and this extraction of information from audio signals is known as content-based audio processing. It is a part of Music Information Retrieval (MIR). MIR is a young and active multidisciplinary research domain that addresses the development of methods for computation of semantics and similarity within music. This project is a single-page application of an audio visualizer that takes in the live-input through your device's microphone and visualizes it. The visualizations have been done by using the canvas tag, WebGL library and THREE.js library. The extraction of the audio features has been done by using Web Audio API. This web application can be used in studying the various aspects of Web Audio API in MIR. This application can also be used as a visible speech unit for the deaf.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:x7X4uJ7IbpoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Music information retrieval",
            "Publication year": 2009,
            "Publication url": "https://www.taylorfrancis.com/chapters/edit/10.1081/E-ELIS3-120043656/music-information-retrieval-kjell-lemstr%C3%B6m-george-tzanetakis?context=ubx",
            "Abstract": "This entry gives an overview of the area of music information retrieval. More specifically, it focuses on what has been done for retrieving polyphonic musical documents both in symbolic and audio forms. It also describes research efforts to combine symbolic-based and audio-based approaches.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:UoSa6IK8DwsC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "A comparison of sensor strategies for capturing percussive gestures",
            "Publication year": 2005,
            "Publication url": "https://www.academia.edu/download/30705059/10.1.1.59.2941.pdf",
            "Abstract": "Drum controllers designed by researchers and commercial companies use a variety of techniques for capturing percussive gestures. It is challenging to obtain both quick response times and low-level data (such as position) that contain expressive information. This research is a comprehensive study of current methods to evaluate the available strategies and technologies. This study aims to demonstrate the benefits and detriments of the current state of percussion controllers as well as yield tools for those who would wish to conduct this type of study in the future.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio analysis using the discrete wavelet transform",
            "Publication year": 2001,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.4555&rep=rep1&type=pdf",
            "Abstract": "The Discrete Wavelet Transform (DWT) is a transformation that can be used to analyze the temporal and spectral properties of non-stationary signals like audio. In this paper we describe some applications of the DWT to the problem of extracting information from non-speech audio. More specifically automatic classification of various types of audio using the DWT is described and compared with other traditional feature extractors proposed in the literature. In addition, a technique for detecting the beat attributes of music is presented. Both synthetic and real world stimuli were used to evaluate the performance of the beat detection algorithm.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Query-by-Beat-Boxing: Music Retrieval For The DJ.",
            "Publication year": 2004,
            "Publication url": "https://scholar.google.com/scholar?cluster=8310227160773378488&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:H30kdXGRGPkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Strike-A-Tune: Fuzzy Music Navigation Using a Drum Interface.",
            "Publication year": 2007,
            "Publication url": "http://webhome.cs.uvic.ca/~gtzan/work/pubs/ismir07gtzanA.pdf",
            "Abstract": "A traditional music library system controlled by a mouse and keyboard is precise, allowing users to select their desired song. Alternatively, randomized playlist or shuffles are used when users have no particular music in mind. We present a new interface and visualization system called Strike-A-Tune for fuzzy music navigation. Fuzzy navigation is an imprecise navigation approach allowing users to choose preference related items. We believe this will help users to play music they want to hear and re-discover infrequently played songs in their music library, thus combining the best aspects of precision navigation and shuffles. We have designed an interface using an electronic drum to communicate with a visualization and playback system.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:NMxIlDl6LWMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Direct and surrogate sensing for the Gyil african xylophone.",
            "Publication year": 2012,
            "Publication url": "https://www.academia.edu/download/37195298/222_Final_Manuscript.pdf",
            "Abstract": "The Gyil is a pentatonic African wooden xylophone with 14-15 keys. The work described in this paper has been motivated by three applications: computer analysis of Gyil performance, live improvised electro-acoustic music incorporating the Gyil, and hybrid sampling and physical modeling. In all three of these cases, detailed information about what is played on the Gyil needs to be digitally captured in real-time. We describe a direct sensing apparatus that can be used to achieve this. It is based on contact microphones and is informed by the specific characteristics of the Gyil. An alternative approach based on indirect acquisition is to apply polyphonic transcription on the signal acquired by a microphone without requiring the instrument to be modified. The direct sensing apparatus we have developed can be used to acquire ground truth for evaluating different approaches to polyphonic transcription and help create a \u201csurrogate\u201d sensor. Some initial results comparing different strategies to polyphonic transcription are presented.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:UHK10RUVsp4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Method and system for analyzing digital audio files",
            "Publication year": 2011,
            "Publication url": "https://patents.google.com/patent/US20110035035A1/en",
            "Abstract": "A fingerprint is generated from an unknown audio signal by dividing the unknown audio signal into bins, where each bin includes points representing a feature space. Each of the points is mapped to one of a plurality of predetermined cluster centers based on the distance between each point and the plurality of cluster centers, each cluster center being associated with an element of a codebook. A string of elements is generated based on the mapping and compressed.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:uH1VZYVfkoQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Music Information Retrieval",
            "Publication year": 2017,
            "Publication url": "https://www.taylorfrancis.com/chapters/music-information-retrieval-kjell-lemstr%C3%B6m-george-tzanetakis/e/10.1081/E-ELIS4-120043656",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:QbuKDewGlxwC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "Improving automatic music tag annotation using stacked generalization of probabilistic svm outputs",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1631272.1631393",
            "Abstract": "Music listeners frequently use words to describe music. Personalized music recommendation systems such as Last. fm and Pandora rely on manual annotations (tags) as a mechanism for querying and navigating large music collections. A well-known issue in such recommendation systems is known as the cold-start problem: it is not possible to recommend new songs/tracks until those songs/tracks have been manually annotated. Automatic tag annotation based on content analysis is a potential solution to this problem and has recently been gaining attention. We describe how stacked generalization can be used to improve the performance of a state-of-the-art automatic tag annotation system for music based on audio content analysis and report results on two publicly available datasets.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Zph67rFs4hoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "One Billion Audio Sounds from GPU-enabled Modular Synthesis",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2104.12922",
            "Abstract": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, which is 100x larger than any audio dataset in the literature. Each sound is paired with the corresponding latent parameters used to generate it. synth1B1 samples are deterministically generated on-the-fly 16200x faster than real-time (714MHz) on a single GPU using torchsynth (https://github.com/torchsynth/torchsynth), an open-source modular synthesizer we release. Additionally, we release two new audio datasets: FM synth timbre (https://zenodo.org/record/4677102) and subtractive synth pitch (https://zenodo.org/record/4677097). Using these datasets, we demonstrate new rank-based synthesizer-motivated evaluation criteria for existing audio representations. Finally, we propose novel approaches to synthesizer hyperparameter optimization, and demonstrate how perceptually-correlated auditory distances could enable new applications in synthesizer design.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Wu2CBzYYpfQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A comparison between audio and IMU data to detect chewing events based on an earable device",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3396339.3396362",
            "Abstract": "The feasibility of collecting various data from built-in wearable sensors has enticed many researchers to use these devices for analyzing human activities and behaviors. In particular, audio, video, and motion data have been utilized for automatic dietary monitoring. In this paper, we investigate the feasibility of detecting chewing activities based on audio and inertial sensor data obtained from an ear-worn device, eSense. We process each sensor data separately and determine the accuracy of each sensing modality for chewing detection when using MFCC and Spectral Centroid as features and Logistic Regression, Decision Tree, and Random Forest as classifiers. We also measure the performance of chewing detection when fusing features extracted from both audio and inertial sensor data. We evaluate the chewing detection algorithm by running a pilot study inside a lab environment on a total of 5 participants. This \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:D52hNgOu9GcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Singing style investigation by residual siamese convolutional neural networks",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8461660/",
            "Abstract": "Investigating singing style is a difficult problem as individual styles are intertwined with melodies from different songs. In this paper, a methodology to investigate singing style is proposed. The proposed approach utilizes convolutional neural networks in a siamese architecture. In addition, we investigate variants of the networks to improve the audio feature extraction process. The potential of the proposed method for analyzing singing style is demonstrated using experiments on pop music singing recordings. The results indicate that the use of the proposed method is indeed effective in learning audio features that are relevant for characterizing singing style.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:SPgoriM2DtkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Guitar model recognition from single instrument audio recordings",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7334864/",
            "Abstract": "The main goal of this paper is to explore the recognition of particular guitar models from single instrument audio recordings. This is different than existing work in music instrument recognition that deals with identifying different instrument types. Through a set of experiments we evaluate different sets of audio features and classifiers for this purpose. To improve accuracy a composite classifier is implemented to first discriminate between electric and acoustic guitars. This affords flexibility in training different models for each guitar type. A data set consisting of audio recordings from 15 guitar models, each recorded with a set of different playing configurations, is used for training and testing. We have found that K Nearest Neighbors and Support Vector Machine (SVM) classifiers perform the best. Testing is done by leaving a specific playing configuration out of the training model. Specific test cases show satisfactory results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:WzbvD7BMtBoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A comparison of human and automatic musical genre classification",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1326806/",
            "Abstract": "Recently there has been an increasing amount of work in the area of automatic genre classification of music in audio format. In addition to automatically structuring large music collections such classification can be used as a way to evaluate features for describing musical content. However the evaluation and comparison of genre classification systems is hindered by the subjective perception of genre definitions by users. In this work, we describe a set of experiments in automatic musical genre classification. An important contribution of this work is the comparison of the automatic results with human genre classifications on the same dataset. The results show that, although there is room for improvement, genre classification is inherently subjective and therefore perfect results can not be expected neither from automatic nor human classification. The experiments also show that features derived from an auditory model \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:N5tVd3kTz84C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Flexible scheduling for dataflow audio processing",
            "Publication year": 2006,
            "Publication url": "https://webhome.cs.uvic.ca/~gtzan/work/pubs/icmc06gtzan.pdf",
            "Abstract": "The notions of audio and control rate have been a pervasive feature of audio programming languages and environments. Real-time computer music systems depend on schedulers to coordinate and order the execution of many tasks over the course of time. In this paper we describe the scheduling infrastructure of Marsyas-0.2, an open source framework for audio analysis and synthesis. We describe how to support multiple, simultaneous, dynamic control rates while retaining the efficiency of block audio processing. In addition we show how timers and events can be abstracted and decoupled from the scheduler in an extensible way. Specific types of supported events such as control updates, implicit patching, wires and expressions are described. In addition, we show how multiple timers based on sample-time, real-time and virtual-time can be utilized. The work in this paper has been motivated by the precise handling of time in the Chuck audio programming language and therefore we show how a simple Chuck program can be expressed in the Marsyas Scripting Language (MSL).",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:RHpTSmoSYBkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "MarsyasX: multimedia dataflow processing with implicit patching",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1459359.1459510",
            "Abstract": "The design and implementation of multimedia signal processing systems is challenging especially when efficiency and real-time performance is desired. In many modern applications, software systems must be able to handle multiple flows of various types of multimedia data such as audio and video. Researchers frequently have to rely on a combination of different software tools for each modality to assemble proof-of-concept systems that are inefficient, brittle and hard to maintain. Marsyas is a software framework originally developed to address these issues in the domain of audio processing. In this paper we describe MarsyasX, a new open-source cross-modal analysis framework that aims at a broader score of applications. It follows a dataflow architecture where complex networks of processing objects can be assembled to form systems that can handle multiple and different types of multimedia flows with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:blknAaTinKkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Integrating hyperinstruments, musical robots & machine musicianship for North Indian classical music",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1279740.1279788",
            "Abstract": "This paper describes a system enabling a human to perform music with a robot in real-time, in the context of North Indian classical music. We modify a traditional acoustic sitar into a hyperinstrument in order to capture performance gestures for musical analysis. A custom built four-armed robotic Indian drummer was built using a microchip, solenoids, aluminum and folk frame drums. Algorithms written towards\" intelligent\" machine musicianship are described.'The final goal of this research is to have a robotic drummer accompany a professional human sitar player live in performance.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Physical modelling and supervised training of a virtual string quartet",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2502081.2502101",
            "Abstract": "This work improves the realism of synthesis and performance of string quartet music by generating audio through physical modelling of the violins, viola, and cello. To perform music with the physical models, virtual musicians interpret the musical score and generate actions which control the physical models. The resulting audio and haptic signals are examined with support vector machines, which adjust the bowing parameters in order to establish and maintain a desirable timbre. This intelligent feedback control is trained with human input, but after the initial training is completed, the virtual musicians perform autonomously. The system can synthesize and control different instruments of the same type (eg, multiple distinct violins) and has been tested on two distinct string quartets (total of 8 violins, 2 violas, 2 cellos). In addition to audio, the system creates a video animation of the instruments performing the sheet music.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:9Nmd_mFXekcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Combining prior\u2010knowledge and grouping cues using a spectral clustering approach.",
            "Publication year": 2008,
            "Publication url": "https://asa.scitation.org/doi/pdf/10.1121/1.4783117",
            "Abstract": "Learning happens at the boundary interactions beween prior knowledge and incoming data. The same interplay takes place when trying to analyze and separate complex mixtures of sound sources such as music. Many approaches to this problem can be broadly categorized as either model based or grouping based. Although it is known that our perceptual system utilizes both of these types of processing, building such systems computationally has been challenging. As a result most existing systems either rely on prior source models or are solely based on grouping cues. In this work it is argued that formulating this integration problem as clustering based on similarities between time\u2010frequency atoms provides an expressive but discipined approach to building sound source characterization and separation systems and to evaluating their performance. After describing the main components of such an architecture \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:SeFeTyx0c_EC",
            "Publisher": "Acoustical Society of America"
        },
        {
            "Title": "Toward an Intelligent Editor for Jazz Music",
            "Publication year": 2003,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/9789812704337_0061",
            "Abstract": "The majority of existing work in Music Information Retrieval (MIR) has been concerned with the similarity relations between different pieces of music rather than their internal structure and content. Although a limited number of automatic techniques for analyzing the internal structure and content of musical signals (such as segmentation and structural analysis) have been proposed, there has been little work in integrating these techniques into a common working environment for music understanding. In addition, because of the emerging nature of MIR research, it is necessary to provide interactive tools for experimentation with new algorithms and ideas. As a first step in this direction, a prototype \u201cintelligent\u201d editor for Jazz music has been implemented and is used as a platform for exploring various analysis algorithms and how they can be integrated into a working interactive system. Jazz standards were chosen as a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:hMod-77fHWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Publication Submission Form",
            "Publication year": 2014,
            "Publication url": "https://www.ri.cmu.edu/pubs/?wpv_view_count=4514-TCPID4515&authors-first-last-and-separated=Jiaji%20Zhou&wpv_post_search=&wpv-publication-taxonomies=0&publication-year=&wpv_filter_submit=Search",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:fTLh7q_iUBEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Guest editorial: special section on music data mining",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6856270/",
            "Abstract": "The five articles in this special section focus on data mining techniques and applications in the music industry. Music has been an important application area for data mining and machine learning techniques for many years. Music data mining is an interdisciplinary area that studies computational methods for understanding and delivering music data and is a topic of growing importance with large commercial relevance and substantial potential. The research area of music data mining has gradually evolved during this time period in order to address the challenge of effectively accessing and interacting with these increasing large collections of music and associated data such as styles, artists, lyrics and music reviews. The algorithms and systems developed frequently employ sophisticated and advanced data mining and machine learning techniques in their attempt to better capture the frequently elusive relevant \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:OWslULmvb_UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Training surrogate sensors in musical gesture acquisition systems",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5610728/",
            "Abstract": "Capturing the gestures of music performers is a common task in interactive electroacoustic music. The captured gestures can be mapped to sounds, synthesis algorithms, visuals, etc., or used for music transcription. Two of the most common approaches for acquiring musical gestures are: 1) \u201chyper-instruments\u201d which are \u201ctraditional\u201d musical instruments enhanced with sensors for directly detecting the gestures and 2) \u201cindirect acquisition\u201d in which the only sensor is a microphone capturing the audio signal. Hyper-instruments require invasive modification of existing instruments which is frequently undesirable. However, they provide relatively straightforward and reliable sensor measurements. On the other hand, indirect acquisition approaches typically require sophisticated signal processing and possibly machine learning algorithms in order to extract the relevant information from the audio signal. The idea of using \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:HoB7MX3m0LUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Content-aware web browsing and visualization tools for cantillation and chant research",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5137820/",
            "Abstract": "Chant and cantillation research is particularly interesting as it explores the transition from oral to written transmission of music. The goal of this work to create Web-based computational tools that can assist the study of how diverse recitation traditions, having their origin in primarily non-notated melodies, later became codified. One of the authors is a musicologist and music theorist who has guided the system design and development by providing manual annotations and participating in the design process. We describe novel content-based visualization and analysis algorithms that can be used for problem-seeking exploration of audio recordings of chant and recitations.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:f2IySw72cVMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Icme 2004 tutorial: Audio feature extraction",
            "Publication year": 2004,
            "Publication url": "http://webhome.cs.uvic.ca/~gtzan/work/talks/icme04/icme04tutorial.pdf",
            "Abstract": "ICME 2004 Tutorial: Audio Feature Extraction Page 1 1 ICME 2004 Tutorial: Audio Feature \nExtraction George Tzanetakis Assistant Professor Computer Science Department University of \nVictoria, Canada gtzan@cs.uvic.ca http://www.cs.uvic.ca/~gtzan Page 2 2 Bits of the history of bits \n01011110101010 Hello world Understanding multimedia content -> Web Multimedia Page 3 3 \nTutorial Goals Overview of state of the art Fundamentals Technical Background \u00a1 Some math, \ncomputer science, music Shift emphasis from audio coding/compression to audio analysis There is \nmore to audio analysis than MFCCs Page 4 4 Some simple but important observations \u00a2 \nAnalysis/Understanding require multiple representations \u2013 no \u201cbest\u201d one \u00a2 \nCoding/Compression/Processing typically search for the \u201cbest\u201d \u201coptimal\u201d way to do things \u00a2 \nParadigm shift is necessary to make multimedia more than just lots of numbers \u00a2 !!! MACHINE 5 5 : '\u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:0KZCP5UExFUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Decoding music in the human brain using EEG data",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8547051/",
            "Abstract": "Semantic vectors, or language embeddings, are used in computational linguistics to represent language for a variety of machine related tasks including translation, speech to text, and natural language understanding. These semantic vectors have also been extensively studied in correlation with human brain data, showing evidence that the representation of language in the human brain can be modeled through these vectors with high correlation. Further, various attempts have been made to study how the human brain represents and understands music. For example, it has been shown that EEG data of subjects listening to music can be used for tempo detection and singer gender recognition. We propose studying the relationship between the EEG data of subjects listening to audio and the audio feature vectors modeled after the semantic vectors in computational linguistics. This could provide new insight into how \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:I96H1Mlar6gC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A new image-based method for event detection and extraction of noisy hydrophone data",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-21596-4_33",
            "Abstract": "In this paper, a new image based method for detecting and extracting events in noisy hydrophone data sequence is developed. The method relies on dominant orientation and its robust reconstruction based on mutual information (MI) measure. This new reconstructed dominant orientation map of the spectrogram image can provide key segments corresponding to various acoustic events and is robust to noise. The proposed method is useful for long-term monitoring and a proper interpretation for a wide variety of marine mammals and human related activities using hydrophone data. The experimental results demonstrate that this image based approach can efficiently detect and extract unusual events, such as whale calls from the highly noisy hydrophone recordings.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:pyW8ca7W8N0C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Marsyas submissions to MIREX 2012",
            "Publication year": 2012,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.439.1402&rep=rep1&type=pdf",
            "Abstract": "Marsyas is an open source software framework for audio analysis, synthesis and retrieval with specific emphasis on Music Information Retrieval. It is developed by an international team of programmers and researchers led by George Tzanetakis. In MIREX 2012 the Marsyas team participated in the following tasks that we have participated in the past: Audio Classical Composer Identification, Audio Genre Classification (Latin and Mixed), Audio Music Mood Classification, Audio Music Similarity and Retrieval, and Audio Tagging Tasks. In addition we participated for the first time with baseline systems to audio key detection and multiple F0 estimation and tracking. Finally, the INESC Beat Tracker (IBT) that is written in Marsyas participated in the beat tracking task but it is described in a separate abstract. In this abstract we describe the specific algorithmic details of our submissions and provide information about how researchers can use our system using the MIREX input/output conventions on their own datasets. Also some comments on the results are provided.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:bKwnt0rjkrwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Espresso: Efficient forward propagation for BCNNs",
            "Publication year": 2017,
            "Publication url": "https://arxiv.org/abs/1705.07175",
            "Abstract": "There are many applications scenarios for which the computational performance and memory footprint of the prediction phase of Deep Neural Networks (DNNs) needs to be optimized. Binary Neural Networks (BDNNs) have been shown to be an effective way of achieving this objective. In this paper, we show how Convolutional Neural Networks (CNNs) can be implemented using binary representations. Espresso is a compact, yet powerful library written in C/CUDA that features all the functionalities required for the forward propagation of CNNs, in a binary file less than 400KB, without any external dependencies. Although it is mainly designed to take advantage of massive GPU parallelism, Espresso also provides an equivalent CPU implementation for CNNs. Espresso provides special convolutional and dense layers for BCNNs, leveraging bit-packing and bit-wise computations for efficient execution. These techniques provide a speed-up of matrix-multiplication routines, and at the same time, reduce memory usage when storing parameters and activations. We experimentally show that Espresso is significantly faster than existing implementations of optimized binary neural networks ( 2 orders of magnitude). Espresso is released under the Apache 2.0 license and is available at http://github.com/fpeder/espresso.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:2Iopv88g0QUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Marsyas3D: a prototype audio browser-editor using a large scale immersive visual and audio display",
            "Publication year": 2001,
            "Publication url": "https://smartech.gatech.edu/handle/1853/50625",
            "Abstract": "Most audio editing tools offer limited capabilities for browsing and editing large collections of files. Moreover working with many audio files tends to clutter the limited screen space of a desktop monitor. In this paper we describe MARSYAS3D, a prototype audio browser and editor for large audio collections. A variety of 2D and 3D graphics interfaces for working with collections and/or individual files have been developed. Many of these interfaces are informed by automatic content-based audio analysis tools. Although MARSYAS3D can be used with a desktop monitor it has been specifically designed as an application for the Princeton Scalable Display Wall project. The current Display Wall system has an 8 x 18-foot rear projection screen with a resolution of 4096 x 1536 pixels. For sound a custom-made 16-speaker surround system is used. The users interact with the Wall using a variety of input methods. This immersive display allows for visual and aural presentation of detailed information for browsing and editing large audio collections and supports natural interactive collaborations among multiple simultaneous users.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:roLk4NBRz8UC",
            "Publisher": "Georgia Institute of Technology"
        },
        {
            "Title": "Query-by-beat-boxing: Music retrieval for the DJ",
            "Publication year": 2004,
            "Publication url": "http://www.ee.columbia.edu/~dpwe/ismir2004/CRFILES/paper217.pdf",
            "Abstract": "BeatBoxing is a type of vocal percussion, where musicians use their lips, cheeks, and throat to create different beats. It is commonly used by hiphop and rap artists. In this paper, we explore the use of BeatBoxing as a query mechanism for music information retrieval and more speci\u00a3 cally the retrieval of drum loops. A classi\u00a3 cation system that automatically identi\u00a3 es the individual beat boxing sounds and can map them to corresponding drum sounds has been developed. In addition, the tempo of BeatBoxing is automatically detected and used to dynamically browse a database of music. We also describe some experiments in extracting structural representations of rhythm and their use for style classi\u00a3 cation of drum loops.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:WF5omc3nYNoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Distributed Audio Feature Extraction for Music.",
            "Publication year": 2005,
            "Publication url": "https://www.academia.edu/download/30705064/2125.pdf",
            "Abstract": "One of the important challenges facing music information retrieval (MIR) of audio signals is scaling analysis algorithms to large collections. Typically, analysis of audio signals utilizes sophisticated signal processing and machine learning techniques that require significant computational resources. Therefore, audio MIR is an area were computational resources are a significant bottleneck. For example, the number of pieces utilized in the majority of existing work in audio MIR is at most a few thousand files. Computing audio features over thousands files can sometimes take days of processing. In this paper, we describe how Marsyas-0.2, a free software framework for audio analysis and synthesis can be used to rapidly implement efficient distributed audio analysis algorithms. The framework is based on a dataflow architecture which facilitates partitioning of audio computations over multiple computers. Experimental results demonstrating the effectiveness of the proposed approach are presented.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:4TOpqqG69KYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cooperative abnormal sound event detection in end-edge-cloud orchestrated systems",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s42045-020-00042-x",
            "Abstract": "In this paper, we propose a novel cooperative abnormal sound event detection framework for city surveillance in end-edge-cloud orchestrated systems. A novel offloading decision-making scheme that leverages hierarchical computational capabilities is proposed to speed up the detection process. The audio pre-processing (feature extraction) and post-processing (sound source localization and sound event classification) can be locally executed or offloaded to the edge or cloud based on the calculation of the so-called communication-to-computation ratio. Furthermore, considering the biased audio information due to source-sensor geometries, a cooperative decision-making algorithm is proposed to aggregate the sound event classification results with adaptive control and ensemble learning. In the audio pre-processing, the log-mel spectrogram and time of arrival information are first extracted from the audio \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:l07DEcJES74C",
            "Publisher": "Springer Singapore"
        },
        {
            "Title": "Ensemble interest point detection for audio matching",
            "Publication year": 2015,
            "Publication url": "https://patents.google.com/patent/US9098576B1/en",
            "Abstract": "Systems and methods for audio matching are disclosed herein. In one embodiment, a system includes both interest point mixing and fingerprint mixing by using multiple interest point detection methods in parallel. Since multiple interest point detection methods are used in parallel, accuracy of audio matching is improved across a wide variety of audio signals. In addition the scalability of the disclosed audio matching system is increased by matching the fingerprint of an audio sample with a fingerprint of a reference sample versus matching an entire spectrogram. Accordingly, a more accurate and more general solution to audio matching can be accomplished.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:s_JjmAzd-pQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Interactive content-aware music browsing using the radio drum",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4036755/",
            "Abstract": "Portable digital music players are becoming pervasive and the size of personal digital music collections has been steadily increasing (5-10 thousand tracks are common today). The emerging area of music information retrieval (MIR) deals with all aspects of managing, analyzing and organizing music in digital formats. The majority of work in MIR follows a search/retrieval paradigm. More recently, the importance of browsing as an interaction paradigm has been realized and several novel interfaces have been proposed. In this paper, we describe a tangible interface for content-aware browsing of music collections. The radio drum is a gestural interface based on capacitance sensors that can detect the x,y,z positions of two drum sticks in a 3D volume. We describe two possible mappings that can be used for browsing music collections without relying on metadata. The first is an explicit mapping of tempo and beat \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:mB3voiENLucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Music Information Retrieval Based on Signal Processing",
            "Publication year": 2007,
            "Publication url": "https://downloads.hindawi.com/journals/specialissues/201964.pdf",
            "Abstract": "The advances experienced in the last decades in areas as information, communication, and media technologies have made available a large amount of all kinds of data. This is particularly true for music, whose databases have grown exponentially since the advent of the first perceptual coders early in the 90\u2019s. This situation demands for tools able to ease searching, retrieving, and handling such a huge amount of data. Among those tools, automatic musical genre classifiers (AMGC) can have a particularly important role, since they could be able to automatically index and retrieve audio data in a human-independent way. This is very useful because a large portion of the metadata used to describe music content is inconsistent or incomplete.Music search and retrieval is the most important application of AGC, but it is not the only one. There are several other technologies that can benefit from AGC. For example, it would be possible to create an automatic equalizer able to choose which frequency bands should be attenuated or reinforced according to the label assigned to the signal being considered. AGC could also be used to automatically select radio stations playing a particular genre of music. The research field of automatic music genre classification has got increasing importance in the last few years. The",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ealulPZkXgsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio genre classification using percussive pattern clustering combined with timbral features",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5202514/",
            "Abstract": "Many musical genres and styles are characterized by distinct representative rhythmic patterns. In most automatic genre classification systems global statistical features based on timbral dynamics such as mel-frequency cepstral coefficients (MFCC) are utilized but so far rhythmic information has not so effectively been used. In order to extract bar-long unit rhythmic patterns for a music collection we propose a clustering method based on one-pass dynamic programming and k-means clustering. After extracting the fundamental rhythmic patterns for each style/genre a pattern occurrence histogram is calculated and used as a feature vector for supervised learning. Experimental results show that the automatically calculated rhythmic pattern information can be used to effectively classify musical genre/style and improve upon current approaches based on timbral features.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:YOwf2qJgpHMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Music Information Robotics: Coping Strategies for Musically Challenged Robots.",
            "Publication year": 2011,
            "Publication url": "https://www.academia.edu/download/37195301/PS4-16.pdf",
            "Abstract": "In the past few years there has been a growing interest in music robotics. Robotic instruments that generate sound acoustically using actuators have been increasingly developed and used in performances and compositions over the past 10 years. Although such devices can be very sophisticated mechanically, in most cases they are passive devices that directly respond to control messages from a computer. In the few cases where more sophisticated control and feedback is employed it is in the form of simple mappings with little musical understanding. Several techniques for extracting musical information have been proposed in the field of music information retrieval. In most cases the focus has been the batch processing of large audio collections rather than real time performance understanding. In this paper we describe how such techniques can be adapted to deal with some of the practical problems we have experienced in our own work with music robotics. Of particular importance is the idea of self-awareness or proprioception in which the robot (s) adapt their behavior based on understanding the connection between their actions and sound generation through listening. More specifically we describe techniques for solving the following problems: 1) controller mapping 2) velocity calibration, and 3) gesture recognition.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Y5dfb0dijaUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Music Browsing Using a Tabletop Display.",
            "Publication year": 2007,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.123.1449&rep=rep1&type=pdf",
            "Abstract": "The majority of work in Music Information Retrieval (MIR) follows a search/retrieval paradigm. More recently, the importance of browsing as an interaction paradigm has been realized, and several novel interfaces have been proposed. In this paper, we describe two novel interaction schemes for content-aware browsing of music collections that use a graphical tabletop interface. We further present findings from qualitative user studies. We describe our work in the context of two primary themes: music collection browsing, and collaborative (multiple simultaneous users) interaction and involvement during the browsing/selection process.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:TQgYirikUcIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Detecting pianist hand posture mistakes for virtual piano tutoring",
            "Publication year": 2016,
            "Publication url": "https://www.researchgate.net/profile/David-Johnson-142/publication/318028952_Detecting_Pianist_Hand_Posture_Mistakes_for_Virtual_Piano_Tutoring/links/596042100f7e9b8194fc1119/Detecting-Pianist-Hand-Posture-Mistakes-for-Virtual-Piano-Tutoring.pdf",
            "Abstract": "Incorrect hand posture is known to cause fatigue and hand injuries in pianists of all levels. Our research is intended to reduce these problems through new methods of providing direct feedback to piano students during their daily practice. This paper presents an approach to detect hand posture in RGB-D recordings of pianists\u2019 hands while practicing for use in a virtual music tutor. We do so through image processing and machine learning. To test this approach we collect data by recording the hands of two pianists during standard piano exercises. Preliminary results show the effectiveness of our methods.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:qzuIxkxWBNsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Manifold Learning Methods for Visualization and Browsing of Drum Machine Samples",
            "Publication year": 2021,
            "Publication url": "https://www.aes.org/e-lib/browse.cfm?elib=21015",
            "Abstract": "The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. The task of organizing and selecting from these large collections can be challenging and time consuming, which points to the need for improved methods for user interaction. This paper presents a system that computationally characterizes and organizes drum machine samples in two dimensions based on sound similarity. The goal of the work is to support the development of intuitive drum sample browsing systems. The methodology presented explores time segmentation, which isolates temporal subsets from the input signal prior to audio feature extraction, as a technique for improving similarity calculations. Manifold learning techniques are compared and evaluated for dimensionality reduction tasks, and used to organize and visualize audio \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:8rj-XRKUtKYC",
            "Publisher": "Audio Engineering Society"
        },
        {
            "Title": "Musical genre classification of audio signals",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1021072/",
            "Abstract": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:u5HHmVD_uO8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Generating targeted rhythmic exercises for music students with constraint satisfaction programming",
            "Publication year": 2008,
            "Publication url": "https://www.researchgate.net/profile/Torsten-Anders/publication/235925970_Generating_Targeted_Rhythmic_Exercises_for_Music_Students_with_Constraint_Satisfaction_Programming/links/00b7d51463c6b21133000000/Generating-Targeted-Rhythmic-Exercises-for-Music-Students-with-Constraint-Satisfaction-Programming.pdf",
            "Abstract": "Generating technical exercises for various levels of playing ability is important for any instrument method book. Writing exercises by hand can be quite tedious, and severely limits the number of exercises which could be created. This is particularly apparent when we consider computer music tutoring systems, which could benefit from a library of thousands of exercises. This library could be used to pick material which addresses the specific weaknesses of students, or to ensure that the student always practices new material while working on sight-reading. We therefore turn to computer-assisted composition to generate these technical drills. This is a challenging problem for which the use of constraints provides an elegant solution.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:NaGl4SEjCO4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reconfigurable autonomous novel guitar effects (range)",
            "Publication year": 2013,
            "Publication url": "https://www.researchgate.net/profile/Shawn-Trail/publication/310644845_Reconfigurable_Autonomous_Novel_Guitar_Effects_RANGE/links/58348bcf08ae102f07395d69/Reconfigurable-Autonomous-Novel-Guitar-Effects-RANGE.pdf",
            "Abstract": "The RANGE guitar is a minimally-invasive hyperinstrument incorporating electronic sensors and integrated digital signal processing (DSP). It introduces an open framework for autonomous music computing eschewing the use of the laptop on stage. The framework uses an embedded Linux microcomputer to provide sensor acquisition, analog-to-digital conversion (ADC) for audio input, DSP, and digital-to-analog conversion (DAC) for audio output. The DSP environment is built in Puredata (Pd). We chose Pd because it is free, widely supported, flexible, and robust. The sensors we selected can be mounted in a variety of ways without compromising traditional playing technique. Integration with a conventional guitar leverages established techniques and preserves the natural gestures of each player\u2019s idiosyncratic performing style. The result is an easy to replicate, reconfigurable, idiomatic sensing and signal processing system for the electric guitar requiring little modification of the original instrument",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:JjBZBFkNMTQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Case Study in Implementing Music",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=5468028192999307895&hl=en&oi=scholarr",
            "Abstract": "MARSYAS is an open source audio processing framework with specific emphasis on building music information retrieval systems. It has been under development since 1998 and has been used for a variety of projects in both academia and industry. In this chapter, the software architecture of Marsyas will be described. The goal is to highlight design challenges and solutions that are relevant to any MIR software.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:nyUGVxR7BcAC",
            "Publisher": "IGI Global"
        },
        {
            "Title": "The wiikembe\u2014performer designed lamellophone hyperinstrument for idiomatic musical-dsp interaction",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6625512/",
            "Abstract": "The Wiikembe is an augmented Likembe from Zaire, believed to be 100+ years old. A Wiimote affords 3D gesture sensing for musical-HCI. An Arduino interface offers explicit control over DSP functions. Puredata (Pd) scales, converts, and routes control data into Ableton Live. A contact mic is used to acquire a direct audio signal from the Likembe. The audio inputs into a conventional computer audio interface and routed into Live which handles event sequencing, DSP, and audio bussing. The result is a compact and intuitive, robust lamellophone hyperinstrument. The Wiikembe extends the sonic possibilities of the acoustic Likembe without compromising traditional sound production methods or performance techniques. We chose specific sensors and their placement based on constraints regarding the instrument's construction, playing techniques, the author's idiosyncratic compositional approach and sound design \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:uc_IGeMz5qoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Stereo panning information for music information retrieval tasks",
            "Publication year": 2010,
            "Publication url": "http://www.aes.org/e-lib/browse.cfm?elib=15454",
            "Abstract": "Recording engineers, mixers, and producers play important yet often overlooked roles in defining the sound of a particular record, artist, or group. The placement of different sound sources in space using stereo panning is an important component of their work. Stereo panning information typically is not utilized in music information retrieval (MIR) tasks such as genre and artist classification. A set of audio features is proposed that can be used to characterize stereo panning information and contrast two different methods of calculation. These features are shown to provide statistically important information for nontrivial audio classification tasks and are compared with the traditional mel-frequency cepstral coefficients for different MIR tasks. They can also be viewed as a first attempt to capture extramusical information related to the production process through music information retrieval techniques.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:RGFaLdJalmkC",
            "Publisher": "Audio Engineering Society"
        },
        {
            "Title": "Vrmin: using mixed reality to augment the theremin for musical tutoring.",
            "Publication year": 2017,
            "Publication url": "https://www.nime.org/proceedings/2017/nime2017_paper0029.pdf",
            "Abstract": "The recent resurgence of Virtual Reality (VR) technologies provide new platforms for augmenting traditional music instruments. Instrument augmentation is a common approach for designing new interfaces for musical expression, as shown through hyperinstrument research. New visual affordances present in VR give designers new methods for augmenting instruments to extend not only their expressivity, but also their capabilities for computer assisted tutoring. In this work, we present VRMin, a mobile Mixed Reality (MR) application for augmenting a physical theremin, with an immersive virtual environment (VE), for real time computer assisted tutoring. We augment a physical theremin with 3D visual cues to indicate correct hand positioning for performing given notes and volumes. The physical theremin acts as a domain specific controller for the resulting MR environment. The initial effectiveness of this approach is measured by analyzing a performer\u2019s hand position while training with and without the VRMin. We also evaluate the usability of the interface using heuristic evaluation based on a newly proposed set of guidelines designed for VR musical environments.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:vs4DU1qUSb8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "El-Lamellophone A Low-cost, DIY, Open Framework for Acoustic Lemellophone Based Hyperinstruments.",
            "Publication year": 2014,
            "Publication url": "https://scholar.google.com/scholar?cluster=16444573418621890666&hl=en&oi=scholarr",
            "Abstract": "The El-Lamellophone (El-La) is a Lamellophone hyperinstrument incorporating electronic sensors and integrated DSP. Initial investigations have been made into digitallycontrolled physical actuation of the acoustic tines. An embedded Linux micro-computer supplants the laptop. A piezoelectric pickup is mounted to the underside of the body of the instrument for direct audio acquisition providing a robust signal with little interference. The signal is used for electric sound-reinforcement, creative signal processing and audio analysis developed in Puredata (Pd). This signal inputs and outputs the micro computer via stereo 1/8th inch phono jacks. Sensors provide gesture recognition affording the performer a broader, more dynamic range of musical human computer interaction (MHCI) over specific DSP functions. Work has been done toward electromagnetic actuation of the tines, aiming to allow performer control and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:twffdjNOitAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "New tools for use in the musicology of record production",
            "Publication year": 2009,
            "Publication url": "http://www.sness.net/papers/mcnally2011musicology.pdf",
            "Abstract": "This paper introduces a stereo 3-D panning visualization tool based on methods borrowed from the field of Music Information Retrieval (MIR). This tool helps to illustrate and quantify production decisions and recording practices used by engineers and producers in the record production process. The tool is also valuable for pedagogical purposes, providing students with a visual feedback of what they are (or are not) hearing in recordings as they develop their critical listening skills. A case study comparing a body of work by Tchad Blake and Rick Rubin illustrates the value of this tool to the musicology of record production.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:1TqUyixsFioC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sezione III. Il contesto extraeuropeo: Capitolo 11. Where is the Music in Music Information Retrieval?",
            "Publication year": 2004,
            "Publication url": "https://www.torrossa.com/gs/resourceProxy?an=2268710&publisher=F19587",
            "Abstract": "Music information retrieval (MIR) of audio signals is an exciting new research area that requires sophisticated signal processing and machine learning tools. Initial work in audio MIR was influenced by other types of audio analysis such as speech recognition. However, recently various ideas and techniques that are specific to musical signals have started appearing in the literature resulting in more powerful and sophisticated retrieval systems. This chapter explores some of these music specific algorithms and relates them to standard musical terminology. More specifically algorithms for the automatic extraction of rhythmicic information, pitch content, musical structure and linking audio signals to musical scores are described.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:qjuL_XCUnM8C",
            "Publisher": "AIDA"
        },
        {
            "Title": "Adaptive N-normalization for enhancing music similarity",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5946422/",
            "Abstract": "The N-Normalization is an efficient method for normalizing a given similarity computed among multimedia objects. It can be considered for clustering and kernel enhancement. However, most approaches to N-Normalization parametrize the method arbitrarily in an ad-hoc manner. In this paper, we show that the optimal parameterization is tightly related to the geometry of the problem at hand. For that purpose, we propose a method for estimating an optimal parameterization given only the associated pair-wise similarities computed from any specific dataset. This allows us to normalize the similarity in a meaningful manner. More specifically, the proposed method allows us to improve retrieval performance as well as minimize unwanted phenomena such as hubs and orphans.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:g5m5HwL7SMYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Snare drum motion capture dataset.",
            "Publication year": 2015,
            "Publication url": "https://www.nime.org/proceedings/2015/nime2015_148.pdf",
            "Abstract": "Comparative studies require a baseline reference and a documented process to capture new subject data. This paper combined with its principal reference [1] presents a definitive dataset in the context of snare drum performances along with a procedure for data acquisition, and a methodology for quantitative analysis. The multi-volume dataset contains video, audio, and discrete two dimensional motion data for forty standardized percussive rudiments.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:lonyfjS0l_UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Music information retrieval based on signal processing",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/content/pdf/10.1155/2007/86874.pdf",
            "Abstract": "The main focus of this special issue is on the application of digital signal processing techniques for music information retrieval (MIR). MIR is an emerging and exciting area of research that seeks to solve a wide variety of problems dealing with preserving, analyzing, indexing, searching, and accessing large collections of digitized music. There are also strong interests in this field of research from music libraries and the recording industry as they move towards digital music distribution. The demands from the general public for easy access to these music libraries challenge researchers to create tools and algorithms that are robust, small, and fast. Music is represented in either encoded audio waveforms (CD audio, MP3, etc.) or symbolic forms (musical score, MIDI, etc.). Audio representations, in particular, require robust signal processing techniques for many applications of MIR since meaningful descriptions need to be \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:D03iK_w7-QYC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "Panel: new directions in music information retrieval",
            "Publication year": 2001,
            "Publication url": "http://www.cs.cmu.edu/afs/.cs.cmu.edu/Web/People/rbd/papers/Panel-ICMC-2001.pdf",
            "Abstract": "This paper and panel discussion will cover the growing and exciting new area of Music Information Retrieval (MIR), as well as the more general topic of Audio Information Retrieval (AIR). The main topics, challenges and future directions of MIR research will be identified and four projects from industry and academia are described.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:8k81kl-MbHgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sound source tracking and formation using normalized cuts",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4217016/",
            "Abstract": "The goal of computational auditory scene analysis (CASA) is to create computer systems that can take as input a mixture of sounds and form packages of acoustic evidence such that each package most likely has arisen from a single sound source. We formulate sound source tracking and formation as a graph partitioning problem and solve it using the normalized cut which is a global criterion for segmenting graphs that has been used in computer vision. It measures both the total dissimilarity between the different groups as well as the total similarity within groups. We describe how this formulation can be used with sinusoidal modeling, a common technique for sound analysis, manipulation and synthesis. Several examples showing the potential of this approach are provided.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:dhFuZR0502QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Using circular models to improve music emotion recognition",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8567988/",
            "Abstract": "The two commonly accepted models of affect used in affective computing are categorical and two-dimensional. However, categorical models are limited to datasets that only contain music for which human annotators fully agree upon, while two-dimensional models use descriptors to which users may not relate to (e.g. Valence and Arousal). This paper explores the hypothesis that the music emotion problem is circular, and shows how circular models can be used for automatic music emotion recognition. This hypothesis is tested through experiments on the two commonly accepted models of affect, as well as on an original circular model proposed by the authors. First, an original dataset was assembled and annotated as a way to investigate agreement among annotators. Then, polygonal approximations of circular regression are proposed as a practical method to investigate whether the circularity of the annotations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:xUT3DyvLuJwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Informedia at TRECVID 2003: Analyzing and searching broadcast news video",
            "Publication year": 2004,
            "Publication url": "https://apps.dtic.mil/sti/citations/ADA456318",
            "Abstract": "A concentrated effort was made by the authors to develop an interface allowing a human to succeed with video topics as defined in TRECVID 2001. This interface was part of the TRECVID 2002 interactive query task, in which a person could issue multiple queries and refinements to the video corpus in formulating the shot answer set for the topic at hand. The interface was designed to present a visually rich set of thumbnail images to the user, tailored for expert control over the number, scale, and attributes of the images. Armed with this interface, an expert user completely familiar with the retrieval system and its features, but having no a priori knowledge of the TRECVID 2002 search test corpus, performed well on the search tasks. This exact system as used in the TRECVID 2002 interactive query task was again used for the TRECVID 2003 evaluation. To facilitate better visual browsing, we extended the storyboard idea to show keyframes across multiple video documents, where a document is automatically derived by segmenting a video production into story units through speech, silence, black frames, and other heuristics. The hierarchy of information units is frame, shot, document and full production. A set of documents is returned by a query. The shots for these documents are presented in a single storyboard, ie, an ordered set of keyframes presented simultaneously on the computer screen, one keyframe per shot. Without further filtering, most queries would overwhelm the user with too many images. Through the use of query context, the cardinality of the image set can be greatly reduced. The search engine for text queries makes use of the Okapi \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "Carnegie-Mellon Univ Pittsburgh PA School of Computer Science"
        },
        {
            "Title": "Subband-based drum transcription for audio signals",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4014047/",
            "Abstract": "Content-based analysis of music can help manage the increasing amounts of music information available digitally and is becoming an important part of multimedia research. The use of drums and percussive sounds is pervasive to popular and world music. In this paper we describe an automatic system for detecting and transcribing low and medium-high frequency drum events from audio signals. Two different subband front-ends are utilized. The first is based on bandpass filters and the second is based on wavelet analysis. Experimental results utilizing music, drum loops and Indian tabla thekas as signals are provided. The proposed system can be used as a preprocessing step for rhythm-based music classification and retrieval. In addition it can be used for pedagogical purposes",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:TFP_iSt0sucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Using fuzzy logic to handle the semantic descriptions of music in a content-based retrieval system",
            "Publication year": 2006,
            "Publication url": "https://biblio.ugent.be/publication/1085507",
            "Abstract": "This paper explores the potential use of fuzzy logic for semantic music recommendation. We show that a set of affective/emotive, structural and kinaesthetic descriptors can be used to formulate a query which allows the retrieval of intended music. A semantic music recommendation system was built, based on an elaborate study of potential users and an analysis of the semantic descriptors that best characterize the user\u2019s understanding of music. Significant relationships between expressive and structural semantic descriptions of music were found. Fuzzy logic was then applied to handle the  quality ratings associated with the semantic descriptions. A working semantic music recommendation system was tested and evaluated. Real-world testing revealed high user satisfaction.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:1yQoGdGgb4wC",
            "Publisher": "Otto-von-Guericke-University Magdeburg"
        },
        {
            "Title": "Physical modeling and hybrid synthesis for the gyil african xylophone",
            "Publication year": 2012,
            "Publication url": "http://webhome.csc.uvic.ca/~gtzan/output/smc2012model.pdf",
            "Abstract": "We propose a physical model for the Gyil, an African pentatonic idiophone with wooden bars that have similar sonic characteristics to the western marimba. The primary focus is modeling the gourds that are suspended beneath each bar and have a similar role to the tuned tubes below the bars in western mallet instruments. The prominent effect of these resonators is the added buzz that results when the bar is struck. This type of intentional sympathetic distortion is inherent to African instrument design as it helps unamplified instruments be heard above crowds of people dancing and singing. The Gyil\u2019s distortion is created by drilling holes on the sides of each gourd and covering them with membranes traditionally made from the silk of spider egg casings stretched across the opening. By analyzing the sonic characteristics of this distortion we have found that the physical mechanisms that create it are highly nonlinear and we have attempted to model them computationally. In addition to the fully synthetic model we consider a hybrid version where the acoustic sound captured by contact microphones on the wooden bars is fed into a virtual model of the gourd resonators. This hybrid approach simplifies significantly the logistic of traveling with the instrument as the gourds are bulky, fragile and hard to pack. We propose several variants of the model, and discuss the feedback we received from expert musicians.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:_Ybze24A_UAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "SpiegeLib: An automatic synthesizer programming library",
            "Publication year": 2020,
            "Publication url": "https://www.aes.org/e-lib/online/browse.cfm?elib=20794",
            "Abstract": "Automatic synthesizer programming is the field of research focused on using algorithmic techniques to generate parameter settings and patch connections for a sound synthesizer. In this paper, we present the Synthesizer Programming with Intelligent Exploration, Generation, and Evaluation Library (spiegelib), an open-source, object oriented software library to support continued development, collaboration, and reproducibility within this field. spiegelib is designed to be extensible, providing an API with classes for conducting automatic synthesizer programming research. The name spiegelib was chosen to pay homage to Laurie Spiegel, an early pioneer in electronic music. In this paper we review the algorithms currently implemented in spiegelib, and provide an example case to illustrate an application of spiegelib in automatic synthesizer programming research.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:rKOGDx9nJ1EC",
            "Publisher": "Audio Engineering Society"
        },
        {
            "Title": "Models for music analysis from a markov logic networks perspective",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7579173/",
            "Abstract": "Analyzing and formalizing the intricate mechanisms of music is a very challenging goal for Artificial Intelligence. Dealing with real audio recordings requires the ability to handle both uncertainty and complex relational structure at multiple levels of representation. Until now, these two aspects have been generally treated separately, probability being the standard way to represent uncertainty in knowledge, while logical representation being the standard way to represent knowledge and complex relational information. Several approaches attempting a unification of logic and probability have recently been proposed. In particular, Markov logic networks (MLNs), which combine first-order logic and probabilistic graphical models, have attracted increasing attention in recent years in many domains. This paper introduces MLNs as a highly flexible and expressive formalism for the analysis of music that encompasses most of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:Fu4hY69slDoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An empirical investigation of PU learning for predicting length of stay",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9565736/",
            "Abstract": "Reliably predicting the length of stay of patients in a hospital based on their demographic and clinical characteristics as well as the care they received can inform hospital planning, particularly in novel response scenarios such as Covid-19. Positive Unlabelled (PU) learning is a type of semi-supervised learning in which only the positive labels in a dataset are reliable. PU learning can be used when the length of stay prediction is formulated as a classification problem, and the prediction needs to be performed dynamically while the patients are being treated. This paper empirically investigates how unlabeling can negatively affect classification accuracy and show how this effect can be mitigated using different algorithms for PU learning. A large dataset of Covid-19 length of hospital stay was used for the experiments. The results show the potential of utilizing PU learning approaches to predicting the length of hospital \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:dpaHy1TF288C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A Force-Sensitive Surface for Intimate Control.",
            "Publication year": 2009,
            "Publication url": "https://www.academia.edu/download/30705044/10.1.1.157.8879.pdf",
            "Abstract": "This paper presents a new force-sensitive surface designed for playing music. A prototype system has been implemented using a passive capacitive sensor, a commodity multichannel audio interface, and decoding software running on a laptop computer. This setup has been a successful, lowcost route to a number of experiments in intimate musical control.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:bEWYMUwI8FkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Chants and orcas: semi-automatic tools for audio annotation and analysis in niche domains",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1460676.1460680",
            "Abstract": "The recent explosion of web-based collaborative applications in business and social media sites demonstrated the power of collaborative internet scale software. This includes the ability to access huge datasets, the ability to quickly update software, and the ability to let people around the world collaborate seamlessly. Multimedia learning techniques have the potential to make unstructured multimedia data accessible, reusable, searchable, and manageable. We present two different web-based collaborative projects: Cantillion, and the Orchive. Cantillion enables ethnomusicology scholars to listen and view data relating to chants from a variety of traditions, letting them view and interact with various pitch contour representations of the chant. The Orchive is a project to digitize over 20,000 hours of Orcinus orca (killer whale) vocalizations, recorded over a period of approximately 35 years, and provide tools to assist \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:maZDTaKrznsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Empirical Analysis of Track Selection and Ordering in Electronic Dance Music using Audio Feature Extraction.",
            "Publication year": 2013,
            "Publication url": "https://ismir2013.ismir.net/wp-content/uploads/2013/09/210_Paper.pdf",
            "Abstract": "Disc jockeys are in some ways the ultimate experts at selecting and playing recorded music for an audience, especially in the context of dance music. In this work, we empirically investigate factors affecting track selection and ordering using DJ-created mixes of electronic dance music. We use automatic content-based analysis and discuss the implications of our findings to playlist generation and ordering. Timbre appears to be an important factor when selecting tracks and ordering tracks, and track order itself matters, as shown by statistically significant differences in the transitions between the original order and a shuffled version. We also apply this analysis to ordering heuristics and suggest that the standard playlist generation model of returning tracks in order of decreasing similarity to the initial track may not be optimal, at least in the context of track ordering for electronic dance music.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:nrtMV_XWKgEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio feature extraction",
            "Publication year": 2011,
            "Publication url": "https://scholar.google.com/scholar?cluster=16621472807934300168&hl=en&oi=scholarr",
            "Abstract": "The automatic analysis of music stored as a digital audio signal requires a sophisticated process of distilling information. For example, a three-minute song stored as uncompressed digital audio is represented digitally by a sequence almost 16 million numbers (3 [minutes]* 60 [seconds]* 2 [stereo channels]* 44100 [sampling rate]). In the case of tempo induction, these 16 million numbers need to somehow be converted to a single numerical estimate of the tempo of the piece.Audio feature extraction forms the foundation for any type of music data mining. It is the process of distilling the huge amounts of raw audio data into much more compact representations that capture higher level information about the underlying musical content. As such, it is much more specific to",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:_B80troHkn4C",
            "Publisher": "CRC Press"
        },
        {
            "Title": "Computational ethnomusicology",
            "Publication year": 2007,
            "Publication url": "https://scholar.google.com/scholar?cluster=10978328362369099322&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:0EnyYjriUFMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Geoshuffle: Location-Aware, Content-based Music Browsing Using Self-organizing Tag Clouds.",
            "Publication year": 2010,
            "Publication url": "http://webhome.csc.uvic.ca/~gtzan/ismir2010gtzan.pdf",
            "Abstract": "In the past few years the computational capabilities of mobile phones have been constantly increasing. Frequently these smartphones are also used as portable music players. In this paper we describe GeoShuffle\u2013a prototype system for content-based music browsing and exploration that targets such devices. One of the most interesting aspects of these portable devices is the inclusion of positioning capabilities based on GPS. GeoShuffle adds location-based and time-based context to a user\u2019s listening preferences. Playlists are dynamically generated based on the location of the user, path and historical preferences. Browsing large music collections having thousands of tracks is challenging. The most common method of interaction is using long lists of textual metadata such as artist name or genre. Current smartphones are characterized by small screen real-estate which limits the amount of textual information that can be displayed. We propose selforganizing tag clouds, a 2D tag cloud representation that is based on an underlying self-organizing map calculated using automatically extracted audio features. To evalute the system the Magnatagatune database is utilized. The evaluation indicates that location and time context can improve the quality of music recommendation and that selforganizing tag clouds provide faster browsing and are more engaging than text-based tag clouds.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:u_35RYKgDlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Soundanchoring: content-based exploration of music collections with anchored self-organized maps",
            "Publication year": 2013,
            "Publication url": "https://www.researchgate.net/profile/George-Tzanetakis/publication/237090695_SoundAnchoring_Content-based_Exploration_of_Music_Collections_with_Anchored_Self-Organized_Maps/links/546f3a630cf2d67fc031027a/SoundAnchoring-Content-based-Exploration-of-Music-Collections-with-Anchored-Self-Organized-Maps.pdf",
            "Abstract": "We present a content-based music collection exploration tool based on a variation of the Self-Organizing Map (SOM) algorithm. The tool, named SoundAnchoring, displays the music collection on a 2D frame and allows users to explicitly choose the locations of some data points known as anchors. By establishing the anchors\u2019 locations, users determine where clusters containing acoustically similar pieces of music will be placed on the 2D frame. User evaluation showed that the cluster location control provided by the anchoring process improved the experience of building playlists and exploring the music collection.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:z62hWG5Wpo0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Manipulation, analysis and retrieval systems for audio signals",
            "Publication year": 2002,
            "Publication url": "https://search.proquest.com/openview/2eb1cefb13a43299e8a3c8560d33c199/1?pq-origsite=gscholar&cbl=18750&diss=y",
            "Abstract": "Digital audio and especially music collections are becoming a major part of the average computer user experience. Large digital audio collections of sound effects are also used by the movie and animation industry. Research areas that utilize large audio collections include: Auditory Display, Bioacoustics, Computer Music, Forensics, and Music Cognition.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimedia technologies for enriched music performance, production, and consumption",
            "Publication year": 2017,
            "Publication url": "https://www.computer.org/csdl/magazine/mu/2017/01/mmu2017010020/13rRUx0xPR9",
            "Abstract": "This special issue gathers state-of-the-art research on multimedia methods and technologies aimed at enriching music performance, production, and consumption.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:FSHXWovK7t4C",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "and Peter Steenkiste",
            "Publication year": 2004,
            "Publication url": "http://www-2.cs.cmu.edu/afs/cs/project/cmcl/archive/2005/music04.pdf",
            "Abstract": "Currently, a large percentage of Internet traffic consists of music files, typically stored in MP3 compressed audio format, shared and exchanged over peer-to-peer (P2P) networks. Searching for music is performed by specifying keywords and na\u0131ve stringmatching techniques. In the past years, the emerging research area of music information retrieval (MIR) has produced a variety of new ways of looking at the problem of music searching. Such MIR techniques can significantly enhance the ways users search for music over P2P networks. For that to happen, there are two main challenges that need to be addressed: scalability to large collections and number of peers; and a richer set of search semantics that can support MIR, especially when the retrieval is content-based. In this article, we describe a scalable P2P system that uses rendezvous points (RPs) for music metadata registration and query resolution. The system supports attribute-value search semantics as well as content-based retrieval. The performance of the system has been evaluated in large-scale usage scenarios using \u2018\u2018real,\u2019\u2019automatically calculated musical content descriptors. One could argue that both the ideas of MIR and P2P became familiar to the general public with Napster (www. napster. com). Although crude both in terms of search capabilities and P2P performance, Napster provided for the first time an example of sharing vast amounts of musical data over large ad-hoc networks. Despite this early connection between MIR and P2P, there has not been much progress in combining these two areas. Although better P2P paradigms have been proposed, searching for music is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:GzlcqhCAosUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Semi-automatic mono to stereo up-mixing using sound source formation",
            "Publication year": 2007,
            "Publication url": "https://www.aes.org/e-lib/online/browse.cfm?elib=14027",
            "Abstract": "In this paper, we propose an original method to include spatial panning information when converting monophonic recordings to stereophonic ones. Sound sources are first identified using perceptively motivated clustering of spectral components. Correlations between these individual sources are then identified to build a middle level representation of the analysed sound. This allows the user to define panning information for major sound sources thus enhancing the stereophonic immersion quality of the resulting sound.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:YFjsv_pBGBYC",
            "Publisher": "Audio Engineering Society"
        },
        {
            "Title": "A comparison of conventional and meta-model based global optimization methods",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7334874/",
            "Abstract": "Motivated by the growing number of applications in engineering, physics, science and other fields, interest in the development of global optimization algorithms is increasing. In this paper, two categories of global optimization methods are considered, namely conventional and meta-model based algorithms. Conventional algorithms require values of the objective function to obtain a solution, while meta-model based algorithms can be used with incomplete information or when there is a limit on the available time or cost. Complex functions pose a challenge to gradient-free algorithms as they may need a significant number of function evaluations, thus meta-model based techniques may be preferred. In the paper, these algorithms are compared using a set of benchmark problems which include convex and non-convex problems, as well as smooth and non-smooth problems.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:mpaOjDK6XBIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Music information retrieval: theory and applications",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1631272.1631450",
            "Abstract": "The goal of this turorial is to provide a thorough theoretical overview of the state-of-the-art in Music Information Retrieval followed by a practical hands-on demonstration of several existing tools and resources that can be used for research in this area. Specific emphasis will be given on how MIR techniques relate to other fields of current multimedia research. MIR is an inherently interdisciplinary area touching on several research areas such as digital signal processing, machine learning, perception, visualization, human-computer interaction, content-based retrieval and digital libraries. Music has several unique characteristics that differentiate it from other areas of multimedia research. The different problems and techniques proposed to solve them that will described in the first part of the tutorial will be followed with concrete practical examples of applying these techniques using existing software tools and datasets.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:NhqRSupF_l8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards the One-Man Indian Computer Music Performance System",
            "Publication year": 2006,
            "Publication url": "https://www.academia.edu/download/30705095/icmc06bgtzan.pdf",
            "Abstract": "This paper presents progress towards a system for human to robot musical performance. Specifically, it focuses on a paradigm based on North Indian classical music, drawing theory from ancient tradition to guide aesthetic and design decisions. Custom built human computer interfaces combined with musical robotic systems using software which take advantage of the state of the art computer music algorithms and theory are presented. The end result of the project is spawning a one-man Indian computer music performance system.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ldfaerwXgEUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Intelligent interest point pruning for audio matching",
            "Publication year": 2014,
            "Publication url": "https://patents.google.com/patent/US8831763B1/en",
            "Abstract": "System and methods for intelligently pruning interest points are disclosed herein. The systems include generating a plurality of distorted audio samples and associated distorted interest points based upon a clean audio sample. Interest points that are common to sets of distorted interest points are retained with interest points not robust to distortion discarded. The disclosed systems and methods therefore can provide for a scalable audio matching solution by eliminating interest points in reference sample fingerprints. The set of pruned interest points are robust to distortion and the benefits of both scalability and accuracy can be had.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:c6chOJGeGucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning-based cooperative sound event detection with edge computing",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8711202/",
            "Abstract": "In this paper, we propose a novel real-time sound event detection framework, which combines multi-label learning and edge computing, to classify and localize abnormal sound events for city surveillance. Multiple devices equipped with acoustic sensors are deployed to collect the audio information. A learning-based approach is introduced to address the difficulties of accurately classifying the temporally overlapping acoustic events in a noisy environment. Then, edge computing is adopted to handle the high processing complexity of the learned analytics. Computation-intensive tasks of classification and localization can be offloaded to the nearby edge server for low-latency sound detection. An ensemble-based cooperative decision-making algorithm is also presented to aggregate the information from distributed devices in order to obtain better classification results. Extensive evaluations show the effectiveness of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:8IsQavbT1M8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The MUSART testbed for query-by-humming evaluation",
            "Publication year": 2004,
            "Publication url": "https://scholar.google.com/scholar?cluster=11354324323628095077&hl=en&oi=scholarr",
            "Abstract": "Music Information Retrieval has become an active area of research motivated by the increasing importance of Internet-based music distribution. In December 2003, Apple Computer announced it was selling almost 1.5 million music downloads per week (www. apple. com/pr/library/2003/dec/15itunes. html), and some analysts predict that downloads will account for 33 percent of the music industry\u2019s sales by 2008 (Zeidler 2003). Online catalogs are already approaching one million songs, so it is important to study new techniques for searching these vast stores of audio. One approach to finding music that has received much attention is Query-by-Humming (QBH). This approach enables users to retrieve songs and information about them by singing, humming, or whistling a melodic fragment. In QBH systems, the query is a digital audio recording of the user, and the ultimate target is a complete digital audio recording \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:Y0pCki6q_DkC",
            "Publisher": "MIT Press"
        },
        {
            "Title": "Princeton Sound Kitchen Open Source Software Report",
            "Publication year": 2001,
            "Publication url": "https://nyuscholars.nyu.edu/en/publications/princeton-sound-kitchen-open-source-software-report",
            "Abstract": "Princeton Sound Kitchen Open Source Software Report \u2014 NYU Scholars Skip to main \nnavigation Skip to search Skip to main content NYU Scholars Logo Help & FAQ Home \nProfiles Research Units Research Output Search by expertise, name or affiliation Princeton \nSound Kitchen Open Source Software Report P. Cook, C. Leider, Tae Park, G. Tzanetakis \nMusic and Performing Arts Professions Urban Initiative Research output: Chapter in Book/Report/Conference \nproceeding \u203a Chapter (peer-reviewed) \u203a peer-review Overview Original language English (US) \nTitle of host publication Proceedings of the 2001 International Computer Music Conference (ICMC) \nPlace of Publication Havana, Cuba State Published - 2001 Cite this APA Standard Harvard \nVancouver Author BIBTEX RIS Cook, P., Leider, C., Park, T., & Tzanetakis, G. (2001). \nPrinceton Sound Kitchen Open Source Software Report. In Proceedings of the 2001 () . / , (\u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:2SmvwDDsShQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A framework for sound source separation using spectral clustering",
            "Publication year": 2007,
            "Publication url": "https://asa.scitation.org/doi/abs/10.1121/1.2942659",
            "Abstract": "Clustering based on the normalized cut criterion, and more generally, spectral clustering methods, are techniques originally proposed to model perceptual grouping tasks, such as image segmentation in computer vision. In this work, it is shown how such techniques can be applied to the problem of dominant melodic source separation in polyphonic music audio signals. One of the main advantages of this approach is the ability to incorporate mutiple perceptually\u2010inspired grouping criteria into a single framework without requiring multiple processing stages, as many existing computational auditory science analysis approaches do. Experimental results for several tasks, including dominant melody pitch detection, are presented. The system is based on a sinusoidal modeling analysis front\u2010end. A novel similarity cue based on harmonicity (harmonically\u2010wrapped peak similariy) is also introduced. The proposed system \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:abG-DnoFyZgC",
            "Publisher": "Acoustical Society of America"
        },
        {
            "Title": "Aesthetic Agents: a Multiagent System for Non-photorealistic Rendering with Multiple Images",
            "Publication year": 2011,
            "Publication url": "https://scholar.google.com/scholar?cluster=10058308826396643425&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:RfQ-KHj5eBsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Implicit patching for dataflow-based audio analysis and synthesis",
            "Publication year": 2005,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.7688&rep=rep1&type=pdf",
            "Abstract": "Programming software for audio analysis and synthesis is challenging. Dataflow-based approaches provide a declarative specification of computation and result in efficient code. Most practitioners of computer music are familiar with some form of dataflow programming where audio applications are constructed by connecting components with \u201cwires\u201d that carry data. Examples include networks of unit generators in Music-V style languages and visual patches in Max/Msp or PD. Even though existing dataflowbased audio systems offer a concise conceptual model of signal computation, this model does have limitations. In many cases, these limitations are a consequence of the programmer having to explicitly specify connections between components. Two such limitations are the difficulty of handling spectral data and the need for fixed-size buffers between components. In this paper we introduce Implicit Patching (IP), a dataflow-based approach to audio analysis and synthesis that attempts to address these limitations. By extending dataflow semantics a large number of connections are automatically created and buffer sizes can be changed dynamically. The resulting model is also particularly suited for distributed systems. We describe Marsyas-0.2, a software framework based on IP, and provide examples that illustrate the strengths and limitations of the proposed approach.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Music analysis, retrieval and synthesis of audio signals MARSYAS",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1631272.1631459",
            "Abstract": "Marsyas is an open source software framework for music analysis, retrieval and synthesis with specific emphasis on Music Information Retreival applications. It has been in development for 10 years and has been used for a variety of projects in both academia and industry in several countries. Based on a novel dataflow architecture named implicit patching it provides a variety of existing processing modules for digital signal processing, machine learning and audio input/output that can be combined at run-time to form complex dataflow networks expressing audio processing algorithms (black-box functionality). In addition it allows the easy addition of new processing modules that need to be compiled for performance purposes. Finally Marsyas is designed with inter-operability in mind and provides various mechanisms for communicating with other software including bindings to the run-time functionality in scripting \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An Easily Removable, wireless Optical Sensing System (EROSS) for the Trumpet.",
            "Publication year": 2013,
            "Publication url": "https://www.nime.org/proceedings/2013/nime2013_261.pdf",
            "Abstract": "This paper presents a minimally-invasive, wireless optical sensor system for use with any conventional piston valve acoustic trumpet. It is designed to be easy to install and remove by any trumpeter. Our goal is to offer the extended control afforded by hyperinstruments without the hard to reverse or irreversible invasive modifications that are typically used for adding digital sensing capabilities. We utilize optical sensors to track the continuous position displacement values of the three trumpet valves. These values are transmitted wirelessly and can be used by an external controller. The hardware has been designed to be reconfigurable by having the housing 3D printed so that the dimensions can be adjusted for any particular trumpet model. The result is a low cost, low power, easily replicable sensor solution that offers any trumpeter the ability to augment their own existing trumpet without compromising the instrument\u2019s structure or playing technique. The extended digital control afforded by our system is interweaved with the natural playing gestures of an acoustic trumpet. We believe that this seamless integration is critical for enabling effective and musical human computer interaction.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:t7zJ5fGR-2UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A new event detection method for noisy hydrophone data",
            "Publication year": 2020,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0003682X19301616",
            "Abstract": "In this paper, a new method for detecting events in noisy hydrophone data is developed. The method takes an image processing approach to the 1D hydrophone data by first converting it into a log-frequency spectrogram image (cepstrum). This image is then filtered by reconstructing it based on mutual information (MI) criteria of the dominant orientation map. The features of the reconstructed cepstrum are then enhanced using a combination of edge-tracking and noise smoothing. Binary feature classification on the processed cepstrum is performed using a least-squares support vector machine (LS-SVM). Compared to other methods, the proposed image-based event detection method exploits both the scale and the orientation information. The method showed that the detection performance in terms of binary classification sensitivity with more than 55% for rare events such as whale calls from noisy hydrophone \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:W2VW_RKN1OwC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "2nd international ACM workshop on music information retrieval with user-centered and multimodal strategies (MIRUM)",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2393347.2396541",
            "Abstract": "The International ACM Workshop on Music Information Retrieval with User-Centered and Multimodal Strategies (MIRUM) at ACM Multimedia was proposed in order to gather experts from the Music and Multimedia Information Retrieval communities, as well as other neighboring fields, and to provide a high-profile platform for presenting current work on Music Information Retrieval with a strong focus on user-centered and multimodal approaches. Following a successful first edition at ACM Multimedia 2011, a second edition of MIRUM was held at ACM Multimedia 2012, which is the focus of this overview. After a description of the rationale and focus areas of the workshop, the accepted submissions and other program elements are summarized.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:5awf1xo2G04C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Document segmentation and classification into musical scores and text",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/article/10.1007/s10032-016-0271-5",
            "Abstract": "A new algorithm for segmenting documents into regions containing musical scores and text is proposed. Such segmentation is a required step prior to applying optical character recognition and optical music recognition on scanned pages that contain both music notation and text. Our segmentation technique is based on the bag-of-visual-words representation followed by random block voting (RBV) in order to detect the bounding boxes containing the musical score and text within a document image. The RBV procedure consists of extracting a fixed number of blocks whose position and size are sampled from a discrete uniform distribution that \u201cover\u201d-covers the input image. Each block is automatically classified as either coming from musical score or text and votes with a particular posterior probability of classification in its spatial domain. An initial coarse segmentation is obtained by summarizing all the votes \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:5Y1KH4bkPm0C",
            "Publisher": "Springer Berlin Heidelberg"
        },
        {
            "Title": "A computationally efficient scheme for dominant harmonic source separation",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4517572/",
            "Abstract": "The leading voice is an important feature of musical pieces and can often be considered as the dominant harmonic source. We propose in this paper a new scheme for the purpose of efficient dominant harmonic source separation. This is achieved by considering an harmonicity cue which is first compared with state-of-the-art cues using a generic evaluation methodology. The proposed separation scheme is then compared to a generic computational auditory scene analysis framework. Computational speed-up and performance comparison is done using source separation and music information retrieval tasks.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Tiz5es2fbqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Cold-start hospital length of stay prediction using positive-unlabeled learning",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9508596/",
            "Abstract": "Accurately predicting the in-hospital length of stay (LOS) at the time of admission can positively impact healthcare metrics. Machine learning (ML) techniques have been used to predict hospital patients\u2019 LOS based on their demographic and clinical characteristics. During the regular steady-state operation, traditional supervised-learning classification algorithms can be used for this purpose to inform hospital planning. However, when there are sudden changes to the admission and patient statistics, such as during the onset of a pandemic or the establishment of a new hospital, these approaches break down because reliable data for training ML models becomes available only gradually over time. This paper shows how LOS predictions can be improved during such a cold-start transition period by utilizing the positive-unlabelled (PU) learning techniques. Experimental results from using two PU learning algorithms \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:9hNLEifDsrsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Factors in factorization: Does better audio source separation imply better polyphonic music transcription?",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6659326/",
            "Abstract": "Spectrogram factorization methods such as Non-Negative Matrix Factorization (NMF) are frequently used as a way to separate individual sound sources from complex sound mixtures. More recently, they have also been used as a first stage for the automatic transcription of polyphonic music. The problem of sound source separation is different (but related) to the problem of automatic music transcription. The output of the first is the separated audio signals corresponding to each sound source, whereas the output of the second is a symbolic representation/music score that encodes the discrete pitches/notes that are played and when they are played. Many variations of factorization methods have been proposed. Two important design choices are the way spectra are represented and what distance measures are used to compare them in the optimization used for factorization. A common assumption has been that a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:zLWjf1WUPmwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Computer-assisted cantillation and chant research using content-aware web visualization tools",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/article/10.1007/s11042-009-0357-x",
            "Abstract": "Chant and cantillation research is particularly interesting as it explores the transition from oral to written transmission of music. The goal of this work to create web-based computational tools that can assist the study of how diverse recitation traditions, having their origin in primarily non-notated melodies, later became codified. One of the authors is a musicologist and music theorist who has guided the system design and development by providing manual annotations and participating in the design process. We describe novel content-based visualization and analysis algorithms that can be used for problem-seeking exploration of audio recordings of chant and recitations.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:xtRiw3GOFMkC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Pedagogical Transcription for Multimodal Sitar Performance.",
            "Publication year": 2007,
            "Publication url": "https://www.academia.edu/download/30705041/2007_ismir_sitar.pdf",
            "Abstract": "Most automatic music transcription research is concerned with producing sheet music from the audio signal alone. However, the audio data does not include certain performance data which is vital for the preservation of instrument performance techniques and the creation of annotated guidelines for students. We propose the use of modified traditional instruments enhanced with sensors which can obtain such data; as a case study we examine the sitar.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:M3NEmzRMIkIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Voice coil actuators for percussion robotics",
            "Publication year": 2017,
            "Publication url": "https://scholar.google.com/scholar?cluster=12797477408355814611&hl=en&oi=scholarr",
            "Abstract": "Percussion robots have successfully used a variety of actuator technologies to activate a wide array of striking mechanisms. Popular types of actuators include solenoids and DC motors. However, the use of industrial strength voice coil actuators provides a compelling alternative given a desirable set of heterogeneous features and requirements that span traditional devices. Their characteristics such as high acceleration and accurate positioning enable the exploration of rendering highly accurate and expressive percussion performances.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ZnWe2zbntUIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Symbolic data mining in musicology",
            "Publication year": 2011,
            "Publication url": "https://scholar.google.com/scholar?cluster=17490800496933915159&hl=en&oi=scholarr",
            "Abstract": "Symbolic data mining in musicology is concerned with extracting musical information from symbolic representations of music: that is, music that is represented by sequences of symbols over time. Representing music symbolically is certainly not a new practice; the majority of Western music exists primarily in a symbolic notation system evolved over more than a thousand years, and there are many examples of symbolic musical notation in other cultures. However, in the majority of cases the symbols used in this kind of study come from common practice musical notation, in the Western European tradition, within the last half millennium. Usually, these are the familiar notes and rests, but sometimes other representations such as chord symbols, neumes, tablature, or various kinds of performance markings are used.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Uo5fLKClJkAC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "Method and system for analyzing digital audio files",
            "Publication year": 2007,
            "Publication url": "https://patents.google.com/patent/US7277766B1/en",
            "Abstract": "A method and system for analyzing audio files is provided. Plural audio file feature vector values based on an audio file's content are determined and the audio file feature vectors are stored in a database that also stores other pre-computed audio file features. The process determines if the audio files feature vectors match the stored audio file vectors. The process also associates a plurality of known attributes to the audio file.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ZHo1McVdvXMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Magnitude ratio descriptors for pitch-resistant audio matching",
            "Publication year": 2015,
            "Publication url": "https://patents.google.com/patent/US9202472B1/en",
            "Abstract": "Systems and methods for generating unique pitch-resistant descriptors for audio clips are provided. In one or more embodiments, a descriptor for an audio clip is generated as a function of relative magnitudes between interest points within the audio clip's time-frequency representation. A number of techniques for leveraging the relative magnitudes to generate descriptors are considered. These techniques include ordering of interest points as a function of ascending or descending magnitude, creation of binary vectors based on magnitude comparisons between pairs of points, and calculation of quantized magnitude ratios between pairs of points. Descriptors generated based on relative magnitudes according to the techniques disclosed herein are relatively invariant to common transformations to the original audio clip, such as pitch shifting, time stretching, global volume changes, equalization, and/or dynamic range \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:yKcmA0jEsUoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Human perception and computer extraction of musical beat strength",
            "Publication year": 2002,
            "Publication url": "http://www.cs.cmu.edu/~./gtzan/work/pubs/dafx02gtzan.pdf",
            "Abstract": "Musical signals exhibit periodic temporal structure that create the sensation of rhythm. In order to model, analyze, and retrieve musical signals it is important to automatically extract rhythmic information. To somewhat simplify the problem, automatic algorithms typically only extract information about the main beat of the signal which can be loosely defined as the regular periodic sequence of pulses corresponding to where a human would tap his foot while listening to the music. In these algorithms, the beat is characterized by its frequency (tempo), phase (accent locations) and a confidence measure about its detection. The main focus of this paper is the concept of Beat Strength, which will be loosely defined as one rhythmic characteristic that could allow to discriminate between two pieces of music having the same tempo. Using this definition, we might say that a piece of Hard Rock has a higher beat strength than a piece of Classical Music at the same tempo. Characteristics related to Beat Strength have been implicitely used in automatic beat detection algorithms and shown to be as important as tempo information for music classification and retrieval. In the work presented in this paper, a user study exploring the perception of Beat Strength was conducted and the results were used to calibrate and explore automatic Beat Strength measures based on the calculation of Beat Histograms.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:_FxGoFyzp5QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning indirect acquisition of instrumental gestures using direct sensors",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4064514/",
            "Abstract": "Sensing instrumental gestures is a common task in interactive electroacoustic music performances. The sensed gestures can then be mapped to sounds, synthesis algorithms, visuals etc. Two of the most common approaches for acquiring these gestures are: 1) Hybrid instruments which are \"traditional\" musical instruments enhanced with sensors that directly detect gestures 2) Indirect acquisition in which the only measurement is the acoustic signal and signal processing techniques are used to acquire the gestures. Hybrid instruments require modification of existing instruments which is frequently undesirable. However they provide relatively straightforward and reliable measuring capability. On the other hand, indirect acquisition approaches typically require sophisticated signal processing and possibly machine learning algorithms in order to extract the relevant information from the audio signals. In this paper the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:O3NaXMp0MMsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Why is Greek music interesting? Towards an ethics of MIR",
            "Publication year": 2014,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.714.2226&rep=rep1&type=pdf",
            "Abstract": "Rembetika: Chord estimation Epirus: Learn harmonic style Parataxis: morphology of melodies Rembetiko/Cretan music: Beat tracking Cretan dance: Music Similarity Mood detection Multimodal analysis Orthodox chant: OMR, score following, performance analysis 4 Universal approaches",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ziW8EwMpto0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Polyphonic audio matching and alignment for music retrieval",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1285862/",
            "Abstract": "We describe a method that aligns polyphonic audio recordings of music to symbolic score information in standard MIDI files without the difficult process of polyphonic transcription. By using this method, we can search through a MIDI database to find the MIDI file corresponding to a polyphonic audio recording.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:9yKSN-GCB0IC",
            "Publisher": "IEEE"
        },
        {
            "Title": "\u97f3\u697d\u60c5\u5831\u51e6\u7406\u6280\u8853\u306e\u6700\u524d\u7dda: 6. \u97f3\u97ff\u30d9\u30fc\u30b9\u306e\u97f3\u697d\u4fe1\u53f7\u5206\u985e",
            "Publication year": 2009,
            "Publication url": "https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=60835&item_no=1&page_id=13&block_id=8",
            "Abstract": "\u8ad6\u6587\u6284\u9332\u81ea\u52d5\u30b8\u30e3\u30f3\u30eb\u5206\u985e\u306f\u9593\u9055\u3044\u306a\u304f\u97f3\u97ff\u97f3\u697d\u60c5\u5831\u691c\u7d22 (Music Information Retireval: MIR) \u306e\u4e2d\u3067\u6700\u3082\u5e83\u304f\u884c\u308f\u308c\u308b\u30bf\u30b9\u30af\u3067\u3042\u308a, \u904e\u53bb 10 \u5e74\u306b\u308f\u305f\u308a\u591a\u304f\u306e\u7d50\u679c\u304c\u8ad6\u6587\u3067\u767a\u8868\u3055\u308c\u3066\u3044\u308b. \u672c\u7a3f\u3067\u306f\u3053\u308c\u3089\u51fa\u7248\u7269\u5168\u4f53\u304b\u3089\u97f3\u97ff\u4fe1\u53f7\u306e\u3055\u307e\u3056\u307e\u306a\u97f3\u697d\u30b8\u30e3\u30f3\u30eb\u5206\u985e\u306e\u7814\u7a76\u306e\u30b5\u30fc\u30d9\u30a4\u3092\u884c\u3044, \u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u7279\u5fb4\u91cf\u3084\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3064\u3044\u3066\u8aac\u660e\u3057, \u8a55\u4fa1\u6307\u6a19\u3084\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u3064\u3044\u3066\u3082\u8b70\u8ad6\u3059\u308b. \u3055\u3089\u306b\u57fa\u672c\u7684\u306a\u30b8\u30e3\u30f3\u30eb\u5206\u985e\u306e\u30bf\u30b9\u30af\u304c\u62e1\u5f35\u3057\u305f\u4ed6\u306e\u97f3\u97ff\u4fe1\u53f7\u5206\u985e\u306e\u30bf\u30b9\u30af\u306b\u3064\u3044\u3066\u3082\u3044\u304f\u3064\u304b\u7d39\u4ecb\u3059\u308b.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:0D9gKr9vLLUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Discrimination between ascending/descending pitch arpeggios",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8417440/",
            "Abstract": "Automatic music transcription can be defined as the analysis of the acoustic signal to extract a symbolic representation of music. Existing transcription systems typically consider just the notes played at a given moment; however, other aspects such as expressiveness and playing technique can also be considered. This work is focused on how chords are played. Specifically, we consider a special type of chords, those played in arpeggio style, or simply arpeggios, in which the notes are played fast, sequentially from the lowest to the highest pitched note or vice versa and with a large overlap of the notes' sound. The main goal of this paper is to determine the pitch direction in which the arpeggiated chord was played. Two different classification methods are considered: a Fisher linear discriminant and an SVM linear classification scheme. Different features are presented for this task: one is based on the Mel-frequency \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:vlECJaBXBlQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "ORCA-SLANG: An Automatic Multi-Stage Semi-Supervised Deep Learning Framework for Large-Scale Killer Whale Call Type Identification}}",
            "Publication year": 2021,
            "Publication url": "https://scholar.google.com/scholar?cluster=17342536052561855783&hl=en&oi=scholarr",
            "Abstract": "Identification of animal-specific vocalization patterns is an imperative requirement to decode animal communication. In bioacoustics, passive acoustic recording setups are increasingly deployed to acquire large-scale datasets. Previous knowledge about established animal-specific call types is usually present due to historically conducted research. However, time-and human-resource constraints, combined with a lack of available machine-based approaches, only allow manual analysis of comparatively small data corpora and strongly distort the actual data representation and information value. Such data limitations cause restrictions in terms of identifying existing population-, group-, and individual-specific call types, subcategories, as well as unseen vocalization patterns. Thus, machine learning forms the basis for animal-specific call type recognition, to facilitate more profound insights into communication. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:-fj4grS0xi0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Retrieval of percussion gestures using timbre classification techniques.",
            "Publication year": 2004,
            "Publication url": "http://www.ee.columbia.edu/~dpwe/ismir2004/CRFILES/paper235.pdf",
            "Abstract": "Musicians are able to recognise the subtle differences in timbre produced by different playing techniques on an instrument, yet there has been little research into achieving this with a computer. This paper will demonstrate an automatic system that can successfully recognise different timbres produced by different performance techniques and classify them using signal processing and classification tools. Success rates over 90% are achieved when classifying snare drum timbres produced by different playing techniques.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:3fE2CSJIrl8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Marsyas: A framework for audio analysis",
            "Publication year": 2000,
            "Publication url": "https://www.cambridge.org/core/journals/organised-sound/article/marsyas-a-framework-for-audio-analysis/43A5D9BCB0F7BB439E1D4D1FF4B563C2",
            "Abstract": "Existing audio tools handle the increasing amount of computer audio data inadequately. The typical tape-recorder paradigm for audio interfaces is inflexible and time consuming, especially for large data sets. On the other hand, completely automatic audio analysis and annotation is impossible using current techniques. Alternative solutions are semi-automatic user interfaces that let users interact with sound in flexible ways based on content. This approach offers significant advantages over manual browsing, annotation and retrieval. Furthermore, it can be implemented using existing techniques for audio content analysis in restricted domains. This paper describes MARSYAS, a framework for experimenting, evaluating and integrating such techniques. As a test for the architecture, some recently proposed techniques have been implemented and tested. In addition, a new method for temporal segmentation based on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:u-x6o8ySG0sC",
            "Publisher": "Cambridge University Press"
        },
        {
            "Title": "Tempo extraction using beat histograms",
            "Publication year": 2005,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.7620&rep=rep1&type=pdf",
            "Abstract": "This abstract describes the tempo extraction algorithm used for the University of Victoria submission to the MIREX (Music Information Retrieval Exchange) 2005. The algorithm is mostly based on self-similarity rather than onset detection. However, an onset detection component is used to calculate the phase of the dominant periodicities. Multiple frequency bands are calculated using a Discrete Wavelet Transform. Subsequently the envelope of each band is extracted and autocorrelation is used to find the dominant periodicities of the audio signal. These dominant periodicities are accumulated into a Beat Histogram which is used to detect the primary and secondary tempo and their relative strength.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:HDshCWvjkbEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Assistive music browsing using self-organizing maps",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1579114.1579117",
            "Abstract": "Music listening is an important activity for many people. Advances in technology have made possible the creation of music collections with thousands of songs in portable music players. Navigating these large music collections is challenging especially for users with vision and/or motion disabilities. In this paper we describe our current efforts to build effective music browsing interfaces for people with disabilities. The foundation of our approach is the automatic extraction of features for describing musical content and the use of self-organizing maps to create two-dimensional representations of music collections. The ultimate goal is effective browsing without using any meta-data. We also describe different control interfaces to the system: a regular desktop application, an iPhone implementation, an eye tracker, and a smart room interface based on Wii-mote tracking.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:j3f4tGmQtD8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Intonation: A dataset of quality vocal performances refined by spectral clustering on pitch congruence",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8683554/",
            "Abstract": "We introduce the \"Intonation\" dataset of amateur vocal performances with a tendency for good intonation, collected from Smule, Inc. The dataset can be used for music information retrieval tasks such as autotuning, query by humming, and singing style analysis. It is available upon request on the Stanford CCRMA DAMP website. 1  We describe a semi-supervised approach to selecting the audio recordings from a larger collection of performances based on intonation patterns. The approach can be applied in other situations where a researcher needs to extract a subset of data samples from a large database. A comparison of the \"Intonation\" dataset and the remaining collection of performances shows that the two have different intonation behavior distributions.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:aXI_bbQgCfgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An effective, simple tempo estimation method based on self-similarity and regularity",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6637645/",
            "Abstract": "Tempo estimation is a fundamental problem in music information retrieval. It also forms the basis of other types of rhythmic analysis such as beat tracking and pattern detection. There is a large body of work in tempo estimation using a variety of different approaches that differ in their accuracy as well as their complexity. Fundamentally they take advantage of two properties of musical rhythm: 1) the music signal tends to be self-similar at periodicities related to the underlying rhythmic structure, 2) rhythmic events tend to be spaced regularly in time. We propose an algorithm for tempo estimation that is based on these two properties. We have tried to reduce the number of steps, parameters and modeling assumptions while retaining good performance and causality. The proposed approach outperforms a large number of existing tempo estimation methods and has similar performance to the best-performing ones. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:XD-gHx7UXLsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Browsing Music and Sound Using Gestures in a Self-Organized 3D Space",
            "Publication year": 2012,
            "Publication url": "http://webhome.csc.uvic.ca/~gtzan/output/icmc2012kinectSOM.pdf",
            "Abstract": "As digital music and sound collections increase in size there has been a lot of work in developing novel interfaces for browsing them. Many of these interfaces rely on automatic content analysis techniques to create representations that reflect similarities between the music pieces or sounds in the collection. Representations in 3D have the potential to convey more information but can be difficult to navigate using the traditional ways of providing input to a computer such as a keyboard and mouse. Utilizing sensors capable of sensing motion in 3-dimensions, we propose a new system for browsing music in augmented reality. Our system places audio files in a virtual cube. The placement of the files into the cube is realized through the use of audio feature extraction and self-organizing maps (SOMs). The system is controlled using gestures, and sound spatialization is utilized to provide auditory cues about the topography of the music or sound collection.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:fXbrI0tPCuEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Anssi klapuri, manuel davy, eds: Signal processing methods for music transcription",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=8867992551525835313&hl=en&oi=scholarr",
            "Abstract": "Established in 1992, The Florida Electroacoustic Music Festival (FEMF) is a highly prestigious festival in the field of electronic music, attracting some of the most important composers from North America, Europe, Asia, and South America. Most of the composers attending are at the professional and faculty level, with approximately 30 percent of the program comprising works from graduate students in composition and electronic music. Taking place in Gainesville, Florida, the \u201cfifth most rocking city in America\u201d(according to Esquire), FEMF is right at home on the hopping University of Florida campus and provides a great atmosphere for the exchange of cutting-edge ideas. In the past, FEMF has featured composers-in-residence as diverse as Jon Appleton, Paul Lansky, Charles Dodge, and Cort Lippe. For the 17th annual festival, held 10\u201312 April 2008, the composer-in-residence was the internationally renowned \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:uWQEDVKXjbEC",
            "Publisher": "MIT Press"
        },
        {
            "Title": "Non-invasive sensing and gesture control for pitched percussion hyper-instruments using the Kinect.",
            "Publication year": 2012,
            "Publication url": "http://www-personal.umich.edu/~yangqi/NIME2012/papers/297_Final_Manuscript.pdf",
            "Abstract": "Hyper-instruments extend traditional acoustic instruments with sensing technologies that capture digitally subtle and sophisticated aspects of human performance. They leverage the long training and skills of performers while simultaneously providing rich possibilities for digital control. Many existing hyper-instruments suffer from being one of a kind instruments that require invasive modifications to the underlying acoustic instrument. In this paper we focus on the pitched percussion family and describe a non-invasive sensing approach for extending them to hyper-instruments. Our primary concern is to retain the technical integrity of the acoustic instrument and sound production methods while being able to intuitively interface the computer. This is accomplished by utilizing the Kinect sensor to track the position of the mallets without any modification to the instrument which enables easy and cheap replication of the proposed hyper-instrument extensions. In addition we describe two approaches to higher-level gesture control that remove the need for additional control devices such as foot pedals and fader boxes that are frequently used in electro-acoustic performance. This gesture control integrates more organically with the natural flow of playing the instrument providing user selectable control over filter parameters, synthesis, sampling, sequencing, and improvisation using a commercially available low-cost sensing apparatus.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:dQ2og3OwTAUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Modeling grouping cues for auditory scene analysis using a spectral clustering formulation",
            "Publication year": 2011,
            "Publication url": "https://www.igi-global.com/chapter/modeling-grouping-cues-auditory-scene/45480",
            "Abstract": "Computational Auditory Scene Analysis (CASA) is challenging problem for which many different approaches have been proposed. These approaches can be based on statistical and signal processing methods such as Independent Component Analysis or can be based on our current knowledge about human auditory perception. Learning happens at the boundary interactions between prior knowledge and incoming data. Separating complex mixtures of sound sources such as music requires a complex interplay between prior knowledge and analysis of incoming data. Many approaches to CASA can also be broadly categorized as either model-based or grouping-based. Although it is known that our perceptual-system utilizes both of these types of processing, building such systems computationally has been challenging. As a result most existing systems either rely on prior source models or are solely based on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:lSLTfruPkqcC",
            "Publisher": "IGI Global"
        },
        {
            "Title": "Adaptive harmonization and pitch correction of polyphonic audio using spectral clustering",
            "Publication year": 2007,
            "Publication url": "https://www.mistic.ece.uvic.ca/publications/2007_dafx_pitch.pdf",
            "Abstract": "There are several well known harmonization and pitch correction techniques that can be applied to monophonic sound sources. They are based on automatic pitch detection and frequency shifting without time stretching. In many applications it is desired to apply such effects on the dominant melodic instrument of a polyphonic audio mixture. However, applying them directly to the mixture results in artifacts, and automatic pitch detection becomes unreliable. In this paper we describe how a dominant melody separation method based on spectral clustering of sinusoidal peaks can be used for adaptive harmonization and pitch correction in mono polyphonic audio mixtures. Motivating examples from a violin tutoring perspective as well as modifying the saxophone melody of an old jazz mono recording are presented.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ns9cj8rnVeAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Natural Human-Computer Interaction with Musical Instruments",
            "Publication year": 2016,
            "Publication url": "https://www.igi-global.com/chapter/natural-human-computer-interaction-with-musical-instruments/157954",
            "Abstract": "The playing of a musical instrument is one of the most skilled and complex interactions between a human and an artifact. Professional musicians spend a significant part of their lives initially learning their instruments and then perfecting their skills. The production, distribution and consumption of music has been profoundly transformed by digital technology. Today music is recorded and mixed using computers, distributed through online stores and streaming services, and heard on smartphones and portable music players. Computers have also been used to synthesize new sounds, generate music, and even create sound acoustically in the field of music robotics. Despite all these advances the way musicians interact with computers has remained relatively unchanged in the last 20-30 years. Most interaction with computers in the context of music making still occurs either using the standard mouse/keyboard/screen \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:2PyGcyYA7d0C",
            "Publisher": "IGI Global"
        },
        {
            "Title": "TOIS reviewers January 2006 through May 2007",
            "Publication year": 2007,
            "Publication url": "https://researchexperts.utmb.edu/en/publications/tois-reviewers-january-2006-through-may-2007",
            "Abstract": "TOIS reviewers January 2006 through May 2007 \u2014 UTMB Health Research Expert Profiles \nSkip to main navigation Skip to search Skip to main content UTMB Health Research Expert \nProfiles Logo Help & FAQ Home Experts Departments Equipment Projects/Grants Publications \nActivities Press / Media Prizes TOIS reviewers January 2006 through May 2007 Gary \nMarchionini, Ahmed Abbasi, Eugene Agichtein, Khurshid Ahmad, Azzah Al-Maskari, Gianni \nAmati, Sihem Amer Yahia, Shlomo Argamon, Daniel Ashbrook, Paolo Atzeni, Michela Bacchin, \nGodmar Back, Antonio Badia, Andras Banczur, Bettina Berendt, Elisa Bertino, B. Bhagyavati, \nSuresh Bhavnani, Devdutta Bhosale, David Bodoff Show 215 others Show less Paolo Boldi, \nJohan Bollen, Angela Bonifati, Pia Borlund, Jit Bose, Athman Bouguettaya, Michael Brinkmeier, \nPeter Brown, Peter Brusilovsky, Peter Bruza, Christopher Burges, Robin Burke, Ben Carterette'\u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:FnaCo-ypupUC",
            "Publisher": "Association for Computing Machinery (ACM)"
        },
        {
            "Title": "Instrument identification in polyphonic music signals based on individual partials",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5495794/",
            "Abstract": "A new approach to instrument identification based on individual partials is presented. It makes identification possible even when the concurrently played instrument sounds have a high degree of spectral overlapping. A pairwise comparison scheme which emphasizes the specific differences between each pair of instruments is used for classification. Finally, the proposed method only requires a single note from each instrument to perform the classification. If more than one partial is available the resulting multiple classification decisions can be summarized to further improve instrument identification for the whole signal. Encouraging classification results have been obtained in the identification of four instruments (saxophone, piano, violin and guitar).",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:nb7KW1ujOQ8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Flexible event scheduling for data-flow audio processing",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1176617.1176694",
            "Abstract": "Real-time audio and music processing frequently requires actions to be taken relative to a number of different control rates such as audio sample rate or sensor inputs. These processing systems frequently use schedulers to manage this complexity. The design of a flexible scheduling system must avoid limiting the types of actions or the range of control rates available. Abstracting time and events from the underlying scheduler provides a high degree of flexibility for designers of processing networks. Marsyas [4] is an open source software framework for analysis, retrieval, and synthesis of audio signals with specific emphasis to Music Information Retrieval applications. The Marsyas scheduler uses these abstractions to provide a highly extensible scheduler.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:tOudhMTPpwUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Enhancing sonic browsing using audio information retrieval",
            "Publication year": 2002,
            "Publication url": "https://smartech.gatech.edu/handle/1853/51352",
            "Abstract": "Collections of sound and music of increasing size and diversity are used both by typical computer users and multimedia designers. Browsing audio collections poses several challenges to the design of effective user interfaces. Recent techniques in audio information retrieval allow the automatic extraction of audio content information. This information can be used to inform and enhance audio browsing tools. In this paper we describe how audio information retrieval can be utilized to create novel user interfaces for browsing of audio collections. More specifically we report on recent work on two system prototypes: the Sonic Browser and Marsyas and our current work on merging the two systems in a common flexible system.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:MXK_kJrjxJIC",
            "Publisher": "Georgia Institute of Technology"
        },
        {
            "Title": "A computational investigation of melodic contour stability in Jewish Torah Trope Performance Traditions",
            "Publication year": 2011,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.868.7778&rep=rep1&type=pdf",
            "Abstract": "The cantillation signs of the Jewish Torah trope are of particular interest to chant scholars interested in the gradual transformation of oral music performance into notation. Each sign, placed above or below the text, acts as a \u201cmelodic idea\u201d which either connects or divides words in order to clarify the syntax, punctuation and, in some cases, meaning of the text. Unlike standard music notation, the interpretations of each sign are flexible and influenced by regional traditions, practices of given Jewish communities, larger musical influences beyond Jewish communities, and improvisatory elements incorporated by a given reader. In this paper we describe our collaborative work in developing and using computational tools to assess the stability of melodic formulas of cantillation signs based on two different performance traditions. We also show that a musically motivated alignment algorithm obtains better results than the more commonly used dynamic time warping method for calculating similarity between pitch contours. Using a participatory design process our team, which includes a domain expert, has developed an interactive web-based interface that enables researches to explore aurally and visually chant recordings and explore the relations between signs, gestures and musical representations.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:eMMeJKvmdy0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Pitch histograms in audio and symbolic music information retrieval",
            "Publication year": 2003,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1076/jnmr.32.2.143.16743",
            "Abstract": "In order to represent musical content, pitch and timing information is utilized in the majority of existing work in Symbolic Music Information Retrieval (MIR). Symbolic representations such as MIDI allow the easy calculation of such information and its manipulation. In contrast, most of the existing work in Audio MIR uses timbral and beat information, which can be calculated using automatic computer audition techniques. In this paper, Pitch Histograms are defined and proposed as a way to represent the pitch content of music signals both in symbolic and audio form. This representation is evaluated in the context of automatic musical genre classification. A multiple-pitch detection algorithm for polyphonic signals is used to calculate Pitch Histograms for audio signals. In order to evaluate the extent and significance of errors resulting from the automatic multiple-pitch detection, automatic musical genre classification results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "Taylor & Francis Group"
        },
        {
            "Title": "Voice coil actuators for percussion robotics.",
            "Publication year": 2017,
            "Publication url": "https://www.researchgate.net/profile/Robert-Van-Rooyen-2/publication/317389063_Voice_Coil_Actuators_for_Percussion_Robotics/links/5938781aa6fdcc58ae5bb928/Voice-Coil-Actuators-for-Percussion-Robotics.pdf",
            "Abstract": "Percussion robots have successfully used a variety of actuator technologies to activate a wide array of striking mechanisms. Popular types of actuators include solenoids and DC motors. However, the use of industrial strength voice coil actuators provides a compelling alternative given a desirable set of heterogeneous features and requirements that span traditional devices. Their characteristics such as high acceleration and accurate positioning enable the exploration of rendering highly accurate and expressive percussion performances.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:pgiGeGwzGf8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A comparative evaluation of search techniques for query\u2010by\u2010humming using the MUSART testbed",
            "Publication year": 2007,
            "Publication url": "https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.20532",
            "Abstract": "Query\u2010by\u2010humming systems offer content\u2010based searching for melodies and require no special musical training or knowledge. Many such systems have been built, but there has not been much useful evaluation and comparison in the literature due to the lack of shared databases and queries. The MUSART project testbed allows various search algorithms to be compared using a shared framework that automatically runs experiments and summarizes results. Using this testbed, the authors compared algorithms based on string alignment, melodic contour matching, a hidden Markov model, n\u2010grams, and CubyHum. Retrieval performance is very sensitive to distance functions and the representation of pitch and rhythm, which raises questions about some previously published conclusions. Some algorithms are particularly sensitive to the quality of queries. Our queries, which are taken from human subjects in a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:eQOLeE2rZwMC",
            "Publisher": "Wiley Subscription Services, Inc., A Wiley Company"
        },
        {
            "Title": "Streamlined tempo estimation based on autocorrelation and cross-correlation with pulses",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6879451/",
            "Abstract": "Algorithms for musical tempo estimation have become increasingly complicated in recent years. These algorithms typically utilize two fundamental properties of musical rhythm: some features of the audio signal are self-similar at periods related to the underlying rhythmic structure, and rhythmic events tend to be spaced regularly in time. We present a streamlined tempo estimation method ( stem) that distills ideas from previous work by reducing the number of steps, parameters, and modeling assumptions while retaining good accuracy. This method is designed for music with a constant or near-constant tempo. The proposed method either outperforms or has similar performance to many existing state-of-the-art algorithms. Self-similarity is captured through autocorrelation of the onset strength signal (OSS), and time regularity is captured through cross-correlation of the OSS with regularly spaced pulses. Our findings \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:04dtUmz_MT0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Building and using a scalable display wall system",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/851747/",
            "Abstract": "Princeton's scalable display wall project explores building and using a large-format display with commodity components. The prototype system has been operational since March 1998. Our goal is to construct a collaborative space that fully exploits a large-format display system with immersive sound and natural user interfaces. Our prototype system is built with low-cost commodity components: a cluster of PCs, PC graphics accelerators, consumer video and sound equipment, and portable presentation projectors. This approach has the advantages of low cost and of tracking technology well, as high-volume commodity components typically have better price-performance ratios and improve at faster rates than special-purpose hardware. We report our early experiences in building and using the display wall system. In particular, we describe our approach to research challenges in several specific research areas \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:d1gkVwhDpl0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Personalizing self-organizing music spaces with anchors: design and evaluation",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/article/10.1007/s11042-017-4465-8",
            "Abstract": "We propose and evaluate a system for content-based visualization and exploration of music collections. The system is based on a modification of Kohonen\u2019s Self-Organizing Map algorithm and allows users to choose the locations of clusters containing acoustically similar tracks on the music space. A user study conducted to evaluate the system shows that the possibility of personalizing the music space was perceived as difficult. Conversely, the user study and objective metrics derived from users\u2019 interactions with the interface demonstrate that the proposed system helped individuals create playlists faster and, under some circumstances, more effectively. We believe that personalized browsing interfaces are an important area of research in Multimedia Information Retrieval, and both the system and user study contribute to the growing work in this field.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:LHtfzE5_5AIC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Music Mining",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/B9780123965028000267",
            "Abstract": "The multi-faceted nature of music information requires sophisticated algorithms and systems, that combine signal processing and machine learning techniques, in order to extract useful information from the large collections of music available today. This chapter overviews work in music mining which is the application of data mining techniques for the purposes of music processing. Topics covered include content-based similarity retrieval, genre classification, emotion/mood classification, music clustering, automatic tag annotation, audio fingerprinting, cover song detection as well as self-organizing maps and visualization. Open problems and future trends as well as pointers for further reading are also provided.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:cCmJLe1CRJUC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Computer evaluation of musical timbre transfer on drum tracks",
            "Publication year": 2021,
            "Publication url": "https://mars.library.uvic.ca/handle/1828/13221",
            "Abstract": "Musical timbre transfer is the task of re-rendering the musical content of a given source using the rendering style of a target sound. The source keeps its musical content, e.g., pitch, microtiming, orchestration, and syncopation. I specifically focus on the task of transferring the style of percussive patterns extracted from polyphonic audio using a MelGAN-VC model [57] by training acoustic properties for each genre. Evaluating audio style transfer is challenging and typically requires user studies. An analytical methodology based on supervised and unsupervised learning including visualization for evaluating musical timbre transfer is proposed. The proposed methodology is used to evaluate the MelGAN-VC model for musical timbre transfer of drum tracks. The method uses audio features to analyze results of the timbre transfer based on classification probability from Random Forest classifier. And K-means algorithm can classify unlabeled instances using audio features and style-transformed results are visualized by t-SNE dimensionality reduction technique, which is helpful for interpreting relations between musical genres and comparing results from the Random Forest classifier.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:QaLwMs-zPFMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Song-specific bootstrapping of singing voice structure",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1394662/",
            "Abstract": "We present some experiments in the semi-automatic extraction of singing voice structure. The main characteristic of the proposed approach is that the segmentation is performed specifically for each individual song using a process we call bootstrapping. In bootstrapping, a small random sampling of the song is annotated by the user. This annotation is used to learn the song-specific voice characteristics and the trained classifier is subsequently used to classify and segment the whole song. We present experimental results on a collection of pieces with jazz singers that show the potential of this approach and compare it with the traditional approach of using multiple songs for training. It is our belief that the idea of song-specific bootstrapping is applicable to other types of music and audio computer-supported annotation.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Hit song science",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=4845553043939607846&hl=en&oi=scholarr",
            "Abstract": "Hit Song Science is an emerging field of investigation that aims at predicting the success of songs before they are released on the market. This chapter defines the context and goals of Hit Song Science (HSS) from the viewpoint of music information retrieval. In the first part, we stress the complexity of the mechanisms underlying individual and social music preference from an experimental psychology viewpoint. In the second part, we describe current attempts at modeling and predicting music hits in a feature oriented view of",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:EXDW3tg14iEC",
            "Publisher": "CRC"
        },
        {
            "Title": "Polyhedral compilation for multi-dimensional stream processing",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3330999",
            "Abstract": "We present a method for compilation of multi-dimensional stream processing programs from affine recurrence equations with unbounded domains into imperative code with statically allocated memory. The method involves a novel polyhedral schedule transformation called periodic tiling. It accommodates existing polyhedral optimizations to improve memory access patterns and expose parallelism. This enables efficient execution of programming languages with unbounded recurrence equations, as well as optimization of existing languages from which this form can be derived. The method is experimentally evaluated on 5 DSP algorithms with large problem sizes. Results show potential for improved throughput compared to hand-optimized C++ (speedups on a 6-core Intel Xeon CPU up to 10\u00d7 with a geometric mean 3.3\u00d7).1",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:P76ttB97BVgC",
            "Publisher": "ACM"
        },
        {
            "Title": "The Orchive: Data mining a massive bioacoustic archive",
            "Publication year": 2013,
            "Publication url": "https://arxiv.org/abs/1307.0589",
            "Abstract": "The Orchive is a large collection of over 20,000 hours of audio recordings from the OrcaLab research facility located off the northern tip of Vancouver Island. It contains recorded orca vocalizations from the 1980 to the present time and is one of the largest resources of bioacoustic data in the world. We have developed a web-based interface that allows researchers to listen to these recordings, view waveform and spectral representations of the audio, label clips with annotations, and view the results of machine learning classifiers based on automatic audio features extraction. In this paper we describe such classifiers that discriminate between background noise, orca calls, and the voice notes that are present in most of the tapes. Furthermore we show classification results for individual calls based on a previously existing orca call catalog. We have also experimentally investigated the scalability of classifiers over the entire Orchive.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:uJ-U7cs_P_0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sonophenology",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/article/10.1007/s12193-011-0066-4",
            "Abstract": "The study of periodic biological processes, such as when plants flower and birds arrive in the spring is known as Phenology. In recent years this field has gained interest from the scientific community because of the applicability of this data to the study of climate change and other ecological processes. In this paper we propose the use of tangible interfaces for interactive sonification with a specific example of a multimodal tangible interface consisting of a physical paper map and tracking of fiducial markers combined with a novel drawing interface. The designed interface enables one or more users to specify point queries with the map interface and to specify time queries with the drawing interface. This allows the user to explore both time and space while receiving immediate sonic feedback of their actions. This system can be used to study and explore the effects of climate change, both as tool to be used by \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:tkaPQYYpVKoC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Normalized cuts for predominant melodic source separation",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4432646/",
            "Abstract": "The predominant melodic source, frequently the singing voice, is an important component of musical signals. In this paper, we describe a method for extracting the predominant source and corresponding melody from ldquoreal-worldrdquo polyphonic music. The proposed method is inspired by ideas from computational auditory scene analysis. We formulate predominant melodic source tracking and formation as a graph partitioning problem and solve it using the normalized cut which is a global criterion for segmenting graphs that has been used in computer vision. Sinusoidal modeling is used as the underlying representation. A novel harmonicity cue which we term harmonically wrapped peak similarity is introduced. Experimental results supporting the use of this cue are presented. In addition, we show results for automatic melody extraction using the proposed approach.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:_kc_bZDykSQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Indirect acquisition of percussion gestures using timbre recognition",
            "Publication year": 2005,
            "Publication url": "https://www.academia.edu/download/45152683/2005_cim_timbre.pdf",
            "Abstract": "There are many techniques available to capture the gestures of a performer. By utilizing digital signal processing and machine learning techniques we are able to capture, process and classify signals in real-time in order to provide data for use by musicians in a performance context. A common technique that achieves similar results is the use of sensors. The system that we describe uses data that is already available in miked environments, therefore not impeding or modifying the performer. The system is accurate and works in real-time and could be used in the future for a live performance. This system has been implemented with a snare drum and tabla for as demonstrations. The specific algorithms used, experiments and advantages and disadvantages of this technology will be discussed.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:r0BpntZqJG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "MARSYAS-0.2: A Case Study in Implementing Music",
            "Publication year": 2007,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=f4b5gKdnRMkC&oi=fnd&pg=PA31&dq=info:Cpa6ICgbCxQJ:scholar.google.com&ots=BgViFwu_vI&sig=dN1AM--Jeq_S_h-3NANuaD-G6Ik",
            "Abstract": "MARSYAS is an open source audio processing framework with specific emphasis on building music information retrieval systems. It has been under development since 1998 and has been used for a variety of projects in both academia and industry. In this chapter, the software architecture of Marsyas will be described. The goal is to highlight design challenges and solutions that are relevant to any MIR software.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Fu2w8maKXqMC",
            "Publisher": "IGI Global"
        },
        {
            "Title": "Global access to ethnic Music: the next big challenge?",
            "Publication year": 2009,
            "Publication url": "https://biblio.ugent.be/publication/844726/file/944419",
            "Abstract": "Although MIR is a relatively new branch of research, it has already approached several musical parameters and many musical styles, using different methodologies, from lowlevel signal analysis up to higher-level symbolic and semantic approaches.Several implementations have come forward, but when we want to apply these tools to ethnic music we are often confronted with fundamental problems. This music mostly does not rely on the common Western musical concepts. Therefore most MIR-applications need to be redesigned in order to give a relevant contribution to the analysis and classification of non-Western music. In an interdisciplinary approach, engineers and ethnomusicologists should be able to achieve considerable progress in the approach of music that lacks commercial means and is difficult to access for the general public. Developers can find new challenges, while the development of interesting tools can give a new impulse to the study and dissemination of a rich heritage of music now hidden in archives.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ZuybSZzF8UAC",
            "Publisher": "International Society for music Information Retrieval"
        },
        {
            "Title": "Strategies for orca call retrieval to support collaborative annotation of a large archive",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6093798/",
            "Abstract": "The Orchive is a large audio archive of hydrophone recordings of Killer whale (Orcinus orca) vocalizations. Researchers and users from around the world can interact with the archive using a collaborative web-based annotation, visualization and retrieval interface. In addition a mobile client has been written in order to crowdsource Orca call annotation. In this paper we describe and compare different strategies for the retrieval of discrete Orca calls. In addition, the results of the automatic analysis are integrated in the user interface facilitating annotation as well as leveraging the existing annotations for supervised learning. The best strategy achieves a mean average precision of 0.77 with the first retrieved item being relevant 95% of the time in a dataset of 185 calls belonging to 4 types.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:eq2jaN3J8jMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Decoding the song: Histogram-based paradigmatic and syntagmatic analysis of melodic formulae in Hungarian laments, Torah trope, tenth century plainchant and Koran recitation",
            "Publication year": 2008,
            "Publication url": "http://sness.net/papers/ness2008decoding.pdf",
            "Abstract": "The development of musical notation and the changing relationship between textual syntax and musical semiotics were inherently connected to the transformation of a culture based on oral transmission and ritual to one based on writing and hermeneutic interpretation. Along this historical continuum, notation functioned either to reconstruct a previous, remembered melody or to construct a newly composed melody. For the chant scholar the question arises as to when and under what conditions melodic formulae became solidified as musical material. In the present study we examine examples from improvised, partially improvised, partially notated and gesture-based notational chant traditions: Ashkenazi Torah cantillation, Ninth Century St. Gallen plainchant, and Koran recitation. We explore examples from these various traditions through computational tools for paradigmatic analysis of melodic formulae and gesture. Exploring the functionality of melodic gesture, musical syntax and musical semiotics in the specific contexts of speaking, singing, reading and writing enhances the comprehension of the relationship between melodic formula and textual syntax within these divergent forms of religious chant.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:bQkGhl1z2hUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Beyond the query-by-example paradigm: New query interfaces for music information retrieval",
            "Publication year": 2002,
            "Publication url": "http://www.music.mcgill.ca/~ich/classes/mumt611_07/Query%20Retrieval/icmc02gtzan.pdf",
            "Abstract": "The majority of existing work in music information retrieval for audio signals has followed the content-based query-by-example paradigm. In this paradigm a musical piece is used as a query and the result is a list of other musical pieces ranked by their content similarity. In this paper we describe algorithms and graphical user interfaces that enable novel alternative ways for querying and browsing large audio collections. Computer audition algorithms are used to extract content information from audio signals. This automatically extracted information is used to configure the graphical user interfaces and to genereate new query audio signals for browsing and retrieval.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:KlAtU1dfN6UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multimodal Sensor Analysis of Sitar Performance: Where is the Beat?",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4412821/",
            "Abstract": "In this paper we describe a system for detecting the tempo of sitar performance using a multimodal signal processing approach. Real-time measurements are obtained from sensors on the instrument and by wearable sensors on the performer's body. Experiments comparing audio-based and sensor-based tempo tracking are described. The real-time tempo tracking method is based on extracting onsets and applying Kalman filtering. We show how late fusion of the audio and sensor tempo estimates can improve tracking. The obtained results are used to inform design parameters for a real-time system for human-robot musical performance.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:BqipwSGYUEgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A computational approach to the modeling and employment of cognitive units of folk song melodies using audio recordings",
            "Publication year": 2010,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1068.5499&rep=rep1&type=pdf",
            "Abstract": "We present a method to classify audio recordings of folk songs into tune families. For this, we segment both the query recording and the recordings in the collection. The segments can be used to relate recordings to each other by evaluating the recurrence of similar melodic patterns. We compare a segmentation that results in what can be considered cognitive units to a segmentation into segments of fixed length. It appears that the use of \u2018cognitive\u2019segments results in higher classification accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:WbkHhVStYXYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Snare Drum Performance Motion Analysis",
            "Publication year": 2016,
            "Publication url": "https://www.researchgate.net/profile/Robert-Van-Rooyen-2/publication/303873184_Snare_Drum_Performance_Motion_Analysis/links/575979fb08ae414b8e43ddc1/Snare-Drum-Performance-Motion-Analysis.pdf",
            "Abstract": "Human motion associated with a percussion performance is incredibly complex and filled with subtle nuances that span timing, dynamics, and timbre. Recreating such a performance robotically or through synthesis requires a systematic method of data acquisition and analysis that leads to a concise set of parameters to drive multidimensional models. By using statistical analysis to derive parameter vectors that configure stochastic models, we can begin to infuse human qualities in a rendering while simultaneously creating uniqueness in each and every performance. This can translate to a more enjoyable performance for the listener as well as a richer pallet for composers.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:SSGWEqmz6gUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automatic event detection for long-term monitoring of hydrophone data",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6032973/",
            "Abstract": "In this paper, we propose an efficient method for long-term monitoring of a wide variety of marine mammals and human related activities using hydrophone data. The proposed method uses a combination of a two-stage denoising process followed by a new event detection function that estimates temporal predictability. The detection function utilizes long-term and short-term predictions in order to detect various acoustic events from the background noise. The first stage of the denoising process uses temporal decomposition via Empirical Mode Decomposition to improve the correct detection rate, while the second stage uses Wavelet Packet spectral decomposition to reduce the false detection rate. Applied to event detection in NEPTUNE hydrophone recordings, the method demonstrates an accuracy of 95% and an F-measure of 94%.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:HE397vMXCloC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Factors in automatic musical genre classification of audio signals",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1285840/",
            "Abstract": "Automatic musical genre classification is an important tool for organizing the large collections of music that are becoming available to the average user. In addition, it provides a structured way of evaluating musical content features that does not require extensive user studies. The paper provides a detailed comparative analysis of various factors affecting automatic classification performance, such as choice of features and classifiers. Using recent machine learning techniques, such as support vector machines, we improve on previously published results using identical data collections and features.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:W7OEmFMy1HYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Marsyas submissions to MIREX 2007",
            "Publication year": 2007,
            "Publication url": "https://www.researchgate.net/profile/George-Tzanetakis/publication/228542053_MARSYAS_submissions_to_MIREX_2009/links/09e4150f71a5cdc52e000000/MARSYAS-submissions-to-MIREX-2009.pdf",
            "Abstract": "Marsyas is an open source software framework for audio analysis, synthesis and retrieval with specific emphasis on Music Information Retrieval. It is developed by an international team of programmers and researchers led by George Tzanetakis. In MIREX 2009 the Marsyas team participated in the following tasks: Audio Classical Composer Identification, Audio Genre Classification (Latin and Mixed), Audio Music Mood Classification, Audio Beat Tracking, Audio Onset Detection, Audio Music Similarity and Retrieval and Audio Tagging Tasks. In this abstract we describe the specific algorithmic details of our submission and provide information about how researchers can use our system using the MIREX input/output conventions on their own datasets. Also some comments on the results are provided especially highlighting the excellent running time performance of our system (an order of magnitude faster than any other submission while remaining competitive in task performance).",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:UebtZRa9Y70C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Analysis of Drum Machine Kick and Snare Sounds",
            "Publication year": 2017,
            "Publication url": "http://www.aes.org/e-lib/browse.cfm?elib=19284",
            "Abstract": "The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. The development of new tools to assist users organizing and managing libraries of this type requires comprehensive audio analysis that is distinct from that used for general classification or onset detection tasks. In this paper, 4230 kick and snare samples, representing 250 individual electronic drum machines are evaluated. Samples are segmented into different lengths and analyzed using comprehensive audio feature analysis. Audio classification is used to evaluate and compare the effect of this time segmentation and establish the overall effectiveness of the selected feature set. Results demonstrate that there is improvement in classification scores when using time segmentation as a pre-processing step.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:J4wmHkHhN-kC",
            "Publisher": "Audio Engineering Society"
        },
        {
            "Title": "Sound analysis using MPEG compressed audio",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/859071/",
            "Abstract": "There is a huge amount of audio data available that is compressed using the MPEG audio compression standard. Sound analysis is based on the computation of short time feature vectors that describe the instantaneous spectral content of the sound. An interesting possibility is the calculation of features directly from compressed data. Since the bulk of the feature calculation is performed during the encoding stage this process has a significant performance advantage if the available data is compressed. Combining decoding and analysis in one stage is also very important for audio streaming applications. In this paper, we describe the calculation of features directly from MPEG audio compressed data. Two of the basic processes of analyzing sound are: segmentation and classification. To illustrate the effectiveness of the calculated features we have implemented two case studies: a general audio segmentation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:YsMSGLbcyi4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "El-Lamellophone-An Open Framework for Low-cost, DIY, Autonomous Lemellophone Based Hyperinstruments",
            "Publication year": 2014,
            "Publication url": "https://www.researchgate.net/profile/Shawn-Trail/publication/310644696_El-Lamellophone_-An_Open_Framework_for_Low-cost_DIY_Autonomous_Lemellophone_Based_Hyperinstruments/links/58348d5408ae138f1c0d7d38/El-Lamellophone-An-Open-Framework-for-Low-cost-DIY-Autonomous-Lemellophone-Based-Hyperinstruments.pdf",
            "Abstract": "The El-Lamellophone (El-La) is a Lamellophone hyperinstrument incorporating electronic sensors and integrated DSP. An embedded Linux micro-computer supplants the laptop. A piezo-electric pickup is mounted to the underside of the body of the instrument for direct audio acquisition providing a robust signal with little interference. The signal is used for electric sound-reinforcement, creative signal processing and audio analysis developed in Puredata (Pd). This signal inputs and outputs the micro-computer via stereo 1/8th inch phono jacks. Sensors provide gesture recognition affording the performer a broader, more dynamic range of\u201d musical human-computer interaction\u201d(MHCI) over specific DSP functions. The instrument\u2019s metal tines (conventionally used for plucking-traditional lamellophone sound production method) tines have been adapted to include capacitive touch in order to control a synthesizer. Initial investigations have been made into digitally-controlled electromagnetic actuation of the acoustic tines, aiming to allow performer control and sensation via both traditional Lamellophone techniques, as well as extended playing techniques that incorporate shared human/computer control of the resulting sound. The goal is to achieve this without compromising the traditional sound production methods of the acoustic instrument while leveraging inherent performance gestures with embedded continuous controller values essential to MHCI. The result is an intuitive, performer designed, hybrid electro-acoustic instrument, idiomatic computer interface, and robotic acoustic instrument in one framework.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:M3zsPnPgUlUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Temporal constraints for sound source formation using the normalized cut",
            "Publication year": 2006,
            "Publication url": "https://hal.archives-ouvertes.fr/hal-02005791/",
            "Abstract": "In this paper, we explore the use of a graph algorithm called the normalized cut in order to organize prominent components of the auditory scene. We focus specifically on defining a time-constrained similarity metric. We show that such a metric can be successfully expressed in terms of the time and frequency masking phenomena and can be used to solve common problems in auditory scene analysis.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:dshw04ExmUIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Evaluating the effectiveness of mixed reality music instrument learning with the theremin",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1007/s10055-019-00388-8",
            "Abstract": "Learning music is a challenging process that requires years of practice to master, either with lessons from a professional teacher or through self-teaching. While practicing, students are expected to self-evaluate their performance which may be difficult without timely feedback from a professional. Research into computer-assisted music instrument tutoring (CAMIT) attempts to address this through the use of emerging technologies. In this paper, we study CAMIT for mixed reality (MR) by developing MR:emin, an immersive MR music learning environment for the theremin, an electronic music instrument that is controlled without physical contact. MR:emin integrates a physical theremin with the immersive learning environment. To better understand the effectiveness of such environments, we perform a user study with MR:emin comparing traditional music learning with two virtual learning environments, an immersive one \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:Rc-B-9qnGaUC",
            "Publisher": "Springer London"
        },
        {
            "Title": "On computational transcription and analysis of oral and semi-oral chant traditions",
            "Publication year": 2012,
            "Publication url": "http://www.sness.net/papers/biro2012computational.pdf",
            "Abstract": "MethodsAs described in Ness et al., 2010, for each of the Torah recordings, we derive a melodic scale by detecting the peaks in a non-parametric density estimation of the distribution of pitches, using a Gaussian kernel. Of these quantized pitches, we choose the two that occur most frequently and use them to scale the pitches in the non-",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:0KyAp5RtaNEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Modeling Chord and Key Structure with Markov Logic.",
            "Publication year": 2012,
            "Publication url": "http://webhome.csc.uvic.ca/~gtzan/ismir2012mln.pdf",
            "Abstract": "We propose the use of Markov Logic Networks (MLNs) as a highly flexible and expressive formalism for the harmonic analysis of audio signals. Using MLNs information about the physical and semantic content of the signal can be intuitively and compactly encoded and expert knowledge can be easily expressed and combined using a single unified formal model that combines probabilities and logic. In particular, we propose a new approach for joint estimation of chord and global key The proposed model is evaluated on a set of popular music songs. The results show that it can achieve similar performance to a state of the art Hidden Markov Model for chord estimation while at the same time estimating global key. In addition when prior information about global key is used it shows a small but statistically significant improvement in chord estimation performance. Our results demonstrate the potential of MLNs for music analysis as they can express both structured relational knowledge as well as uncertainty.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ZfRJV9d4-WMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning audio features for singer identification and embedding",
            "Publication year": 2018,
            "Publication url": "https://openreview.net/forum?id=H1cT3NTBM",
            "Abstract": "There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ZuSUVyMx-TgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio-based gender identification using bootstrapping",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1517318/",
            "Abstract": "Annotation of audio content is an important component of modern multimedia information retrieval systems. Automatic gender identification is used for video indexing and can improve speech recognition results by using gender-specific classifiers. Gender identification in large datasets is difficult because of the large variability in speaker characteristics. Bootstrapping is an approach that attempts to combine minimal user annotations with automatic techniques for audio classification. In bootstrapping a small random sampling of the training data is annotated by the user and this annotation is used to train a classifier that annotates the remaining data. This technique is useful when the training set is too large to be fully annotated by the user. Experimental results showing that bootstrapping is effective for automatic audio-based gender identification are provided.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:k_IJM867U9cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Computational ethnomusicology: a music information retrieval perspective.",
            "Publication year": 2014,
            "Publication url": "http://smc.afim-asso.org/smc-icmc-2014/images/proceedings/OS1-IS07-Computationalethnomusicology.pdf",
            "Abstract": "Computational ethnomusicology (CE) refers to the use of computational techniques for the study of musics from around the world. It has been a growing field that has benefited from the the many advances that have been made in music information retrieval (MIR). The historical development of CE and the types of tasks that have been addressed so far, is traced in this paper. The use of computational techniques enables types of analysis and processing that would be either impossible or very hard to perform using only audio recordings and human listening. The small but growing subset of music cultures that have been investigated is also overviewed. Research in computational ethnomusicology is still at early stage and the engagement of musicologists and musicians is still limited. The paper ends with interesting directions for future work and suggestions for how to engange musicologists and musicians. The material presented formed the basis of an invited talk by the author presented at the 2014 joint International Computer Music/Sound and Music Computing conference in Athens, Greece, 2014.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Oc-rVwKPngoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Blending the physical and the virtual in music technology: from interface design to multi-modal signal processing",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2502081.2502238",
            "Abstract": "Recent years have seen a significant increase of interest in rich multi-modal user interfaces going beyond conventional mouse/keyboard/screen interaction. The new interface technologies are broadly impacting music technology and culture. New musical interfaces use a variety of sensing (and actuating) modalities to receive and present information to users, and often require techniques from signal processing and machine learning in order to extract and fuse high level information from noisy, high dimensional signals over time. Hence they pose many interesting signal processing challenges while offering fascinating possibilities for new research. At the same time the richness of possibilities for new forms of musical interaction requires a new approach to the design of musical technologies and has implications for performance aesthetics and music pedagogy. This tutorial begins with a general and gentle \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:EkHepimYqZsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Marsyas-0.2: a case study in implementing music information retrieval systems",
            "Publication year": 2008,
            "Publication url": "https://www.igi-global.com/chapter/marsyas-case-study-implementing-music/24427",
            "Abstract": "Marsyas, is an open source audio processing framework with specific emphasis on building Music Information Retrieval systems. It has been been under development since 1998 and has been used for a variety of projects in both academia and industry. In this chapter, the software architecture of Marsyas will be described. The goal is to highlight design challenges and solutions that are relevant to any MIR software. Keywords: Information Processing, Music, Information Retrieval, System Design, Evaluation, Fast Fourier Transfer (FFT), Feature Extraction, MFCC",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:kNdYIx-mwKoC",
            "Publisher": "IGI Global"
        },
        {
            "Title": "Gesture Analysis of Radiodrum Data",
            "Publication year": 2011,
            "Publication url": "http://webhome.csc.uvic.ca/~gtzan/icmc2011gesturesgtzan.pdf",
            "Abstract": "The radiodrum is a virtual controller/interface that has existed in various forms since its initial design at Bell Laboratories in the 1980\u2019s, and it is still being developed. It is a percussion instrument, while at the same time an abstract 3D gesture/position sensor. There are two main modalities of the instrument that are used by composers and performers: the first is similar to a percussive interface, where the performer hits the surface, and the instrument reports position (x, y, z) and velocity (u) of the hit; thereby it has 6 degrees of freedom. The other mode, which is unique to this instrument (at least in the domain of percussive interfaces), is moving the sticks in the space above the pad, whereby the instrument also reports (x, y, z) position in space above the surface.In this paper we describe techniques for identifying different gestures using the Radio Drum, which could include signals like a circle or square, or other physically intuitive gestures, like the pinch-to-zoom metaphor used on mobile devices such as the iPhone. Two approaches to gesture analysis are explored. The first one is based on feature classification using support vector machines and the second is using Dynamic Time Warping. By allowing users to interact with the system using a complex set of gestures, we have produced a system that will allow for a richer vocabulary for composers and performers of electro-acoustic music. These techniques and vocabulary are useful not only for this particular instrument, but can be modified for other 3D sensors.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:5ugPr518TE4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Osc-xr: A toolkit for extended reality immersive music interfaces",
            "Publication year": 2019,
            "Publication url": "http://smc2019.uma.es/articles/S3/S3_04_SMC2019_paper.pdf",
            "Abstract": "Currently, developing immersive music environments for extended reality (XR) can be a tedious process requiring designers to build 3D audio controllers from scratch. OSCXR is a toolkit for Unity intended to speed up this process through rapid prototyping, enabling research in this emerging field. Designed with multi-touch OSC controllers in mind, OSC-XR simplifies the process of designing immersive music environments by providing prebuilt OSC controllers and Unity scripts for designing custom ones. In this work, we describe the toolkit\u2019s infrastructure and perform an evaluation of the controllers to validate the generated control data. In addition to OSC-XR, we present UnityOscLib, a simplified OSC library for Unity utilized by OSC-XR. We implemented three use cases, using OSCXR, to inform its design and demonstrate its capabilities. The Sonic Playground is an immersive environment for controlling audio patches. Hyperemin is an XR hyperinstrument environment in which we augment a physical theremin with OSC-XR controllers for real-time control of audio processing. Lastly, we add OSC-XR controllers to an immersive T-SNE visualization of music genre data for enhanced exploration and sonification of the data. Through these use cases, we explore and discuss the affordances of OSC-XR and immersive music interfaces.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Q3_nmhWTCy0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A scalable peer-to-peer system for music information retrieval",
            "Publication year": 2004,
            "Publication url": "https://scholar.google.com/scholar?cluster=5737243392268014529&hl=en&oi=scholarr",
            "Abstract": "Currently, a large percentage of Internet traffic consists of music files, typically stored in MP3 compressed audio format, shared and exchanged over peer-to-peer (P2P) networks. Searching for music is performed by specifying keywords and na\u0131ve stringmatching techniques. In the past years, the emerging research area of music information retrieval (MIR) has produced a variety of new ways of looking at the problem of music searching. Such MIR techniques can significantly enhance the ways users search for music over P2P networks. For that to happen, there are two main challenges that need to be addressed: scalability to large collections and number of peers; and a richer set of search semantics that can support MIR, especially when the retrieval is content-based. In this article, we describe a scalable P2P system that uses rendezvous points (RPs) for music metadata registration and query resolution. The system \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:Se3iqnhoufwC",
            "Publisher": "MIT Press"
        },
        {
            "Title": "An experimental comparison of audio tempo induction algorithms",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1678001/",
            "Abstract": "We report on the tempo induction contest organized during the International Conference on Music Information Retrieval (ISMIR 2004) held at the University Pompeu Fabra in Barcelona, Spain, in October 2004. The goal of this contest was to evaluate some state-of-the-art algorithms in the task of inducing the basic tempo (as a scalar, in beats per minute) from musical audio signals. To our knowledge, this is the first published large scale cross-validation of audio tempo induction algorithms. Participants were invited to submit algorithms to the contest organizer, in one of several allowed formats. No training data was provided. A total of 12 entries (representing the work of seven research teams) were evaluated, 11 of which are reported in this document. Results on the test set of 3199 instances were returned to the participants before they were made public. Anssi Klapuri's algorithm won the contest. This evaluation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:UeHWp8X0CEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "SOMba: Multiuser Music Creation Using Self-Organizing Maps and Motion Tracking.",
            "Publication year": 2009,
            "Publication url": "http://webhome.csc.uvic.ca/~gtzan/icmc2009gtzan-somba.pdf",
            "Abstract": "SOMba is a system where multiple users create new rhythms and music by moving around a physical space while being tracked in real time using the infrared sensor on the Wii remote control. The physical space they move around in is mapped to a 2D Self-Organizing map. This SOM is created using the Marsyas audio processing framework from a collection of aligned 1-bar Samba rhythms of a variety of Brazilian musical instruments. A user is mapped to a unique point in the SOM, and this point contains a single rhythm. As the user moves around the 2D space, different rhythms are played. Multiple users can move around a space, and each would generate a different rhythm.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:KxtntwgDAa4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Transformation invariant media matching",
            "Publication year": 2016,
            "Publication url": "https://patents.google.com/patent/US9508023B1/en",
            "Abstract": "This disclosure relates to transformation invariant media matching. A fingerprinting component can generate a transformation invariant identifier for media content by adaptively encoding the relative ordering of interest points in media content. The interest points can be grouped into subsets, and stretch invariant descriptors can be generated for the subsets based on ratios of coordinates of interest points included in the subsets. The stretch invariant descriptors can be aggregated into a transformation invariant identifier. An identification component compares the identifier against a set of identifiers for known media content, and the media content can be matched or identified as a function of the comparison.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:IT5EXw6i2GUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Comparative Study on Wearable Sensors for Signal Processing on the North Indian Tabla",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4313313/",
            "Abstract": "This paper describes experimentation using a variety of sensor techniques to capture body gestures and train a student performing the North Indian hand drums known as the Tabla. A comparative study of motion capture systems, wearable accelerometer units, and wireless inertial sensor packages, is described. Each acquisition method has it advantages and disadvantages which are explored through trial and error. The paper describes a number of applications using real-time signal processing techniques for analysis, performance, performer posture detection and machine perception of human interaction.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:P5F9QuxV20EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Visualization tools for musical timing applied to Afro-Cuban percussion",
            "Publication year": 2008,
            "Publication url": "https://www.researchgate.net/profile/George-Tzanetakis/publication/238620955_Visualization_tools_for_musical_timing_applied_to_afro-cuban_percussion/links/0deec52242b6749629000000/Visualization-tools-for-musical-timing-applied-to-afro-cuban-percussion.pdf",
            "Abstract": "Timing is perhaps the most fundamental aspect of music, and visualization tools can help in formulating hypotheses and exploring questions regarding musical timing. We present a series of novel graphical representations of musical timing, generated by computer from signal analysis of audio recordings and from listeners\u2019 annotations collected in real time. We have tested our methods on recordings of Afro-Cuban percussion with particular emphasis on the \u201cclave\u201d rhythmic patterns used for temporal organization. The proposed visualizations are based on the idea of Bar Wrapping, which is the breaking and stacking of a linear time axis at a fixed metric location.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:35N4QoGY0k4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Adapting personal music for synesthetic game play",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1822348.1822370",
            "Abstract": "Music can significantly effect game play and help players understand underlying patterns in the game, or the effects of their actions on the characters. Conversely, inappropriate music can have a negative effect on players by creating additional difficulties. While game makers recognize the effects of music on game play, solutions that provide users with a choice in personal music are not forthcoming. We design, implement and evaluate an algorithm for automatically adapting an arbitrary music track from a personal library and synchronizing play back to the user, without requiring any access to the video game source code.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:fPk4N6BV_jEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Effective use of multimedia for computer-assisted musical instrument tutoring",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1290144.1290156",
            "Abstract": "This paper presents a survey of recent work in computer-assisted musical instrumental tutoring and outlines several questions to consider when developing future projects. In particular, we suggest that the area ingreatest need of computer assistance is enhancing daily practice: both motivating students to practice through games and multimedia, and providing an objective analysis of the students' performance. Many existing projects attempt to replace human teachers by providing lessons during daily practice; in most cases, these\" daily lessons\" are not necessary.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:IWHjjKOFINEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Raydiance: A tangible interface for teaching computer vision",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-24031-7_26",
            "Abstract": "This paper presents a novel paradigm for prototyping Computer Vision algorithms; this paradigm is suitable for students with very limited programming experience. Raydiance includes a tangible user interface controlled by a spatial arrangement of physical tokens which are detected using computer vision techniques. Constructing an algorithm is accomplished by creating a directed graph of token connections. Data is processed, then propagated from one token to another by using a novel Light Ray metaphor. Our case study shows how Raydiance can be used to construct a computer vision algorithm for a particular task.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:1qzjygNMrQYC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Tutorial on Music Information Retrieval for Audio Signals",
            "Publication year": 2002,
            "Publication url": "http://ismir2002.ircam.fr/proceedings/05-TUT-2.pdf",
            "Abstract": "1. OBJECTIVESThe main objective of this tutorial is to provide an overview of the current status of music information retrieval (MIR) for audio signals. The intended audience are people with a technical background who are interested to learn the main approaches and current status of MIR for audio signals. An important part of the intended audience would consist of researchers who have a background in symbolic MIR and/or musicology and music cognition and are interested to learn more about the similarities/differences of audio MIR to their respective fields. Demonstrations of several of the described algorithms and techniques will be part of the tutorial presentation.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:isC4tDSrTZIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Musescape: A tool for changing music collections into libraries",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-45175-4_38",
            "Abstract": "Increases in hard disk capacity and audio compression technology have enabled the storage of large collections of music on personal computers and portable devices. As an example a portable device with 20 Gigabytes of storage can hold up to 4000 songs in compressed audio format. Currently the only way of structuring these collections is using a file system hierarchy which allows very limited forms of searching and retrieval. These limitations are even more pronounced in the case of portable devices where there is less screen real estate and user attention is limited compared to a personal computer. Musescape is a prototype tool for organizing and interacting with large music collections in audio format with specific emphasis on portable devices. It provides a variety of automatic and manual ways to organize and interact with large music collections using a consistent continuous \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Audio information retrieval (AIR) tools",
            "Publication year": 2000,
            "Publication url": "http://www.ee.columbia.edu/~dpwe/papers/TzanC00-airtools.pdf",
            "Abstract": "The majority of work in music information retrieval (IR) has been focused on symbolic representations of music. However, most of the digitally available music is in the form of raw audio signals. Although various attempts at monophonic and polyphonic transcription have been made, none has been successful and general enough to work with real world signals. In this paper we describe some initial efforts at building IR tools for real world audio signals. Our approach is based on signal processing, statistical pattern recognition and visualization techniques. We try to gather as much information as possible without attempting to perform polyphonic transcription.A frequently ignored aspect in emerging fields like music IR is the importance of the user in building a successful system. We describe some new graphical user interfaces that accommodate different modes of interaction with the user. More specifically we describe an augmented sound editor for annotating, classifying and segmenting music and we define TimbreGrams a new visual representation for audio files.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ufrVoPGSRksC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sieve: A plugin for the automatic classification and intelligent browsing of kick and snare samples",
            "Publication year": 2017,
            "Publication url": "https://www.researchgate.net/profile/Kirk-Mcnally/publication/319622779_SIEVE_A_PLUGIN_FOR_THE_AUTOMATIC_CLASSIFICATION_AND_INTELLIGENT_BROWSING_OF_KICK_AND_SNARE_SAMPLES/links/59b607250f7e9b374355257c/SIEVE-A-PLUGIN-FOR-THE-AUTOMATIC-CLASSIFICATION-AND-INTELLIGENT-BROWSING-OF-KICK-AND-SNARE-SAMPLES.pdf",
            "Abstract": "The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. To be efficient, users of these large collections require new tools to assist them in sorting, selection and auditioning tasks. This paper presents a new plugin for working with a large collection of kick and snare samples within a music production context. A database of 4230 kick and snare samples, representing 250 individual electronic drum machines are analyzed by segmenting the audio samples into different sample lengths and characterizing these segments using audio feature analysis. The resulting multidimensional feature space is reduced using principle component analysis (PCA). Samples are mapped to a 2D grid interface within an audio plug-in built using the JUCE software framework.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:vlMkzkLhH4wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Noise based interest point density pruning",
            "Publication year": 2014,
            "Publication url": "https://patents.google.com/patent/US8805560B1/en",
            "Abstract": "Systems and methods for noise based interest point density pruning are disclosed herein. The systems include determining an amount of noise in an audio sample and adjusting the amount of interest points within an audio sample fingerprint based on the amount of noise. Samples containing high amounts of noise correspondingly generate fingerprints with more interest points. The disclosed systems and methods allow reference fingerprints to be reduced in size while increasing the size of sample fingerprints. The benefits in scalability do not compromise the accuracy of an audio matching system using noise based interest point density pruning.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:GfAJFcoWUJEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Declarative Composition and Reactive Control in Marsyas",
            "Publication year": 2014,
            "Publication url": "http://speech.di.uoa.gr/ICMC-SMC-2014/images/VOL_1/0325.pdf",
            "Abstract": "We present a new coordination language for audio processing applications, designed for the dynamic dataflow capabilities of the Marsyas C++ framework. We refer to the language as Marsyas Script. It is a declarative coordination language that enables intuitive and quick composition of dataflow networks and reactive processing control. It separates the tasks of dataflow coordination and computation, while increasing the expressivity of the coordination level. This allows more dynamic dataflow behavior and more powerful interaction with other multimedia applications and the physical world. It also increases code portability and allows multiple tools to operate on the same network definition with the purpose of real-time or nonreal-time execution, network visualization, operational inspection and debugging, etc. This naturally enhances and extends the functionality within the domain of the Marsyas framework and makes it more accessible to users of other audio software frameworks and languages.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:LHWLPdAD5FMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The E-Drum: A case study for machine learning in new musical controllers.",
            "Publication year": 2012,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.2432&rep=rep1&type=pdf",
            "Abstract": "Background in Music Performance. Advances in Music Technology seem to pass percussion by: there is no entry for it in the standard book Percussion Instruments and their Histories (Blades, 1997). Current percussion controllers monitor impact velocity only, dimensionally impoverishing complex musical gestures. Conversely, percussionists select sticks and vary technique to influence the timbre of their performance. Commercial drum synthesizers ignore this, which is inadequate if the device is to be useful for a professional percussionist. Background in Computer Science. Rebecca Fiebrink has presented several works on integrating machine learning into the live performance context (Fiebrink, 2008, Wang et al., 2008). Fiebrink's system incorporates a machine learning framework with a real-time Open Sound Control interface in order to allow an agnostic approach towards feature extraction. Roberto Aimi received his Ph. D. from the MIT Media Lab for developing novel percussion interfaces (Aimi, 2007). Aimi's convdrum uses piezo microphones attached to a drum. Convolution of their signals creates a timbre depending on stick, strike position and force. Aims. The E-Drum primarily uses an acoustically driven physical model permitting acquisition of brushing and scraping gestures eluding existing drum controllers. Main contribution. The E-Drum has two methods of recognizing gestures: implicit and explicit position tracking. The former uses machine learning to determine the timbre produced by a drum controller and then infer position based on labeled training data. The later is made application-specific by selecting the classification algorithm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:UxriW0iASnsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving music transcription by pre-stacking a u-net",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9052987/",
            "Abstract": "We propose to pre-stack a U-Net as a way of improving the polyphonic music transcription performance of various baseline Convolutional Neural Networks (CNNS). The U-Net, a network architecture based on skip-connections between layers acts as a transformation network followed by a transcription network. Notably, we do not introduce any additional loss terms specific to the transformation network, but instead, jointly train the entire combined model with the original loss function that was designed for the back-end transcription network. We argue that this U-Net network transforms the input signal into a representation that is more effective for transcription, and thus enables the observed improvements in accuracy. We empirically confirm with several experiments using the MusicNet dataset, that the proposed configuration consistently improves the accuracy of transcription networks. This enhancement cannot be \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:Xc-mKOjpdrwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Learning the Semantics of Audio Signals",
            "Publication year": 2006,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.1332&rep=rep1&type=pdf",
            "Abstract": "Introduction of techniques for compressing and streaming of audio data in recent time has significantly changed the way music is consumed and archived. Personal music collections may nowadays comprise ten-thousands of music titles and even mobile devices are able to store some thousands of songs. But these magnitudes are nothing compared to the vast amount of music data digitally available on the internet. Several features have been proposed to describe music on a low, signal-processing based level. Some of these have already been incorporated as description schemes for annotation of audio data into the MPEG-7 standard. However, in contrast to text documents that can be sufficiently well represented by statistics about the contained terms, audio data seems far too complex to be described by statistics on signals alone. Additionally, such a representation does only allow query-by-example. Learning a mapping between audio features and contextual interpretations would be the key to solve this problem, enabling a user to formulate a query in a way that is close to his way of describing music contents, eg using natural language or at least combinations of terms. For this task, models describing how music is perceived are needed, as well as methods for the extraction, analysis and representation of linguistic descriptions of music. On the other hand, more sophisticated audio features and analysis of the music structure can narrow the gap. But even if a mapping can be found, it cannot be considered as universally valid. It will rather be biased depending on the user\u2019s preferences, making it necessary to think about personalization at \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:CHSYGLWDkRkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Digital sensing of musical instruments",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-662-55004-5_46",
            "Abstract": "Acoustic musical instruments enable very rich and subtle control when used by experienced musicians. Musicology has traditionally focused on analysis of scores and more recently audio recordings. However, most music from around the world is not notated, and many nuances of music performance are hard to recover from audio recordings. In this chapter, we describe hyperinstruments, i.\u202fe., acoustic instruments that are augmented with digital sensors for capturing performance information and in some cases offering additional playing possibilities. Direct sensors are integrated onto the physical instrument, possibly requiring modifications. Indirect sensors such as cameras and microphones can be used to analyze performer gestures without requiring modifications to the instrument. We describe some representative case studies of hyperinstruments from our own research as well as some representative \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:WD7AgJrCjNIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Music analysis and retrieval systems for audio signals",
            "Publication year": 2004,
            "Publication url": "https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.20060",
            "Abstract": "The constantly increasing amount of audio available in digital form necessitates the development of software systems for analyzing and retrieving digital audio. In this work, we describe our efforts in developing such systems. More specifically, we describe the design philosophy behind our approach, the specific problems we try to solve, and how we evaluate the performance of our algorithms. Automatic music analysis and retrieval of non\u2010speech digital audio is a relatively new field, and the existing techniques are far from perfect. To improve the performance of the developed techniques, two main techniques are used: (1) integration of information from multiple analysis and retrieval algorithms and (2) the use of graphical user interfaces that enable the user to provide feedback to the design, development, and evaluation of the algorithms. All the developed algorithms and user interfaces are integrated under \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:hC7cP41nSMkC",
            "Publisher": "Wiley Subscription Services, Inc., A Wiley Company"
        },
        {
            "Title": "Audio-visual vibraphone transcription in real time",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6343443/",
            "Abstract": "Music transcription refers to the process of detecting musical events (typically consisting of notes, starting times and durations) from an audio signal. Most existing work in automatic music transcription has focused on offline processing. In this work we describe our efforts in building a system for real time music transcription for the vibraphone. We describe experiments with three audio-based methods for music transcription that are representative of the state of the art. One method is based on multiple pitch estimation and the other two methods are based on factorization of the audio spectrogram. In addition we show how information from a video camera can be used to impose constraints on the symbol search space based on the gestures of the performer. Experimental results with various system configurations show that this multi-modal approach leads to a significant reduction of false positives and increases the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:uLbwQdceFCQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Dawintegrated beat tracking for music production",
            "Publication year": 2019,
            "Publication url": "http://www.smc2019.uma.es/articles/P1/P1_01_SMC2019_paper.pdf",
            "Abstract": "Rhythm analysis is a well researched area in music information retrieval that has many useful applications in music production. In particular, it can be used to synchronize the tempo of audio recordings with a digital audio workstation (DAW). Conventionally this is done by stretching recordings over time, however, this can introduce artifacts and alter the rhythmic characteristics of the audio. Instead, this research explores how rhythm analysis can be used to do the reverse by synchronizing a DAW\u2019s tempo to a source recording. Drawing on research by Percival and Tzanetakis, a simple beat extraction algorithm was developed and integrated with the Renoise DAW. The results of this experiment show that, using user input from a DAW, even a simple algorithm can perform on par with popular packages for rhythm analysis such as BeatRoot, IBT, and aubio.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:0MsrIDK0EWAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "29 Music Information Retrieval",
            "Publication year": 2011,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=krkInw_libkC&oi=fnd&pg=PA409&dq=info:Def0VNoZf5EJ:scholar.google.com&ots=ytLe7Xv5zB&sig=S9vWVKVdLj9V3A-wr56Oaxe2c8o",
            "Abstract": "IntroductIon contentsIntroduction.................................................................................................................................... 409 Symbolic Techniques..................................................................................................................... 410 Using String Matching Framework........................................................................................... 410 Problem Definition............................................................................................................... 410 Results.................................................................................................................................. 411 Using Geometric Framework.................................................................................................... 411 Problem Definition............................................................................................................... 411 Results.................................................................................................................................. 412 Theorem 1: The Largest Common Pointset Problem under Translations Is 3SUM-Hard............. 412 Audio Techniques.......................................................................................................................... 413 Query Type................................................................................................................................ 414 Representations and Extracted Information.............................................................................. 414 Specificity.................................................................................................................................. 415 Evaluation.................................................................................................................................. 416 User Interfaces.......................................................................................................................... 416 Bridging Symbolic and Audio Content-Based Music Retrieval.................................................... 416 Acknowledgment........................................................................................................................... 417 References...................................................................................................................................... 417",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:LjlpjdlvIbIC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "Visualization in audio-based music information retrieval",
            "Publication year": 2006,
            "Publication url": "https://scholar.google.com/scholar?cluster=2612278015939513274&hl=en&oi=scholarr",
            "Abstract": "BackgroundThere has been considerable interest in making music visible. Many artists have attempted to realize the images elicited by sound (Walt Disney\u2019s Fantasia being an early, well-known example). Another approach is to quantitatively render the time or frequency content of the audio signal, using methods such as the oscillograph and sound spectrograph (Koening, Dunn, and Lacey 1946; Potter, Kopp, and Green 1947). These are intended primarily for scientific or quantitative analysis, although artists like Mary Ellen Bute have used quantitative methods such as the cathode ray oscilloscope toward artistic ends (Moritz 1996). Other visualizations are derived from note-based or score-representations of music, typically MIDI note events (Malinowski 1988; Smith and Williams 1997; Sapp 2001). The idea of representing sound as a visual object in a two-or three-dimensional space with properties related to the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:mVmsd5A6BfQC",
            "Publisher": "MIT Press"
        },
        {
            "Title": "New Music Interfaces for Rhythm-Based Retrieval.",
            "Publication year": 2005,
            "Publication url": "https://webhome.cs.uvic.ca/~gtzan/work/pubs/ismir05gtzan.pdf",
            "Abstract": "In the majority of existing work in music information retrieval (MIR) the user interacts with the system using standard desktop components such as the keyboard, mouse or sometimes microphone input. It is our belief that moving away from the desktop to more physically tangible ways of interacting can lead to novel ways of thinking about MIR. In this paper, we report on our work in utilizing new non-standard interfaces for MIR purposes. One of the most important but frequently neglected ways of characterizing and retrieving music is through rhythmic information. We concentrate on rhythmic information both as user input and as means for retrieval. Algorithms and experiments for rhythm-based information retrieval of music, drum loops and indian tabla thekas are described. This work targets expert users such as DJs and musicians which tend to be more curious about new technologies and therefore can serve as catalysts for accelerating the adoption of MIR techniques. In addition, we describe how the proposed rhythm-based interfaces can assist in the annotation and preservation of perfomance practice.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:iH-uZ7U-co4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Jingdong Wang",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=4605226967817925238&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:xqRlItQsuMMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Aesthetic agents swarm-based non-photorealistic rendering using multiple images",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2030441.2030452",
            "Abstract": "The creation of expressive styles for digital art is one of the primary goals in non-photorealistic rendering. In this paper, we introduce a swarm-based multi-agent system that is capable of producing expressive imagery through the use of multiple digital images. At birth, agents in our system are assigned a digital image that represents theiraesthetic ideal'. As agents move throughout a digital canvas they try torealize'their ideal by modifying the pixels in the digital canvas to be closer to the pixels in their aesthetic ideal. When groups of agents with different aesthetic ideals occupy the same canvas, a new image is created through the convergence of their conflicting aesthetic goals. We use our system to explore the concepts and techniques from a number of Modern Art movements. The simple implementation and effective results produced by our system makes a compelling argument for more research using swarm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:AXPGKjj_ei8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sonophenology: a tangible interface for sonification of geo-spatial phenological data at multiple time-scales",
            "Publication year": 2010,
            "Publication url": "https://aragorn.library.gatech.edu/handle/1853/50061",
            "Abstract": "Phenology is the study of periodic biological processes, such as when plants flower and birds arrive in the spring. In this paper we sonify phenology data and control the sonification process through a tangible interface consisting of a physical paper map and tracking of fiducial markers. The designed interface enables one or more users to concurrently specify point and range queries in both time and space and receive immediate sonic feedback. This system can be used to study and explore the effects of climate change, both as tool to be used by scientists, and as a way to educate members of the general public.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:sSrBHYA8nusC",
            "Publisher": "Georgia Institute of Technology"
        },
        {
            "Title": "Example design process for audio signals in a digital camera",
            "Publication year": 2002,
            "Publication url": "https://smartech.gatech.edu/handle/1853/51353",
            "Abstract": "Design guidelines were proposed to provide a general workflow for the creation of audio signals for digital cameras, which recently continue to diversify. The steps for the initial phase\u2013- from the formulation of basic sound design concepts to the extraction of the design item and the construction of an outline for creating an actual sound\u2013-have been defined, and trial audio signals that are expected to be incorporated into a wide variety of electrical appliances have been created. In addition, hypotheses on the circumstances under which the equipment will be used have been deduced, and the evaluation methods of trial audio signals, points of improvement in the sound quality and the necessary precautions for incorporating the audio signal into the equipment have been investigated.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:tai-Ft5GzhwC",
            "Publisher": "Georgia Institute of Technology"
        },
        {
            "Title": "Analyzing Afro-Cuban Rhythms using Rotation-Aware Clave Template Matching with Dynamic Programming.",
            "Publication year": 2008,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.184.9232&rep=rep1&type=pdf",
            "Abstract": "The majority of existing research in Music Information Retrieval (MIR) has focused on either popular or classical music and frequently makes assumptions that do not generalize to other music cultures. We use the term Computational Ethnomusicology (CE) to describe the use of computer tools to assist the analysis and understanding of musics from around the world. Although existing MIR techniques can serve as a good starting point for CE, the design of effective tools can benefit from incorporating domain-specific knowledge about the musical style and culture of interest. In this paper we describe our realization of this approach in the context of studying Afro-Cuban rhythm. More specifically we show how computer analysis can help us characterize and appreciate the complexities of tracking tempo and analyzing micro-timing in these particular music styles. A novel template-based method for tempo tracking in rhythmically complex Afro-Cuban music is proposed. Although our approach is domain-specific, we believe that the concepts and ideas used could also be used for studying other music cultures after some adaptation.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ZeXyd9-uunAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Speaker Segmentation of Interviews Using Integrated Video and Audio Change Detectors",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4275078/",
            "Abstract": "In this paper, we study the use of audio and visual cues to perform speaker segmentation of audiovisual recordings of formal meetings such as interviews, lectures, or courtroom sessions. The sole use of audio cues for such recordings can be ineffective due to low recording quality and high level of background noise. We propose to use additional cues from the video stream by exploiting the relative static locations of speakers among the scene. The experiments show that the combination of those multiple cues helps to identify more robustly the transitions among speakers.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:RYcK_YlVTxYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Exploiting structural relationships in audio music signals using Markov Logic Networks",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6637597/",
            "Abstract": "We propose an innovative approach for music description at several time-scales in a single unified formalism. More specifically, chord information at the analysis-frame level and global semantic structure are integrated in an elegant and flexible model. Using Markov Logic Networks (MLNs) low-level signal features are encoded with high-level information expressed by logical rules, without the need of a transcription step. Our results demonstrate the potential of MLNs for music analysis as they can express both structured relational knowledge through logic as well as uncertainty through probabilities.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:j8SEvjWlNXcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Noise based interest point density pruning",
            "Publication year": 2016,
            "Publication url": "https://patents.google.com/patent/US9411884B1/en",
            "Abstract": "Systems and methods for noise based interest point density pruning are disclosed herein. The systems include determining an amount of noise in an audio sample and adjusting the amount of interest points within an audio sample fingerprint based on the amount of noise. Samples containing high amounts of noise correspondingly generate fingerprints with more interest points. The disclosed systems and methods allow reference fingerprints to be reduced in size while increasing the size of sample fingerprints. The benefits in scalability do not compromise the accuracy of an audio matching system using noise based interest point density pruning.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:mq6pegT_rlEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Polyphonic instrument recognition using spectral clustering.",
            "Publication year": 2007,
            "Publication url": "https://www.academia.edu/download/30705042/4.pdf",
            "Abstract": "The identification of the instruments playing in a polyphonic music signal is an important and unsolved problem in Music Information Retrieval. In this paper, we propose a framework for the sound source separation and timbre classification of polyphonic, multi-instrumental music signals. The sound source separation method is inspired by ideas from Computational Auditory Scene Analysis and formulated as a graph partitioning problem. It utilizes a sinusoidal analysis front-end and makes use of the normalized cut, applied as a global criterion for segmenting graphs. Timbre models for six musical instruments are used for the classification of the resulting sound sources. The proposed framework is evaluated on a dataset consisting of mixtures of a variable number of simultaneous pitches and instruments, up to a maximum of four concurrent notes.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:M3ejUd6NZC8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audioscapes: Exploring Surface Interfaces for Music Exploration.",
            "Publication year": 2009,
            "Publication url": "http://webhome.csc.uvic.ca/~gtzan/icmc2009gtzan-freesound.pdf",
            "Abstract": "There is a growing interest in touch-based and gestural interfaces as alternatives to the dominant mouse, keyboard and monitor interaction. Content and context-aware visualizations of audio collections have been proposed as a more effective way to interact with the increasing amounts of audio data available digitally. Audioscapes is a framework for prototyping and exploring how touch-based and gestural controllers can be used with state-of-the-art content and context-aware visualizations. By providing well-defined interfaces and conventions a variety of different audio collections, controllers and visualization methods can be combined to create innovative ways of interacting with large audio collections. We describe the overall system architecture, the currently available components and specific case studies.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:70eg2SAEIzsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "3D graphics tools for sound collections",
            "Publication year": 2000,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.9236&rep=rep1&type=pdf",
            "Abstract": "Most of the current tools for working with sound work on single soundfiles, use 2D graphics and offer limited interaction to the user. In this paper we describe a set of tools for working with collections of sounds that are based on interactive 3D graphics. These tools form two families: sound analysis visualization displays and model-based controllers for sound synthesis algorithms. We describe the general techniques we have used to develop these tools and give specific case studies from each family. Several collections of sounds were used for development and evaluation. These are: a set of musical instrument tones, a set of sound effects, a set of FM radio audio clips belonging to several music genres, and a set of mp3 rock song snippets.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:4DMP91E08xMC",
            "Publisher": "Fernstr\u00f6m, Mikael"
        },
        {
            "Title": "Histogram-Based Asymmetric Relabeling for Learning from Only Positive and Unlabeled Data",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8260784/",
            "Abstract": "In this paper, we demonstrate how to use asymmetric data relabeling based on feature histograms as a pre-processing step for improving the overall classification performance of different classifiers in situations when only positive and unlabeled data is available. Additionally, this strategy can be used to identify with some level of confidence those data instances that should probably be labeled as positive. Moreover, this approach can be adapted to assess the quality of a given dataset, in terms of how many positive instances are not labeled. We examine our approach using synthetic data and demonstrate its applicability using real, publicly available data.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:wW8w_uPXRNAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Estimation of the Direction of Strokes and Arpeggios.",
            "Publication year": 2014,
            "Publication url": "https://scholar.google.com/scholar?cluster=7535276562870561016&hl=en&oi=scholarr",
            "Abstract": "Whenever a chord is played in a musical instrument, the notes are not commonly played at the same time. Actually, in some instruments, it is impossible to trigger multiple notes simultaneously. In others, the player can consciously select the order of the sequence of notes to play to create a chord. In either case, the notes in the chord can be played very fast, and they can be played from the lowest to the highest pitch note (upstroke) or from the highest to the lowest pitch note (downstroke). In this paper, we describe a system to automatically estimate the direction of strokes and arpeggios from audio recordings. The proposed system is based on the analysis of the spectrogram to identify meaningful changes. In addition to the estimation of the up or down stroke direction, the proposed method provides information about the number of notes that constitute the chord, as well as the chord playing speed. The system has been tested with four different instruments: guitar, piano, autoharp and organ.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:RSvx26AYfG8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A scalable peer-to-peer system for music content and information retrieval",
            "Publication year": 2003,
            "Publication url": "https://jscholarship.library.jhu.edu/handle/1774.2/29",
            "Abstract": "Currently a large percentage of internet traffice consists of music files, typically stored in MP3 compressed audio format, shared and exchanged over Peer-to-Peer (P2P) networks. Searching for music is performed by specifying keywords and naive string matching techniques. In the past years the emerging research area of Music Information Retrieval (MIR) has produced a variety of new ways of looking at the problem of music search. Such MIR techniques can significantly enhance the ways users search for music over P2P networks. In order for that to happen there are two main challenges that need to be addressed: 1) scalability to large collections and number of peers 2) richer set of search semantics that can support MIR especially when retrieval is content-based. In this paper, we describe a scalable P2P system that uses Rendezvouz Points (RPs) for music metadata registration and query resolution, that supports atribute-value search semantics as well as content-based retrieval. The performance of the system has been evaluated in large scale usage scenarios using \"real\" automatically calculated musical content descriptors.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:JoZmwDi-zQgC",
            "Publisher": "Johns Hopkins University"
        },
        {
            "Title": "Music data mining",
            "Publication year": 2011,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=_zc3vKDLUNIC&oi=fnd&pg=PP1&dq=info:Tv2JkIOjKMUJ:scholar.google.com&ots=dWlWvGZ-GR&sig=zBachhP-SgfLAiuJscTrtdyvB74",
            "Abstract": "The research area of music information retrieval has gradually evolved to address the challenges of effectively accessing and interacting large collections of music and associated data, such as styles, artists, lyrics, and reviews. Bringing together an interdisciplinary array of top researchers, Music Data Mining presents a variety of approaches to successfully employ data mining techniques for the purpose of music processing. The book first covers music data mining tasks and algorithms and audio feature extraction, providing a framework for subsequent chapters. With a focus on data classification, it then describes a computational approach inspired by human auditory perception and examines instrument recognition, the effects of music on moods and emotions, and the connections between power laws and music aesthetics. Given the importance of social aspects in understanding music, the text addresses the use of the Web and peer-to-peer networks for both music data mining and evaluating music mining tasks and algorithms. It also discusses indexing with tags and explains how data can be collected using online human computation games. The final chapters offer a balanced exploration of hit song science as well as a look at symbolic musicology and data mining. The multifaceted nature of music information often requires algorithms and systems using sophisticated signal processing and machine learning techniques to better extract useful information. An excellent introduction to the field, this volume presents state-of-the-art techniques in music data mining and information retrieval to create novel ways of interacting with large music collections.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:z_wVstp3MssC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "Audio-based gesture extraction on the esitar controller",
            "Publication year": 2004,
            "Publication url": "https://www.academia.edu/download/30705071/10.1.1.138.7587.pdf",
            "Abstract": "Using sensors to extract gestural information for control parameters of digital audio effects is common practice. There has also been research using machine learning techniques to classify specific gestures based on audio feature analysis. In this paper, we will describe our experiments in training a computer to map the appropriate audio-based features to look like sensor data, in order to potentially eliminate the need for sensors. Specifically, we will show our experiments using the ESitar, a digitally enhanced sensor based controller modeled after the traditional North Indian sitar. We utilize multivariate linear regression to map continuous audio features to continuous gestural data.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:-f6ydRqryjwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Gesture-based affective computing on motion capture data",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11573548_1",
            "Abstract": "This paper presents research using full body skeletal movements captured using video-based sensor technology developed by Vicon Motion Systems, to train a machine to identify different human emotions. The Vicon system uses a series of 6 cameras to capture lightweight markers placed on various points of the body in 3D space, and digitizes movement into x, y, and z displacement data. Gestural data from five subjects was collected depicting four emotions: sadness, joy, anger, and fear. Experimental results with different machine learning techniques show that automatic classification of this data ranges from 84% to 92% depending on how it is calculated. In order to put these automatic classification results into perspective a user study on the human perception of the same data was conducted with average classification accuracy of 93%.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:LkGwnXOMwfcC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Logorhythms: Introductory audio programming for computer musicians in a functional language paradigm",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4116952/",
            "Abstract": "Teaching computer music presents opportunities and challenges at both secondary and university levels by bringing together students with widely varying exposures to and interests for mathematics and computer programming. Visual languages like MAX/MSP are popular with many musicians, but the idiom doesn't necessarily transfer well to a text language such as Java or C++, languages that might be used in a wider variety of programming problems. Our design challenge with LogoRhythms was to create a forgiving text based API that allows the neophyte programmer to explore programming and low-level digital audio manipulations. Since any musical composition is essentially a novel program, the opportunity for custom software is endless and the programming task given as a creative endeavor. LogoRhythms encourages functional style programming. Examples are provided showing lists and higher order \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:JV2RwH3_ST0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fast genre classification and artist identification",
            "Publication year": 2005,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8879&rep=rep1&type=pdf",
            "Abstract": "This abstract describes the audio feature extraction and classification algorithm used for the University of Victoria submission to the MIREX (Music Information Retrieval Exchange) 2005. The same audio features and classification algorithm are used for the audio genre classification and artist identification tasks and arge therefore combined in this abstract. The feature set used is similar to one described in Tzanetakis and Cook (2002) but not including the Beat and Pitch Histogram features. This decision was made in order to address stability problems with the above features and computation speed requirements. Even though our submission did not win the contest it was by far the fastest one.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:2P1L_qKh6hAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Aesthetic agents",
            "Publication year": 2011,
            "Publication url": "https://scholar.google.com/scholar?cluster=2940894654251186153&hl=en&oi=scholarr",
            "Abstract": "The creation of expressive styles for digital art is one of the primary goals in non-photorealistic rendering. In this paper, we introduce a swarm-based multi-agent system that is capable of producing expressive imagery through the use of multiple digital images. At birth, agents in our system are assigned a digital image that represents their `aesthetic ideal'. As agents move throughout a digital canvas they try to `realize' their ideal by modifying the pixels in the digital canvas to be closer to the pixels in their aesthetic ideal. When groups of agents with different aesthetic ideals occupy the same canvas, a new image is created through the convergence of their conflicting aesthetic goals. We use our system to explore the concepts and techniques from a number of Modern Art movements. The simple implementation and effective results produced by our system makes a compelling argument for more research using swarm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:Kr3pDLWb32UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Transformation invariant media matching",
            "Publication year": 2014,
            "Publication url": "https://patents.google.com/patent/US8738633B1/en",
            "Abstract": "This disclosure relates to transformation invariant media matching. A fingerprinting component can generate a transformation invariant identifier for media content by adaptively encoding the relative ordering of interest points in media content. The interest points can be grouped into subsets, and stretch invariant descriptors can be generated for the subsets based on ratios of coordinates of interest points included in the subsets. The stretch invariant descriptors can be aggregated into a transformation invariant identifier. An identification component compares the identifier against a set of identifiers for known media content, and the media content can be matched or identified as a function of the comparison.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Fd6TstiuZzAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Transforming perceived vocal effort and breathiness using adaptive pre-emphasis linear prediction",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4566083/",
            "Abstract": "This paper presents a technique to transform high-effort voices into breathy voices using adaptive pre-emphasis linear prediction (APLP). The primary benefit of this technique is that it estimates a spectral emphasis filter that can be used to manipulate the perceived vocal effort. The other benefit of APLP is that it estimates a formant filter that is more consistent across varying voice qualities. This paper describes how constant pre-emphasis linear prediction (LP) estimates a voice source with a constant spectral envelope even though the spectral envelope of the true voice source varies over time. A listening experiment demonstrates how differences in vocal effort and breathiness are audible in the formant filter estimated by constant pre-emphasis LP. APLP is presented as a technique to estimate a spectral emphasis filter that captures the combined influence of the glottal source and the vocal tract upon the spectral \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:OU6Ihb5iCvQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Deep Autotuner: A data-driven approach to natural-sounding pitch correction for singing voice in karaoke performances",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1902.00956",
            "Abstract": "We describe a machine-learning approach to pitch correcting a solo singing performance in a karaoke setting, where the solo voice and accompaniment are on separate tracks. The proposed approach addresses the situation where no musical score of the vocals nor the accompaniment exists: It predicts the amount of correction from the relationship between the spectral contents of the vocal and accompaniment tracks. Hence, the pitch shift in cents suggested by the model can be used to make the voice sound in tune with the accompaniment. This approach differs from commercially used automatic pitch correction systems, where notes in the vocal tracks are shifted to be centered around notes in a user-defined score or mapped to the closest pitch among the twelve equal-tempered scale degrees. We train the model using a dataset of 4,702 amateur karaoke performances selected for good intonation. We present a Convolutional Gated Recurrent Unit (CGRU) model to accomplish this task. This method can be extended into unsupervised pitch correction of a vocal performance, popularly referred to as autotuning.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:PRLG7g5oK-wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cluster aware normalization for enhancing audio similarity",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6288292/",
            "Abstract": "An important task in Music Information Retrieval is content-based similarity retrieval in which given a query music track, a set of tracks that are similar in terms of musical content are retrieved. A variety of audio features that attempt to model different aspects of the music have been proposed. In most cases the resulting audio feature vector used to represent each music track is high dimensional. It has been observed that high dimensional music similarity spaces exhibit some anomalies: hubs which are tracks that are similar to many other tracks, and orphans which are tracks that are not similar to most other tracks. These anomalies are an artifact of the high dimensional representation rather than actually based on the musical content. In this work we describe a distance normalization method that is shown to reduce the number of hubs and orphans. It is based on post-processing the similarity matrix that encodes the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:W5xh706n7nkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Deep Autotuner: A Pitch Correcting Network for Singing Performances",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9054308/",
            "Abstract": "We introduce a data-driven approach to automatic pitch correction of solo singing performances. The proposed approach predicts note-wise pitch shifts from the relationship between the respective spectrograms of the singing and accompaniment. This approach differs from commercial systems, where vocal track notes are usually shifted to be centered around pitches in a user-defined score, or mapped to the closest pitch among the twelve equal-tempered scale degrees. The proposed system treats pitch as a continuous value rather than relying on a set of discretized notes found in musical scores, thus allowing for improvisation and harmonization in the singing performance. We train our neural network model using a dataset of 4,702 amateur karaoke performances selected for good intonation. Our model is trained on both incorrect intonation, for which it learns a correction, and intentional pitch variation, which it \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:0lma3NU_mqEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The Selling Sound: Country Music, Commercialism, and the Politics of Popular Culture, 1920-1974",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=8501529183212007613&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:Vu1dURnyNv8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Physical modeling meets machine learning: Teaching bow control to a virtual violinist",
            "Publication year": 2011,
            "Publication url": "https://www.researchgate.net/profile/George-Tzanetakis/publication/267236503_Physical_modeling_meets_machine_learning_Teaching_bow_control_to_a_virtual_violinist/links/546df2c60cf2d5ae3670cd24/Physical-modeling-meets-machine-learning-Teaching-bow-control-to-a-virtual-violinist.pdf",
            "Abstract": "The control of musical instrument physical models is difficult; it takes many years for professional musicians to learn their craft. We perform intelligent control of a violin physical model by analyzing the audio output and adjusting the physical inputs to the system using trained Support Vector Machines (SVM).Vivi, the virtual violinist is a computer program which can perform music notation with the same skill as a beginning violin student. After only four hours of interactive training, Vivi can play all of Suzuki violin volume 1 with quality that is comparable to a human student. Although physical constants are used to generate audio with the model, the control loop takes a \u201cblack-box\u201d approach to the system. The controller generates the finger position, bow-bridge distance, bow velocity, and bow force without knowing those physical constants. This method can therefore be used with other bowed-string physical models and even musical robots.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:cFHS6HbyZ2cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Stari: A self tuning auto-monochord robotic instrument",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6625511/",
            "Abstract": "This paper outlines the motivation, design and development of a self-tuning, robotic monochord. This work presents a portable, autonomous musical robotic string instrument intended for creative and pedagogical use. Detailed tests performed to optimize technical aspects of STARI are described to highlight usability and performance specifications for artists and educators. STARI is intended to be open-source so that the results are reproducible and expandable using common components with minimal financial constraints. Because the field of musical robotics is so new, standardized systems need to be designed from existing paradigms. Such paradigms are typically singular in nature, solely reflecting the idiosyncrasies of the artist and often difficult to reproduce. STARI is an attempt to standardize certain existing actuated string techniques in order to establish a formal system for experimentation and pedagogy.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ipzZ9siozwsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robotic drummer",
            "Publication year": 2018,
            "Publication url": "https://patents.google.com/patent/US20180326588A1/en",
            "Abstract": "Robotic drummers include voice coil actuators that are coupled to linear-to-rotary motion convertors to produce drumstick rotations so as to strike a drum head. Such rotations can be triggered via a microprocessor using stored performance data, by a user with a mouse, trackpad, joystick, or other user input device. Performances are enhanced by driving the VCA with drive signals have random variations associated with strike timing, amplitude, location, and speed. Multiple strikes are provided by reducing, eliminating, or reversing drumstick rotation with a corresponding drive signal upon detection of drumstick contact with the drum head,",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:ZbiiB1Sm8G8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "ORCHIVE: Digitizing and Analyzing Orca Vocalizations.",
            "Publication year": 2007,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8253&rep=rep1&type=pdf",
            "Abstract": "This paper describes the process of creating a large digital archive of killer whale or orca vocalizations. The goal of the project is to digitize approximately 20000 hours of existing analog recordings of these vocalizations in order to facilitate access to researchers internationally. We are also developing tools to assist content-based access and retrieval over this large digital audio archive. After describing the logistics of the digitization process we describe algorithms for denoising the vocalizations and for segmenting the recordings into regions of interest. It is our hope that the creation of this archive and the associated tools will lead to better understanding of the acoustic communications of Orca communities worldwide.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:4JMBOYKVnBMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Correlation-Based Amplitude Estimation of Coincident Partials in Monaural Musical Signals",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/content/pdf/10.1155/2010/523791.pdf",
            "Abstract": "This paper presents a method for estimating the amplitude of coincident partials generated by harmonic musical sources (instruments and vocals). It was developed as an alternative to the commonly used interpolation approach, which has several limitations in terms of performance and applicability. The strategy is based on the following observations: (a) the parameters of partials vary with time; (b) such a variation tends to be correlated when the partials belong to the same source; (c) the presence of an interfering coincident partial reduces the correlation; and (d) such a reduction is proportional to the relative amplitude of the interfering partial. Besides the improved accuracy, the proposed technique has other advantages over its predecessors: it works properly even if the sources have the same fundamental frequency, it is able to estimate the first partial (fundamental), which is not possible using the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:tS2w5q8j5-wC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "MarsyasX: Multim\u00e9dia Datflow Processing vith Implicit Patching",
            "Publication year": 2008,
            "Publication url": "https://hal.archives-ouvertes.fr/hal-01697481/",
            "Abstract": "The design and implementation of multimedia signal processing systems is challenging especially when efficiency and real-time performance is desired. In many modern applications , software systems must be able to handle multiple flows of various types of multimedia data such as audio and video. Researchers frequently have to rely on a combination of different software tools for each modality to assemble proof-of-concept systems that are inefficient, brittle and hard to maintain. Marsyas is a software framework originally developed to address these issues in the domain of audio processing. In this paper we describe MarsyasX, a new open-source cross-modal analysis framework that aims at a broader score of applications. It follows a dataflow architecture where complex networks of processing objects can be assembled to form systems that can handle multiple and different types of multimedia flows with expressiveness and efficiency.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:22I2CSi1iVUC",
            "Publisher": "ACM Press"
        },
        {
            "Title": "Experiments in computer-assisted annotation of audio",
            "Publication year": 2000,
            "Publication url": "https://smartech.gatech.edu/handle/1853/50671",
            "Abstract": "Advances in digital storage technology and the wide use of digital audio compression standards like MPEG have made possible the creation of large archives of audio material. In order to work efficiently with these large archives much more structure than what is currently available is needed. The creation of the necessary text indices is difficult to fully automate. However, significant amounts of user time can be saved by having the computer assist the user during the annotation process. In this paper, we describe a prototype audio browsing tool that was used to perform user experiments in semi-automatic audio segmentation and annotation. In addition to the typical sound-editor functionality the system can automatically suggest time lines that the user can edit and annotate. We examine the effect that this automatically suggested segmentation has on the user decisions as well as timing information about segmentation and annotation. Finally we discuss thumbnailing and semantic labeling of annotated audio.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "Georgia Institute of Technology"
        },
        {
            "Title": "Stereo Panning Features for Classifying Recording Production Style.",
            "Publication year": 2007,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.205.8283&rep=rep1&type=pdf",
            "Abstract": "Recording engineers, mixers and producers play important yet often overlooked roles in defining the sound of a particular record, artist or group. The placement of different sound sources in space using stereo panning information is an important component of the production process. Audio classification systems typically convert stereo signals to mono and to the best of our knowledge have not utilized information related to stereo panning. In this paper we propose a set of audio features that can be used to capture stereo information. These features are shown to provide statistically important information for non-trivial audio classification tasks and are compared with the traditional Mel-Frequency Cepstral Coefficients. The proposed features can be viewed as a first attempt to capture extra-musical information related to the production process through music information retrieval techniques.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:qUcmZB5y_30C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Audio genre classification by clustering percussive patterns",
            "Publication year": 2009,
            "Publication url": "http://hil.t.u-tokyo.ac.jp/publications/download.php?bib=Tsunoo2009ASJ03.pdf",
            "Abstract": "Due to the increasing size of digital music collections available on computers, interest in music information retrieval (MIR), especially automatic genre classification has recently surged. In this task, not only instrumental but also rhythmic information is thought to be important. If representative unit rhythmic patterns in music can be identified they can be used to genre classification directly from audio signals.In previous research, timbral, rhythmic and pitch features have been used for audio genre classification [1]. However, the rhythmic features were based on overall statistics of periodicities, not directly on temporal information. More closely related research includes Dixon [2] and which extract a periodical pattern from acoustic signals. These approaches can successfully discriminate styles such samba or tango. In this paper, we describe an approach for extracting unit percussive patterns from a number of audio tracks and propose a patterns occurrence histogram as a feature for genre classification. Finally, the effectiveness of the proposed feature is verified experimentally.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:M05iB0D1s5AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The need for music information retrieval with user-centered and multimodal strategies",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2072529.2072531",
            "Abstract": "Music is a widely enjoyed content type, existing in many multifaceted representations. With the digital information age, a lot of digitized music information has theoretically become available at the user's fingertips. However, the abundance of information is too large-scaled and too diverse to annotate, oversee and present in a consistent and human manner, motivating the development of automated Music Information Retrieval (Music-IR) techniques.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:SdhP9T11ey4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Adaptive music technology using the Kinect",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2769493.2769583",
            "Abstract": "In this paper a new approach to music-making for people with disabilities is discussed. Until recently, the technology to enable people with disabilities to make music has been relatively limited, consisting primarily of mechanical approaches. With new developments in computing, including the Microsoft Kinect, touchless sensors are providing a new way for people with disabilities to interface with instruments in novel ways. There have been few papers that made empirical measurements of adaptive musical instruments, including latency. This paper will fill this gap by detailing an adaptive musical interface using the Microsoft Kinect. Then the overall latency, including response time of the musician, will be measured, and methods to decrease this latency will be proposed.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:KKiikWAUrRgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Building audio classifiers for broadcast news retrieval",
            "Publication year": 2004,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.7099&rep=rep1&type=pdf",
            "Abstract": "The process of building audio classifiers for high-level content descriptors, especially in large datasets, is not trivial. In this paper we describe the design and development of audio classification algorithms for broadcast news retrieval in the context of the TREC 2003 video retrieval evaluation. The main focus of this paper is the actual building process itself rather than the final results, although some representative results will be provided. It is our belief that the insights obtained and tools developed in order to work with real world large audio collections are important and frequently unmentioned in existing published work. An important and critical aspect of this process is obtaining ground truth annotations for training the classifiers. Therefore tools and techniques that assist the human annotation of news audio will be described.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:R3hNpaxXUhUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spatial sound rendering using measured room impulse responses",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4042282/",
            "Abstract": "Spatial sound rendering has many applications such as music production, movies, electronic gaming and teleconferencing. Each of the applications may have different quality and complexity requirements. This paper presents a new spatial sound rendering framework that aims at producing realistic multichannel audio while being flexible and scalable so that is can be extended and adopted by various applications. The proposed framework uses multi-channel measured room impulse response (MMRIR) as the basis for building a room acoustic model which is used to synthesize multi-channel audio. The proposed framework has been evaluated by informal listening tests",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:GnPB-g6toBAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "STUDIO REPORT: UNIVERSITY OF VICTORIA MUSIC INTELLIGENCE AND SOUND TECHNOLOGY INTERDISCIPLINARY CENTRE (MISTIC)",
            "Publication year": 2005,
            "Publication url": "http://www.music.mcgill.ca/~ich/research/misc/papers/cr1261.pdf",
            "Abstract": "The University of Victoria has a new centre for computer music and digital media called MISTIC (Music Intelligence and Sound Technology Interdisciplinary Centre). Research and educational activities pertaining to MISTIC are described.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:u9iWguZQMMsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Human and machine annotation in the Orchive, a large scale bioacoustic archive",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7032299/",
            "Abstract": "Advances in computer technology have enabled the collection, digitization, and automated processing of huge archives of bioacoustic sound. Many of the tools previously used in bioacoustics research work well with small to medium-sized audio collections, but are challenged when processing large collections ranging from tens of terabytes to petabyte size. The Orchive is a system that assists researchers to listen to, view, annotate and run advanced audio feature extraction and machine learning algorithms on large bioacoustic archives. Annotation is one of the biggest challenges in our work. In this paper, we describe our efforts to utilize experts as well as citizen scientists to participate in the process of annotating recordings. The Orchive contains over 23,000 hours of orca vocalizations collected over the course of 30 years, and represents one of the largest continuous collections of bioacoustic recordings in the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:45AZ0Vt6gvEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Content-based retrieval of music in scalable peer-to-peer networks",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1220916/",
            "Abstract": "A large portion of data exchanged in today's peer-to-peer (P2P) networks consists of music stored as MP3 compressed audio. Existing P2P systems typically are not scalable and only support primitive methods for the searching of music files, e.g., by looking up exact filenames or using simple metadata information such as artist or album name. In this paper, we present the design and evaluation of a scalable P2P system that uses rendezvous points (RPs) for music file registration and query resolution, and supports content-based music information retrieval (MIR) of audio signals.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:aqlVkmm33-oC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Musical instrument classification using individual partials",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5428856/",
            "Abstract": "In a musical signals, the spectral and temporal contents of instruments often overlap. If the number of channels is at least the same as the number of instruments, it is possible to apply statistical tools to highlight the characteristics of each instrument, making their identification possible. However, in the underdetermined case, in which there are fewer channels than sources, the task becomes challenging. One possible way to solve this problem is to seek for regions in the time and/or frequency domains in which the content of a given instrument appears isolated. The strategy presented in this paper explores the spectral disjointness among instruments by identifying isolated partials, from which a number of features are extracted. The information contained in those features, in turn, is used to infer which instrument is more likely to have generated that partial. Hence, the only condition for the method to work is that at least \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Computer-supported analysis of religious chant",
            "Publication year": 2017,
            "Publication url": "https://www.taylorfrancis.com/chapters/edit/10.4324/9781315776989-12/computer-supported-analysis-religious-chant-d%C3%A1niel-p%C3%A9ter-bir%C3%B3-george-tzanetakis",
            "Abstract": "This is an overview of transcription methods via computational means based on research conducted from 2007 to 2015 at the University of Victoria, Utrecht University and the Meertens Institute in Amsterdam, Netherlands. In particular, we have developed new computational models for analysing chant in order to continue the project of folk music transcription, initiated by B\u00e9la Bart\u00f3k (1881\u201345), by incorporating twenty-first century technology. We have applied our established analytical and computational tools to examples of Hungarian laments, Jewish Torah cantillation and Qur\u2019an recitation. In analysing relationships among pitch, melodic gesture and melodic scale with computational tools, we have also created a new paradigm for chant transcription.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:auQHJw8QJBgC",
            "Publisher": "Routledge"
        },
        {
            "Title": "An empirical investigation of stacking for music tag annotation",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6146949/",
            "Abstract": "Automatic tag annotation is one of the most important problems in multimedia information retrieval. It has been motivated by the large amount of unstructured tag annotation data provided by internet users and can be viewed as a variation of multi-label classification with special characteristics and constraints. Stacking is a technique in which the outputs (binary or probabilistic) of a set of binary classifiers (one for each tag) are used as input to a second stage of classification that attempts to exploit latent relationships between tags. This technique (known under a variety of names) has been used in a variety of multimedia tag annotation systems. In this paper we survey these approaches, clarify how stacking system are structured, and empirically investigate stacking using a variety of classifier combinations in the context of tagging pieces of music.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:VL0QpB8kHFEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A framework for sonification of vicon motion capture data",
            "Publication year": 2005,
            "Publication url": "https://www.mistic.ece.uvic.ca/publications/2005_dafx_vicon.pdf",
            "Abstract": "This paper describes experiments on sonifying data obtained using the VICON motion capture system. The main goal is to build the necessary infrastructure in order to be able to map motion parameters of the human body to sound. For sonification the following three software frameworks were used: Marsyas, traditionally used for music information retrieval with audio analysis and synthesis, CHUCK, an on-the-fly real-time synthesis language, and Synthesis Toolkit (STK), a toolkit for sound synthesis that includes many physical models of instruments and sounds. An interesting possibility is the use of motion capture data to control parameters of digital audio effects. In order to experiment with the system, different types of motion data were collected. These include traditional performance on musical instruments, acting out emotions as well as data from individuals having impairments in sensor motor coordination. Rhythmic motion (ie walking) although complex, can be highly periodic and maps quite naturally to sound. We hope that this work will eventually assist patients in identifying and correcting problems related to motor coordination through sound.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:hFOr9nPyWt4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Stability and variation in cadence formulas in oral and semi-oral chant traditions\u2013a computational approach",
            "Publication year": 2012,
            "Publication url": "https://www.lodebar.nl/pvk/wp-content/uploads/dpbiro_poster_icmpc2012.pdf",
            "Abstract": "Stability and Variation in Cadence Formulas in Oral and Semi-\u2010Oral Chant Tradit Page 1 Stability \nand Variation in Cadence Formulas in Oral and Semi-\u2010Oral Chant Traditions \u2014 a Computational \nApproach D\u00e1niel P. Bir\u00f31, Peter van Kranenburg2, Steven R. Ness1, George Tzanetakis1, \nAnja Volk3 1University of Victoria. 2Meertens Institute, Amsterdam. 3Utrecht University, ICS 0 \n5 10 15 20 25 30 35 40 45 4000 4500 5000 5500 6000 Density Pitch 40.31 48.38 50.43 \n52.60 55.65 57.67 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 pitch (MIDI encoding) \nscale degrees Maria Ulfa Taty Abbas Figure 4. Scale degrees derived from Indonesian \n(solid) and Dutch early his outlines pitch no We a folksong odic sta in song dences o presentl \nAs with scales to way, we Torah Trope Pitch-\u2010Histograms of Genesis 1\u20134 (left) and Genesis 5 \n(right) as read in The Netherlands (by Amir Naamani in 2011), showing a high degree of -\u2010. (\u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:yB1At4FlUx8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automatic Melody Composition Inspired by Short Melodies Using a Probabilistic Model and Harmonic Rules",
            "Publication year": 2019,
            "Publication url": "https://riuma.uma.es/xmlui/handle/10630/18754",
            "Abstract": "This demo shows how automatic melody composition of melodies that follow the style of a certain singleshort melodic excerpt can be achieved in such a way that the sample excerpt can be considered an inspirationalpiece of music for the automatic compositor.Music composition has been faced from diverse perspectives and using different approaches, among them theutilization of probabilistic schemes is common since early stages. For example, Brooks et al. [1] considerthe direct utilization of m-order Markov chains for music analysis and generation, and other authors havefollowed a relatively similar path [2]. Often large data sets are necessary to train music generation systems,though in some rare cases algorithms are developed to perform automatic melody composition based on asingle sample melody [5]; in such case, the application of explicit rules is of great importance.Considering the probabilistic approach to music models and music rules on the basis of the melody generationmodel developed in [4], a melody generation scheme is designed that is capable of generating music excerptswith the style and resemblance of short individual MIDI melody samples.Since only one sample melody of arbitrarily small duration will be considered, some modification must bedone to the system described in [4].  Specifically, modifications are necessary in the extraction of musicalparameters and in the generation scheme in order to provide with the necessary musical variability for theadaptation of extracted musical parameters and patterns. .",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:YK4ucWkmU_UC",
            "Publisher": "International Society for Music Information Retrieval (ISMIR)"
        },
        {
            "Title": "Detecting hand posture in piano playing using depth data",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8952960/",
            "Abstract": "We present research for automatic assessment of pianist hand posture that is intended to help beginning piano students improve their piano-playing technique during practice sessions. To automatically assess a student's hand posture, we propose a system that is able to recognize three categories of postures from a single depth map containing a pianist's hands during performance. This is achieved through a computer vision pipeline that uses machine learning on the depth maps for both hand segmentation and detection of hand posture. First, we segment the left and right hands from the scene captured in the depth map using per-pixel classification. To train the hand-segmentation models, we experiment with two feature descriptors, depth image features and depth context features, that describe the context of individual pixels' neighborhoods. After the hands have been segmented from the depth map, a posture \u2026",
            "Abstract entirety": 0,
            "Author pub id": "yPgxxpwAAAAJ:fveVehIkgekC",
            "Publisher": "MIT Press"
        },
        {
            "Title": "Frequency ratio fingerprint characterization for audio matching",
            "Publication year": 2014,
            "Publication url": "https://patents.google.com/patent/US8886543B1/en",
            "Abstract": "System and methods for characterizing interest points within a fingerprint are disclosed herein. The systems include generating a set of interest points and an anchor point related to an audio sample. A quantized absolute frequency of an anchor point can be calculated and used to calculate a set of quantized ratios. A fingerprint can then be generated based upon the set of quantized ratios and used in comparison to reference fingerprints to identify the audio sample. The disclosed systems and methods provide for an audio matching system robust to pitch-shift distortion by using quantized ratios within fingerprints rather than solely using absolute frequencies of interest points. Thus, the disclosed system and methods result in more accurate audio identification.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:DxlTmyU89zoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "SPmat: A Framework and Data Representation for Binary Image Processing",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8547142/",
            "Abstract": "We propose an optimized framework for binary image processing, characterized by a highly bit-packed representation of pixels and their square neighbourhood. The Super-Packed (SPmat) representation for binary images enables the easy use of bit-wise computations for developing fast processing algorithms, such as: morphology, contours, run-length, and thinning, in a unified framework. With several experiments, we show that the aforementioned algorithms can be consistently sped-up, and outperform by a large margin available software implementations. The software package is freely available on github at the url https://github.com/fpeder/spmat to support reproducibility.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:w9HJn4dzO2wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Interoperability and the marsyas 0.2 runtime",
            "Publication year": 2008,
            "Publication url": "https://hal.archives-ouvertes.fr/hal-01697498/",
            "Abstract": "Marsyas is a software framework for building efficient complex audio processing systems and applications. Although originally designed for Music Information Retrieval (MIR) tasks in the past few years it has been expanded to include any type of audio analysis or synthesis. Complex Audio processing systems are defined hierarchically through composition using implicit patching. Both the specification of the processing network and the control of it while data is flowing through can be performed at runtime without requiring recompilation. Compilation is required only when new processing objects need to be defined. Therefore the Marsyas runtime provides considerable functionality and flexibility. In this paper we demonstrate how the Marsyas runtime can be accessed using a variety of different ways allowing non-trivial interactions with common software frameworks and environments.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Musical Acoustics",
            "Publication year": 2008,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=YaNCAAAAQBAJ&oi=fnd&pg=PA275&dq=info:1TW1BkpIMh4J:scholar.google.com&ots=LkG_66U9wY&sig=tIdZW1SKh2PQ0lMM6eMCiJidhT0",
            "Abstract": "Written musical notation describes music in a symbolic form that is suitable for performing a piece using the available musical instruments. Traditionally, musical notation indicates the pitch, target instrument, timing, and duration of each sound to be played. The aim of music transcription either by humans or by a machine is to infer these musical parameters, given only the acoustic recording of a performance. In terms of data representations, this can be seen as transforming an audio signal into a MIDI1 file. Signals of particular interest here are polyphonic music signals where several sounds are playing simultaneously. Automatic recovery of the musical notation of an audio signal allows modifying, rearranging, and processing music at a high abstraction level and then resynthesizing it again. Structured audio coding is another important application: For example, a MIDI-like representation is extremely compact yet retains the characteristics of a piece of music to an important degree. Other uses of music transcription comprise information retrieval,",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:vM5yiaU9oLoC",
            "Publisher": "Springer Science & Business Media"
        },
        {
            "Title": "Multimedia structuring using trees.",
            "Publication year": 2000,
            "Publication url": "https://www.academia.edu/download/30705118/riao2000.pdf",
            "Abstract": "Traditionally work on multimedia structuring has been centered on the creation of indices and their use for searching. Although searching is important there are many cases where the user just wants to browse through the data to find something interesting without having any particular search goal. Multimedia data exhibits hierarchical structure that can be exploited for more natural user interaction with the content.In order to handle the large amounts of multimedia data more structure than what is currently available is required. In this paper, we have focused on structuring multimedia data using trees to describe both temporal and categorical relations. The pervasive use of trees to express hierarchies facilitates browsing, profiling, and authoring. Our main target application is the implementation of a personalized TV-guide. The constraints imposed by this application caused the development of a new simple compact graphical user interface for tree browsing.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:J_g5lzvAfSwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Pragmatic drum motion capture system.",
            "Publication year": 2015,
            "Publication url": "https://www.nime.org/proceedings/2015/nime2015_173.pdf",
            "Abstract": "The ability to acquire and analyze a percussion performance in an efficient, affordable, and non-invasive manner has been made possible by a unique composite of off-the-shelf products. Through various methods of calibration and analysis, human motion as imparted on a striking implement can be tracked and correlated with traditional audio data in order to compare performances. Ultimately, conclusions can be drawn that drive pedagogical studies as well as advances in musical robots.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:renmPk63pRgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A comparison of solenoid-based strategies for robotic drumming",
            "Publication year": 2007,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.5321&rep=rep1&type=pdf",
            "Abstract": "Solenoids are important components of robotic drumming systems and several designs have been proposed. In this paper, we compare different designs in terms of speed and dynamic range and discuss the tradeoffs involved. The evaluation is performed in the context of MahaDeviBot, a custom built 12-armed MIDI controlled mechanical device that performs a variety of Indian folk instruments, including frame-drums, bells, and shakers. To measure speed and dynamic range a haptic robotic feedback system using piezo sensors was built. The evaluation methods presented are modular and can be administered to any hyperinstrument, new interface or robotic system to inform composers about the strengths and limitation of different designs to guide composition and performance.",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ISMIR 2009",
            "Publication year": 2009,
            "Publication url": "https://scholar.google.com/scholar?cluster=15940716385832540425&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "yPgxxpwAAAAJ:fEOibwPWpKIC",
            "Publisher": "Unknown"
        }
    ]
}]