[{
    "name": "\u039d\u03b9\u03ba\u03cc\u03bb\u03b1\u03bf\u03c2 \u03a0\u03b1\u03c0\u03b1\u03bc\u03ac\u03c1\u03ba\u03bf\u03c2",
    "romanize name": "Nikolaos Papamarkos",
    "School-Department": "\u0397\u03bb\u03b5\u03ba\u03c4\u03c1\u03bf\u03bb\u03cc\u03b3\u03c9\u03bd \u039c\u03b7\u03c7 & \u039c\u03b7\u03c7 \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ce\u03bd",
    "University": "duth",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 18596,
    "Scholar name": "Nikos Papamarkos",
    "Scholar id": "E8a90xYAAAAJ",
    "Affiliation": "Democritus University of Thrace Department of Electrical and Computer Engineering Laboratory of",
    "Citedby": 4056,
    "Interests": [
        "Digital image processing",
        "computer vision",
        "document processing",
        "analysis and recognition",
        "pattern recognition"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=E8a90xYAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "A gray-scale inverse Hough transform algorithm",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/857423/",
            "Abstract": "This paper proposes a gray-scale Inverse Hough Transform. The proposed algorithm can be applied to any gray-scale image and allows the edge extraction according to filtering conditions. The algorithm exploits all the gray-scale information and avoids conversion of the gray-scale image to a binary one. The edge pixels are exactly determined according to their position and gray-level as they appear in the original gray-scale image.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:2P1L_qKh6hAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automatic classification of earthquake-induced building damages",
            "Publication year": 2018,
            "Publication url": "https://www.researchgate.net/profile/Eleni-Vrochidou/publication/323429630_Automatic_Classification_of_Earthquake-Induced_Building_Damages/links/5aa676acaca27291845fdde1/Automatic-Classification-of-Earthquake-Induced-Building-Damages.pdf",
            "Abstract": "Summary, CO NTRIBUTIONTOTHE State OF Art v from the introduced strong motion duration of the seismic accelerogram so as to extract the new set of seismic features. The aforementioned analysis is carried out only to those signal components which have a direct impact on the damages of the construction under study. A correlation study is conducted and shows that the extracted seismic intensity parameters are strongly correlated to the structural damage. Therefore, the new seismic intensity parameters constitute an efficient data set, which can be utilized to represent the seismic intensity of signals in an intelligent system for structural damage estimation.The aforementioned research is essentially the background work for creating a model for structural damage assessment. Having artificially generated accelerograms of any desired intensity available, along with an efficient data set of seismic parameters strongly related to structural damage, the research is completed with the implementation of a system suitable for damage estimation. All extracted seismic parameters and damage indices refer to a certain reinforced concrete (RC) frame model utilized throughout the present project. However, the research methodology is structured in such a way as to be general in nature thus, being in a form that renders it ready to be implemented in different engineering design provisions with only minor changes.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:uGrg30pLAbkC",
            "Publisher": "LAP LAMBERT Academic Publishing"
        },
        {
            "Title": "Image retrieval using a fractal signature extraction technique",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1028312/",
            "Abstract": "Content-based image retrieval has become a reliable tool for digital image applications. There are several advantages of the image retrieval techniques compared to other simple retrieval approaches such as text-based retrieval techniques. This paper proposes a new method for color image retrieval, suitable for color image databases. It is based on a fractal scanning technique, which is used to extract signatures for each one of the RGB color components. Features extracted from these signatures are next used for image comparison and retrieval. The system is suitable for retrieving query images even in distortion cases such as deformations, noise, color reduction and smoothing.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:BqipwSGYUEgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fast communication",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=15489350496794040888&hl=en&oi=scholarr",
            "Abstract": "SIGNAL PROCESSING Volume 85 No. 9, September 2005 CONTENTS M. Bouzid, A. Djeradi and B. Boudraa Optimized trellis coded vector quantization of LSF parameters, application to the 4.8 kbps FS1016 speech coder 1675 K. Dog ancay Bearings-only target localization using total least squares 1695 W. Naanaa and J.-M. Nuzillard Blind source separation of positive and partially correlated data 1711 C. Paulus, J. Mars and P. Gounon Wideband spectral matrix filtering for multicomponent sensors array 1723 J. Even and E. Moisan Blind source separation using order statistics 1744 G.-O. Glentis Efficient fast recursive least-squares adaptive complex filtering using real valued arithmetic 1759 J. Sole-Casals, C. Jutten and DT Pham Fast approximation of nonlinearities for improving inversion algorithms of PNL mixtures and Wiener systems 1780 SJ Searle Efficient matched processing for localisation of a moving \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:olpn-zPbct0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Color quantization using principal components for initialization of Kohonen SOFM",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5413720/",
            "Abstract": "A new method is proposed for initializing Kohonen's self-organizing feature maps (SOFM) of fixed zero neighborhood radius for use in color quantization. The method employs the two largest principal components of the input image so that the initial weights of a number of neurons approach the input image color distribution. The rest of the neurons are initialized using the smallest principal component of the input image. Namely, standard SOFM is applied to the projection of the input image pixels onto the plane spanned by the two largest principal components and to pixels of the original image defined by the smallest principal component. The neuron values which emerge initialize the final SOFM of fixed zero neighborhood radius that performs the color quantization of the original image. Experimental results show that the proposed method can often produce smaller quantization errors than standard SOFM and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:RYcK_YlVTxYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Text identification in color documents",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1296366/",
            "Abstract": "In complex color documents, text, drawings and graphics are appeared with millions of different colors. In many cases, text regions are overlaid onto drawings or graphics. In this paper, a new method is proposed to automatically detect and extract text in mixed type color documents. The proposed method is based on a combination of an adaptive color reduction (ACR) technique and a page layout analysis (PLA) approach. The ACR technique is used to obtain the optimal number of colors. Then, image is split to separable binary images, each one corresponding to every principal color. The PLA technique is applied independently to each one of the color plains and identifies the text regions. A merging procedure is applied in the final stage to merge the text regions derived from the color plains and to produce the final document.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:NMxIlDl6LWMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A new technique for hand gesture recognition",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4107115/",
            "Abstract": "A new method for hand gesture recognition is proposed which is based on an innovative self-growing and self-organized neural gas (SGONG) network. Initially, the region of the hand is detected by using a color segmentation technique that depends on a skin-color distribution map. Then, the SGONG network is applied on the segmented hand so as to approach its topology. Based on the output grid of neurons, palm geometric characteristics are obtained which in accordance with powerful finger features allow the identification of the raised fingers. Finally, the hand gesture recognition is accomplished through a probability-based classification method.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:0EnyYjriUFMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Handwritten and machine printed text separation in document images using the bag of visual words paradigm",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6424377/",
            "Abstract": "In a number of types of documents, ranging from forms to archive documents and books with annotations, machine printed and handwritten text may be present in the same document image, giving rise to significant issues within a digitisation and recognition pipeline. It is therefore necessary to separate the two types of text before applying different recognition methodologies to each. In this paper, a new approach is proposed which strives towards identifying and separating handwritten from machine printed text using the Bag of Visual Words paradigm (BoVW). Initially, blocks of interest are detected in the document image. For each block, a descriptor is calculated based on the BoVW. The final characterization of the blocks as Handwritten, Machine Printed or Noise is made by a Support Vector Machine classifier. The promising performance of the proposed approach is shown by using a consistent evaluation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:q3oQSFYPqjQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the gray-scale inverse Hough transform",
            "Publication year": 2000,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0262885699000670",
            "Abstract": "This paper proposes a gray-scale inverse Hough transform (GIHT) algorithm which is combined with a modified gray-scale Hough transform (GHT). Given only the data of the Hough transform (HT) space and the dimensions of the image, the GIHT algorithm reconstructs correctly the original gray-scale image. As a first application, the GIHT is used for line detection and filtering according to conditions associated with the polar parameters, the size and the gray-scale values of the lines. The main advantage of the GIHT is the determination of the image lines exactly as they appear, i.e. pixel by pixel and with the correct gray-scale values. To avoid the quantization effects in the accumulator array of the GHT space, inversion conditions are defined which are associated only with the image size. The GIHT algorithm consists of two phases, which are the collection of gray-scale information stored in the accumulator array and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:UebtZRa9Y70C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Color segmentation of complex document images",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-75274-5_17",
            "Abstract": "In this paper we present a new method for color segmentation of complex document images which can be used as a preprocessing step of a text information extraction application. From the edge map of an image, we choose a representative set of samples of the input color image and built the 3D histogram of the RGB color space. These samples are used to locate a relatively large number of proper points in the 3D color space and use them in order to initially reduce the colors. From this step an oversegmented image is produced which usually has no more than 100 colors. To extract the final result, a mean shift procedure starts from the calculated points and locates the final color clusters of the RGB color distribution. Also, to overcome noise problems, a proposed edge preserving smoothing filter is used to enhance the quality of the image. Experimental results showed the method\u2019s capability of producing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:L8Ckcad2t8MC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Text extraction using component analysis and neuro-fuzzy classification on complex backgrounds",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-21227-7_69",
            "Abstract": "This paper proposes a new technique for text extraction on complex color documents and cover books. The novelty of the proposed technique is that contrary to many existing techniques, it has been designed to deal successfully with documents having complex background, character size variations and different fonts. The number of colors of each document image is reduced automatically into a relative small number (usually below ten colors) and each document is divided into binary images. Then, connected component analysis is performed and homogenous groups of connected components (CCs) are created. A set of features is extracted for each group of CCs. Finally each group is classified into text or non-text classes using a neuro-fuzzy classifier. The proposed technique can be summarized into four consequent stages. In the first stage, a pre-processing algorithm filters noisy CCs. Afterwards, CC \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:1qzjygNMrQYC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Local skew correction in documents",
            "Publication year": 2008,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/S0218001408006417",
            "Abstract": "In this paper we propose a technique for detecting and correcting the skew of text areas in a document. The documents we work with may contain several areas of text with different skew angles. First, a text localization procedure is applied based on connected components analysis. Specifically, the connected components of the document are extracted and filtered according to their size and geometric characteristics. Next, the candidate characters are grouped using a nearest neighbor approach to form words and then based on these words text lines of any skew are constructed. Then, the top-line and baseline for each text line are estimated using linear regression. Text lines in near locations, having similar skew angles, are grown to form text areas. For each text area a local skew angle is estimated and then these text areas are skew corrected independently to horizontal or vertical orientation. The technique has \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:R3hNpaxXUhUC",
            "Publisher": "World Scientific Publishing Company"
        },
        {
            "Title": "Exact Grayscale Image Reconstruction from Projections",
            "Publication year": 2006,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/S0218001406004892",
            "Abstract": "This paper proposes a new method for the exact reconstruction of gray-scale images from projections. The image projections construct an accumulator array, which is used afterwards to reconstruct the original grayscale image by applying the proposed decomposition algorithm. The proposed method determines the number of projections and the number of rays in each projection that are required in order to achieve the reconstruction. These two parameters also define the dimensions of the accumulator array. Using an accumulator array with proper dimensions ensures that there is always a unique characteristic sample for each pixel, which is used during the reconstruction process to extract the pixel's grayscale value. During the reconstruction phase, the sinusoidal contribution of each pixel is removed from the accumulator array. At the end of the decomposition process the accumulator array becomes empty and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:WbkHhVStYXYC",
            "Publisher": "World Scientific Publishing Company"
        },
        {
            "Title": "Automatic summarization and annotation of videos with lack of metadata information",
            "Publication year": 2013,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0957417413001322",
            "Abstract": "The advances in computer and network infrastructure together with the fast evolution of multimedia data has resulted in the growth of attention to the digital video\u2019s development. The scientific community has increased the amount of research into new technologies, with a view to improving the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. The main goal of this paper is the construction of a system that automatically generates and provides all the essential information, both in visual and textual form, of a video. By using the visual or the textual information, a user is facilitated on the one hand to locate a specific video and on the other hand is able to comprehend rapidly the basic points and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:kRWSkSYxWN8C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Document image binarization using local features and Gaussian mixture modeling",
            "Publication year": 2015,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0262885615000360",
            "Abstract": "In this paper, we address the document image binarization problem with a three-stage procedure. First, possible stains and general document background information are removed from the image through a background removal stage. The remaining misclassified background and character pixels are then separated using a Local Co-occurrence Mapping, local contrast and a two-state Gaussian Mixture Model. Finally, some isolated misclassified components are removed by a morphology operator. The proposed scheme offers robust and fast performance, especially for both handwritten and printed documents, which compares favorably with other binarization methods.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:LzOrNEA7mwcC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "On the approximation of real rational functions via mixed-integer linear programming",
            "Publication year": 2000,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0096300399000521",
            "Abstract": "This paper is introducing a new method for the approximation of real rational functions via mixed-integer linear programming. The formulation of the linear approximation problem is based on the minimization of a suitable minimax criterion, in combination with a branch and bound linear integer technique. The proposed algorithm can be used in many rational approximation problems, where some coefficients of the rational function are required to take only integer values. The formulation of the problem ensures always the global solution. The proposed algorithm was extensively tested on a variety of problems. An analytical example is presented to illustrate the use and effectiveness of the algorithm.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:b0M2c_1WBrUC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A multi-segment image coding and transmission scheme",
            "Publication year": 2005,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0165168405001076",
            "Abstract": "In this paper, we present a multi-segment image coding scheme suitable for lossless progressive transmission over noisy or congested networks. The method is a block-wise forward-adaptive context selection technique that employs a low-complexity classification process to separate an image into multiple segments according to local statistics, so that compression efficiency by an entropy coder is improved and progressive transmission becomes possible. At the same time, loss of information in case of error occurrence during transmission results in less obvious quality degradation as the lost blocks are scattered in the final image. The scheme was extensively tested and turned out to give results comparable to those given by state-of-the-art standards, such as L-JPEG, JPEG-LS, CALIC and FELICS, in terms of compression efficiency, while providing with scalability and error concealment mechanisms.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:pqnbT2bcN3wC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "img (Anaktisi): A web content based image retrieval system",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5271935/",
            "Abstract": "img(Anaktisi) is a C#/.NET content base image retrieval application suitable for the Web. It provides efficient retrieval services for various image databases using as a query a sample image, an image sketched by the user and keywords. The image retrieval engine is powered by innovative compact and effective descriptors. Also, an Auto Relevance Feedback (ARF) technique is provided to the user. This technique readjusts the initial retrieval results based on user preferences improving the retrieval score significantly. img(Anaktisi) can be found at http://www.anaktisi.net.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:4DMP91E08xMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A neuro-fuzzy technique for document binarisation",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/article/10.1007/s00521-003-0382-z",
            "Abstract": "This paper proposes a new neuro-fuzzy technique suitable for binarisation or, in general, the colour reduction of digital documents. The proposed approach uses the image colour values and additional local spatial features extracted in the neighbourhood of the pixels. Both image and local features values feed a Kohonen self-organised feature map (SOFM) neural network classifier. After training, the neurons of the output competition layer of the SOFM define a first approach of the final classes. Using the content of these classes, fuzzy membership functions are obtained that are next used by the fuzzy C-means (FCM) algorithm in order to obtain the colours of the final document. The method can be applied to greyscale and colour documents; it is suitable for improving blurring and badly illuminated documents and can be easily modified to accommodate any type of spatial characteristics.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Histogram based color reduction through self-organized neural networks",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-44668-0_66",
            "Abstract": "A new technique suitable for reduction of the number of colors in an image is presented in this paper. It is based on histogram processing and the use of Kohonen Self Organizing Feature Map (SOFM) neural networks. Initially, the dominant colors of each primary image are extracted through a simple linear piece-wise histogram approximation process. Then, using a SOFM the dominant color components of each primary color band are obtained and a look up table is constructed containing all possible color triplets. The final dominant colors are extracted from the look-up table entries using a SOFM by specifying the number of output neurons equal to the number of the dominant colors. Thus, the final image has all the dominant color classes. Experimental and comparative results demonstrate the applicability of the proposed technique.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "A document image retrieval system",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197610000771",
            "Abstract": "In this paper, a system is presented that locates words in document image archives. This technique performs the word matching directly in the document images bypassing character recognition and using word images as queries. First, it makes use of document image processing techniques, in order to extract powerful features for the description of the word images. The features used for the comparison are capable of capturing the general shape of the query, and escape details due to noise or different fonts. In order to demonstrate the effectiveness of our system, we used a collection of noisy documents and we compared our results with those of a commercial optical character recognition (OCR) package.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:hFOr9nPyWt4C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "A new adaptive color quantization technique",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-44668-0_139",
            "Abstract": "This paper proposes a new algorithm for color quantization. The proposed approach achieves color quantization using an adaptive tree clustering procedure. In each node of the tree a self-organized Neural Network Classifier (NNC) is used which is fed by image color values and additional local spatial features. The NNC consists of a Principal Component Analyzer (PCA) and a Kohonen self-organized feature map (SOFM) neural network. The output neurons of the NNC define the color classes for each node. The final image not only has the dominant image colors, but also its texture approaches the image local characteristics used. For better classification, split and merging conditions are used in order to define if color classes must be split or merged. To speed-up the entire algorithm and reduce memory requirements, a fractal scanning subsampling technique is used.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:ldfaerwXgEUC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Developing document image retrieval system",
            "Publication year": 2008,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.488.7249&rep=rep1&type=pdf",
            "Abstract": "A system was developed able to retrieve specific documents from a document collection. In this system the query is given in text by the user and then transformed into image. Appropriate features were in order to capture the general shape of the query, and ignore details due to noise or different fonts. In order to demonstrate the effectiveness of our system, we used a collection of noisy documents and we compared our results with those of a commercial OCR package.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:uWQEDVKXjbEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Color image retrieval using a fractal signature extraction technique",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197602000283",
            "Abstract": "Content-based image retrieval systems have become a reliable tool for many image database applications. There are several advantages of the image retrieval techniques compared to other simple retrieval approaches such as text-based retrieval techniques. This paper proposes a new image retrieval technique that can be used for retrieving color images. The proposed technique is based on a fractal scanning procedure, which extracts 1-D signatures for each one of the image color components. These signatures contain not only color information, but also shape and textural image information. Using Fourier descriptors and discrete transform, powerful features are extracted from the signatures that permit the efficient retrieval of color images. The system is suitable for retrieving query images even in distortion cases such as deformations, noise, color, cosine reduction and smoothing.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:3fE2CSJIrl8C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "A dynamic gesture and posture recognition system",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/article/10.1007/s10846-013-9983-7",
            "Abstract": "This paper presents a real time dynamic hand gesture and posture recognition system based on a neural network and a Hidden Markov Model. For skin color segmentation an adaptive online trained skin color model is used, while the hand posture recognition is accomplished through a likelihood-based classification technique of geometric features. A novel trajectory smoothing technique based on Self Organized Neural Network is introduced to improve HMM classification performance of dynamic gestures. The aim of the proposed system is the creation of a visual dictionary combining hand postures and dynamic gestures. The system has been successfully tested with many people under varying light conditions and different web cameras.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:AXPGKjj_ei8C",
            "Publisher": "Springer Netherlands"
        },
        {
            "Title": "Text extraction using document structure features and support vector machines",
            "Publication year": 2010,
            "Publication url": "https://www.researchgate.net/profile/Nikos-Papamarkos/publication/315529848_Text_Extraction_using_Document_Structure_Features_and_Support_Vector_Machines/links/58d4fee24585153378528a30/Text-Extraction-using-Document-Structure-Features-and-Support-Vector-Machines.pdf",
            "Abstract": "In order to successfully locate and retrieve document images such as technical articles and newspapers, a text localization technique must be employed. The proposed method detects and extracts homogeneous text areas in document images indifferent to font types and size by using connected components analysis to detect blocks of foreground objects. Next, a descriptor that consists of a set of structural features is extracted from the merged blocks and used as input to a trained Support Vector Machines (SVM). Finally, the output of the SVM classifies the block as text or not.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exact reconstruction of gray-scale images from projections",
            "Publication year": 2002,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/9789812776266_0048",
            "Abstract": "This paper proposes a new method for the exact reconstruction of gray-scale images from projections. The image projections construct an accumulator array, which by applying a consequently decomposition technique reconstructs the original gray-scale image. The proposed method defines the number of projections and the number of rays in each projection that are required in order to achieve the reconstruction. These two parameters also define the dimensions of the accumulator array. According to the projection procedure, each pixel of the original image corresponds to a range of sinusoidal curves in the accumulator array. Using an accumulator array with proper dimensions ensures that there is always a unique characteristic sample for each pixel, which is used in the reconstruction process to determine its grayscale value. During the reconstruction process, the votes of the sinusoidal curves are removed from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:u_35RYKgDlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards memristive crossbar-based neuromorphic HW accelerators for signal processing",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7937678/",
            "Abstract": "Research progress in neuromorphic hardware, capable of biological perception and cognitive information processing, is leading the way towards a revolution in computing technology. Current research efforts have focused mainly on resistive switching devices, the electronic analog of synapses in artificial neural networks (ANNs), and the crossbar nanoarchitecture, for its huge connectivity and maximum integration density. In this context, this work presents the design and simulation of a memristive crossbar-based ANN for text recognition tasks, implementing a novel computing algorithm. In such case study, important issues during the application mapping process are identified and properly addressed at device and circuit level. The computing capabilities of the proposed system are highlighted through SPICE-level circuit simulations, which show excellent agreement with theoretical simulation results.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:bFuYayV9R1gC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Text localization using standard deviation analysis of structure elements and support vector machines",
            "Publication year": 2011,
            "Publication url": "http://hephaestus.nup.ac.cy/handle/11728/10170",
            "Abstract": "A text localization technique is required to successfully exploit document images such as technical articles and letters. The proposed method detects and extracts text areas from document images. Initially a connected components analysis technique detects blocks of foreground objects. Then, a descriptor that consists of a set of suitable document structure elements is extracted from the blocks. This is achieved by incorporating an algorithm called Standard Deviation Analysis of Structure Elements (SDASE) which maximizes the separability between the blocks. Another feature of the SDASE is that its length adapts according to the requirements of the application. Finally, the descriptor of each block is used as input to a trained support vector machines that classify the block as text or not. The proposed technique is also capable of adjusting to the text structure of the documents. Experimental results on benchmarking databases demonstrate the effectiveness of the proposed method.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:0kLwNjf3oFwC",
            "Publisher": "Springer"
        },
        {
            "Title": "Color quantization based on pca and kohonen sofm",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-03767-2_59",
            "Abstract": "A method for initializing optimally Kohonen\u2019s Self-Organizing Feature Maps (SOFM) of a fixed zero neighborhood radius for use in color quantization is presented. Standard SOFM is applied to the projection of the input image pixels onto the plane spanned by the two largest principal components and to pixels of the original image defined by the smallest principal component via a thresholding procedure. The neuron values which emerge initialize the final SOFM of a fixed zero neighborhood radius that performs the color quantization of the original image. Experimental results show that the proposed method is able to produce smaller quantization errors than standard SOFM and other existing color quantization methods.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:4fKUyHm3Qg0C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Grayscale Image Reconstruction from Projections with Linear Noise Response",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=17809153522122854737&hl=en&oi=scholarr",
            "Abstract": "This paper presents concisely two reconstruction algorithms that provide exact image reconstruction from its projections and then analyzes the performance of these algorithms when noisy projection data are present. In both the reconstruction methods the projection samples are stored in a 2-D accumulator array where each column corresponds to the projection data at a certain view angle. However, the second method uses a significantly smaller number of projection samples. The response of the reconstruction methods is examined when Gaussian noise is applied in the projection data stored in the accumulator array. Several cases are examined regarding the original image size, the level of the input noise and the rounding of the grayscale values in the reconstructed image. The experimental results show that the two reconstruction methods have the same noise response and that in both cases the reconstructed image is linearly related to the input noise.2-D accumulator array where each column corresponds to the projection data at a certain view angle. The second reconstruction method, namely the limited projections method, provides exact reconstruction using a small number of projection samples in certain view angles. The maximum number of these samples equals the number of image pixels. This reconstruction procedure reduces significantly the memory and time requirements since a very small set of projection samples is used instead of all the data in the accumulator array. The main advantage of both methods is that when the projection data in the accumulator array are noise free then the original image is exactly reconstructed ie \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:qwOXE0mbtu4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Estimation of proper parameter values for document binarization",
            "Publication year": 2008,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.230.6604&rep=rep1&type=pdf",
            "Abstract": "Most of the existing document-binarization techniques deal with many parameters that require a priori setting of their values. Due to the unknown of the ground-truth images, the evaluation of document binarization techniques is subjective and employs human observers for the estimation of the appropriate parameter values. The selection of the appropriate values for these parameters is crucial and influences to the final binarization. However, there is no predetermined set of parameters that guarantees optimal binarization for all document images. This paper proposes a new technique that allows the estimation of proper parameters values for each one of the document binarization techniques. The proposed approach is based on a statistical performance analysis of a set of binarization results, which are obtained by applying various binarization techniques with different parameter values. The proposed statistical performance analysis can also depicts the best document binarization result obtained by a set of document binarization techniques.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multithresholding of mixed-type documents",
            "Publication year": 2000,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S095219760000004X",
            "Abstract": "Mixed-type documents include text, drawings and graphics regions. It is obvious that a technique that can reduce the number of the gray-levels in accordance with the type of each document region could be important for many document applications, such as storage, transmission and recognition. To solve this problem, this paper proposes a new method, called the document multithresholding technique. The method is based on a page layout analysis (PLA) technique and on a neural-network multilevel threshold-selection approach. The proposed technique is applicable to any mixed-type document and achieves document multithresholding by taking advantage of the types of the document blocks. Thus, in the final document different block types are stored with the appropriate and limited numbers of gray-level values. The proposed method includes two main steps. First, a PLA technique is applied, which classifies \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:eQOLeE2rZwMC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Image retrieval systems based on compact shape descriptor and relevance feedback information",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1047320311000368",
            "Abstract": "One of the most important and most used low-level image feature is the shape employed in a variety of systems such as document image retrieval through word spotting. In this paper an MPEG-like descriptor is proposed that contains conventional contour and region shape features with a wide applicability from any arbitrary shape to document retrieval through word spotting. Its size and storage requirements are kept to minimum without limiting its discriminating ability. In addition to that, a relevance feedback technique based on Support Vector Machines is provided that employs the proposed descriptor with the purpose to measure how well it performs with it. In order to evaluate the proposed descriptor it is compared against different descriptors at the MPEG-7 CE1 Set B database.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:vRqMK49ujn8C",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Text binarization in color documents",
            "Publication year": 2006,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ima.20092",
            "Abstract": "This article presents a new method for the binarization of color document images. Initially, the colors of the document image are reduced to a small number using a new color reduction technique. Specifically, this technique estimates the dominant colors and then assigns the original image colors to them in order that the background and text components to become uniform. Each dominant color defines a color plane in which the connected components (CCs) are extracted. Next, in each color plane a CC filtering procedure is applied which is followed by a grouping procedure. At the end of this stage, blocks of CCs are constructed which are next redefined by obtaining the direction of connection (DOC) property for each CC. Using the DOC property, the blocks of CCs are classified as text or nontext. The identified text blocks are binarized properly using suitable binarization techniques, considering the rest of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:kNdYIx-mwKoC",
            "Publisher": "Wiley Subscription Services, Inc., A Wiley Company"
        },
        {
            "Title": "Automatic evaluation of document binarization results",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11578079_103",
            "Abstract": "Most of the document binarization techniques have many parameters that can initially be specified. Usually, subjective document binarization evaluation, employs human observes for the estimation of the best parameter values of the techniques. Thus, the selection of the best values for these parameters is crucial for the final binarization result. However, there is not any set of parameters that guarantees the best binarization result for all document images. It is important, the estimation of the best values to be adaptive for each one of the processing images. This paper proposes a new method which permits the estimation of the best parameter values for each one of the document binarization techniques and also the estimation of the best document binarization result of all techniques. In this way, document binarization techniques can be compared and evaluated using, for each one of them, the best parameter \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:8k81kl-MbHgC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Object-Panorama using SIFT/SURF descriptors and Tamura texture features",
            "Publication year": 2015,
            "Publication url": "https://scholar.google.com/scholar?cluster=15574503242460621709&hl=en&oi=scholarr",
            "Abstract": "The digital visualization of an object derived from the Cultural Heritage Domain has always been of great importance in order to provide a harmless way of examining and studying various aspects of the artefact. Panoramas provide a way of highlighting some important aspects of the object. Apart from the examination purpose which is enriched by the use of panoramas, they are also important for documentation purposes. [...]",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:DOLguN9Lh8sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Document gray-scale reduction using a neuro-fuzzy technique",
            "Publication year": 2003,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/S0218001403002502",
            "Abstract": "This paper proposes a new neuro-fuzzy technique suitable for binarization and gray-scale reduction of digital documents. The proposed approach uses both the image gray-scales and additional local spatial features. Both, gray-scales and local feature values feed a Kohonen Self-Organized Feature Map (SOFM) neural network classifier. After training, the neurons of the output competition layer of the SOFM define two bilevel classes. Using the content of these classes, fuzzy membership functions are obtained that are next used by the fuzzy C-means (FCM) algorithm in order to reduce the character-blurring problem. The method is suitable for improving blurring and badly illuminated documents and can be easily modified to accommodate any type of spatial characteristics.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:ns9cj8rnVeAC",
            "Publisher": "World Scientific Publishing Company"
        },
        {
            "Title": "A new technique for global and local skew correction in binary documents",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-74607-2_80",
            "Abstract": "A new technique for global and local skew correction in binary documents is proposed. The proposed technique performs a connected component analysis and for each connected component, document\u2019s local skew angle is estimated, based on detecting a sequence of other consecutive connected components, at certain directions, within a specified neighborhood. A histogram of all local skew angles is constructed. If the histogram has one peak then global skew correction is performed, otherwise the document has more than one skews. For local skew correction, a page layout analysis is performed based on a boundary growth algorithm at different directions. The exact global or local skew is approached with a least squares line fitting procedure. The accuracy of the technique has been tested using many documents of different skew and it is compared with two other similar techniques.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:isC4tDSrTZIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Conversion of color documents to grayscale",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6608937/",
            "Abstract": "In this paper, a novel method to convert color documents to grayscale is proposed. This approach takes as criteria that a suitable form of a grayscale document must have locally uniform background, well separated characters from the background and reduced noise. The main stages of the proposed technique are color reduction to a limited number of dominant colors and transformation of the gray classes obtained 3-D to a more compact form. The resultant grayscale document gives better OCR results and better compression ratio.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:WA5NYHcadZ8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Color Reduction by Using a new Self-Growing and Self-Organized Neural Network.",
            "Publication year": 2005,
            "Publication url": "https://www.researchgate.net/profile/Nikos-Papamarkos/publication/287500257_Color_reduction_by_using_a_new_self-growing_and_self-organized_neural_network/links/5927dc34a6fdcc444350b765/Color-reduction-by-using-a-new-self-growing-and-self-organized-neural-network.pdf",
            "Abstract": "A new method for the reduction of the number of colors in a digital image is proposed. The new method is based on the developed of a new neural network classifier that combines the advantages of the Growing Neural Gas (GNG) and the Kohonen Self-Organized Feature Map (SOFM) neural networks. We call the new neural network: Self-Growing and Self-Organized Neural Gas (SGONG). Its main advantage is that it defines the number of the created neurons and their topology in an automatic way. As a consecutive, isolated color classes, which may correspond to significant image details, can be obtained. The SGONG is fed by the color components and additional spatial features. To speed up the entire algorithm and to reduce memory requirements, a fractal scanning sub-sampling technique is used. The method is applicable to any type of color images and it can accommodate any type of color space.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:SeFeTyx0c_EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On estimation of the number of image principal colors and color reduction through self\u2010organized neural networks",
            "Publication year": 2002,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ima.10019",
            "Abstract": "A new technique suitable for reduction of the number of colors in a color image is presented in this article. It is based on the use of the image Principal Color Components (PCC), which consist of the image color components and additional image components extracted with the use of proper spatial features. The additional spatial features are used to enhance the quality of the final image. First, the principal colors of the image and the principal colors of each PCC are extracted. Three algorithms were developed and tested for this purpose. Using Kohonen self\u2010organizing feature maps (SOFM) as classifiers, the principal color components of each PCC are obtained and a look\u2010up table, containing the principal colors of the PCC, is constructed. The final colors are extracted from the look\u2010up table entries through a SOFM by setting the number of output neurons equal to the number of the principal colors obtained for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:5nxA0vEk-isC",
            "Publisher": "Wiley Subscription Services, Inc., A Wiley Company"
        },
        {
            "Title": "Pattern Recognition Referees 2009",
            "Publication year": 2010,
            "Publication url": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:928895",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:t6usbXjVLHcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Development and evaluation of text localization techniques based on structural texture features and neural classifiers",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5277681/",
            "Abstract": "This paper presents a text localization approach for binarized printed document images. Emphasis is given to the feature extraction and feature selection stages. In the former, several document structure elements and spatial features, likely to convey useful information, are extracted. In the latter, evolutionary multi-objective feature selection is employed to identify combinations of features with simultaneous good performance in terms of text localization sensitivity and specificity. The selected features are applied to a range of classifiers. Performance results over document image sets from known databases are presented, employing the classifiers with or without feature selection. The results suggest that the hybrid techniques, which utilize the classifiers in combination with the customized pre-processing, feature extraction and feature selection stages, exhibit promising performance on a range of document images.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:TFP_iSt0sucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A system for document binarization",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1296408/",
            "Abstract": "This paper presents a system for binarization of digital documents. The system comprises the benefits of a set of other binarization techniques by combining their results. This is necessary for bad illuminated and degraded document where there are many pixels that cannot be easily classified as foreground or background. For this reason, it is necessary to perform the final binarization by exploiting the results of a set of binarization algorithms, especially for the document pixels that have high vagueness. Also, in this paper significant improvements are proposed for two of the methods used, i.e. for the Adaptive Logical Level Technique (ALLT) and the Improvement of Integrated Function Algorithm (IIFA). The entire system is extensively tested with a variety of degraded and bad-illuminated documents.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Offline Signature recognition using multiple Neural Network classification structures",
            "Publication year": 2001,
            "Publication url": "https://scholar.google.com/scholar?cluster=18229598657310824404&hl=en&oi=scholarr",
            "Abstract": "In this work we implement an off-line recognition system, specialized in signature recognition and verification. The system uses three kinds of information extracted from the digital image of the signature: global features, grid information features and texture features. For each of them a special OCONs (One-Class-One-Network) classification structure has been implemented. In order the system to come to a decision, it uses the results from all the three neural network structures, combined with a simple Euclidean norm of all the three kinds of characteristics together. INTRODUCTION",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:tS2w5q8j5-wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using local features in a neural network based gray-level reduction technique",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/903720/",
            "Abstract": "Proposes a method for reduction of the number of gray-levels in an image. The proposed approach achieves gray-level reduction using the image gray-levels and additional local spatial features. Both gray-level and local feature values feed a self-organized neural network classifier. The final image has not only the dominant image gray-levels, but also has texture approaching the image local characteristics used. To speed up the entire multithresholding algorithm and reduce memory requirements, a fractal scanning sub-sampling technique can be used.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:P5F9QuxV20EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimal combination of document binarization techniques using a self-organizing map neural network",
            "Publication year": 2007,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197606000789",
            "Abstract": "This paper proposes an integrated system for the binarization of normal and degraded printed documents for the purpose of visualization and recognition of text characters. In degraded documents, where considerable background noise or variation in contrast and illumination exists, there are many pixels that cannot be easily classified as foreground or background pixels. For this reason, it is necessary to perform document binarization by combining and taking into account the results of a set of binarization techniques, especially for document pixels that have high vagueness. The proposed binarization technique takes advantage of the benefits of a set of selected binarization algorithms by combining their results using a Kohonen self-organizing map neural network. Specifically, in the first stage the best parameter values for each independent binarization technique are estimated. In the second stage and in order to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:MXK_kJrjxJIC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Multi-spectral document image binarization using image fusion and background subtraction techniques",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7026047/",
            "Abstract": "In this paper, the authors exploit a multispectral image representation to perform more accurate document image binarisation compared to previous color representations. In the first stage, image fusion is employed to create a \u201cdocument\u201d and a \u201cbackground\u201d image. In the second stage, the FastICA algorithm is used to perform background subtraction. In the third stage, a spatial kernel K-harmonic means classifier binarizes the FastICA output. The proposed system outperforms previous efforts on document image binarization.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:FtNbRaqWXr4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automatic image annotation and retrieval using the joint composite descriptor",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5600451/",
            "Abstract": "Capable tools are needed in order to successfully search and retrieve a suitable image from large image collections. Many content-based image retrieval systems employ low-level image features such as color, texture and shape in order to locate the image. Although the above approaches are successful, they lack the ability to include human perception in the query for retrieval because the query must be an image. In this paper a new image annotation technique and a keyword-based image retrieval system are presented, which map the low-level features of the Joint Composite Descriptor to the high-level features constituted by a set of keywords. One set consists of colors-keywords and the other set consists of words. Experiments were performed to demonstrate the effectiveness of the proposed technique.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:ZHo1McVdvXMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Locating text in color documents",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/959233/",
            "Abstract": "In complex color documents, text, drawings and graphics occur with millions of different colors. In many cases, text regions are overlaid onto drawings or graphics. A new method is proposed for the automatic detection and extraction of text in mixed type color documents. The proposed method is based on a combination of an adaptive color reduction (ACR) technique and a page layout analysis (PLA) approach. The ACR technique is used to obtain the optimal number of colors. Then, the image is split to separable binary images, each one corresponding to every principal color. The PLA technique is applied independently to each one of the color planes and identifies the text regions. A merging procedure is applied in the final stage to merge the text regions derived from the color planes and to produce the final document.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:M3NEmzRMIkIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Accurate image retrieval based on compact composite descriptors and relevance feedback information",
            "Publication year": 2010,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/S0218001410007890",
            "Abstract": "In this paper a new set of descriptors appropriate for image indexing and retrieval is proposed. The proposed descriptors address the tremendously increased need for efficient content-based image retrieval (CBIR) in many application areas such as the Internet, biomedicine, commerce and education. These applications commonly store image information in large image databases where the image information cannot be accessed or used unless the database is organized to allow efficient storage, browsing and retrieval. To be applicable in the design of large image databases, the proposed descriptors are compact, with the smallest requiring only 23 bytes per image. The proposed descriptors' structure combines color and texture information which are extracted using fuzzy approaches. To evaluate the performance of the proposed descriptors, the objective Average Normalized Modified Retrieval Rank (ANMRR) is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:hqOjcs7Dif8C",
            "Publisher": "World Scientific Publishing Company"
        },
        {
            "Title": "A window-based color quantization technique and its architecture implementation",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1027889/",
            "Abstract": "A new color quantization (CQ) technique and its VLSI implementation is introduced. It is based on splitting an image into windows and uses a Kohonen self organized neural network classifier (SONNC). Initially, the dominant colors of each window are extracted through the SONNC and then are used for the quantization of the colors of the entire image. The image split into windows offers reduction of the memory requirements and feasibility of a suitable VLSI implementation of the most time consuming part of the technique, i.e. the SONNC. Applying a systematic design methodology into the developed CQ algorithm, an efficient system-on-chip based on an ARM (Advanced RISC Machines Ltd) processor, which achieves high speed processing and less energy consumption, is derived.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:g5m5HwL7SMYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A window-based inverse Hough transform",
            "Publication year": 2000,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0031320399001673",
            "Abstract": "In this paper a new Hough transform inversion technique is proposed. It is a window-based inverse Hough transform algorithm, which reconstructs the original image using only the data of the Hough space and the dimensions of the image. In order to minimize memory and computing requirements, the original image is split into windows. Thus, the algorithm can be used to large-size images as a general purpose tool. In this paper, the proposed technique is applied for edge extraction and filtering. The edges are detected not just as continuous straight lines but as they really appear in the original image, i.e. pixel by pixel. Experimental results indicate that the method is robust, accurate and fast.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:M3ejUd6NZC8C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "A New Sharpening Technique for Medical Images using Wavelets and Image Fusion.",
            "Publication year": 2016,
            "Publication url": "https://pdfs.semanticscholar.org/7af4/14d58dc7413b1614f9732169344ffe3c0518.pdf",
            "Abstract": "In this work a new image sharpening technique based on multiscale analysis and wavelet fusion is presented. The proposed technique is suitable for visibility optimization of biomedical images obtained from MRI sensors. The proposed approach combines, with a wavelet based fusion algorithm, the sharpening results accrued from a number of independent image sharpening techniques. Initially, the input image is preprocessed by a denoising filter based on a complex Two Dimensional Dual-Tree Discrete Wavelet Transform. Then, the denoised image is passed through a cluster of five sharpening filters and subsequently, the final image is obtained with the help of a wavelet fusion technique. The main novelty of the proposed technique lies on using only one input image for sharpening and that the fusion is performed on images extracted in different frequency bands. This technique could be used as a preprocessing step in many applications. In this paper we focus on the application of the proposed technique in brain MR images. Specific image sharpening and quality indices are employed for the quantitative assessment of the proposed technique.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:BKYZGPsuSFYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "img (Anaktisi): A Web Content Based Image Retrieval System",
            "Publication year": 2009,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.873.4084&rep=rep1&type=pdf",
            "Abstract": "img (Anaktisi) is a C#/.NET content base image retrieval application suitable for the web. It provides efficient retrieval services for various image databases using as a query a sample image, an image sketched by the user and keywords. The image retrieval engine is powered by innovative compact and effective descriptors. Also, an Auto Relevance Feedback (ARF) technique is provided to the user. This technique readjusts the initial retrieval results based on user preferences improving the retrieval score significantly. img (Anaktisi) can be found at http://www. anaktisi. net.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:Np1obAXpBq8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Distinction between handwritten and machine-printed text based on the bag of visual words model",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0031320313003774",
            "Abstract": "In a variety of documents, ranging from forms to archive documents and books with annotations, machine printed and handwritten text may coexist in the same document image, raising significant issues within the recognition pipeline. It is, therefore, necessary to separate the two types of text so that it becomes feasible to apply different recognition methodologies to each modality. In this paper, a new approach is proposed which strives towards identifying and separating handwritten from machine printed text using the Bag of Visual Words model (BoVW). Initially, blocks of interest are detected in the document image. For each block, a descriptor is calculated based on the BoVW. The final characterization of the blocks as Handwritten, Machine Printed or Noise is made by a decision scheme which relies upon the combination of binary SVM classifiers. The promising performance of the proposed approach is shown by \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:HE397vMXCloC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Colour quantisation technique based on image decomposition and its embedded system implementation",
            "Publication year": 2004,
            "Publication url": "https://digital-library.theiet.org/content/journals/10.1049/ip-vis_20040552",
            "Abstract": "A new colour quantisation (CQ) technique and its corresponding embedded system realisation are introduced. The CQ technique is based on image split into sub-images and the use of Kohonen self-organised neural network classifiers (SONNC). Initially, the dominant colours of each sub-image are extracted through SONNCs and then are used for the quantisation of the colours of the entire image. The proposed CQ technique can use both colour components and spatial features, achieving better approximation of the final image to the spatial characteristics of the original one. In addition, for the estimation of the proper number of dominant image colours, a new algorithm based on the projection of the image colours into the first two principal components is proposed. The image split into sub-images offers reduction of the on-chip memory requirements and is suitable for embedded system (or system-on-chip \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:IWHjjKOFINEC",
            "Publisher": "IET Digital Library"
        },
        {
            "Title": "Exact image reconstruction using a limited number of projection samples",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1296942/",
            "Abstract": "A new method for the exact reconstruction of any grayscale image from a limited number of projection samples is proposed. The original image is projected into certain projection angles using a proper projection angle step and a proper number of ray samples on the projection axis. If these conditions are fulfilled the proposed method ensures that for each image pixel a characteristic sample exists which allows the determination of the pixel's grayscale value during the reconstruction phase. Thus, instead of calculating all the projection samples of the image on several projection angles, in the proposed method only a specific number of samples is required that equals the number of pixels in the original image. During the reconstruction procedure the characteristic samples are examined consequently starting from those corresponding to pixels further from the origin and continuing with the closer image pixels. This \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:3s1wT3WcHBgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Hand gesture recognition via a new self-organized neural network",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11578079_92",
            "Abstract": "A new method for hand gesture recognition is proposed which is based on an innovative Self-Growing and Self-Organized Neural Gas (SGONG) network. Initially, the region of the hand is detected by using a color segmentation technique that depends on a skin-color distribution map. Then, the SGONG network is applied on the segmented hand so as to approach its topology. Based on the output grid of neurons, palm geometric characteristics are obtained which in accordance with powerful finger features allow the identification of the raised fingers. Finally, the hand gesture recognition is accomplished through a probability-based classification method.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:qUcmZB5y_30C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Robust document binarization with OFF center-surround cells",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/article/10.1007/s10044-011-0214-1",
            "Abstract": "This paper presents a new method for degraded-document binarization, inspired by the attributes of the Human Visual System (HVS). It can deal with various types of degradations, such as uneven illumination, shadows, low contrast, smears, and heavy noise densities. The proposed algorithm combines the characteristics of the OFF center-surround cells of the HVS with the classic Otsu binarization technique. Cells of two different scales are combined, increasing the efficiency of the algorithm and reducing the extracted noise in the final output. A new response function, which regulates the output of the cell according to the local contrast and the local lighting conditions is also introduced. The Otsu technique is used to binarize the outputs of the OFF center-surround cells. Quantitative experiments performed on a set of various computer-generated degradations, such as noise, shadow, and low contrast \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:RHpTSmoSYBkC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Skew correction in documents with several differently skewed text areas.",
            "Publication year": 2007,
            "Publication url": "https://www.scitepress.org/Papers/2007/20418/20418.pdf",
            "Abstract": "In this paper we propose a technique for detecting and correcting the skew of text areas in a document. The documents we work with may contain several areas of text with different skew angles. In the first stage, a text localization procedure is applied based on connected components analysis. Specifically, the connected components of the document are extracted and filtered according to their size and geometric characteristics. Next, the candidate characters are grouped using a nearest neighbour approach to form words, in a first step, and then text lines of any skew, in a second step. Using linear regression, two lines are estimated for each text line representing its top and bottom boundaries. The text lines in near locations with similar skew angles are grown to form text areas. These text areas are rotated independently to a horizontal or vertical plane. This technique has been tested and proved efficient and robust on a wide variety of documents including spreadsheets, book and magazine covers and advertisements.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:DrOLxFoABAwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exact image reconstruction from a limited number of projections",
            "Publication year": 2008,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1047320308000266",
            "Abstract": "A new method for the exact reconstruction of any gray-scale image from its projections is proposed. The original image is projected into several view angles and the projection samples are stored in an accumulator array. In order to reconstruct the image, the accumulator array is considered as an accumulation of sinusoidal contributions each one corresponding to a certain pixel of the original image. The proposed method defines conditions for the necessary number of projections and the density of ray samples on the projection axis. These conditions insure that, for each pixel, there is at least one sample in the accumulator array where only this particular pixel contributes. This characteristic projection sample is used during the reconstruction phase to determine the coordinates and the gray-scale value of the corresponding image pixel. A variation of the method is also proposed where the reconstruction is performed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:j3f4tGmQtD8C",
            "Publisher": "Academic Press"
        },
        {
            "Title": "A new technique for solving puzzles",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5299177/",
            "Abstract": "This paper proposes a new technique for solving jigsaw puzzles. The novelty of the proposed technique is that it provides an automatic jigsaw puzzle solution without any initial restriction about the shape of pieces, the number of neighbor pieces, etc. The proposed technique uses both curve- and color-matching similarity features. A recurrent procedure is applied, which compares and merges puzzle pieces in pairs, until the original puzzle image is reformed. Geometrical and color features are extracted on the characteristic points (CPs) of the puzzle pieces. CPs, which can be considered as high curvature points, are detected by a rotationally invariant corner detection algorithm. The features which are associated with color are provided by applying a color reduction technique using the Kohonen self-organized feature map. Finally, a postprocessing stage checks and corrects the relative position between puzzle \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:CHSYGLWDkRkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A new technique for solving a jigsaw puzzle",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4106951/",
            "Abstract": "A new technique for solving jigsaw puzzles is proposed, which takes advantage of both geometrical and color features. It is considered that an image is being divided into a number of pieces (sub-images). The proposed technique is based on extraction of a set of boundary characteristic points and on a Kohonen self-organized feature map (KSOFM) color reduction technique. For each characteristic point a set of color and geometrical features are extracted. The technique compares these sets of features and decides whether two sub-images match or not. When a matching pair has been found, a corrective procedure is applied in order for these sub-images to fit exactly. Next, the proposed technique creates a new sub-image, which consists of the two matched sub-images. The whole matching procedure is being repeated until only one sub-image remains or no more matching sub-images can be found.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:Zph67rFs4hoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An innovative algorithm for solving jigsaw puzzles using geometrical and color features",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11578079_99",
            "Abstract": "The proposed technique deals with jigsaw puzzles and takes advantage of both geometrical and color features. It is considered that an image is being divided into pieces. The shape of these pieces is not predefined, yet the background\u2019s color is. The whole method concerns a recurrent algorithm, which initially, finds the most important corner points around the contour of a piece, afterwards performs color segmentation with a Kohonen\u2019s SOFM based technique and finally uses a comparing routine. This routine is based on the corner points found before. It compares a set of angles, the color of the image around the region of the corner points, the color of the contour and finally compares sequences of points by calculating the Euclidean distance of luminance between them. At a final stage the method decides which pieces match. If the result is not satisfying, the algorithm is being repeated with new adaptive \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:bEWYMUwI8FkC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "A window-based color quantization technique and its embedded implementation",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1039963/",
            "Abstract": "A new color quantization (CQ) technique and its VLSI implementation is introduced. It is based on image split into windows and uses Kohonen self organized neural network classifier (SONNC). Initially, the dominant colors of each window are extracted through the SONNC and then are used for the quantization of the colors of the entire image. The image split in windows offers reduction of the memory requirements and feasibility of suitable VLSI implementation of the most time consuming part of the technique. Applying a systematic design methodology into the developed CQ algorithm, an efficient system-on-chip based on the ARM processor, which achieves high speed processing and less energy consumption, is derived.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:7PzlFSSx8tAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Guidance, navigation, and control of an unmanned hovercraft",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6608750/",
            "Abstract": "This paper introduces a simulation and evaluation of guidance, navigation, and control algorithms applied to an autonomous hovercraft. A line-of-sight guidance law is adopted in conjunction with a neural network based adaptive dynamic inversion control scheme for the underactuated hovercraft following a prescribed path. The simulation result demonstrates that the guidance and control scheme can be effective in waypoint following of the underactuated hovercraft, especially, when external disturbances exist. It is also shown that the error signals are bounded using Lyapunov's direct method.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:Mojj43d5GZwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A Vector Quantization-Entropy Coder Image",
            "Publication year": 2001,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=nYKRIwqROWgC&oi=fnd&pg=PA25&dq=info:1NH52t9gyHUJ:scholar.google.com&ots=nvm7Cp_1vU&sig=nk6vkEAFn80wk-U30T5m7P-S_uA",
            "Abstract": "Vector quantization (VQ) has been used extensively in the past for image compression. The quantized image can be further compressed via a standard entropy coder (such as the arithmetic coder). In this paper, we present a simple equivalent to VQ, where unsupervised neural nets (NN) are used to find the appropriate codevectors. Furthermore, by imposing additional constraints to the VQ-NN system, we match the entropy coder characteristics and improve the overall image compression by an additional 10%.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:rO6llkc54NcC",
            "Publisher": "Universitat Jaume I"
        },
        {
            "Title": "Multithresholding of mixed-type documents",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/857424/",
            "Abstract": "Mixed type documents include text, drawings and graphics regions. It is obvious that a technique that can reduce the number of the gray-levels in accordance to the type of each document region could be important for many document applications, such as storage, transmission and recognition. To solve this problem this paper proposes a new method that is called the document multithresholding technique. The method is based on a Page Layout Analysis (PLA) technique and on a neural network multilevel threshold selection approach. In the final document the different block types are stored with the appropriate and limited number of gray-level values. In text and line-drawing blocks, only one threshold is determined whereas in the graphics blocks the optimal number of thresholds is first determined. The performance of the method was extensively tested on a variety of documents.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:_xSYboBqXhAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Hand gesture recognition using a neural network shape fitting technique",
            "Publication year": 2009,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197609000694",
            "Abstract": "A new method for hand gesture recognition that is based on a hand gesture fitting procedure via a new Self-Growing and Self-Organized Neural Gas (SGONG) network is proposed. Initially, the region of the hand is detected by applying a color segmentation technique based on a skin color filtering procedure in the YCbCr color space. Then, the SGONG network is applied on the hand area so as to approach its shape. Based on the output grid of neurons produced by the neural network, palm morphologic characteristics are extracted. These characteristics, in accordance with powerful finger features, allow the identification of the raised fingers. Finally, the hand gesture recognition is accomplished through a likelihood-based classification technique. The proposed system has been extensively tested with success.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:roLk4NBRz8UC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Local co-occurrence and contrast mapping for document image binarization",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6981086/",
            "Abstract": "Document Image Binarization refers to the task of transforming a scanned image of a handwritten or printed document into a bi-level representation containing only characters and background. Here, we address the historic document image binarization problem using a three-stage methodology. Firstly, we remove possible stains and noise from the document image by estimating the document background image. The remaining background and character pixels are separated using a Local Co-occurrence Mapping, local contrast and a two-state Gaussian Mixture Model. In the last stage, possible isolated misclassified blobs are removed by a morphology operator. The proposed scheme offers robust and fast performance, especially for handwritten documents.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:6ScxedgR18sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An Evaluation Technique for Binarization Algorithms.",
            "Publication year": 2008,
            "Publication url": "http://www.jucs.org/jucs_14_18/an_evaluation_technique_for/jucs_14_18_3011_3030_stathis.pdf",
            "Abstract": "Document binarization is an active research area for many years. The choice of the most appropriate binarization algorithm for each case proved to be a very difficult procedure itself. In this paper, we propose a new technique for the validation of document binarization algorithms. Our method is simple in its implementation and can be performed on any binarization algorithm since it doesn\u2019t require anything more than the binarization stage. As a demonstration of the proposed technique, we use the case of degraded historical documents. Then we apply the proposed technique to 30 binarization algorithms. Experimental results and conclusions are presented.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:_FxGoFyzp5QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Segmentation of historical machine-printed documents using adaptive run length smoothing and skeleton segmentation paths",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0262885609002005",
            "Abstract": "In this paper, we strive towards the development of efficient techniques in order to segment document pages resulting from the digitization of historical machine-printed sources. This kind of documents often suffer from low quality and local skew, several degradations due to the old printing matrix quality or ink diffusion, and exhibit complex and dense layout. To face these problems, we introduce the following innovative aspects: (i) use of a novel Adaptive Run Length Smoothing Algorithm (ARLSA) in order to face the problem of complex and dense document layout, (ii) detection of noisy areas and punctuation marks that are usual in historical machine-printed documents, (iii) detection of possible obstacles formed from background areas in order to separate neighboring text columns or text lines, and (iv) use of skeleton segmentation paths in order to isolate possible connected characters. Comparative experiments \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:YOwf2qJgpHMC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A window-based gray-scale inverse Hough transform algorithm and its applications on gray-scale line filtering",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/958074/",
            "Abstract": "This paper proposes a window-based gray-scale inverse Hough transform. The proposed algorithm can be applied to any gray-scale image and allows line extraction according to filtering conditions. The technique reduces the computational time and the memory storage requirements of the inversion procedure considering the image as a set of sub-windows and working only with the necessary gray-scale values. Also, a new improved definition of the representation of a straight line in the accumulator array is used that takes into account the image quantization effects and allows the extraction of lines with variable thickness. The pixels of the resulting lines in the final image appear in their exact position and have gray-scale values as in the original gray-scale image.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:SP6oXDckpogC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Document binarisation using Kohonen SOM",
            "Publication year": 2007,
            "Publication url": "https://digital-library.theiet.org/content/journals/10.1049/iet-ipr_20050311",
            "Abstract": "An integrated system for the binarisation of normal and degraded printed documents for the purpose of visualisation and recognition of text characters is proposed. In degraded documents, where considerable background noise or variation in contrast and illumination exists, there are many pixels that cannot be easily classified as foreground or background pixels. For this reason, it is necessary to perform document binarisation by combining and taking into account the results of a set of binarisation techniques, especially for document pixels that have high vagueness. The proposed binarisation technique takes advantages of the benefits of a set of selected binarisation algorithms by combining their results using a Kohonen self-organising map neural network. In order to improve further the binarisation results, significant improvements are proposed for two of the most powerful document binarisation techniques used \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:maZDTaKrznsC",
            "Publisher": "IET Digital Library"
        },
        {
            "Title": "JPEG2000 over noisy communication channels thorough evaluation and cost analysis",
            "Publication year": 2003,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0923596503000389",
            "Abstract": "In this paper, we examine the behavior of the JPEG2000 coding scheme over noisy or congested communication channels and highlight a cost policy aspect. Two error schemes are considered, involving bit errors (noisy channel) and packet-dropping (congested channel) effects. Two bit error methods are used, consisting of flipping or eliminating the bits, and various packet sizes are put to the test of packet dropping. Extensive performance results are presented accompanied by an overall cost analysis.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:r0BpntZqJG4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Applying conformal geometry for creating a 3d model spatial-consistent texture map",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7574914/",
            "Abstract": "The aim of this research is to achieve spatial consistency of the UV map. We present an approach to produce a fully spatially consistent UV mapping based on the planar parameterisation of the mesh. We apply our method on a 3D digital replica of an ancient Greek Lekythos vessel. We parameterise the mesh of a 3D model onto a unit square 2D plane using computational conformal geometry techniques. The proposed method is genus independent, due to an iterative 3D mesh cutting procedure. Having now the texture of a 3D model depicted on a spatially continuous two dimensional structure enables us to efficiently apply a vast range of image processing based techniques and algorithms.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:Cy13deThEpcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An adaptive technique for global and local skew correction in color documents",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0957417410002228",
            "Abstract": "In this paper, a new technique is proposed for global and local skew detection in complex color documents. The proposed technique, which can be applied also to grayscale and binary documents, consists of four main stages; color reduction, text localization, document binarization and skew correction. Color reduction limits the initial number of colors to a small number, usually smaller than 10 colors. Thus, the original documents are decomposed in homogenous regions. Text localization initially divides the document into a number of binary planes (color planes) equal to the number of the reduced colors. Then, connected component analysis is performed and text is extracted according to similarity features between adjacent connected components. In the third stage the binary document is composed by the processed binary planes. Finally, skew correction is achieved by detecting the direction of connection of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:KxtntwgDAa4C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Real time hand detection in a complex background",
            "Publication year": 2014,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197614001286",
            "Abstract": "Hand gesture recognition has gained the interest of many researchers in recent years, as it has become one of the most popular Human Computer Interfaces. The first step in most vision-based gesture recognition systems is the hand region detection and segmentation. This segmentation can be a particularly challenging task when it comes to complex backgrounds and varying illumination. In such environments, most hand detection techniques fail to obtain the exact region of the hand shape, especially in cases of dynamic gestures. Meeting these requirements becomes even more difficult, due to real-time operation demand. To overcome these problems, in this paper, we propose a new method for real-time hand detection in a complex background. We employ a combination of existing techniques, based on motion detection and introduce a novel skin color classifier to improve segmentation accuracy. Motion \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:g9YHGIZn7mcC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Fast implementation of morphological operations using binary image block decomposition",
            "Publication year": 2004,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/S0219467804001361",
            "Abstract": "Morphological transformations are commonly used to perform a variety of image processing tasks. However, morphological operations are time-consuming procedures since they involve ordering and min/max computation of numbers resulting from image interaction with structuring elements. This paper presents a new method that can be used to speed up basic morphological operations for binary images. To achieve this, the binary images are first decomposed in a set of non-overlapping rectangular blocks of foreground pixels that have predefined maximum dimensions. Then off-line dilation and erosion of all rectangular blocks are arbitrary obtained and stored into suitable look-up array tables. By using the look up tables, the results of the morphological operations to the rectangular blocks are directly obtained. Thus, first all image blocks are replaced by their look-up array tables. Then the morphological \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:YFjsv_pBGBYC",
            "Publisher": "World Scientific Publishing Company"
        },
        {
            "Title": "Colour image skeletonisation",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7075216/",
            "Abstract": "In this paper a new morphological technique suitable for colour image skeleton extraction is presented. Vector morphological operations are defined by means of a new ordering of vectors of the HSV colour space, which is a combination of conditional and partial sub-ordering. Then, these are used to extract skeletons of colour images in terms of erosions and openings. The proposed method was tested with a variety of images and such experimental results are provided. Its applications include image compression and recognition problems.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:5ugPr518TE4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Estimation of appropriate parameter values for document binarization techniques",
            "Publication year": 2009,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.8833&rep=rep1&type=pdf",
            "Abstract": "Most of the document binarization techniques have many parameters that must initially be specified. Usually, document binarization evaluation is subjective and employs human observers for the estimation of the proper set of parameters, for each one of the binarization techniques. The selection of the proper values for these parameters is crucial for the final binarization result. However, there is no set of parameters that guarantees the best binarization result for all document images. Thus, the proper parameters must be adapted to each one of the document images. This paper proposes a new technique which permits the estimation of the proper parameters values for each one of the document binarization techniques. The proposed approach is based on a statistical performance analysis of a set of binarization results obtained by the application of a binarization technique, using different parameters values. Using the statistical performance analysis, the best document binarization result of a set of document binarization techniques can also be estimated. From several experimental results, we verify that the proposed evaluation technique is successfully applied to different types of document images. In addition, psycho-visual experiments show that the selection of the best binarization technique, obtained by the proposed approach, agrees in most of the cases, with the human perception ability.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:B3FOqHPlNUQC",
            "Publisher": "ACTA Press"
        },
        {
            "Title": "Web document image retrieval system based on word spotting",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4106570/",
            "Abstract": "Nowadays, the huge non-indexing quantities of image archives (especially document images) require the development of intelligent tools for their retrieval with convenience comparable of the texts search engines. The proposed technique addresses the document retrieval problem by a word matching procedure. It performs matching directly in the images bypassing OCR and using word-images as queries. It is constituted of two different parts: The offline and the online operation. In the offline operation, the archive of document images is examined and the results are stored in a database. The online operation consists of the Web interface, the creation of the word's image and finally, the matching stage. The proposed matching process it can be described shortly as a two-threshold rating system. Finally, the proposed system has been build and it can be found in at the Web address: http://orpheus.ee.duth.gr/irs2.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:4JMBOYKVnBMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Video summarization using a self-growing and self-organized neural gas network",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-24136-9_19",
            "Abstract": "In this paper, a novel method to generate video summaries is proposed, which is allocated mainly for being applied to on-line videos. The novelty of this approach lies in the fact that the video summarization problem is considered as a single query image retrieval problem. According to the proposed method, each frame is considered as a separate image and is described by the recently proposed Compact Composite Descriptors(CCDs) and a visual word histogram. In order to classify the frames into clusters, the method utilizes a powerful Self-Growing and Self-Organized Neural Gas (SGONG) network. Its main advantage is that it adjusts the number of created neurons and their topology in an automatic way. Thus, after training, the SGONG give us the appropriate number of output classes and their centers. The extraction of a representative key frame from every cluster leads to the generation of the video \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "A new signature verification technique based on a two-stage neural network classifier",
            "Publication year": 2001,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197600000646",
            "Abstract": "This paper presents a new technique for off-line signature recognition and verification. The proposed system is based on global, grid and texture features. For each one of these feature sets a special two stage Perceptron OCON (one-class-one-network) classification structure has been implemented. In the first stage, the classifier combines the decision results of the neural networks and the Euclidean distance obtained using the three feature sets. The results of the first-stage classifier feed a second-stage radial base function (RBF) neural network structure, which makes the final decision. The entire system was extensively tested and yielded high recognition and verification rates.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:u5HHmVD_uO8C",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Image dominant colors estimation and color reduction via a new self-growing and self-organized neural gas",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11578079_100",
            "Abstract": "A new method for the reduction of the number of colors in a digital image is proposed. The new method is based on the development of a new neural network classifier that combines the advantages of the Growing Neural Gas (GNG) and the Kohonen Self-Organized Feature Map (SOFM) neural networks. We call the new neural network: Self-Growing and Self-Organized Neural Gas (SGONG). Its main advantage is that it defines the number of the created neurons and their topology in an automatic way. Besides, a new method is proposed for the Estimation of the Most Important of already created Classes (EMIC). The combination of SGONG and EMIC in color images results in retaining the isolated and significant colors with the minimum number of color classes. The above techniques are able to be fed by both color and spatial features. For this reason a similarity function is used for vector comparison. To \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:hC7cP41nSMkC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Automatic edge detection by combining kohonen som and the canny operator",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11578079_98",
            "Abstract": "In this paper a new method for edge detection in grayscale images is presented. It is based on the use of the Kohonen self-organizing map (SOM) neural network combined with the methodology of Canny edge detector. Gradient information obtained from different masks and at different smoothing scales is classified in three classes (Edge, Non Edge and Fuzzy Edge) using an hierarchical Kohonen network. Using the three classes obtained, the final stage of hysterisis thresholding is performed in a fully automatic way. The proposed technique is extensively tested with success.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:a0OBvERweLwC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Color reduction for complex document images",
            "Publication year": 2009,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ima.20174",
            "Abstract": "A new technique for color reduction of complex document images is presented in this article. It reduces significantly the number of colors of the document image (less than 15 colors in most of the cases) so as to have solid characters and uniform local backgrounds. Therefore, this technique can be used as a preprocessing step by text information extraction applications. Specifically, using the edge map of the document image, a representative set of samples is chosen that constructs a 3D color histogram. Based on these samples in the 3D color space, a relatively large number of colors (usually no more than 100 colors) are obtained by using a simple clustering procedure. The final colors are obtained by applying a mean\u2010shift based procedure. Also, an edge preserving smoothing filter is used as a preprocessing stage that enhances significantly the quality of the initial image. Experimental results prove the method \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "Wiley Subscription Services, Inc., A Wiley Company"
        },
        {
            "Title": "Adaptive color reduction",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/979959/",
            "Abstract": "The paper proposes an algorithm for reducing the number of colors in an image. The proposed adaptive color reduction (ACR) technique achieves color reduction using a tree clustering procedure. In each node of the tree, a self-organized neural network classifier (NNC) is used which is fed by image color values and additional local spatial features. The NNC consists of a principal component analyzer (PCA) and a Kohonen self-organized feature map (SOFM) neural network (NN). The output neurons of the NNC define the color classes for each node. The final image not only has the dominant image colors, but its texture also approaches the image local characteristics used. Using the adaptive procedure and different local features for each level of the tree, the initial color classes can be split even more. For better classification, split and merging conditions are used in order to define whether color classes must be \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:u-x6o8ySG0sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Color Reduction using the Combination on the Kohonen Self-Organized Feature Map and the Gustafson Kessel Fuzzy Algorithm",
            "Publication year": 2008,
            "Publication url": "https://www.academia.edu/download/44132939/mldm_1_1_31-46.pdf",
            "Abstract": "The color reduction in digital images is an active research area in digital image processing. In many applications such as image segmentation, analysis, compression and transmission, it is preferable to have images with a limited number of colors. In this paper, a color clustering technique which is a combination of a Kohonen Self Organized Featured Map (KSOFM) and a fuzzy clustering algorithm is proposed. Initially, we reduce the number of image\u2019s colors by using a KSOFM. Then, using the KSOFM color clustering results as starting values, we obtain the final colors by a Gustafson-Kessel Fuzzy Classifier (GKFC). Doing this, we lead to better color classification results because the final color classes obtained are not spherical.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:738O_yMBCRsC",
            "Publisher": "ibai Publishing"
        },
        {
            "Title": "Color reduction and estimation of the number of dominant colors by using a self-growing and self-organized neural gas",
            "Publication year": 2006,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197606000972",
            "Abstract": "A new method for color reduction in a digital image is proposed, which is based on the development of a new neural network classifier and on a new method for Estimation of the Most Important Classes (EMIC). The proposed neural network combines the features of the well-known Growing Neural Gas (GNG) and the Kohonen Self-Organized Feature Map (KSOFM) neural networks. We call the new neural network Self-Growing and Self-Organized Neural Gas (SGONG). This combination produces a new neural network with outstanding features. The proposed technique utilizes the GNG mechanism of growing the neural lattice and the KSOFM leaning adaptation mechanism. Besides, introducing a number of criteria that have an effect on inserting or removing neurons, it is able to automatically define the number of the created neurons and their topology. Moreover, applying the EMIC method, the produced classes \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:WF5omc3nYNoC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Index to Volume 18 (2000)",
            "Publication year": 2000,
            "Publication url": "https://scholar.google.com/scholar?cluster=581135169654286408&hl=en&oi=scholarr",
            "Abstract": "D. Craievich, B. Barnett, AC Bovik 21 Cephalogram analysis applying template matching and fuzzy logic S. Sanei, P. Sanaei, M. Zahabsaniei 39 Automated surface inspection for directional textures D.-M. Tsai, C.-Y. Hsieh Detecting human faces in color images J. Cai, A. Goshtasby 63",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:n1qY4L4uFdgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "JPEG2000 over noisy communication channels-The cost analysis aspect",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1039032/",
            "Abstract": "We present the behavior of the JPEG2000 coding scheme over noisy or congested communication channels and highlight the cost analysis aspect. Two error schemes are considered including bit errors and packet dropping effects. Two bit error methods are used, consisting of flipping or dropping the bits, and various packet sizes are put to the test of packet dropping. Extensive performance results are presented and the overall cost analysis is emphasized.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:mB3voiENLucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automatic summarization and annotation of videos with lack of metadata information",
            "Publication year": 2013,
            "Publication url": "http://hephaestus.nup.ac.cy/handle/11728/10153",
            "Abstract": "The advances in computer and network infrastructure together with the fast evolution of multimedia data has resulted in the growth of attention to the digital video\u2019s development. The scientific community has increased the amount of research into new technologies, with a view to improving the digital video utilization: its archiving, indexing, accessibility, acquisition, store and even its process and usability. All these parts of the video utilization entail the necessity of the extraction of all important information of a video, especially in cases of lack of metadata information. The main goal of this paper is the construction of a system that automatically generates and provides all the essential information, both in visual and textual form, of a video. By using the visual or the textual information, a user is facilitated on the one hand to locate a specific video and on the other hand is able to comprehend rapidly the basic points and generally, the main concept of a video without the need to watch the whole of it. The visual information of the system emanates from a video summarization method, while the textual one derives from a key-word-based video annotation approach. The video annotation technique is based on the key-frames, that constitute the video abstract and therefore, the first part of the system consists of the new video summarization method.  According to the proposed video abstraction technique, initially, each frame of the video is described by the Compact Composite Descriptors (CCDs) and a visual word histogram. Afterwards, the proposed approach utilizes the Self-Growing and Self-Organized Neural Gas (SGONG) network, with a view to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:l7iSsH4_Im4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Microcalcification oriented content-based mammogram retrieval for breast cancer diagnosis",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6958484/",
            "Abstract": "Microcalcifications (MCs) provide a significant early indication of breast malignancy. This work introduces a supervised scheme for malignancy risk assessment of mammograms containing MCs. The proposed scheme employs shape and textural features as input to a support vector machine (SVM) ensemble, in order to perform content-based image retrieval (CBIR) of mammograms. The retrieval performance of the proposed scheme has been evaluated by taking into account the variation of MCs morphology as defined in BI-RADS. In our experiments, we use a set of 87 mammograms containing MCs, obtained from the widely adopted DDSM database for screening mammography. The experimental results demonstrate that the proposed supervised CBIR scheme addresses effective retrieval of MCs mammograms outperforming relevant unsupervised schemes.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:f13iAvnbnnYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An evaluation survey of binarization algorithms on historical documents",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4761546/",
            "Abstract": "Document binarization is an active research area for many years. There are many difficulties associated with satisfactory binarization of document images and especially in cases of degraded historical documents. In this paper, we try to answer the question ldquohow well an existing binarization algorithm can binarize a degraded document image?rdquo We propose a new technique for the validation of document binarization algorithms. Our method is simple in its implementation and can be performed on any binarization algorithm since it doesnpsilat require anything more than the binarization stage. Then we apply the proposed technique to 30 existing binarization algorithms. Experimental results and conclusions are presented.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:KlAtU1dfN6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Adaptive document binarization",
            "Publication year": 2007,
            "Publication url": "https://www.researchgate.net/profile/Antonios-Gasteratos/publication/221415249_Adaptive_document_binarization_A_human_vision_approach/links/0912f505a1e127129a000000/Adaptive-document-binarization-A-human-vision-approach.pdf",
            "Abstract": "This paper presents a new approach to adaptive document binarization, inspired by the attributes of the Human Visual System (HVS). The proposed algorithm combines the characteristics of the OFF ganglion cells of the HVS with the classic Otsu binarization technique. Ganglion cells with four receptive field sizes tuned to different spatial frequencies are employed, which, adopting a new activation function, are independent of gradual illumination changes, such as shadows. The Otsu technique is then used for thresholding the outputs of the ganglion cells, resulting to the final segmentation of the characters from the background. The proposed method was quantitatively and qualitatively tested against other contemporary adaptive binarization techniques in various shadow levels and noise densities, and it was found to outperform them.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:wbdj-CoPYUoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Applying fast segmentation techniques at a binary image represented by a set of non-overlapping blocks",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/953965/",
            "Abstract": "Run length smoothing algorithm (RLSA) and projection profiles are among the fundamental algorithms in binary image processing, mainly used for segmentation of monochrome images. In this paper, fast RLSA and projection profiles are applied to binary images represented by a set of nonoverlapping rectangular blocks. The representation of binary images using rectangular blocks as primitives has been used with great success for several image processing tasks, such as image compression, Hough transform fast implementation and skeletonization. We show that this representation can be applied with great success for fast RLSA application and fast projection profiles evaluation. The experimental results demonstrate that starting from a block represented binary image we can apply RLSA and evaluate projection profiles in significant less CPU time. The average time gain is recorded at 60% and 88%, respectively.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:TQgYirikUcIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An adaptive layer-based local binarization technique for degraded documents",
            "Publication year": 2010,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/S0218001410007889",
            "Abstract": "This paper presents a new technique for adaptive binarization of degraded document images. The proposed technique focuses on degraded documents with various background patterns and noise. It involves a preprocessing local background estimation stage, which detects for each pixel that is considered as background one, a proper grayscale value. Then, the estimated background is used to produce a new enhanced image having uniform background layers and increased local contrast. That is, the new image is a combination of background and foreground layers. Foreground and background layers are then separated by using a new transformation which exploits efficiently, both grayscale and spatial information. The final binary document is obtained by combining all foreground layers. The proposed binarization technique has been extensively tested on numerous documents and successfully compared with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:HoB7MX3m0LUC",
            "Publisher": "World Scientific Publishing Company"
        },
        {
            "Title": "An intelligent hardware structure for impulse noise suppression",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1296937/",
            "Abstract": "In this paper an intelligent hardware module suitable for the computation of an adaptive median filter (AMF) is presented. The proposed digital hardware structure is pipelined and parallel processing is used to minimize computational time. It is capable of processing gray-scale images of 8-bit resolution with 3/spl times/3 or 5/spl times/5-pixel image neighborhoods as options for the computation of the filter output. However, the system can be easily expanded to accommodate windows of larger sizes. The function of the proposed circuitry is to detect the existence of impulse noise in an image neighborhood and apply the median filter operator only when necessary. Moreover, the noise detection procedure can be customized so that a range of pixel values is considered as impulse noise. In this way, the integrity of edge and detail information of the image under process is preserved and blurring is avoided. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:V3AGJWp-ZtQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multithresholding of color and gray-level images through a neural network technique",
            "Publication year": 2000,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0262885699000153",
            "Abstract": "One of the most frequently used methods in image processing is thresholding. This can be a highly efficient means of aiding the interpretation of images. A new technique suitable for segmenting both gray-level and color images is presented in this paper. The proposed approach is a multithresholding technique implemented by a Principal Component Analyzer (PCA) and a Kohonen Self-Organized Feature Map (SOFM) neural network. To speedup the entire multithresholding algorithm and reduce the memory requirements, a sub-sampling technique can be used. Several experimental and comparative results exhibiting the performance of the proposed technique are presented.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Text extraction in complex color documents",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0031320301001674",
            "Abstract": "Text extraction in mixed-type documents is a pre-processing and necessary stage for many document applications. In mixed-type color documents, text, drawings and graphics appear with millions of different colors. In many cases, text regions are overlaid onto drawings or graphics. In this paper, a new method to automatically detect and extract text in mixed-type color documents is presented. The proposed method is based on a combination of an adaptive color reduction (ACR) technique and a page layout analysis (PLA) approach. The ACR technique is used to obtain the optimal number of colors and to convert the document into the principal of them. Then, using the principal colors, the document image is split into the separable color plains. Thus, binary images are obtained, each one corresponding to a principal color. The PLA technique is applied independently to each of the color plains and identifies the text \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Text localization in color documents",
            "Publication year": 2006,
            "Publication url": "https://www.scitepress.org/Papers/2006/13658/13658.pdf",
            "Abstract": "* Image Processing and Multimedia Laboratory, Department of Electrical & Computer Engineering Democritus University of Thrace, 67100 Xanthi, Greece,# Department of Informatics and Communications, Technological Educational Institution of Serres 62123 Serres, Greece,",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:-f6ydRqryjwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Grayscale image reconstruction from projections with linear noise response",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1579891/",
            "Abstract": "This paper presents concisely two reconstruction algorithms that provide exact image reconstruction from its projections and then analyzes the performance of these algorithms when noisy projection data are present. In both the reconstruction methods the projection samples are stored in a 2-D accumulator array where each column corresponds to the projection data at a certain view angle. However, the second method uses a significantly smaller number of projection samples. The response of the reconstruction methods is examined when Gaussian noise is applied in the projection data stored in the accumulator array. Several cases are examined regarding the original image size, the level of the input noise and the rounding of the grayscale values in the reconstructed image. The experimental results show that the two reconstruction methods have the same noise response and that in both cases the reconstructed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:J-pR_7NvFogC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A technique for fuzzy document binarization",
            "Publication year": 2001,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/502187.502210",
            "Abstract": "This paper proposes a new method for fuzzy binarization of digital document. The proposed approach achieves binarization using both the image gray-levels and additional local spatial features. Both, gray-level and local features values feed a Kohonen Self-Organized Feature Map (SOFM) neural network classifier. After training, the neurons of the output competition layer of the SOFM define two bilevel classes. Using content of these classes, fuzzy membership functions are obtained that are next used with the Fuzzy C-means (FCM) algorithm in order to reduce the character-blurring problem. The method is suitable for binarization of blurring documents and can be easily modified to accommodate any type of spatial characteristics.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:ULOm3_A8WrAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Text localization using standard deviation analysis of structure elements and support vector machines",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/article/10.1186/1687-6180-2011-47",
            "Abstract": "A text localization technique is required to successfully exploit document images such as technical articles and letters. The proposed method detects and extracts text areas from document images. Initially a connected components analysis technique detects blocks of foreground objects. Then, a descriptor that consists of a set of suitable document structure elements is extracted from the blocks. This is achieved by incorporating an algorithm called Standard Deviation Analysis of Structure Elements (SDASE) which maximizes the separability between the blocks. Another feature of the SDASE is that its length adapts according to the requirements of the application. Finally, the descriptor of each block is used as input to a trained support vector machines that classify the block as text or not. The proposed technique is also capable of adjusting to the text structure of the documents. Experimental results on benchmarking \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:xtRiw3GOFMkC",
            "Publisher": "SpringerOpen"
        },
        {
            "Title": "A novel image sharpening technique based on 2D-DWT and image fusion",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6916146/",
            "Abstract": "This paper presents a new image sharpening technique that can be applied to both gray-scale and color images. The proposed approach uses a multi-scale scheme and a wavelet based fusion algorithm. Specifically, the input image is initially processed by a cluster of un-sharp filters with different variance. The final image is then obtained with the aid of wavelet fusion. The application of the unsharp filters with different size Gaussian filters emphasizes important information in different frequency bands. It is shown that the proposed technique can be used as a preprocessing stage to general image fusion approaches. The quality of the resulting images is evaluated using three different sharpening indices. The experimental investigation demonstrates the effectiveness of the proposed technique.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:S_fw-_riRmcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "VeSTIS: a versatile semi-automatic taxon identification system from digital images",
            "Publication year": 2010,
            "Publication url": "https://www.openstarts.units.it/handle/10077/3782",
            "Abstract": "In this work we present a flexible Open Source software platform for training classifiers capable of identifying the taxonomy of a specimen from digital images. We demonstrate the performance of our system in a pilot study, building a feed-forward artificial neural network to effectively classify five different species of marine annelid worms of the class Polychaeta. We also discuss on the extensibility of the system, and its potential uses either as a research tool or in assisting routine taxon identification procedures.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:J_g5lzvAfSwC",
            "Publisher": "EUT Edizioni Universit\u00e0 di Trieste"
        },
        {
            "Title": "Gray-level reduction using local spatial features",
            "Publication year": 2000,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1077314200908385",
            "Abstract": "This paper proposes a new method for reduction of the number of gray-levels in an image. The proposed approach achieves gray-level reduction using both the image gray-levels and additional local spatial features. Both gray-level and local feature values feed a self-organized neural network classifier. After training, the neurons of the output competition layer of the SOFM define the gray-level classes. The final image has not only the dominant image gray-levels, but also has a texture approaching the image local characteristics used. To split the initial classes further, the proposed technique can be used in an adaptive mode. To speed up the entire multithresholding algorithm and reduce memory requirements, a fractal scanning subsampling technique is adopted. The method is applicable to any type of gray-level image and can be easily modified to accommodate any type of spatial characteristic. Several \u2026",
            "Abstract entirety": 0,
            "Author pub id": "E8a90xYAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Color reduction using the combination of the Kohonen self-organized feature map and the Gustafson-Kessel fuzzy algorithm",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-73499-4_53",
            "Abstract": "The color of the digital images is one of the most important components of the image processing research area. In many applications such as image segmentation, analysis, compression and transition, it is preferable to reduce the colors as much as possible. In this paper, a color clustering technique which is the combination of a neural network and a fuzzy algorithm is proposed. Initially, the Kohonen Self Organized Featured Map (KSOFM) is applied to the original image. Then, the KSOFM results are fed to the Gustafson-Kessel (GK) fuzzy clustering algorithm as starting values. Finally, the output classes of GK algorithm define the numbers of colors of which the image will be reduced.",
            "Abstract entirety": 1,
            "Author pub id": "E8a90xYAAAAJ:HDshCWvjkbEC",
            "Publisher": "Springer, Berlin, Heidelberg"
        }
    ]
}]