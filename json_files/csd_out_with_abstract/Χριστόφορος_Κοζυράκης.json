[{
    "name": "\u03a7\u03c1\u03b9\u03c3\u03c4\u03cc\u03c6\u03bf\u03c1\u03bf\u03c2 \u039a\u03bf\u03b6\u03c5\u03c1\u03ac\u03ba\u03b7\u03c2",
    "romanize name": "Christoforos Kozyrakis",
    "School-Department": " Electrical Engineering and Computer Science",
    "University": "Stanford University",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 6966,
    "Scholar name": "Christos Kozyrakis",
    "Scholar id": "G2EJz5kAAAAJ",
    "Affiliation": "Stanford University",
    "Citedby": 25827,
    "Interests": [
        "Computer Architecture",
        "Computer Systems",
        "Cloud Computing"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=G2EJz5kAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Security implications of data mining in cloud scheduling",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7167661/",
            "Abstract": "Cloud providers host an increasing number of popular applications, on the premise of resource flexibility and cost efficiency. Most of these systems expose virtualized resources of different types and sizes. As instances share the same physical host to increase utilization, they contend on hardware resources, e.g., last-level cache, making them vulnerable to side-channel attacks from co-scheduled applications. In this work we show that using data mining techniques can help an adversarial user of the cloud determine the nature and characteristics of co-scheduled applications and negatively impact their performance through targeted contention injections. We design Bolt, a simple runtime that extracts the sensitivity of co-scheduled applications to various types of interference and uses this signal to determine the type of these applications by applying a set of data mining techniques. We validate the accuracy of Bolt on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:0N-VGjzr574C",
            "Publisher": "IEEE"
        },
        {
            "Title": "ShEF: Shielded Enclaves for Cloud FPGAs",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2103.03500",
            "Abstract": "FPGAs are now used in public clouds to accelerate a wide range of applications, including many that operate on sensitive data such as financial and medical records. We present ShEF, a trusted execution environment (TEE) for cloud-based reconfigurable accelerators. ShEF is independent from CPU-based TEEs and allows secure execution under a threat model where the adversary can control all software running on the CPU connected to the FPGA, has physical access to the FPGA, and can compromise the FPGA interface logic of the cloud provider. ShEF provides a secure boot and remote attestation process that relies solely on existing FPGA mechanisms for root of trust. It also includes a Shield component that provides secure access to data while the accelerator is in use. The Shield is highly customizable and extensible, allowing users to craft a bespoke security solution that fits their accelerator's memory access patterns, bandwidth, and security requirements at minimum performance and area overheads. We describe a prototype implementation of ShEF for existing cloud FPGAs and measure the performance benefits of customizable security using five accelerator designs.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:kF1pexMAQbMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Uncovering the Security Implications of Cloud Multi-Tenancy with Bolt",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8358043/",
            "Abstract": "Cloud providers routinely schedule multiple applications per physical host to increase efficiency. The resulting interference on shared resources often leads to performance degradation and, more importantly, security vulnerabilities. Interference can leak important information about an application, ranging from a services placement to confidential data, such as private keys. We present Bolt, a practical system that accurately detects the type and characteristics of applications sharing a cloud platform based on the interference an adversary sees on shared resources. Bolt leverages online data mining techniques that only require 2-5 seconds for detection. In a multi-user study on Amazon Elastic Compute Cloud (EC2), Bolt correctly identifies the characteristics of 385 out of a set of 436 diverse workloads. Extracting this information enables a wide spectrum of previously impractical cloud attacks, including denial of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:wMgC3FpKEyYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Spatial: A language and compiler for application accelerators",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3192366.3192379",
            "Abstract": "Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs for improved performance and energy efficiency. Unfortunately, adoption of these architectures has been limited by their programming models. HDLs lack abstractions for productivity and are difficult to target from higher level languages. HLS tools are more productive, but offer an ad-hoc mix of software and hardware abstractions which make performance optimizations difficult.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:1lhNe0rCu4AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A case for managed and model-less inference serving",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3317550.3321443",
            "Abstract": "The number of applications relying on inference from machine learning models, especially neural networks, is already large and expected to keep growing. For instance, Facebook applications issue tens-of-trillions of inference queries per day with varying performance, accuracy, and cost constraints. Unfortunately, today's inference serving systems are neither easy to use nor cost effective. Developers must manually match the performance, accuracy, and cost constraints of their applications to a large design space that includes decisions such as selecting the right model and model optimizations, selecting the right hardware architecture, selecting the right scale-out factor, and avoiding cold-start effects. These interacting decisions are difficult to make, especially when the application load varies over time, applications evolve over time, and the available resources vary over time.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:-mN3Mh-tlDkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deconstructing hardware architectures for security",
            "Publication year": 2006,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.2165",
            "Abstract": "Researchers have recently proposed novel hardware architectures for enhancing system security. The proposed architectures address security threats such as buffer overflows, format string bugs, and information disclosure. The main advantage of hardware support is increased visibility into system state, low overheads for security checks, and, in some cases, compatibility with legacy binaries. Nevertheless, hardware support is not a panacea for system security. We review two architectures for preventing memory corruption and two for preventing information leaks. We identify significant vulnerabilities and shortcomings in these designs. We also discuss solutions and mitigation strategies. 1.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:SeFeTyx0c_EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Real-World Buffer Overflow Protection for Userspace and Kernelspace.",
            "Publication year": 2008,
            "Publication url": "https://www.usenix.org/events/sec08/tech/full_papers/dalton/dalton_html",
            "Abstract": "Despite having been around for more than 25 years, buffer overflow attacks are still a major security threat for deployed software. Existing techniques for buffer overflow detection provide partial protection at best as they detect limited cases, suffer from many false positives, require source code access, or introduce large performance overheads. Moreover, none of these techniques are easily applicable to the operating system kernel.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:35N4QoGY0k4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Qos-aware admission control in heterogeneous datacenters",
            "Publication year": 2013,
            "Publication url": "https://www.usenix.org/conference/icac13/technical-sessions/presentation/delimitrou",
            "Abstract": "Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. Apart from determining where to schedule workloads, the cluster manager should also decide when to constrain application admission to prevent system oversubscription. At the same time datacenter users care not only for fast execution time but for low waiting time (fast scheduling) as well. Recent work has addressed the first challenge in the presence of unknown workloads, but not the second one.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:nrtMV_XWKgEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "AppSwitch: Resolving the Application Identity Crisis",
            "Publication year": 2017,
            "Publication url": "https://arxiv.org/abs/1711.02294",
            "Abstract": "Networked applications traditionally derive their identity from the identity of the host on which they run. The default application identity acquired from the host results in subtle and substantial problems related to application deployment, discovery and access, especially for modern distributed applications. A number of mechanisms and workarounds, often quite elaborate, are used to address those problems but they only address them indirectly and incompletely. This paper presents AppSwitch, a novel transport layer network element that decouples applications from underlying network at the system call layer and enables them to be identified independently of the network. Without requiring changes to existing applications or infrastructure, it removes the cost and complexity associated with operating distributed applications while offering a number of benefits including an efficient implementation of common network functions such as application firewall and load balancer. Experiments with our implementation show that AppSwitch model also effectively removes the performance penalty associated with unnecessary data path processing that is typical in those application environments.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:5MTHONV0fEkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fast memory snapshot for concurrent programmingwithout synchronization",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1542275.1542297",
            "Abstract": "The industry-wide turn toward chip-multiprocessors (CMPs) provides an increasing amount of parallel resources for commodity systems. However, it is still difficult to harness the available parallelism in user applications and system software code.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:Y0pCki6q_DkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Hardware acceleration of transactional memory on commodity systems",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1961296.1950372",
            "Abstract": "The adoption of transactional memory is hindered by the high overhead of software transactional memory and the intrusive design changes required by previously proposed TM hardware. We propose that hardware to accelerate software transactional memory (STM) can reside outside an unmodified commodity processor core, thereby substantially reducing implementation costs. This paper introduces Transactional Memory Acceleration using Commodity Cores (TMACC), a hardware-accelerated TM system that does not modify the processor, caches, or coherence protocol.We present a complete hardware implementation of TMACC using a rapid prototyping platform. Using this hardware, we implement two unique conflict detection schemes which are accelerated using Bloom filters on an FPGA. These schemes employ novel techniques for tolerating the latency of fine-grained asynchronous communication with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:nb7KW1ujOQ8C",
            "Publisher": "ACM"
        },
        {
            "Title": "A few ways can take you a long way: Efficient and highly associative caches with scalable partitioning for many-core CMPs",
            "Publication year": 2011,
            "Publication url": "https://www.computer.org/csdl/proceedings-article/hcs/2011/07477514/12OmNz4SOCF",
            "Abstract": "This article consists of a collection of slides from the author's conference presentation on a comparative analysis of ZCache versus Vantage, single chip multi-core processors.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:dBIO0h50nwkC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "Asmdb: Understanding and mitigating front-end stalls in warehouse-scale computers",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9069187/",
            "Abstract": "It is well known that the datacenters hosting today's cloud services waste a significant number of cycles on front-end stalls. However, prior work has provided little insights about the source of these front-end stalls and how to address them. This work analyzes the cause of instruction cache misses at a fleet-wide scale and proposes a new compiler-driven software code prefetching strategy to reduce instruction caches misses by 90%.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:mKu_rENv82IC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Scalable, vector processors for embedded systems",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1261385/",
            "Abstract": "For embedded applications with data-level parallelism, a vector processor offers high performance at low power consumption and low design complexity. Unlike superscalar and VLIW designs, a vector processor is scalable and can optimally match specific application requirements.To demonstrate that vector architectures meet the requirements of embedded media processing, we evaluate the Vector IRAM, or VIRAM (pronounced \"V-IRAM\"), architecture developed at UC Berkeley, using benchmarks from the Embedded Microprocessor Benchmark Consortium (EEMBC). Our evaluation covers all three components of the VIRAM architecture: the instruction set, the vectorizing compiler, and the processor microarchitecture. We show that a compiler can vectorize embedded tasks automatically without compromising code density. We also describe a prototype vector processor that outperforms high-end superscalar and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Transactional collection classes",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1229428.1229441",
            "Abstract": "While parallel programmers find it easier to reason about large atomic regions, the conventional mutual exclusion-based primitives for synchronization force them to interleave many small operations to achieve performance. Transactional memory promises that programmer scan use large atomic regions while achieving similar performance. However, these large transactions can conflict when operating on shared data structures, even for logically independent operations. Transactional collection classes address this problem by allowing long-running transactions to operate on shared data while eliminating unnecessary conflicts. Transactional collection classes wrap existing data structures, without the need for custom implementations or knowledge of data structure internals.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:lSLTfruPkqcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Server engineering insights for large-scale online services",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5550995/",
            "Abstract": "The rapid growth of online services in the last decade has led to the development of large data centers to host these workloads. These large-scale online, user-facing services have unique engineering and capacity provisioning design requirements. The authors explore these requirements, focusing on system balancing, the impact of technology trends, and the challenges of online service workloads.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:EUQCXRtRnyEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Energy-efficient and high-performance instruction fetch using a block-aware ISA",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1077603.1077614",
            "Abstract": "The front-end in superscalar processors must deliver high application performance in an energy-effective manner. Impediments such as multi-cycle instruction accesses, instruction-cache misses, and mispredictions reduce performance by 48% and increase energy consumption by 21%. This paper presents a block-aware instruction set architecture (BLISS) that defines basic block descriptors in addition to the actual instructions in a program. BLISS allows for a decoupled front-end that reduces the time and energy spent on misspeculated instructions. It also allows for accurate instruction prefetching and energy efficient instruction access. A BLISS-based front-end leads to 14% IPC, 16% total energy, and 83% energy-delay-squared product improvements for wide-issue processors",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:XiSMed-E-HIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "DRAF: A low-power DRAM-based reconfigurable acceleration fabric",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3007787.3001191",
            "Abstract": "FPGAs are a popular target for application-specific accelerators because they lead to a good balance between flexibility and energy efficiency. However, FPGA lookup tables introduce significant area and power overheads, making it difficult to use FPGA devices in environments with tight cost and power constraints. This is the case for datacenter servers, where a modestly-sized FPGA cannot accommodate the large number of diverse accelerators that datacenter applications need.This paper introduces DRAF, an architecture for bit-level reconfigurable logic that uses DRAM subarrays to implement dense lookup tables. DRAF overlaps DRAM operations like bitline precharge and charge restoration with routing within the reconfigurable routing fabric to minimize the impact of DRAM latency. It also supports multiple configuration contexts that can be used to quickly switch between different accelerators with minimal \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:XvxMoLDsR5gC",
            "Publisher": "ACM"
        },
        {
            "Title": "JouleSort: a balanced energy-efficiency benchmark",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1247480.1247522",
            "Abstract": "The energy efficiency of computer systems is an important concern in a variety of contexts. In data centers, reducing energy use improves operating cost, scalability, reliability, and other factors. For mobile devices, energy consumption directly affects functionality and usability. We propose and motivate JouleSort, an external sort benchmark, for evaluating the energy efficiency of a wide range of computer systems from clusters to handhelds. We list the criteria, challenges, and pitfalls from our experience in creating a fair energy-efficiency benchmark. Using a commercial sort, we demonstrate a JouleSort system that is over 3.5 x as energy-efficient as last year's estimated winner. This system is quite different from those currently used in data centers. It consists of a commodity mobile CPU and 13 laptop drives connected by server-style I/O interfaces.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:bEWYMUwI8FkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scalable and efficient fine-grained cache partitioning with vantage",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6175882/",
            "Abstract": "The Vantage cache-partitioning technique enables configurability and quality-of-service guarantees in large-scale chip multiprocessors with shared caches. Caches can have hundreds of partitions with sizes specified at cache line granularity, while maintaining high associativity and strict isolation among partitions.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:SdhP9T11ey4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Thanks also to our HotChips Volunteers!",
            "Publication year": 2007,
            "Publication url": "https://old.hotchips.org/wp-content/uploads/hc_archives/hc19/HC19.committee.pdf",
            "Abstract": "Thanks also to our HotChips Volunteers! Page 1 Conference Chair John Sell, Microsoft \nConference Vice-Chair Don Draper, Rambus Inc. CTO Yusuf Abdulghani, Apple \nRegistration Ravi Rajamani, Oracle Sujata Ramasubramanian, Intel Publicity Kevin Krewell, \nNVIDIA Gail Sachs, Telairity Local Arrangements - Food Allen Baum, Intel Local \nArrangements - Facilities Lance Hammond, Apple Advertising Don Draper, Rambus Inc. \nFely Krewell, Spansion Webmaster Alexis Cordova Finance Lily Jow, HP Sponsorship Amr \nZaky, Broadcom Publications Randall Neff, Volunteer Coordinator Charlie Neuhauser, \nNeuhauser Associates Co-Chairs Rajeevan Amirtharajah, UC, Davis John Mashey, \nTechviser Committee Forest Baskett, New Enterprise Associates Dileep Bhandarkar, \nMicrosoft Doug Burger, University of Texas - Austin Norm Jouppi, HP Labs Christos \nKozyrakis, Stanford University John Kubiatowicz, UC, Berkeley , -\u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:IUKN3-7HHlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Faa $ T: A Transparent Auto-Scaling Cache for Serverless Applications",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2104.13869",
            "Abstract": "Function-as-a-Service (FaaS) has become an increasingly popular way for users to deploy their applications without the burden of managing the underlying infrastructure. However, existing FaaS platforms rely on remote storage to maintain state, limiting the set of applications that can be run efficiently. Recent caching work for FaaS platforms has tried to address this problem, but has fallen short: it disregards the widely different characteristics of FaaS applications, does not scale the cache based on data access patterns, or requires changes to applications. To address these limitations, we present Faa\\$T, a transparent auto-scaling distributed cache for serverless applications. Each application gets its own Faa\\$T cache. After a function executes and the application becomes inactive, the cache is unloaded from memory with the application. Upon reloading for the next invocation, Faa\\$T pre-warms the cache with objects likely to be accessed. In addition to traditional compute-based scaling, Faa\\$T scales based on working set and object sizes to manage cache space and I/O bandwidth. We motivate our design with a comprehensive study of data access patterns in a large-scale commercial FaaS provider. We implement Faa\\$T for the provider's production FaaS platform. Our experiments show that Faa\\$T can improve performance by up to 92% (57% on average) for challenging applications, and reduce cost for most users compared to state-of-the-art caching systems, i.e. the cost of having to stand up additional serverful resources.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:2l5NCbZemmgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Boosting SMT trace processors performance with data cache misssensitive thread scheduling mechanism",
            "Publication year": 2006,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S014193310500092X",
            "Abstract": "The penalty associated with data cache misses is one of the obstacles to the performance of SMT trace processors. The increased latency is not only required to resolve the missing data, the miss will also have negative impact on the PE resources utilization rate of the SMT trace processors. When data cache miss occurs in SMT trace processors, all the completed traces following the data-miss-trace (a trace with at least one data cache miss) will be delayed to commit for the data cache miss event. PE resources occupied by those traces can not be released until traces are committed, which wastes the PE execution resources and hampers the performance of SMT trace processors.In this paper, we propose several data cache miss sensitive thread scheduling mechanisms with the aim to tolerate the penalties of data cache misses. By choosing the thread wisely in trace dispatch and trace commit stages, the SMT trace \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:cK4Rrx0J3m0C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Phoenix rebirth: Scalable MapReduce on a large-scale shared-memory system",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5306783/",
            "Abstract": "Dynamic runtimes can simplify parallel programming by automatically managing concurrency and locality without further burdening the programmer. Nevertheless, implementing such runtime systems for large-scale, shared-memory systems can be challenging. This work optimizes Phoenix, a MapReduce runtime for shared-memory multi-cores and multiprocessors, on a quad-chip, 32-core, 256-thread UltraSPARC T2+ system with NUMA characteristics. We show how a multi-layered approach that comprises optimizations on the algorithm, implementation, and OS interaction leads to significant speedup improvements with 256 threads (average of 2.5times higher speedup, maximum of 19times). We also identify the roadblocks that limit the scalability of parallel runtimes on shared-memory systems, which are inherently tied to the OS scalability on large-scale systems.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:J_g5lzvAfSwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimizing resource provisioning in shared cloud systems",
            "Publication year": 2014,
            "Publication url": "https://web.stanford.edu/group/mast/cgi-bin/drupal/system/files/2014.techreport.hybrid.pdf",
            "Abstract": "Cloud computing promises flexibility and high performance for users and cost efficiency for operators. To achieve this premise, cloud providers offer several provisioning strategies including long-term reserved resources and short-term ondemand resources. Determining the most appropriate provisioning strategy is a complex, multi-dimensional problem that depends on the load fluctuation, interference sensitivity and duration of incoming jobs and the performance unpredictability and cost of the provisioned resources. We first compare the two main provisioning strategies (reserved and on-demand resources) on Google Compute Engine (GCE) using three representative workload scenarios with mixes of batch and latency-critical applications and increasing levels of load variability. We show that either approach is suboptimal from the performance or cost perspective. We then explore hybrid provisioning strategies with both reserved and on-demand resources. We design policies that account for the resource preferences of incoming jobs to automatically determine which jobs should be mapped to reserved versus on-demand resources based on overall load, and resource unpredictability. We demonstrate that hybrid configurations improve both performance and cost-efficiency compared to fully reserved and fully on-demand systems. Specifically they improve performance by 2.1 x compared to fully on-demand provisioning, and reduce cost by 46% compared to fully reserved systems. We also show that hybrid strategies are robust to variation in system and job parameters, such as cost, and system load.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:SpbeaW3--B0C",
            "Publisher": "Standford University"
        },
        {
            "Title": "Tarcil: High Quality and Low Latency Scheduling in Large, Shared Clusters",
            "Publication year": 2015,
            "Publication url": "https://www.csl.cornell.edu/~delimitrou/papers/2014.techreport.tarcil.pdf",
            "Abstract": "Scheduling diverse applications in large, shared clusters is particularly challenging. Recent research on cluster management focuses either on scheduling speed, using sampling techniques to quickly assign tasks to resources, or on scheduling quality, using centralized algorithms that examine the cluster state to find the most suitable resources that improve both task performance and cluster utilization. We present Tarcil, a distributed scheduler that targets both scheduling speed and quality, making it appropriate for large, highly-loaded clusters running both short and long jobs. Tarcil uses an analytically derived sampling framework that dynamically adjusts the sample size based on load and provides guarantees on the quality of scheduling decisions with respect to resource heterogeneity and workload interference. It also implements admission control when sampling is unlikely to find suitable resources for a task. We evaluate Tarcil on clusters with hundreds of servers on EC2. For highly-loaded clusters running short jobs, Tarcil improves task execution time by 41% over a distributed, sampling-based scheduler. For more general workload scenarios, Tarcil increases the fraction of tasks that achieve near ideal performance by 4x and 2x compared to sampling-based and centralized scheduling respectively.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:wKETBy42zhYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "DBOS: A DBMS-oriented Operating System",
            "Publication year": 2021,
            "Publication url": "https://petereliaskraft.net/res/dbos-vldb.pdf",
            "Abstract": "This paper lays out the rationale for building a completely new operating system (OS) stack. Rather than build on a single node OS together with separate cluster schedulers, distributed filesystems, and network managers, we argue that a distributed transactional DBMS should be the basis for a scalable cluster OS. We show herein that such a database OS (DBOS) can do scheduling, file management, and inter-process communication with competitive performance to existing systems. In addition, significantly better analytics can be provided as well as a dramatic reduction in code complexity through implementing OS services as standard database queries, while implementing low-latency transactions and high availability only once.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:69ZgNCALVd0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tetris: Scalable and efficient neural network acceleration with 3d memory",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3037697.3037702",
            "Abstract": "The high accuracy of deep neural networks (NNs) has led to the development of NN accelerators that improve performance by two orders of magnitude. However, scaling these accelerators for higher performance with increasingly larger NNs exacerbates the cost and energy overheads of their memory systems, including the on-chip SRAM buffers and the off-chip DRAM channels.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:LhH-TYMQEocC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Ramcloud: Scalable high-performance storage entirely in dram",
            "Publication year": 2009,
            "Publication url": "http://people.eecs.berkeley.edu/~istoica/classes/cs294/15/notes/08-ramcloud.pdf",
            "Abstract": "RAMCloud: Scalable High-Performance Storage Entirely in DRAM Page 1 RAMCloud: \nScalable High-Performance Storage Entirely in DRAM John Ousterhout Stanford University (with \nNandu Jayakumar, Diego Ongaro, Mendel Rosenblum, Stephen Rumble, and Ryan Stutsman) \nPage 2 DRAM in Storage Systems March 28, 2011 RAMCloud Slide 2 1970 1980 1990 2000 \n2010 UNIX buffer cache Main-memory databases Large file caches Web indexes entirely in \nDRAM memcached Facebook: 200 TB total data 150 TB cache! Main-memory DBs, again Page \n3 DRAM in Storage Systems \u25cf DRAM usage limited/specialized \u25cf Clumsy (consistency with \nbacking store) \u25cf Lost performance (cache misses, backing store) March 28, 2011 RAMCloud \nSlide 3 1970 1980 1990 2000 2010 UNIX buffer cache Main-memory databases Large \nfile caches Web indexes entirely in DRAM memcached Facebook: 200 TB total data 150 ! \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:TIZ-Mc8IlK0C",
            "Publisher": "HPTS"
        },
        {
            "Title": "Thread-safe dynamic binary translation using transactional memory",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4658646/",
            "Abstract": "Dynamic binary translation (DBT) is a runtime instrumentation technique commonly used to support profiling, optimization, secure execution, and bug detection tools for application binaries. However, DBT frameworks may incorrectly handle multithreaded programs due to races involving updates to the application data and the corresponding metadata maintained by the DBT. Existing DBT frameworks handle this issue by serializing threads, disallowing multithreaded programs, or requiring explicit use of locks. This paper presents a practical solution for correct execution of multithreaded programs within DBT frameworks. To eliminate races involving metadata, we propose the use of transactional memory (TM). The DBT uses memory transactions to encapsulate the data and metadata accesses in a trace, within one atomic block. This approach guarantees correct execution of concurrent threads of the translated \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:M05iB0D1s5AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Measuring Latency: Am I doing it right?",
            "Publication year": 2017,
            "Publication url": "https://infoscience.epfl.ch/record/231868/files/nsdi17_poster.pdf",
            "Abstract": "The ever increasing hardware speeds has led systems and application designers to push software to its limits and create applications with microsecond response times. Moreover, such interactive applications, eg websearch, run at massive scales, with large fan-in and fan-out patterns, where the importance of tail-latency becomes crucial to guarantee Service Level Agreements (SLAs). So being able to measure latency accurately is vital to debug those system, identify their behaviour and specify proper SLAs. Accurate latency measurement proves to be very challenging. Unlike throughput, latency is a very sensitive metric that is affected by multiple factors, both within the application and the operating system [2], but also the measuring client itself. Consequently, we are faced with two major challenges related to latency measurement. We need tools that are able to measure latency in such a low microsecond scale, while independently of the tools, we need an accurate an sound methodology that is able to produce unbiased and realistic results that are collected under settings that highly resemble a production environment. In this work we first collect and analyze some common methodology pitfalls that have been overlooked by previous approaches [9] and then compare existing tools and identify missing features, related to the experiment methodology, that can either lead to more statistically sound results or reduce necessary resources (time and compute) for latency experiments.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:u-coK7KVo8oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Executing Java programs with transactional memory",
            "Publication year": 2006,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0167642306001146",
            "Abstract": "Parallel programming is difficult due to the complexity of dealing with conventional lock-based synchronization. To simplify parallel programming, there have been a number of proposals to support transactions directly in hardware and eliminate locks completely. Although hardware support for transactions has the potential to completely change the way parallel programs are written, initially transactions will be used to execute existing parallel programs. In this paper we investigate the implications of using transactions to execute existing parallel Java programs. Our results show that transactions can be used to support all aspects of Java multithreaded programs, and once Java virtual machine issues have been addressed, the conversion of a lock-based application into transactions is largely straightforward. The performance that these converted applications achieve is equal to or sometimes better than the original \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:bFI3QPDXJZMC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Mind the gap: A case for informed request scheduling at the nic",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3365609.3365856",
            "Abstract": "Recent research in high-throughput networked systems has established the need for centralized and preemptive request scheduling in order to achieve good hardware utilization and low tail latency for a wide variety of workloads. However, this approach is expensive to scale as it requires an increasing number of CPU cores dedicated to scheduling. Moreover, passing every request through a scheduling core introduces latency for inter-core communication and reduces the effectiveness of data preloading and caching optimizations.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:jFemdcug13IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Outsourcing everyday jobs to thousands of cloud functions with gg",
            "Publication year": 2020,
            "Publication url": "https://www.usenix.org/system/files/login/articles/login_fall19_02_fouladi.pdf",
            "Abstract": "Dan Iter is a PhD student at Stanford University. He is advised by Professor Dan Jurafsky and is a member of the NLP Group and AI Lab. He is interested in generative models for text representation, relation extraction, knowledgebase construction, and mental health applications. Previously, Dan also worked on lambda computing and virtualized storage for datacenters. daniter@ stanford. eduQian Li is a PhD student in computer science at Stanford University, advised by Professor Christos Kozyrakis. She has broad interests in computer systems and architecture. Her current research focuses on efficient resource management and scheduling for heterogeneous cloud computing platforms. Before coming to Stanford, Qian received her Bachelor of Science from Peking University. qianli@ cs. stanford. edu",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:LPtt_HFRSbwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Measuring and analyzing the energy use of enterprise computing systems",
            "Publication year": 2013,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S2210537913000103",
            "Abstract": "Until now, green computing research has largely relied on few, short-term power measurements to characterize the energy use of enterprise computing. This paper brings new and comprehensive power datasets through Powernet, a hybrid sensor network that monitors the power and utilization of the IT systems in a large academic building. Over more than two years, we have collected power data from 250+ individual computing devices and have monitored a subset of CPU and network loads. This dense, long-term monitoring allows us to extrapolate the data to a detailed breakdown of electricity use across the building's computing systems.Our datasets provide an opportunity to examine data analysis and methodology techniques used in green computing research. We show that power variability both between similar devices and over time for a single device can lead to cost or savings estimates that are off by 15 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:-_dYPAW6P2MC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Eigenbench: A simple exploration tool for orthogonal TM characteristics",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5648812/",
            "Abstract": "There are a significant number of Transactional Memory(TM) proposals, varying in almost all aspects of the design space. Although several transactional benchmarks have been suggested, a simple, yet thorough, evaluation framework is still needed to completely characterize a TM system and allow for comparison among the various proposals. Unfortunately, TM system evaluation is difficult because the application characteristics which affect performance are often difficult to isolate from each other. We propose a set of orthogonal application characteristics that form a basis for transactional behavior and are useful in fully understanding the performance of a TM system. In this paper, we present EigenBench, a lightweight yet powerful microbenchmark for fully evaluating a transactional memory system. We show that EigenBench is useful for thoroughly exploring the orthogonal space of TM application characteristics \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:P5F9QuxV20EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Hardware Acceleration of Transactional Memory on Commodity Systems",
            "Publication year": 2011,
            "Publication url": "https://web.stanford.edu/group/mast/cgi-bin/drupal/system/files/2011.tmacc_.asplos.pdf",
            "Abstract": "The adoption of transactional memory is hindered by the high overhead of software transactional memory and the intrusive design changes required by previously proposed TM hardware. We propose that hardware to accelerate software transactional memory (STM) can reside outside an unmodified commodity processor core, thereby substantially reducing implementation costs. This paper introduces Transactional Memory Acceleration using Commodity Cores (TMACC), a hardware-accelerated TM system that does not modify the processor, caches, or coherence protocol. We present a complete hardware implementation of TMACC using a rapid prototyping platform. Using this hardware, we implement two unique conflict detection schemes which are accelerated using Bloom filters on an FPGA. These schemes employ novel techniques for tolerating the latency of fine-grained asynchronous communication with an out-of-core accelerator. We then conduct experiments to explore the feasibility of accelerating TM without modifying existing system hardware. We show that, for all but short transactions, it is not necessary to modify the processor to obtain substantial improvement in TM performance. In these cases, TMACC outperforms an STM by an average of 69% in applications using moderate-length transactions, showing maximum speedup within 8% of an upper bound on TM acceleration. Overall, we demonstrate that hardware can substantially accelerate the performance of an STM on unmodified commodity processors.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:j7_hQOaDUrUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Full-system power analysis and modeling for server environments",
            "Publication year": 2006,
            "Publication url": "http://sonoma-dspace.calstate.edu/handle/10211.1/715",
            "Abstract": "The increasing costs of power delivery and cooling, as well as the trend toward higher-density computer systems, have created a growing demand for better power management in server environments. Despite the increasing interest in this issue, little work has been done in quantitatively understanding power consumption trends and developing simple yet accurate models to predict full-system power. We study the component-level power breakdown and variation, as well as temporal workload-specific power consumption of an instrumented power-optimized blade server. Using this analysis, we examine the validity of prior adhoc approaches to understanding power breakdown and quantify several interesting trends important for power modeling and management in the future. We also introduce Mantis, a nonintrusive method for modeling full-system power consumption and providing real-time power prediction. Mantis uses a onetime calibration phase to generate a model by correlating AC power measurements with user-level system utilization metrics. We experimentally validate the model on two server systems with drastically different power footprints and characteristics (a low-end blade and high-end compute-optimized server) using a variety of workloads. Mantis provides power estimates with high accuracy for both overall and temporal power consumption, making it a valuable tool for power-aware scheduling and analysis.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:isC4tDSrTZIC",
            "Publisher": "International Symposium on Computer Architecture (IEEE)"
        },
        {
            "Title": "RAMP: Research accelerator for multiple processors",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4287395/",
            "Abstract": "The RAMP project's goal is to enable the intensive, multidisciplinary innovation that the computing industry will need to tackle the problems of parallel processing. RAMP itself is an open-source, community-developed, FPGA-based emulator of parallel architectures. its design framework lets a large, collaborative community develop and contribute reusable, composable design modules. three complete designs - for transactional memory, distributed systems, and distributed-shared memory - demonstrate the platform's potential.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "MARS: adaptive remote execution for multi-threaded mobile devices",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2043106.2043107",
            "Abstract": "Mobile devices face a growing demand to support computationally intensive applications like 3D graphics and computer vision. However, these devices are inherently limited by processor power density and device battery life. Dynamic remote execution addresses this problem, by enabling mobile devices to opportunistically offload computations to a remote server. We envision remote execution as a new type of cloud-based heterogeneous computing resource, or a\" Cloud-on-Chip\", which would be managed as a system resource as if it were a local CPU, with a highly variable wireless interconnect. To realize this vision, we introduce MARS, the first adaptive, online and lightweight RPC-based remote execution scheduler supporting multi-threaded and multi-core systems. MARS uses a novel efficient offloading decision algorithm that takes into account the inherent trade-offs between communication and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:J-pR_7NvFogC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Selecta: Heterogeneous cloud storage configuration for data analytics",
            "Publication year": 2018,
            "Publication url": "https://www.usenix.org/conference/atc18/presentation/klimovic-selecta",
            "Abstract": "Data analytics are an important class of data-intensive workloads on public cloud services. However, selecting the right compute and storage configuration for these applications is difficult as the space of available options is large and the interactions between options are complex. Moreover, the different data streams accessed by analytics workloads have distinct characteristics that may be better served by different types of storage devices.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:ubry08Y2EpUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Power management of datacenter workloads using per-core power gating",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5224259/",
            "Abstract": "While modern processors offer a wide spectrum of software-controlled power modes, most datacenters only rely on dynamic voltage and frequency scaling (DVFS, a.k.a. P-states) to achieve energy efficiency. This paper argues that, in the case of datacenter workloads, DVFS is not the only option for processor power management. We make the case for per-core power gating (PCPG) as an additional power management knob for multi-core processors. PCPG is the ability to cut the voltage supply to selected cores, thus reducing to almost zero the leakage power for the gated cores. Using a testbed based on a commercial 4-core chip and a set of real-world application traces from enterprise environments, we have evaluated the potential of PCPG. We show that PCPG can significantly reduce a processor's energy consumption (up to 40%) without significant performance overheads. When compared to DVFS, PCPG is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:pyW8ca7W8N0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The software stack for transactional memory",
            "Publication year": 2006,
            "Publication url": "http://ppl.stanford.edu/papers/tcc_stmcs2006.pdf",
            "Abstract": "Transactional memory systems apply the experience of the database community to the general problem of parallel programming with the goal of providing a simple parallel programing model that delivers on the performance potential of multi-processor systems. Although initial research into both software-only and hardwaresupported transactional memory has shown promising results, there are many challenges to creating a fully transactional software stack. Although today\u2019s software stack has some limited use of transactional programming, many parts of the stack from basic data structures to the operating system and program runtimes contain at least some lock-based code. In code with coarse-grained locking, transactions provide an opportunity to improve performance. In code with fine-grained lock, transactions provide an opportunity to simplify code while reducing overhead.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:WZBGuue-350C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Pascal and Francis Bibliographic Databases",
            "Publication year": 2003,
            "Publication url": "https://pascal-francis.inist.fr/vibad/index.php?action=getRecordDetail&idt=15082852",
            "Abstract": "Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:6bLC7aUMtPcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Green enterprise computing data: Assumptions and realities",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6322264/",
            "Abstract": "Until now, green computing research has largely relied on few, short-term power measurements to characterize the energy use of enterprise computing. This paper brings new and comprehensive power datasets through Powernet, a hybrid sensor network that monitors the power and utilization of the IT systems in a large academic building. Over more than two years, we have collected power data from 250+ individual computing devices and have monitored a subset of CPU and network loads. This dense, long-term monitoring allows us to extrapolate the data to a detailed breakdown of electricity use across the building's computing systems. Our datasets provide an opportunity to examine assumptions commonly made in green computing. We show that power variability both between similar devices and over time for a single device can lead to cost or savings estimates that are off by 15-20%. Extending the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:ye4kPcJQO24C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improving instruction delivery with a block-aware ISA",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11549468_60",
            "Abstract": "Instruction delivery is a critical component for wide-issue processors since its bandwidth and accuracy place an upper limit on performance. The processor front-end accuracy and bandwidth are limited by instruction cache misses, multi-cycle instruction cache accesses, and target or direction mispredictions for control-flow operations. This paper introduces a block-aware ISA (BLISS) that helps accurate instruction delivery by defining basic block descriptors in addition to and separate from the actual instructions in a program. We show that BLISS allows for a decoupled front-end that tolerates cache latency and allows for higher speculation accuracy. This translates to a 20% IPC and 14% energy improvements over conventional front-ends. We also demonstrate that a BLISS-based front-end outperforms by 13% decoupled front-ends that detect fetched blocks dynamically in hardware, without any information \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:WbkHhVStYXYC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Transactional memory: The hardware-software interface",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4205125/",
            "Abstract": "As multicore chips become ubiquitous, the need to provide architectural support for practical parallel programming is reaching critical. Conventional lock-based concurrency control techniques are difficult to use, requiring the programmer to navigate through the minefield of coarse-versus fine-grained locks, deadlock, livelock, lock convoying, and priority inversion. This explicit management of concurrency is beyond the reach of the average programmer, threatening to waste the additional parallelism available with multicore architectures. This comprehensive architecture supports nested transactions, transactional handlers, and two-phase commit. The result is a seamless integration of transactional memory with modern programming languages and runtime environments",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:mB3voiENLucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Low power programmable image processor",
            "Publication year": 2016,
            "Publication url": "https://patents.google.com/patent/US9477999B2/en",
            "Abstract": "A convolution image processor includes a load and store unit, a shift register unit, and a mapping unit. The load and store unit is configured to load and store image pixel data and allow for unaligned access of the image pixel data. The shift register is configured to load and store at least a portion of the image pixel data from the load and store unit and concurrently provide access to each image pixel value in the portion of the image pixel data. The mapping unit is configured to generate a number of shifted versions of image pixel data and corresponding stencil data from the portion of the image pixel data, and concurrently perform one or more operations on each image pixel value in the shifted versions of the portion of the image pixel data and a corresponding stencil value in the corresponding stencil data.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:q3CdL3IzO_QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Paragon: QoS-aware scheduling for heterogeneous datacenters",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2499368.2451125",
            "Abstract": "Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty to match applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online and do not scale beyond few applications.We present Paragon, an online and scalable DC scheduler that is heterogeneity and interference-aware. Paragon is derived from robust analytical methods and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:35r97b3x0nAC",
            "Publisher": "ACM"
        },
        {
            "Title": "SmartHarvest: harvesting idle CPUs safely and efficiently in the cloud",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3447786.3456225",
            "Abstract": "We can increase the efficiency of public cloud datacenters by harvesting allocated but temporarily idling CPU cores from customer virtual machines (VMs) to run batch or analytics workloads. Even small efficiency gains translate into substantial savings, since provisioning and operating a datacenter costs hundreds of millions of dollars per year. The main challenge is to harvest idle cores with little or no impact on customer VMs, which could be running latency-sensitive services and are essentially black-boxes to the cloud provider.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:EPG8bYD4jVwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving resource efficiency in cloud computing",
            "Publication year": 2015,
            "Publication url": "https://search.proquest.com/openview/0831f08fd7ffefa29dc4871b913e3710/1?pq-origsite=gscholar&cbl=44156",
            "Abstract": "Cloud computing is at a critical juncture. An increasing amount of computation is now hosted in private and public clouds. At the same time, datacenter resource efficiency, ie, the effective utility we extract from system resources has remained notoriously low, with utilization rarely exceeding 20-30%. Low utilization coupled with the lack of scaling in hardware due to technology limitations poses threatening scalability roadblocks for cloud computing.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:GtLg2Ama23sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Syrup: User-Defined Scheduling Across the Stack",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3477132.3483548",
            "Abstract": "Suboptimal scheduling decisions in operating systems, networking stacks, and application runtimes are often responsible for poor application performance, including higher latency and lower throughput. These poor decisions stem from a lack of insight into the applications and requests the scheduler is handling and a lack of coherence and coordination between the various layers of the stack, including NICs, kernels, and applications.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:SnGPuo6Feq8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "INFaaS: A model-less inference serving system",
            "Publication year": 2019,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2019arXiv190513348R/abstract",
            "Abstract": "Despite existing work in machine learning inference serving, ease-of-use and cost efficiency remain key challenges. Developers must manually match the performance, accuracy, and cost constraints of their applications to decisions about selecting the right model and model optimizations, suitable hardware architectures, and auto-scaling configurations. These interacting decisions are difficult to make for users, especially when the application load varies, applications evolve, and the available resources vary over time. Thus, users often end up making decisions that overprovision resources. This paper introduces INFaaS, a model-less inference-as-a-service system that relieves users of making these decisions. INFaaS provides a simple interface allowing users to specify their inference task, and performance and accuracy requirements. To implement this interface, INFaaS generates and leverages model-variants \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:RoXSNcbkSzsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Plasticine: A reconfigurable architecture for parallel patterns",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8192487/",
            "Abstract": "Reconfigurable architectures have gained popularity in recent years as they allow the design of energy-efficient accelerators. Fine-grain fabrics (e.g. FPGAs) have traditionally suffered from performance and power inefficiencies due to bit-level reconfigurable abstractions. Both fine-grain and coarse-grain architectures (e.g. CGRAs) traditionally require low level programming and suffer from long compilation times. We address both challenges with Plasticine, a new spatially reconfigurable architecture designed to efficiently execute applications composed of parallel patterns. Parallel patterns have emerged from recent research on parallel programming as powerful, high-level abstractions that can elegantly capture data locality, memory access patterns, and parallelism across a wide range of dense and sparse applications. We motivate Plasticine by first observing key application characteristics captured by parallel \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:8xutWZnSdmoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A scalable, non-blocking approach to transactional memory",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4147652/",
            "Abstract": "Transactional memory (TM) provides mechanisms that promise to simplify parallel programming by eliminating the need for locks and their associated problems (deadlock, livelock, priority inversion, convoying). For TM to be adopted in the long term, not only does it need to deliver on these promises, but it needs to scale to a high number of processors. To date, proposals for scalable TM have relegated livelock issues to user-level contention managers. This paper presents the first scalable TM implementation for directory-based distributed shared memory systems that is livelock free without the need for user-level intervention. The design is a scalable implementation of optimistic concurrency control that supports parallel commits with a two-phase commit protocol, uses write-back caches, and filters coherence messages. The scalable design is based on transactional coherence and consistency (TCC), which \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:k_IJM867U9cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Comparative evaluation of memory models for chip multiprocessors",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1455650.1455651",
            "Abstract": "There are two competing models for the on-chip memory in Chip Multiprocessor (CMP) systems: hardware-managed coherent caches and software-managed streaming memory. This paper performs a direct comparison of the two models under the same set of assumptions about technology, area, and computational capabilities. The goal is to quantify how and when they differ in terms of performance, energy consumption, bandwidth requirements, and latency tolerance for general-purpose CMPs. We demonstrate that for data-parallel applications on systems with up to 16 cores, the cache-based and streaming models perform and scale equally well. For certain applications with little data reuse, streaming scales better due to better bandwidth use and macroscopic software prefetching. However, the introduction of techniques such as hardware prefetching and nonallocating stores to the cache-based model \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:_xSYboBqXhAC",
            "Publisher": "ACM"
        },
        {
            "Title": "Implementing and evaluating nested parallel transactions in software transactional memory",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1810479.1810528",
            "Abstract": "Transactional Memory (TM) is a promising technique that simplifies parallel programming for shared-memory applications. To date, most TM systems have been designed to efficiently support single-level parallelism. To achieve widespread use and maximize performance gains, TM must support nested parallelism available in many applications and supported by several programming models.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:SP6oXDckpogC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Guest Editors' Introduction: Hot Chips Turns 20",
            "Publication year": 2009,
            "Publication url": "https://www.computer.org/csdl/magazine/mi/2009/02/mmi2009020004/13rRUxcbnEC",
            "Abstract": "In this special issue, IEEE Micro presents a selection of articles from the best presentations from the Hot Chips 2008 conference.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:-7ulzOJl1JYC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "Architectural semantics for practical transactional memory",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1150019.1136491",
            "Abstract": "Transactional Memory (TM) simplifies parallel programming by allowing for parallel execution of atomic tasks. Thus far, TM systems have focused on implementing transactional state buffering and conflict resolution. Missing is a robust hardware/software interface, not limited to simplistic instructions defining transaction boundaries. Without rich semantics, current TM systems cannot support basic features of modern programming languages and operating systems such as transparent library calls, conditional synchronization, system calls, I/O, and runtime exceptions. This paper presents a comprehensive instruction set architecture (ISA) for TM systems. Our proposal introduces three key mechanisms: two-phase commit; support for software handlers on commit, violation, and abort; and full support for open- and closed-nested transactions with independent rollback. These mechanisms provide a flexible interface to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:4JMBOYKVnBMC",
            "Publisher": "ACM"
        },
        {
            "Title": "Advancing computer systems without technology progress.",
            "Publication year": 2013,
            "Publication url": "http://www.ispass.org/ispass2013/2013.advancingsystems.ispass_keynote.pdf",
            "Abstract": "Advancing Computer Systems without Technology Progress Page 1 Advancing Computer \nSystems without Technology Progress Christos Kozyrakis Stanford University http://csl.stanford.edu/~christos \nISPASS Keynote \u2013 April 23rd 2013 Page 2 2 Computing is the Innovation Catalyst Science \nGovernment Commerce Healthcare Education Entertainment Faster, greener, cheaper Page \n3 3 The Key Enabler \u220e Turning exponentially increasing transistor counts into \u220e \nExponentially improving performance \u220e At constant cost and power consumption Page 4 4 \nCMOS Scaling: The Past \u220e Moore\u2019s law (more transistors) + Dennard scaling (lower V dd ) \u220e \n2.8x in chip capability per CMOS generation at constant power Chip Capability Chip Power \n1.0 1.5 2.0 2.5 3.0 2.5 1.5 2.0 3.0 2x transistors 1.4x frequency 0.7x voltage 0.7x capacitance \n2.8x capability, same power [S. Keckler, 2011] Page 5 5 CMOS Scaling: The Present \u220e \u2019s \u220e [/\u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:Z5m8FVwuT1cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Testing Implementations of Transactional Memory",
            "Publication year": 2006,
            "Publication url": "https://scholar.google.com/scholar?cluster=14997758719773709683&hl=en&oi=scholarr",
            "Abstract": "Transactional memory is an attractive design concept for scalable multiprocessors because it offers efficient lock-free synchronization and greatly simplifies parallel software. Given the subtle issues involved with concurrency and atom-icity, however, it is important that transactional memory systems be carefully designed and aggressively tested to en-sure their correctness. In this paper, we propose an ax-iomatic framework to model the formal specification of a realistic transactional memory system which may contain a mix of transactional and non-transactional operations. Using this framework and extensions to analysis algorithms originally developed for checking traditional memory consistency, we show that the widely practiced pseudo-random testing methodology can be effectively applied to transac-tional memory systems. Our testing methodology was successful in finding previously unknown bugs in the implemen-tation of TCC, a transactional memory system. We study two flavors of the underlying analysis algorithm, one incom-plete and the other complete, and show that the complete algorithm while being theoretically intractable is very effi-cient in practice.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:oi2SiIJ9l4AC",
            "Publisher": "Association for Computing Machinery (ACM)"
        },
        {
            "Title": "HRL: Efficient and flexible reconfigurable logic for near-data processing",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7446059/",
            "Abstract": "The energy constraints due to the end of Dennard scaling, the popularity of in-memory analytics, and the advances in 3D integration technology have led to renewed interest in near-data processing (NDP) architectures that move processing closer to main memory. Due to the limited power and area budgets of the logic layer, the NDP compute units should be area and energy efficient while providing sufficient compute capability to match the high bandwidth of vertical memory channels. They should also be flexible to accommodate a wide range of applications. Towards this goal, NDP units based on fine-grained (FPGA) and coarse-grained (CGRA) reconfigurable logic have been proposed as a compromise between the efficiency of custom engines and the flexibility of programmable cores. Unfortunately, FPGAs incur significant area overheads for bit-level reconfiguration, while CGRAs consume significant power in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:NXb4pA-qfm4C",
            "Publisher": "Ieee"
        },
        {
            "Title": "IX Open-source version 1.1-Deployment and Evaluation Guide",
            "Publication year": 2016,
            "Publication url": "https://infoscience.epfl.ch/record/218568",
            "Abstract": "This Technical Report provides the deployment and evaluation guide of the ix dataplane operating system, as of its first open-source release on May 27, 2016. To facilitate the reproduction of our results, we include in this report the precise steps needed to install, deploy and configure ix and its workloads. We reproduce all benchmarks previously published in two peer-reviewed publications at OSDI\u201914 [3] and SoCC\u201915 [6] using this up-to-date, open-source code base.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:cWzG1nlazyYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Parallelizing specjbb2000 with transactional memory",
            "Publication year": 2006,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.2812&rep=rep1&type=pdf",
            "Abstract": "As chip-multiprocessors become ubiquitous, it is critical to provide architectural support for practical parallel programming. Transactional Memory (TM)[4] has the potential to simplify concurrency management by supporting parallel tasks (transactions) that appear to execute atomically and in isolation. By virtue of optimistic concurrency, transactional memory promises good parallel performance with easy-to-write, coarse-grain transactions. Furthermore, transactions can address other challenges of lock-based parallel code such as deadlock avoidance and robustness to failures. In this paper, we use transactional memory to parallelize SPECjbb2000 [9], a popular benchmark for Java middleware. SPECjbb2000 combines in a single Java program many common features of 3-tier enterprise systems, hence it is significantly more complicated than the data structure microbenchmarks frequently used for TM research [3, 5, 8]. Since SPECjbb2000 models the operation of a wholesale company with multiple warehouses, the original code is already parallel, with a separate Java thread managing each warehouse. Different warehouses accesses mostly disjoint data structures, hence dependencies are rare. We focus on parallelism within a single warehouse, which is a more challenging case1. Conceptually, there are significant amounts of parallelism within a single warehouse as different customers order or pay for different objects. Nevertheless, all operations within a warehouse access the same data structures (B-trees), hence dependencies are possible and difficult to predict. Overall, single-warehouse SPECjbb2000 is an excellent candidate to explore \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:dfsIfKJdRG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "RackSched: A microsecond-scale scheduler for rack-scale computers (technical report)",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2010.05969",
            "Abstract": "Low-latency online services have strict Service Level Objectives (SLOs) that require datacenter systems to support high throughput at microsecond-scale tail latency. Dataplane operating systems have been designed to scale up multi-core servers with minimal overhead for such SLOs. However, as application demands continue to increase, scaling up is not enough, and serving larger demands requires these systems to scale out to multiple servers in a rack. We present RackSched, the first rack-level microsecond-scale scheduler that provides the abstraction of a rack-scale computer (i.e., a huge server with hundreds to thousands of cores) to an external service with network-system co-design. The core of RackSched is a two-layer scheduling framework that integrates inter-server scheduling in the top-of-rack (ToR) switch with intra-server scheduling in each server. We use a combination of analytical results and simulations to show that it provides near-optimal performance as centralized scheduling policies, and is robust for both low-dispersion and high-dispersion workloads. We design a custom switch data plane for the inter-server scheduler, which realizes power-of-k-choices, ensures request affinity, and tracks server loads accurately and efficiently. We implement a RackSched prototype on a cluster of commodity servers connected by a Barefoot Tofino switch. End-to-end experiments on a twelve-server testbed show that RackSched improves the throughput by up to 1.44x, and scales out the throughput near linearly, while maintaining the same tail latency as one server until the system is saturated.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:uVUOdF_882EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Making pull-based graph processing performant",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3200691.3178506",
            "Abstract": "Graph processing engines following either the push-based or pull-based pattern conceptually consist of a two-level nested loop structure. Parallelizing and vectorizing these loops is critical for high overall performance and memory bandwidth utilization. Outer loop parallelization is simple for both engine types but suffers from high load imbalance. This work focuses on inner loop parallelization for pull engines, which when performed naively leads to a significant increase in conflicting memory writes that must be synchronized.Our first contribution is a scheduler-aware interface for parallel loops that allows us to optimize for the common case in which each thread executes several consecutive iterations. This eliminates most write traffic and avoids all synchronization, leading to speedups of up to 50X.Our second contribution is the Vector-Sparse format, which addresses the obstacles to vectorization that stem from the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:BJbdYPG6LGMC",
            "Publisher": "ACM"
        },
        {
            "Title": "Decoupling datacenter studies from access to large-scale applications: A modeling approach for storage workloads",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6114196/",
            "Abstract": "The cost and power impact of suboptimal storage configurations is significant in datacenters (DCs) as inefficiencies are aggregated over several thousand servers and represent considerable losses in capital and operating costs. Designing performance, power and cost-optimized systems requires a deep understanding of target workloads, and mechanisms to effectively model different storage design choices. Traditional benchmarking is invalid in cloud data-stores, representative storage profiles are hard to obtain, while replaying the entire application in all storage configurations is impractical both from a cost and time perspective. Despite these issues, current workload generators are not able to accurately reproduce key aspects of real application patterns. Some of these features include spatial and temporal locality, as well as tuning the intensity of the workload to emulate different storage system configurations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:eQOLeE2rZwMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Quality-Of-Service Aware Resource Control in Networked Computing Systems",
            "Publication year": 2011,
            "Publication url": "https://search.proquest.com/openview/7f818916b45837f445366a635c425b78/1?pq-origsite=gscholar&cbl=44156",
            "Abstract": "The emerging trends in computing have increasingly had a network-centric focus. Networked services offered through cloud computing paradigms have replaced applications that would traditionally run on local machines. In addition, the growing usage of applications such as social networking and platforms such as smartphones has resulted in greater need for ubiquitous network access. The consequent heightened demand for networked computing warrants efficient utilization of the limited network resources and more intelligent resource control algorithms, with a focus on providing an enhanced user experience. This thesis examines quality-of-service aware resource control for both wireless and wired networks.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:4fGpz3EwCPoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Future scaling of processor-memory interfaces",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1654059.1654102",
            "Abstract": "Continuous evolution in process technology brings energy-efficiency and reliability challenges, which are harder for memory system designs since chip multiprocessors demand high bandwidth and capacity, global wires improve slowly, and more cells are susceptible to hard and soft errors. Recently, there are proposals aiming at better main-memory energy efficiency by dividing a memory rank into subsets.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:b0M2c_1WBrUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "RAMP: Research Accelerator for Multiple Processors-A Community Vision for a Shared Experimental. Parallel HW/SW Platform",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=8449943734110080839&hl=en&oi=scholarr",
            "Abstract": "Desktop processor architectures have crossed a critical threshold. Manufactures have given up attempting to extract ever more performance from a single core and instead have turned to multi-core designs. While straightforward approaches to the architecture of multi-core processors are sufficient for small designs (2-4 cores), little is really known how to build, program, or manage systems of 64 to 1024 processors. Unfortunately, the computer architecture community lacks the basic infrastructure tools required to carry out this research. While simulation has been adequate for single-processor research, significant use of simplified modeling and statistical sampling is required to work in the 2-16 processing core space. Invention is required for architecture research at the level of 64-1024 cores.Fortunately, Moore's law has not only enabled these dense multi-core chips, it has also enabled extremely dense FPGAs \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:IWHjjKOFINEC",
            "Publisher": "Technical Report, UCB/CSD'() S-1412, University of California, Berkeley"
        },
        {
            "Title": "INFaaS: Automated Model-less Inference Serving",
            "Publication year": 2021,
            "Publication url": "https://www.usenix.org/conference/atc21/presentation/romero",
            "Abstract": "Despite existing work in machine learning inference serving, ease-of-use and cost efficiency remain challenges at large scales. Developers must manually search through thousands of model-variants\u2014versions of already-trained models that differ in hardware, resource footprints, latencies, costs, and accuracies\u2014to meet the diverse application requirements. Since requirements, query load, and applications themselves evolve over time, these decisions need to be made dynamically for each inference query to avoid excessive costs through naive autoscaling. To avoid navigating through the large and complex trade-off space of model-variants, developers often fix a variant across queries, and replicate it when load increases. However, given the diversity across variants and hardware platforms in the cloud, a lack of understanding of the trade-off space can incur significant costs to developers.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:rTD5ala9j4wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ZSim: Fast and accurate microarchitectural simulation of thousand-core systems",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2508148.2485963",
            "Abstract": "Architectural simulation is time-consuming, and the trend towards hundreds of cores is making sequential simulation even slower. Existing parallel simulation techniques either scale poorly due to excessive synchronization, or sacrifice accuracy by allowing event reordering and using simplistic contention models. As a result, most researchers use sequential simulators and model small-scale systems with 16-32 cores. With 100-core chips already available, developing simulators that scale to thousands of cores is crucial.We present three novel techniques that, together, make thousand-core simulation practical. First, we speed up detailed core models (including OOO cores) with instruction-driven timing models that leverage dynamic binary translation. Second, we introduce bound-weave, a two-phase parallelization technique that scales parallel simulation on multicore hosts efficiently with minimal loss of accuracy \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:yB1At4FlUx8C",
            "Publisher": "ACM"
        },
        {
            "Title": "The netflix challenge: Datacenter edition",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6212411/",
            "Abstract": "The hundreds of thousands of servers in modern warehouse-scale systems make performance and efficiency optimizations pressing design challenges. These systems are traditionally considered homogeneous. However, that is not typically the case. Multiple server generations compose a heterogeneous environment, whose performance opportunities have not been fully explored since techniques that account for platform heterogeneity typically do not scale to the tens of thousands of applications hosted in large-scale cloud providers. We present ADSM, a scalable and efficient recommendation system for application-to-server mapping in large-scale datacenters (DCs) that is QoS-aware. ADSM overcomes the drawbacks of previous techniques, by leveraging robust and computationally efficient analytical methods to scale to tens of thousands of applications with minimal overheads. It is also QoS-aware, mapping \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:VL0QpB8kHFEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Vector vs. superscalar and VLIW architectures for embedded multimedia benchmarks",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1176257/",
            "Abstract": "Multimedia processing on embedded devices requires an architecture that leads to high performance, low power consumption, reduced design complexity, and small code size. In this paper, we use EEMBC, an industrial benchmark suite, to compare the VIRAM vector architecture to superscalar and VLIW processors for embedded multimedia applications. The comparison covers the VIRAM instruction set, vectorizing compiler and the prototype chip that integrates a vector processor with DRAM main memory. We demonstrate that executable code for VIRAM is up to 10 times smaller than VLIW code and comparable to /spl times/86 CISC code. The simple, cache-less VIRAM chip is 2 times faster than a 4-way superscalar RISC processor that uses a 5 times faster clock frequency and consumes 10 times more power VIRAM is also 10 times faster than cache-based VLIW processors. Even after manual optimization of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:roLk4NBRz8UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Reflex: Remote flash\u2248 local flash",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3093337.3037732",
            "Abstract": "Remote access to NVMe Flash enables flexible scaling and high utilization of Flash capacity and IOPS within a datacenter. However, existing systems for remote Flash access either introduce significant performance overheads or fail to isolate the multiple remote clients sharing each Flash device. We present ReFlex, a software-based system for remote Flash access, that provides nearly identical performance to accessing local Flash. ReFlex uses a dataplane kernel to closely integrate networking and storage processing to achieve low latency and high throughput at low resource requirements. Specifically, ReFlex can serve up to 850K IOPS per core over TCP/IP networking, while adding 21us over direct access to local Flash. ReFlex uses a QoS scheduler that can enforce tail latency and throughput service-level objectives (SLOs) for thousands of remote clients. We show that ReFlex allows applications to use \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:r_AWSJRzSzQC",
            "Publisher": "ACM"
        },
        {
            "Title": "Register pointer architecture for efficient embedded processors",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4211864/",
            "Abstract": "Conventional register file architectures cannot optimally exploit temporal locality in data references due to their limited capacity and static encoding of register addresses in instructions. In conventional embedded architectures, the register file capacity cannot be increased without resorting to longer instruction words. Similarly, loop unrolling is often required to exploit locality in the register file accesses across iterations because naming registers statically is inflexible. Both optimizations lead to significant code size increases, which is undesirable in embedded systems. In this paper, the authors introduce the register pointer architecture (RPA), which allows registers to be accessed indirectly through register pointers. Indirection allows a larger register file to be used without increasing the length of instruction words. Additional register file capacity allows many loads and stores, such as those introduced by spill code, to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:p2g8aNsByqUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Quasar: Resource-efficient and qos-aware cluster management",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2644865.2541941",
            "Abstract": "Cloud computing promises flexibility and high performance for users and high cost-efficiency for operators. Nevertheless, most cloud facilities operate at very low utilization, hurting both cost effectiveness and future scalability.We present Quasar, a cluster management system that increases resource utilization while providing consistently high application performance. Quasar employs three techniques. First, it does not rely on resource reservations, which lead to underutilization as users do not necessarily understand workload dynamics and physical resource requirements of complex codebases. Instead, users express performance constraints for each workload, letting Quasar determine the right amount of resources to meet these constraints at any point. Second, Quasar uses classification techniques to quickly and accurately determine the impact of the amount of resources (scale-out and scale-up), type of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:L7CI7m0gUJcC",
            "Publisher": "ACM"
        },
        {
            "Title": "Interference-Aware Scheduling for Inference Serving",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3437984.3458837",
            "Abstract": "Machine learning inference applications have proliferated through diverse domains such as healthcare, security, and analytics. Recent work has proposed inference serving systems for improving the deployment and scalability of models. To improve resource utilization, multiple models can be co-located on the same backend machine. However, co-location can cause latency degradation due to interference and can subsequently violate latency requirements. Although interference-aware schedulers for general workloads have been introduced, they do not scale appropriately to heterogeneous inference serving systems where the number of co-location configurations grows exponentially with the number of models and machine types.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:An6A6Jpfc1oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Infaas: Managed & model-less inference serving",
            "Publication year": 2019,
            "Publication url": "https://scholar.google.com/scholar?cluster=12306322909376611367&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:yxmsSjX2EkcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Selected research from Hot Chips 24",
            "Publication year": 2013,
            "Publication url": "https://www.computer.org/csdl/magazine/mi/2013/02/mmi2013020006/13rRUxcbnEE",
            "Abstract": "This introduction to the special issue introduces the articles selected for publication from Hot Chips 24.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:XD-gHx7UXLsC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "Identifying energy waste through dense power sensing and utilization monitoring",
            "Publication year": 2010,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.174.7717&rep=rep1&type=pdf",
            "Abstract": "PowerNet is a hybrid sensor network for monitoring the power and utilization of computing systems in a large academic building. PowerNet comprises approximately 140 single-plug wired and wireless hardware power meters and 23 software sensors that monitor PCs, laptops, network switches, servers, LCD screens, and other office equipment. PowerNet has been operational for 14 months, and the wireless meters for three months.This dense, long-term monitoring allows us to extrapolate the energy consumption breakdown of the whole building. Using our measurements together with device inventory we find that approximately 56% of the total building energy budget goes toward computing systems, at a cost of\u2248 $22,000 per month. PowerNet\u2019s measurements of CPU activity and network traffic reveal that a large fraction of this power is wasted and shows where there are savings opportunities. In addition to these sensor data results, we present our experiences designing, deploying, and maintaining PowerNet. We include a longterm characterization of CTP, the standard TinyOS collection protocol.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:Tiz5es2fbqcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Hcloud: Resource-efficient provisioning in shared cloud systems",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2872362.2872365",
            "Abstract": "Cloud computing promises flexibility and high performance for users and cost efficiency for operators. To achieve this, cloud providers offer instances of different sizes, both as long-term reservations and short-term, on-demand allocations. Unfortunately, determining the best provisioning strategy is a complex, multi-dimensional problem that depends on the load fluctuation and duration of incoming jobs, and the performance unpredictability and cost of resources. We first compare the two main provisioning strategies (reserved and on-demand resources) on Google Compute Engine (GCE) using three representative workload scenarios with batch and latency-critical applications. We show that either approach is suboptimal for performance or cost. We then present HCloud, a hybrid provisioning system that uses both reserved and on-demand resources. HCloud determines which jobs should be mapped to reserved \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:_axFR9aDTf0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "COMPUTER ARCHITECTURE LETTERS",
            "Publication year": 2016,
            "Publication url": "https://www.computer.org/csdl/letters/ca/2016/02/07797569.pdf",
            "Abstract": "COMPUTER ARCHITECTURE LETTERS Page 1 IEEE COMPUTER ARCHITECTURE LETTER S V \nol. 15, No. 2, July-December 2016 PAPERS A Coarse-Grained Reconfigurable Architecture for \nCompute-Intensive MapReduce Acceleration S. Liang, S. Yin, L. Liu, Y. Guo, and S. Wei \n......................................................................................................................... 69 A Quantitative Method to Data \nReuse Patterns of SIMT Applications B.-C. Charles Lai, LG Platero, and H.-K. Kuo \n.................................................................................................................. 73 Cyclic Power-Gating as an \nAlternative to Voltage and Frequency Scaling Y. \u00c7akmak\u00e7i, W. Toms, J. Navaridas, and M. Luj\u00e1n \n........................................................................................................... 77 Diversity: A Design Goal for \nHeterogeneous Processors E. Tomusk, C. Dubach, and M. O\u2019Boyle \n............................................................................................................................ 81 Efficient Execution of Bursty \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:yFnVuubrUp4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "The Atomos transactional programming language",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1133981.1133983",
            "Abstract": "Atomos is the first programming language with implicit transactions, strong atomicity, and a scalable multiprocessor implementation. Atomos is derived from Java, but replaces its synchronization and conditional waiting constructs with simpler transactional alternatives. The Atomos watch statement allows programmers to specify fine-grained watch sets used with the Atomos retry conditional waiting statement for efficient transactional conflict-driven wakeup even in transactional memory systems with a limited number of transactional contexts. Atomos supports open-nested transactions, which are necessary for building both scalable application programs and virtual machine implementations. The implementation of the Atomos scheduler demonstrates the use of open nesting within the virtual machine and introduces the concept of transactional memory violation handlers that allow programs to recover from data \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:j3f4tGmQtD8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "The case for RAMClouds: scalable high-performance storage entirely in DRAM",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1713254.1713276",
            "Abstract": "Disk-oriented approaches to online storage are becoming increasingly problematic: they do not scale gracefully to meet the needs of large-scale Web applications, and improvements in disk capacity have far outstripped improvements in access latency and bandwidth. This paper argues for a new approach to datacenter storage called RAMCloud, where information is kept entirely in DRAM and large-scale systems are created by aggregating the main memories of thousands of commodity servers. We believe that RAMClouds can provide durable and available storage with 100-1000x the throughput of disk-based systems and 100-1000x lower access latency. The combination of low latency and large scale will enable a new breed of dataintensive applications.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:O3NaXMp0MMsC",
            "Publisher": "ACM"
        },
        {
            "Title": "Library-based prefetching for pointer-intensive applications",
            "Publication year": 2006,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.9667&rep=rep1&type=pdf",
            "Abstract": "Processor speed has been improving faster than memory latency for over two decades. Thus, an increased portion of execution time is spent stalling on loads, waiting for data from the memory hierarchy. Prefetching is an effective mechanism to hide memory latency for applications with low temporal locality. However, existing hardware prefetching techniques work well for array-based programs but not effective effective for pointer-intensive applications. Existing software prefetching techniques require recompilation or even modifications to application code before running on a new system with different hardware parameters. In this paper, we exploit two opportunities to address this challenge. First, chip-multiprocessors (CMPs) are quickly becoming the norm, providing spare processors for prefetching tasks. Second, productivity reasons encourage programmers to make frequent use of standard or popular data-structure libraries. Hence, we propose a novel software prefetching scheme for pointer-based data-structures in which prefetching is performed by a helper thread included in the data-structure library code. The prefetch thread runs on a spare processor and relies on the library\u2019s knowledge of the data-structure type and access pattern to perform effective prefetching. All the development effort for prefetching is concentrated in the library code and is done once. The user application is not modified at all, as the library API remains the same. We implement and evaluate the library-based prefetch scheme. We demonstrate that it performs accurate prefetching and leads to an average reduction in execution time of 26% for pointer-intensive \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:08ZZubdj9fEC",
            "Publisher": "Computer Systems Laboratory, Stanford University"
        },
        {
            "Title": "The Hot Chips Renaissance",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9042373/",
            "Abstract": "The articles in this special section report on the technologies and events that were part of the 31st Annual Hot Chips symposium was held at Stanford University in August 2019.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:U_HPUtbDl20C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Comparing memory systems for chip multiprocessors",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1273440.1250707",
            "Abstract": "There are two basic models for the on-chip memory in CMP systems:hardware-managed coherent caches and software-managed streaming memory. This paper performs a direct comparison of the two modelsunder the same set of assumptions about technology, area, and computational capabilities. The goal is to quantify how and when they differ in terms of performance, energy consumption, bandwidth requirements, and latency tolerance for general-purpose CMPs. We demonstrate that for data-parallel applications, the cache-based and streaming models perform and scale equally well. For certain applications with little data reuse, streaming scales better due to better bandwidth use and macroscopic software prefetching. However, the introduction of techniques such as hardware prefetching and non-allocating stores to the cache-based model eliminates the streaming advantage. Overall, our results indicate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:maZDTaKrznsC",
            "Publisher": "ACM"
        },
        {
            "Title": "Smart memories: A configurable processor architecture for high productivity parallel programming",
            "Publication year": 2005,
            "Publication url": "https://www.researchgate.net/profile/Alexandre-Solomatnikov/publication/228948700_Smart_memories_A_configurable_processor_architecture_for_high_productivity_parallel_programming/links/02e7e53b6f3ddc71bd000000/Smart-memories-A-configurable-processor-architecture-for-high-productivity-parallel-programming.pdf",
            "Abstract": "With single processor systems running into instruction-level parallelism (ILP) limits and fundamental VLSI constraints, multiprocessor chips provide a realistic path towards scalable performance by allowing one to take advantage of thread-level (TLP) and data-level parallelism (DLP) in emerging applications. Nevertheless, parallel architectures are limited by the difficulty of parallel application development. This challenge has led to the invention of new programming models to simplify the way in which parallel programs are developed correctly and efficiently. Smart Memories is a scalable, hierarchical architecture which, using a modular design, addresses the process technology issues, such as power consumption and wire latency. Its reconfigurability allows executing applications described in different programming models with high performance. Simulations have shown that considerable speed ups (2x to 10x) can be achieved over a broad range of applications, while a small amount of power and area penalty is tolerated for reconfiguration.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:5Ul4iDaHHb8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Convolution engine: balancing efficiency & flexibility in specialized computing",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2485922.2485925",
            "Abstract": "This paper focuses on the trade-off between flexibility and efficiency in specialized computing. We observe that specialized units achieve most of their efficiency gains by tuning data storage and compute structures and their connectivity to the data-flow and data-locality patterns in the kernels. Hence, by identifying key data-flow patterns used in a domain, we can create efficient engines that can be programmed and reused across a wide range of applications.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:zLWjf1WUPmwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Powernet: A magnifying glass for computing system energy",
            "Publication year": 2008,
            "Publication url": "http://171.64.64.194/events/posterslides/PowerNetAMagnifyingGlassforComputingSystemEnergy.pdf",
            "Abstract": "PowerNet: A Magnifying Glass for Computing System Energy Page 1 PowerNet: A Magnifying \nGlass for Computing System Energy http://powernet.stanford.edu Maria Kazandjieva, Brandon \nHeller, David Gal Philip Levis, Christos Kozyrakis, Nick McKeown Supported by: Stanford \nUniversity Microsoft External Research Initiative Problem: Electricity powers our everyday \ncomputing, yet we have no visibility into how this power is consumed. Detailed power usage \ndata can improve how we design, select, manage, and use our computing devices and \ninfrastructure. Solution: The goal of PowerNet is to provide data, together with utilization \nstatistics, to answer questions about total power usage, variation, and efficiency. We will release \nall the collected data and created tools back to the research community. PowerNet is a platform \nfor collecting, viewing, and analyzing plug-level power data collected from both wired and in Hall-\u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:sSrBHYA8nusC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Optimizing memory transactions for multicore systems",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-1-4419-0263-4_5",
            "Abstract": "The shift to multicore architectures will require new programming technologies that enable mainstream developers to write parallel programs that can safely take advantage of the parallelism offered by multicore processors. One challenging aspect of shared memory parallel programming is synchronization. Programmers have traditionally used locks for synchronization, but lock-based synchronization has well-known pitfalls that make it hard to use for building thread-safe and scalable software components. Memory transactions have emerged as a promising alternative to lock-based synchronization because they promise to eliminate many of the problems associated with locks. Transactional programming constructs, however, have overheads and require optimizations to make them practical. Transactions can also benefit significantly from hardware support, and multicore processors with their large \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:d1gkVwhDpl0C",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "Characterization of TCC on chip-multiprocessors",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1515581/",
            "Abstract": "Transactional coherence and consistency (TCC) is a novel coherence scheme for shared memory multiprocessors that uses programmer-defined transactions as the fundamental unit of parallel work, synchronization, coherence, and consistency. TCC has the potential to simplify parallel program development and optimization by providing a smooth transition from sequential to parallel programs. In this paper, we study the implementation of TCC on chip-multiprocessors (CMPs). We explore design alternatives such as the granularity of state tracking, double-buffering, and write-update and write-invalidate protocols. Furthermore, we characterize the performance of TCC in comparison to conventional snoopy cache coherence (SCC) using parallel applications optimized for each scheme. We conclude that the two coherence schemes perform similarly, with each scheme having a slight advantage for some applications \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:blknAaTinKkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics Pipelines",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2102.01887",
            "Abstract": "The proliferation of camera-enabled devices and large video repositories has given rise to a diverse set of video analytics applications. The video pipelines for these applications are DAGs of operations that transform videos, process extracted metadata, and answer questions such as, \"Is this intersection congested?\" The latency and resource efficiency of pipelines can be optimized using configurable knobs for each operation such as the sampling rate, batch size, or type of hardware used. However, determining efficient configurations is challenging because (a) the configuration search space is exponentially large, and (b) the optimal configuration depends on the desired latency target and the input video contents that may exercise different paths in the DAG and produce different volumes of intermediate results. Existing video analytics and processing systems leave it to the users to manually configure operations and select hardware resources. Hence, we observe that they often execute inefficiently and fail to meet latency and cost targets. We present Llama: a heterogeneous and serverless framework for auto-tuning video pipelines. Llama optimizes the overall video pipeline latency by (a) dynamically calculating latency targets per-operation invocation, and (b) dynamically running a cost-based optimizer to determine efficient configurations that meet the target latency for each invocation. This makes the problem of auto-tuning large video pipelines tractable and allows us to handle input dependent behavior, conditional branches in the DAG, and execution variability. We describe the algorithms in Llama and evaluate it on a cloud platform using \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:raTqNPD5sRQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Understanding ephemeral storage for serverless analytics",
            "Publication year": 2018,
            "Publication url": "https://www.usenix.org/conference/atc18/presentation/klimovic-serverless",
            "Abstract": "Serverless computing frameworks allow users to launch thousands of concurrent tasks with high elasticity and fine-grain resource billing without explicitly managing computing resources. While already successful for IoT and web microservices, there is increasing interest in leveraging serverless computing to run data-intensive jobs, such as interactive analytics. A key challenge in running analytics workloads on serverless platforms is enabling tasks in different execution stages to efficiently communicate data between each other via a shared data store. In this paper, we explore the suitability of different cloud storage services (eg, object stores and distributed caches) as remote storage for serverless analytics. Our analysis leads to key insights to guide the design of an ephemeral cloud storage system, including the performance and cost efficiency of Flash storage for serverless application requirements and the need for a pay-what-you-use storage service that can support the high throughput demands of highly parallel applications.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:jgBuDB5drN8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Accurate modeling and generation of storage i/o for datacenter workloads",
            "Publication year": 2011,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.1900&rep=rep1&type=pdf",
            "Abstract": "Tools that confidently recreate I/O workloads have become a critical requirement in designing efficient storage systems for datacenters (DCs), since potential inefficiencies get aggregated over several thousand servers. Designing performance, power and cost optimized systems requires a deep understanding of target workloads, and mechanisms to effectively model different design choices. Traditional benchmarking is invalid in cloud data-stores, representative storage profiles are hard to obtain, while replaying the entire application in all storage configurations is impractical. Despite these issues, current workload generators are not comprehensive enough to accurately reproduce key aspects of real application patterns. Some of these features include spatial and temporal locality, as well as tuning the intensity of the workload to emulate different storage system behaviors. To address these limitations, we use a state diagram-based storage model, extend it to a hierarchical representation and implement a tool that consistently recreates I/O loads of DC applications. We present the design of the tool and the validation process performed against six original DC applications traces. We explore the practical applications of this methodology in two important storage challenges-1) SSD caching and 2) defragmentation benefits on enterprise storage. In both cases we observe significant storage speedup for most of the DC applications. Since knowledge of the workload\u2019s spatial locality is necessary to model these use cases, our tool was instrumental in quantifying their performance benefits.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:eJXPG6dFmWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Transactional programming in a multi-core environment",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1229428.1229484",
            "Abstract": "With single thread performance starting to plateau, HW architects have turned to chip level multiprocessing (CMP) to increase processing power. All major microprocessor companies are aggressively shipping multi-core products in the mainstream computing market. Moore's law will largely be used to increase HW thread-level parallelism through higher core counts in a CMP environment. CMPs bring new challenges into the design of the software system stack.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:dshw04ExmUIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Nemesis: Preventing Authentication & [and] Access Control Vulnerabilities in Web Applications",
            "Publication year": 2009,
            "Publication url": "https://www.usenix.org/legacy/event/sec09/tech/full_papers/dalton.pdf",
            "Abstract": "This paper presents Nemesis, a novel methodology for mitigating authentication bypass and access control vulnerabilities in existing web applications. Authentication attacks occur when a web application authenticates users unsafely, granting access to web clients that lack the appropriate credentials. Access control attacks occur when an access control check in the web application is incorrect or missing, allowing users unauthorized access to privileged resources such as databases and files. Such attacks are becoming increasingly common, and have occurred in many high-profile applications, such as IIS [10] and WordPress [31], as well as 14% of surveyed web sites [30]. Nevertheless, none of the currently available tools can fully mitigate these attacks. Nemesis automatically determines when an application safely and correctly authenticates users, by using Dynamic Information Flow Tracking (DIFT) techniques to track the flow of user credentials through the application\u2019s language runtime. Nemesis combines authentication information with programmer-supplied access control rules on files and database entries to automatically ensure that only properly authenticated users are granted access to any privileged resources or data. A study of seven popular web applications demonstrates that a prototype of Nemesis is effective at mitigating attacks, requires little programmer effort, and imposes minimal runtime overhead. Finally, we show that Nemesis can also improve the precision of existing security tools, such as DIFT analyses for SQL injection prevention, by providing runtime information about user authentication.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:3s1wT3WcHBgC",
            "Publisher": "USENIX Association"
        },
        {
            "Title": "FARM: A prototyping environment for tightly-coupled, heterogeneous architectures",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5474046/",
            "Abstract": "Computer architectures are increasingly turning to parallelism and heterogeneity as solutions for boosting performance in the face of power constraints. As this trend continues, the challenges of simulating and evaluating these architectures have grown. Hardware prototypes provide deeper insight into these systems when compared to simulators, but are traditionally more difficult and costly to build. We present the Flexible Architecture Research Machine (FARM), a hardware prototyping system based on an FPGA coherently connected to a multiprocessor system. FARM substantially reduces the difficulty and cost of building hardware prototypes by providing a ready-made framework for communicating with a custom design on the FPGA. FARM ensures efficient, low-latency communication with the FPGA via a variety of mechanisms, allowing a wide range of applications to effectively utilize the system. FARM's \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:UxriW0iASnsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A case of system-level hardware/software co-design and co-verification of a commodity multi-processor system with custom hardware",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2380445.2380524",
            "Abstract": "This paper presents an interesting system-level co-design and co-verification case study for a non-trivial design where multiple high-performing x86 processors and custom hardware were connected through a coherent interconnection fabric. In functional verification of such a system, we used a processor bus functional model (BFM) to combine native software execution with a cycle-accurate interconnect simulator and an HDL simulator. However, we found that significant extensions need to be made to the conventional BFM methodology in order to capture various data-race cases in simulation, which eventually happen in modern multi-processor systems. Especially essential were faithful implementations of the memory consistency model and cache coherence protocol, as well as timing randomization. We demonstrate how such a co-simulation environment can be constructed from existing tools and software \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:1yQoGdGgb4wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "{IX}: A Protected Dataplane Operating System for High Throughput and Low Latency",
            "Publication year": 2014,
            "Publication url": "https://www.usenix.org/conference/osdi14/technical-sessions/presentation/belay",
            "Abstract": "The conventional wisdom is that aggressive networking requirements, such as high packet rates for small messages and microsecond-scale tail latency, are best addressed outside the kernel, in a user-level networking stack. We present IX, a dataplane operating system that provides high I/O performance, while maintaining the key advantage of strong protection offered by existing kernels. IX uses hardware virtualization to separate management and scheduling functions of the kernel (control plane) from network processing (dataplane). The dataplane architecture builds upon a native, zero-copy API and optimizes for both bandwidth and latency by dedicating hardware threads and networking queues to dataplane instances, processing bounded batches of packets to completion, and by eliminating coherence traffic and multi-core synchronization. We demonstrate that IX outperforms Linux and state-of-the-art, user-space network stacks significantly in both throughput and end-to-end latency. Moreover, IX improves the throughput of a widely deployed, key-value store by up to 3.6 and reduces tail latency by more than 2.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:i2xiXl-TujoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scalable vector media-processors for embedded systems",
            "Publication year": 2002,
            "Publication url": "https://search.proquest.com/openview/d6b287268409cd7ac841eb5cd90a6dc3/1?pq-origsite=gscholar&cbl=18750&diss=y",
            "Abstract": "Over the past twenty years, processor designers have concentrated on superscalar and VLIW architectures that exploit the instruction-level parallelism (ILP) available in engineering applications for workstation systems. Recently, however, the focus in computing has shifted from engineering to multimedia applications and from workstations to embedded systems. In this new computing environment, the performance, energy consumption, and development cost of ILP processors renders them ineffective despite their theoretical generality.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:hqOjcs7Dif8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bolt: I know what you did last summer... in the cloud",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3093337.3037703",
            "Abstract": "Cloud providers routinely schedule multiple applications per physical host to increase efficiency. The resulting interference on shared resources often leads to performance degradation and, more importantly, security vulnerabilities. Interference can leak important information ranging from a service's placement to confidential data, like private keys. We present Bolt, a practical system that accurately detects the type and characteristics of applications sharing a cloud platform based on the interference an adversary sees on shared resources. Bolt leverages online data mining techniques that only require 2-5 seconds for detection. In a multi-user study on EC2, Bolt correctly identifies the characteristics of 385 out of 436 diverse workloads. Extracting this information enables a wide spectrum of previously-impractical cloud attacks, including denial of service attacks (DoS) that increase tail latency by 140x, as well as \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:HeT0ZceujKMC",
            "Publisher": "ACM"
        },
        {
            "Title": "A new frontier for pull-based graph processing",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1903.07754",
            "Abstract": "The trade-off between pull-based and push-based graph processing engines is well-understood. On one hand, pull-based engines can achieve higher throughput because their workloads are read-dominant, rather than write-dominant, and can proceed without synchronization between threads. On the other hand, push-based engines are much better able to take advantage of the frontier optimization, which leverages the fact that often only a small subset of the graph needs to be accessed to complete an iteration of a graph processing application. Hybrid engines attempt to overcome this trade-off by dynamically switching between push and pull, but there are two key disadvantages with this approach. First, applications must be implemented twice (once for push and once for pull), and second, processing throughput is reduced for iterations that run with push. We propose a radically different solution: rebuild the frontier optimization entirely such that it is well-suited for a pull-based engine. In doing so, we remove the only advantage that a push-based engine had over a pull-based engine, making it possible to eliminate the push-based engine entirely. We introduce Wedge, a pull-only graph processing framework that transforms the traditional source-oriented vertex-based frontier into a pull-friendly format called the Wedge Frontier. The transformation itself is expensive even when parallelized, so we introduce two key optimizations to make it practical. First, we perform the transformation only when the resulting Wedge Frontier is sufficiently sparse. Second, we coarsen the granularity of the representation of elements in the Wedge Frontier. These \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:prdVHNxh-e8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sustainable Computing: Informatics and Systems",
            "Publication year": 2013,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.2461&rep=rep1&type=pdf",
            "Abstract": "Until now, green computing research has largely relied on few, short-term power measurements to characterize the energy use of enterprise computing. This paper brings new and comprehensive power datasets through Powernet, a hybrid sensor network that monitors the power and utilization of the IT systems in a large academic building. Over more than two years, we have collected power data from 250+ individual computing devices and have monitored a subset of CPU and network loads. This dense, long-term monitoring allows us to extrapolate the data to a detailed breakdown of electricity use across the building\u2019s computing systems.Our datasets provide an opportunity to examine data analysis and methodology techniques used in green computing research. We show that power variability both between similar devices and over time for a single device can lead to cost or savings estimates that are off by 15\u201320%. Extending the coverage of measured devices and the duration (to at least one month) significantly reduces errors. Lastly, our experiences with collecting data and the subsequent analysis lead to a better understanding of how one should go about power characterization studies. We provide several methodology guidelines for the green computing community. The data from the Powernet deployment can be found at http://sing. stanford. edu/maria/powernet.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:0KyAp5RtaNEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Polystore Based Database Operating System (DBOS)",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-71055-2_1",
            "Abstract": "Current operating systems are complex systems that were designed before today\u2019s computing environments. This makes it difficult for them to meet the scalability, heterogeneity, availability, and security challenges in current cloud and parallel computing environments. To address these problems, we propose a radically new OS design based on data-centric architecture: all operating system state should be represented uniformly as database tables, and operations on this state should be made via queries from otherwise stateless tasks. This design makes it easy to scale and evolve the OS without whole-system refactoring, inspect and debug system state, upgrade components without downtime, manage decisions using machine learning, and implement sophisticated security features. We discuss how a database OS (DBOS) can improve the programmability and performance of many of today\u2019s most \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:kzcSZmkxUKAC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "Understanding sources of inefficiency in general-purpose chips",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1815961.1815968",
            "Abstract": "Due to their high volume, general-purpose processors, and now chip multiprocessors (CMPs), are much more cost effective than ASICs, but lag significantly in terms of performance and energy efficiency. This paper explores the sources of these performance and energy overheads in general-purpose processing systems by quantifying the overheads of a 720p HD H. 264 encoder running on a general-purpose CMP system. It then explores methods to eliminate these overheads by transforming the CPU into a specialized system for H. 264 encoding. We evaluate the gains from customizations useful to broad classes of algorithms, such as SIMD units, as well as those specific to particular computation, such as customized storage and functional units.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:YsMSGLbcyi4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Qos-aware scheduling in heterogeneous datacenters with paragon",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2556583",
            "Abstract": "Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty of matching applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online, and do not scale beyond a few applications.We present Paragon, an online and scalable DC scheduler that is heterogeneity- and interference-aware. Paragon is derived from robust analytical methods, and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:4MWp96NkSFoC",
            "Publisher": "ACM"
        },
        {
            "Title": "Automatic generation of efficient accelerators for reconfigurable hardware",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7551387/",
            "Abstract": "Acceleration in the form of customized datapaths offer large performance and energy improvements over general purpose processors. Reconfigurable fabrics such as FPGAs are gaining popularity for use in implementing application-specific accelerators, thereby increasing the importance of having good high-level FPGA design tools. However, current tools for targeting FPGAs offer inadequate support for high-level programming, resource estimation, and rapid and automatic design space exploration. We describe a design framework that addresses these challenges. We introduce a new representation of hardware using parameterized templates that captures locality and parallelism information at multiple levels of nesting. This representation is designed to be automatically generated from high-level languages based on parallel patterns. We describe a hybrid area estimation technique which uses template-level \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:4hFrxpcac9AC",
            "Publisher": "Ieee"
        },
        {
            "Title": "Bolt: Uncovering and Reducing the Security Vulnerabilities of Shared Clouds",
            "Publication year": 2016,
            "Publication url": "http://web.stanford.edu/group/mast/cgi-bin/drupal/system/files/Bolt_techreport16.pdf",
            "Abstract": "Cloud providers routinely schedule multiple applications per physical host to increase cost efficiency. The resulting interference in shared resources leads to performance degradation and, more importantly, security vulnerabilities. Interference can leak important information ranging from the placement of a service to confidential data, like private keys. We present Bolt, a practical system that accurately detects the type and characteristics of applications sharing a cloud platform, based on the interference an adversary sees on shared resources. Bolt leverages practical data mining techniques for detection that operate online and require 2-5 seconds. In a 40-server shared cluster, Bolt correctly detects 81% out of 108 diverse batch and interactive workloads. Extracting this information enables a wide spectrum of previouslyimpractical cloud attacks, including denial of service (DoS), resource freeing (RFA) and co-residency attacks. For example, Bolt can successfully launch difficult to detect, host-based DoS attacks, with only a fraction of the resources and time needed by a conventional distributed DoS that cause the tail latency of the victim to increase by up to 140x. Finally, we show that, while advanced isolation techniques, such as cache partitioning, lower detection accuracy, they are insufficient to eliminate these vulnerabilities. To do so, one must either disallow core sharing, or only allow it between threads of the same application, leading to significant inefficiencies and performance penalties respectively.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:OP4eGU-M3BUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Models and metrics to enable energy-efficiency optimizations",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4404808/",
            "Abstract": "Power consumption and energy efficiency are important factors in the initial design and day-to-day management of computer systems. Researchers and system designers need benchmarks that characterize energy efficiency to evaluate systems and identify promising new technologies. To predict the effects of new designs and configurations, they also need accurate methods of modeling power consumption.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:ldfaerwXgEUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Theme Articles",
            "Publication year": 2020,
            "Publication url": "https://www.computer.org/csdl/magazine/mi/2020/02/09042379/1ika5tWGlfq",
            "Abstract": "Table of Contents IEEE.org Help Cart Jobs Board Create Account Toggle navigation IEEE \nComputer Society Digital Library Jobs Tech News Resource Center Press Room Browse By \nDate Advertising About Us IEEE IEEE Computer Society IEEE Computer Society Digital Library \nMy Subscriptions Magazines Journals Conference Proceedings Institutional Subscriptions \nIEEE IEEE Computer Society More Jobs Tech News Resource Center Press Room Browse By \nDate Advertising About Us Cart Advanced Search IEEE Micro IEEE Micro Home Current Issue \nEarly Access About Editorial Board Advisory Board Staff History Author Information Calls for \nPapers Peer Review Information Submit a Manuscript IEEE DataPort IEEE Micro Download \n1.Home 2.magazines 3.IEEE Micro 4.Table of Contents 2020, pp. 2-3, vol. 40 DOI Bookmark: \n10.1109/MM.Keywords Authors Abstract Presents the table of contents for this issue of the ,\u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:NDuN12AVoxsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The common case transactional behavior of multithreaded programs",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1598135/",
            "Abstract": "Transactional memory (TM) provides an easy-to-use and high-performance parallel programming model for the upcoming chip-multiprocessor systems. Several researchers have proposed alternative hardware and software TM implementations. However, the lack of transaction-based programs makes it difficult to understand the merits of each proposal and to tune future TM implementations to the common case behavior of real application. This work addresses this problem by analyzing the common case transactional behavior for 35 multithreaded programs from a wide range of application domains. We identify transactions within the source code by mapping existing primitives for parallelism and synchronization management to transaction boundaries. The analysis covers basic characteristics such as transaction length, distribution of read-set and write-set size, and the frequency of nesting and I/O operations. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:TFP_iSt0sucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Software or hardware: The future of green enterprise computing",
            "Publication year": 2011,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.366.5840&rep=rep1&type=pdf",
            "Abstract": "Over the last few years, interest in \u201cgreen computing\u201d has motivated research into energy-saving techniques for enterprise systems, from network proxies and virtual machine migration to the return of thin clients. This paper tries to answer a possibly contentious question: would we be better served by the embarrassingly simple approach of replacing every desktop with a laptop? To answer this question, we use power and utilization data collected from more than 100 devices over durations up to 15 months. We find that choosing the right computing systems\u2013laptops\u2013would save more energy than stateof-the-art power management software or thin clients. Furthermore, the marginal savings of applying software techniques on top of laptops is so small that it is probably not worth the trouble.When selecting computers, there are many other considerations than just energy, such as computational resources, and price. We find that these factors generally do not reduce the attractiveness of a laptop-based enterprise. We discuss current trends in enterprises today, and how our conclusions might affect their directions, sketching a future of how a cost-efficient enterprise might become a hybrid system entwining laptops and tablets with a computing cloud.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:UeHWp8X0CEIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Transactional memory coherence and consistency",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1028176.1006711",
            "Abstract": "In this paper, we propos a new shared memory model: Transactionalmemory Coherence and Consistency (TCC).TCC providesa model in which atomic transactions are always the basicunit of parallel work, communication, memory coherence, andmemory reference consistency.TCC greatly simplifies parallelsoftware by eliminating the need for synchronization using conventionallocks and semaphores, along with their complexities.TCC hardware must combine all writes from each transaction regionin a program into a single packet and broadcast this packetto the permanent shared memory state atomically as a large block.This simplifies the coherence hardware because it reduces theneed for small, low-latency messages and completely eliminatesthe need for conventional snoopy cache coherence protocols, asmultiple speculatively written versions of a cache line may safelycoexist within the system.Meanwhile \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:R3hNpaxXUhUC",
            "Publisher": "ACM"
        },
        {
            "Title": "Towards soft optimization techniques for parallel cognitive applications",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/pdf/10.1145/1248377.1248389",
            "Abstract": "The world\u2019s data is growing at exponential rates. To deal with this glut of information, computer systems must be able to understand and interpret data in ways that provide their users with practical knowledge [1]. To process ever increasing data sets, often, within real-time constraints, cognitive applications are stressing the performance, memory bandwidth, and storage capabilities of modern computers. Uniprocessor systems cannot meet the performance requirements even for relatively small data sets [3]. Parallel computers such as multi-core systems or larger-scale shared memory multiprocessors can provide the scalable performance necessary. In theory, cognitive tasks should map well on parallel systems as each node can process a subset of the input data or operate on a portion of the current state of knowledge. In practice, however, there are many issues that can lead to sub-optimal scalability for cognitive \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:1sJd4Hv_s6UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Building and using the atlas transactional memory system",
            "Publication year": 2006,
            "Publication url": "http://csl.stanford.edu/~christos/publications/2006.atlas.warfp.pdf",
            "Abstract": "At WARFP 2005, we proposed ATLAS as a scalable implementation for transactional parallel systems [5]. The impetus for the development of ATLAS is to address the significant hurdles that software simulators face in multiprocessor architectural research. In particular, ATLAS is an FPGA-based system that primarily serves as a rapid software development platform for our transactional memory model, TCC [4]. Leveraging commodity hardware and software tools, ATLAS is poised to support research exploring the impact of transactional memory on architecture, operating systems, compilers and programming models.To date, we have implemented a fully functional 2-processor ATLAS system on Xilinx\u2019s XUP board [8], employing the two embedded PowerPC cores in the Virtex II Pro FPGA. To interface with the TCC caches in the FPGA fabric, we have also implemented the TCC API [3]. Additionally, we have developed infrastructure to support debugging and profiling with the aim of increasing visibility and ease of performance tuning of applications.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:CHSYGLWDkRkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The stanford pervasive parallelism lab",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7478358/",
            "Abstract": "Presents a collection of slides covering the following topics: pervasive parallelism laboratory; parallel programming; PPL vision; PPL applications; virtual worlds; domain specific languages; Liszt DSL; Liszt Code; DSL infrastructure; parallel object language; Delite parallel runtime; hardware architecture; heterogeneous hardware; and fine-grain parallelism.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:sNmaIFBj_lkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Tainting is not pointless",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1773912.1773933",
            "Abstract": "Pointer tainting is a form of Dynamic Information Flow Tracking used primarily to prevent software security attacks such as buffer overflows. Researchers have also applied pointer tainting to malware and virus analysis.A recent paper by Slowinska and Bos has criticized pointer tainting as a security mechanism, arguing that it is has serious, inherent false positive and false negative defects. We present a rebuttal that addresses the confusion due to the two uses of pointer tainting in security literature. We clarify that many of the arguments against pointer tainting apply only to its use as a malware and virus analysis platform, but do not apply to the application of pointer tainting to memory corruption protection. Hence, we argue that pointer tainting remains a useful and promising technique for robust protection against memory corruption attacks.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:OU6Ihb5iCvQC",
            "Publisher": "ACM"
        },
        {
            "Title": "Automatic power management schemes for internet servers and data centers",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1577776/",
            "Abstract": "We investigate autonomic power control policies for Internet servers and data centers. In particular, by monitoring the system load and thermal status, we decide how to vary the utilized processing resources to achieve acceptable delay and power performance. We formulate the problem using a dynamic programming approach that captures the power-performance tradeoff. We study the structural properties of the optimal solution and develop low-complexity justified heuristics, which achieve significant performance gains over standard benchmarks. The performance gains are higher when the load exhibits stronger temporal variations. We also demonstrate that the heuristics are very efficient, in the sense that they perform very close to the optimal solution obtained via dynamic programming.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:g5m5HwL7SMYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Decoupling dynamic information flow tracking with a dedicated coprocessor",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5270347/",
            "Abstract": "Dynamic information flow tracking (DIFT) is a promising security technique. With hardware support, DIFT prevents a wide range of attacks on vulnerable software with minimal performance impact. DIFT architectures, however, require significant changes in the processor pipeline that increase design and verification complexity and may affect clock frequency. These complications deter hardware vendors from supporting DIFT. This paper makes hardware support for DIFT cost effective by decoupling DIFT functionality onto a simple, separate coprocessor. Decoupling is possible because DIFT operations and regular computation need only synchronize on system calls. The coprocessor is a small hardware engine that performs logical operations and caches 4-bit tags. It introduces no changes to the design or layout of the main processor's logic, pipeline, or caches, and can be combined with various processors. Using a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:yD5IFk8b50cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Memory hierarchy for web search",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8327044/",
            "Abstract": "Online data-intensive services, such as search, serve billions of users, utilize millions of cores, and comprise a significant and growing portion of datacenter-scale workloads. However, the complexity of these workloads and their proprietary nature has precluded detailed architectural evaluations and optimizations of processor design trade-offs. We present the first detailed study of the memory hierarchy for the largest commercial search engine today. We use a combination of measurements from longitudinal studies across tens of thousands of deployed servers, systematic microarchitectural evaluation on individual platforms, validated trace-driven simulation, and performance modeling - all driven by production workloads servicing real-world user requests. Our data quantifies significant differences between production search and benchmarks commonly used in the architecture community. We identify the memory \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:F1b5ZUV5XREC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Towards energy proportionality for large-scale latency-critical workloads",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6853237/",
            "Abstract": "Reducing the energy footprint of warehouse-scale computer (WSC) systems is key to their affordability, yet difficult to achieve in practice. The lack of energy proportionality of typical WSC hardware and the fact that important workloads (such as search) require all servers to remain up regardless of traffic intensity renders existing power management techniques ineffective at reducing WSC energy use. We present PEGASUS, a feedback-based controller that significantly improves the energy proportionality of WSC systems, as demonstrated by a real implementation in a Google search cluster. PEGASUS uses request latency statistics to dynamically adjust server power management limits in a fine-grain manner, running each server just fast enough to meet global service-level latency objectives. In large cluster experiments, PEGASUS reduces power consumption by up to 20%. We also estimate that a distributed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:-FonjvnnhkoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "RAMBO: Resource Allocation for Microservices Using Bayesian Optimization",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9380428/",
            "Abstract": "Microservices are becoming the defining paradigm of cloud applications, which raises urgent challenges for efficient datacenter management. Guaranteeing end-to-end Service Level Agreement (SLA) while optimizing resource allocation is critical to both cloud service providers and users. However, one application may contain hundreds of microservices, which constitute an enormous search space that is unfeasible to explore exhaustively. Thus, we propose RAMBO, an SLA-aware framework for microservices that leverages multi-objective Bayesian Optimization (BO) to allocate resources and meet performance/cost goals. Experiments conducted on a real microservice workload demonstrate that RAMBO can correctly characterize each microservice and efficiently discover Pareto-optimal solutions. We envision that the proposed methodology and results will benefit future resource planning, cluster orchestration \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:5bg8sr1QxYwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Welcome to Hot Chips 24",
            "Publication year": 2012,
            "Publication url": "https://www.infona.pl/resource/bwmeta1.element.ieee-art-000007476477",
            "Abstract": "Presents the opening message from the Hot Chips 24 Proceedings that was held 27-29 August 2012, in Cupertino, CA. Also includes conference program committee members, program statistics, keynote speakers, conference tutorials, conference proceedings, conference etiquette, and persons named in memoriam.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:WJVC3Jt7v1AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the energy (in) efficiency of hadoop clusters",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1740390.1740405",
            "Abstract": "Distributed processing frameworks, such as Yahoo!'s Hadoop and Google's MapReduce, have been successful at harnessing expansive datacenter resources for large-scale data analysis. However, their effect on datacenter energy efficiency has not been scrutinized. Moreover, the filesystem component of these frameworks effectively precludes scale-down of clusters deploying these frameworks (i.e. operating at reduced capacity). This paper presents our early work on modifying Hadoop to allow scale-down of operational clusters. We find that running Hadoop clusters in fractional configurations can save between 9% and 50% of energy consumption, and that there is a tradeoff between performance energy consumption. We also outline further research into the energy-efficiency of these frameworks.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:BqipwSGYUEgC",
            "Publisher": "ACM"
        },
        {
            "Title": "Interstellar: Using halide's scheduling language to analyze dnn accelerators",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3373376.3378514",
            "Abstract": "We show that DNN accelerator micro-architectures and their program mappings represent specific choices of loop order and hardware parallelism for computing the seven nested loops of DNNs, which enables us to create a formal taxonomy of all existing dense DNN accelerators. Surprisingly, the loop transformations needed to create these hardware variants can be precisely and concisely represented by Halide's scheduling language. By modifying the Halide compiler to generate hardware, we create a system that can fairly compare these prior accelerators. As long as proper loop blocking schemes are used, and the hardware can support mapping replicated loops, many different hardware dataflows yield similar energy efficiency with good performance. This is because the loop blocking can ensure that most data references stay on-chip with good locality and the processing units have high resource utilization \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:-jrNzM816MMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving software concurrency with hardware-assisted memory snapshot",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1378533.1378596",
            "Abstract": "We propose a hardware-assisted memory snapshot to improve software concurrency. It is built on top of the hardware resources for transactional memory and allows for easy development of system software modules such as concurrent garbage collector and dynamic profiler.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:LPZeul_q3PIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Flash storage disaggregation",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2901318.2901337",
            "Abstract": "PCIe-based Flash is commonly deployed to provide datacenter applications with high IO rates. However, its capacity and bandwidth are often underutilized as it is difficult to design servers with the right balance of CPU, memory and Flash resources over time and for multiple applications. This work examines Flash disaggregation as a way to deal with Flash overprovisioning. We tune remote access to Flash over commodity networks and analyze its impact on workloads sampled from real datacenter applications. We show that, while remote Flash access introduces a 20% throughput drop at the application level, disaggregation allows us to make up for these overheads through resource-efficient scale-out. Hence, we show that Flash disaggregation allows scaling CPU and Flash resources independently in a cost effective manner. We use our analysis to draw conclusions about data and control plane issues in remote \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:a9-T7VOCCH8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Hardware/compiler codevelopment for an embedded media processor",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/964446/",
            "Abstract": "Embedded and portable systems running multimedia applications create a new challenge for hardware architects. A microprocessor for such applications needs to be easy to program like a general-purpose processor and have the performance and power efficiency of a digital signal processor. This paper presents the codevelopment of the instruction set, the hardware, and the compiler for the Vector IRAM media processor. A vector architecture is used to exploit the data parallelism of multimedia programs, which allows the use of highly modular hardware and enables implementations that combine high performance, low power consumption, and reduced design complexity. It also leads to a compiler model that is efficient both in terms of performance and executable code size. The memory system for the vector processor is implemented using embedded DRAM technology, which provides high bandwidth in an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:3fE2CSJIrl8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The OpenTM transactional application programming interface",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4336227/",
            "Abstract": "Transactional Memory (TM) simplifies parallel programming by supporting atomic and isolated execution of user-identified tasks. To date, TM programming has re quired the use of libraries that make it difficult to achieve scalable performance with code that is easy to develop and maintain. For TM programming to become practical, it is important to integrate TM into familiar, high-level environments for parallel programming. This paper presents OpenTM, an application programming interface (API) for parallel programming with transactions. OpenTM extends OpenMP, a widely used API for shared-memory parallel programming, with a set of compiler directives to express non-blocking synchronization and speculative parallelization based on memory transactions. We also present a portable OpenTM implementation that produces code for hardware, software, and hybrid TM systems. The implementation builds upon the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:GnPB-g6toBAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Persona: A high-performance bioinformatics framework",
            "Publication year": 2017,
            "Publication url": "https://www.usenix.org/conference/atc17/technical-sessions/presentation/byma",
            "Abstract": "Next-generation genome sequencing technology has reached a point at which it is becoming cost-effective to sequence all patients. Biobanks and researchers are faced with an oncoming deluge of genomic data, whose processing requires new and scalable bioinformatics architectures and systems. Processing raw genetic sequence data is computationally expensive and datasets are large. Current software systems can require many hours to process a single genome and generally run only on a single computer. Common file formats are monolithic and row-oriented, a barrier to distributed computation.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:ZzlSgRqYykMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fast memory snapshot for concurrent programming without synchronization",
            "Publication year": 2009,
            "Publication url": "https://scholarworks.unist.ac.kr/handle/201301/46840",
            "Abstract": "The industry-wide turn toward chip-multiprocessors (CMPs) provides an increasing amount of parallel resources for commodity systems. However, it is still difficult to harness the available parallelism in user applications and system software code. We propose MShot, a hardware-assisted memory snapshot for concurrent programming without synchronization code. It supports atomic multi-word read operations on a large dataset. Since modern processors support atomic access only to a single word, programmers should add synchronization code to process a multiword dataset concurrently in multithreading environment. With snapshot, programmers read the dataset atomically and process the snapshot image without synchronization code. We implement MShot using hardware resources for transactional memory and reduce the storage overhead from 2.98% to 0.07%. To demonstrate the usefulness of fast snapshot, we use MShot to implement concurrent versions of garbage collection and call-path profiling. Without the need for synchronization code, MShot allows such system services to run in parallel with user applications on spare cores in CMP systems. As a result, the overhead of these services is minimized, approaching that of an ideal implementation. Copyright 2009 ACM.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:mUJArPsKIAAC",
            "Publisher": "23rd International Conference on Supercomputing, ICS'09"
        },
        {
            "Title": "INFaaS: A Model-less and Managed Inference Serving System",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1905.13348",
            "Abstract": "Despite existing work in machine learning inference serving, ease-of-use and cost efficiency remain challenges at large scales. Developers must manually search through thousands of model-variants -- versions of already-trained models that differ in hardware, resource footprints, latencies, costs, and accuracies -- to meet the diverse application requirements. Since requirements, query load, and applications themselves evolve over time, these decisions need to be made dynamically for each inference query to avoid excessive costs through naive autoscaling. To avoid navigating through the large and complex trade-off space of model-variants, developers often fix a variant across queries, and replicate it when load increases. However, given the diversity across variants and hardware platforms in the cloud, a lack of understanding of the trade-off space can incur significant costs to developers. This paper introduces INFaaS, a managed and model-less system for distributed inference serving, where developers simply specify the performance and accuracy requirements for their applications without needing to specify a specific model-variant for each query. INFaaS generates model-variants, and efficiently navigates the large trade-off space of model-variants on behalf of developers to meet application-specific objectives: (a) for each query, it selects a model, hardware architecture, and model optimizations, (b) it combines VM-level horizontal autoscaling with model-level autoscaling, where multiple, different model-variants are used to serve queries within each machine. By leveraging diverse variants and sharing hardware resources across models \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:DkZNVXde3BIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Towards energy-proportional datacenter memory with mobile DRAM",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6237004/",
            "Abstract": "To increase datacenter energy efficiency, we need memory systems that keep pace with processor efficiency gains. Currently, servers use DDR3 memory, which is designed for high bandwidth but not for energy proportionality. A system using 20% of the peak DDR3 bandwidth consumes 2.3\u00d7 the energy per bit compared to the energy consumed by a system with fully utilized memory bandwidth. Nevertheless, many datacenter applications stress memory capacity and latency but not memory bandwidth. In response, we architect server memory systems using mobile DRAM devices, trading peak bandwidth for lower energy consumption per bit and more efficient idle modes. We demonstrate 3-5\u00d7 lower memory power, better proportionality, and negligible performance penalties for data-center workloads.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:5awf1xo2G04C",
            "Publisher": "IEEE"
        },
        {
            "Title": "ibench: Quantifying interference for datacenter applications",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6704667/",
            "Abstract": "Interference between co-scheduled applications is one of the major reasons that causes modern datacenters (DCs) to operate at low utilization. DC operators traditionally side-step interference either by disallowing colocation altogether and providing isolated server instances, or by requiring the users to express resource reservations, which are often exaggerated to counter-balance the unpredictability in the quality of allocated resources. Understanding, reducing and managing interference can significantly impact the manner in which these large-scale systems operate. We present iBench, a novel workload suite that helps quantify the pressure different applications put in various shared resources, and similarly the pressure they can tolerate in these resources. iBench consists of a set of carefully-crafted benchmarks that induce interference of increasing intensity in resources that span the CPU, cache hierarchy \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:ML0RJ9NH7IQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Energy dumpster diving",
            "Publication year": 2009,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.423.4688&rep=rep1&type=pdf",
            "Abstract": "Power data alone cannot identify sources of energy inefficiency. However, correlating power data with utilization statistics can reveal where power is used well and where it is wasted. We describe a sensing infrastructure, PowerNet, that monitors power and utilization in a building environment. The deployment includes both wired and wireless sensors and covers offices, networking closets, and server racks. We present PowerNet\u2019s architecture, then generate initial insights from each monitored environment. Analyzing PowerNet data traces identifies contexts where electricity consumption can be reduced without cost, and others which call for rethinking system designs altogether.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:abG-DnoFyZgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "From laptop to lambda: Outsourcing everyday jobs to thousands of transient functional containers",
            "Publication year": 2019,
            "Publication url": "https://www.usenix.org/conference/atc19/presentation/fouladi",
            "Abstract": "We present gg, a framework and a set of command-line tools that helps people execute everyday applications\u2014eg, software compilation, unit tests, video encoding, or object recognition\u2014using thousands of parallel threads on a cloud-functions service to achieve near-interactive completion time. In the future, instead of running these tasks on a laptop, or keeping a warm cluster running in the cloud, users might push a button that spawns 10,000 parallel cloud functions to execute a large job in a few seconds from start. gg is designed to make this practical and easy.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:hsZV8lGYWTMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Programming with transactional coherence and consistency (TCC)",
            "Publication year": 2004,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1024393.1024395",
            "Abstract": "Transactional Coherence and Consistency (TCC) offers a way to simplify parallel programming by executing all code within transactions. In TCC systems, transactions serve as the fundamental unit of parallel work, communication and coherence. As each transaction completes, it writes all of its newly produced state to shared memory atomically, while restarting other processors that have speculatively read stale data. With this mechanism, a TCC-based system automatically handles data synchronization correctly, without programmer intervention. To gain the benefits of TCC, programs must be decomposed into transactions. We describe two basic programming language constructs for decomposing programs into transactions, a loop conversion syntax and a general transaction-forking mechanism. With these constructs, writing correct parallel programs requires only small, incremental changes to correct sequential \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:iH-uZ7U-co4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Heracles: Improving resource efficiency at scale",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2749469.2749475",
            "Abstract": "User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low traffic. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy-efficiency of large-scale datacenters. With technology scaling slowing down, it becomes important to address this opportunity.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:XoXfffV-tXoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Time and cost-efficient modeling and generation of large-scale tpcc/tpce/tpch workloads",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-32627-1_11",
            "Abstract": "Large-scale TPC workloads are critical for the evaluation of datacenter-scale storage systems. However, these workloads have not been previously characterized, in-depth, and modeled in a DC environment. In this work, we categorize the TPC workloads into storage threads that have unique features and characterize the storage activity of TPCC, TPCE and TPCH based on I/O traces from real server installations. We also propose a framework for modeling and generation of large-scale TPC workloads, which allows us to conduct a wide spectrum of storage experiments without requiring knowledge on the structure of the application or the overhead of fully deploying it in different storage configurations. Using our framework, we eliminate the time for TPC setup and reduce the time for experiments by two orders of magnitude, due to the compression in storage activity enforced by the model. We demonstrate the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "A low power front-end for embedded processors using a block-aware instruction set",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1289881.1289926",
            "Abstract": "Energy, power, and area efficiency are critical design concerns for embedded processors. Much of the energy of a typical embedded processor is consumed in the front-end since instruction fetching happens on nearly every cycle and involves accesses to large memory arrays such as instruction and branch target caches. The use of small front-end arrays leads to significant power and area savings, but typically results in significant performance degradation. This paper evaluates and compares optimizations that improve the performance of embedded processors with small front-end caches. We examine both software techniques, such as instruction re-ordering and selective caching, and hardware techniques, such as instruction prefetching, tagless instruction cache, and unified caches for instruction and branch targets. We demonstrate that, building on top of a block-aware instruction set, these optimizations can \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:vRqMK49ujn8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unlocking concurrency: Multicore programming with transactional memory",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1189276.1189288",
            "Abstract": "Multicore architectures are an inflection point in mainstream software development because they force developers to write parallel programs. In a previous article in Queue, Herb Sutter and James Larus pointed out, \u201cThe concurrency revolution is primarily a software revolution. The difficult problem is not building multicore hardware, but programming it in a way that lets mainstream applications benefit from the continued exponential growth in CPU performance.\u201d In this new multicore world, developers must write explicitly parallel applications that can take advantage of the increasing number of cores that each successive multicore generation will provide.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:hMod-77fHWUC",
            "Publisher": "ACM"
        },
        {
            "Title": "3 Guest Editors\u2019 Introduction: Top Picks from the 2011 Computer Architecture Conferences Paolo Faraboschi and TN Vijaykumar 7 Kilo TM: Hardware Transactional Memory for GPU Architectures",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6208903/",
            "Abstract": "Table of Contents Page 1 [3B2-9] mmi2012030001.3d 16/5/012 17:26 Cover 2 IEEE Micro (ISSN \n0272-1732) is published bimonthly by the IEEE Computer Society. IEEE Headquarters, \nThree Park Ave., 17th Floor, New York, NY 10016-5997; IEEE Computer Society \nHeadquarters, 2001 L St., Ste. 700, Washington, DC 20036; IEEE Computer Society \nPublications Office, 10662 Los Vaqueros Circle, PO Box 3014, Los Alamitos, CA 90720. \nAnnual subscription rates: IEEE Computer Society members get the lowest rates, US$45 (print \nand electronic). Go to http://www.computer.org/ subscribe to order and for more information \non other subscription prices. Back issues: members, $20; nonmembers, $148. This \nmagazine is also available on the Web. Postmaster: Send address changes and \nundelivered copies to IEEE, Membership Processing Dept., 445 Hoes Ln., Piscataway, NJ \n08855. Periodicals postage is paid at New , NY\u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:LjlpjdlvIbIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reconciling high server utilization and sub-millisecond quality-of-service",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2592798.2592821",
            "Abstract": "The simplest strategy to guarantee good quality of service (QoS) for a latency-sensitive workload with sub-millisecond latency in a shared cluster environment is to never run other workloads concurrently with it on the same server. Unfortunately, this inevitably leads to low server utilization, reducing both the capability and cost effectiveness of the cluster.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:LI9QrySNdTsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Simultaneously improving code size, performance, and energy in embedded processors",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1656880/",
            "Abstract": "Code size and energy consumption are critical design concerns for embedded processors as they determine the cost of the overall system. Techniques such as reduced length instruction sets lead to significant code size savings but also introduce performance and energy consumption impediments such as additional dynamic instructions or decompression latency. In this paper, we show that a block-aware instruction set (BLISS) which stores basic block descriptors in addition to and separately from the actual instructions in the program allows embedded processors to achieve significant improvements in all three metrics: reduced code size and improved performance and lower energy consumption",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:NhqRSupF_l8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The ZCache: Decoupling ways and associativity",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5695536/",
            "Abstract": "The ever-increasing importance of main memory latency and bandwidth is pushing CMPs towards caches with higher capacity and associativity. Associativity is typically improved by increasing the number of ways. This reduces conflict misses, but increases hit latency and energy, placing a stringent trade-off on cache design. We present the zcache, a cache design that allows much higher associativity than the number of physical ways (e.g. a 64-associative cache with 4 ways). The zcache draws on previous research on skew-associative caches and cuckoo hashing. Hits, the common case, require a single lookup, incurring the latency and energy costs of a cache with a very low number of ways. On a miss, additional tag lookups happen off the critical path, yielding an arbitrarily large number of replacement candidates for the incoming block. Unlike conventional designs, the zcache provides associativity by \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:K3LRdlH-MEoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Making nested parallel transactions practical using lightweight hardware support",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1810085.1810097",
            "Abstract": "Transactional Memory (TM) simplifies parallel programming by supporting parallel tasks that execute in an atomic and isolated way. To achieve the best possible performance, TM must support the nested parallelism available in real-world applications and supported by popular programming models. A few recent papers have proposed support for nested parallelism in software TM (STM) and hardware TM (HTM). However, the proposed designs are still impractical, as they either introduce excessive runtime overheads or require complex hardware structures.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:tS2w5q8j5-wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "RAMP: Research accelerator for multiple processors-a community vision for a shared experimental parallel HW/SW platform",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=12594571216015282802&hl=en&oi=scholarr",
            "Abstract": "Desktop processor architectures have crossed a critical threshold. Manufactures have given up attempting to extract ever more performance from a single core and instead have turned to multi-core designs. While straightforward approaches to the architecture of multi-core processors are sufficient for small designs (2-4 cores), little is really known how to build, program, or manage systems of 64 to 1024 processors. Unfortunately, the computer architecture community lacks the basic infrastructure tools required to carry out this research. While simulation has been adequate for single-processor research, significant use of simplified modeling and statistical sampling is required to work in the 2-16 processing core space. Invention is required for architecture research at the level of 64-1024 cores.Fortunately, Moore's law has not only enabled these dense multi-core chips, it has also enabled extremely dense FPGAs \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:MXK_kJrjxJIC",
            "Publisher": "UC Berkeley technical report, UCB/CSD-05-1412"
        },
        {
            "Title": "Amdahl's law for tail latency",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/fullHtml/10.1145/3232559",
            "Abstract": "Queueing theoretic models can guide design trade-offs in systems targeting tail latency, not just average performance.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:7Hz3ACDFbsoC",
            "Publisher": "ACM"
        },
        {
            "Title": "Classifying memory access patterns for prefetching",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3373376.3378498",
            "Abstract": "Prefetching is a well-studied technique for addressing the memory access stall time of contemporary microprocessors. However, despite a large body of related work, the memory access behavior of applications is not well understood, and it remains difficult to predict whether a particular application will benefit from a given prefetcher technique. In this work we propose a novel methodology to classify the memory access patterns of applications, enabling well-informed reasoning about the applicability of a certain prefetcher. Our approach leverages instruction dataflow information to uncover a wide range of access patterns, including arbitrary combinations of offsets and indirection. These combinations or prefetch kernels represent reuse, strides, reference locality, and complex address generation. By determining the complexity and frequency of these access patterns, we enable reasoning about prefetcher timeliness \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:OBSaB-F7qqsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "RAIL: Predictable, Low Tail Latency for NVMe Flash",
            "Publication year": 2021,
            "Publication url": "https://people.ucsc.edu/~hlitz/papers/rail.pdf",
            "Abstract": "Flash-based storage is replacing disk for an increasing number of data center applications, providing orders of magnitude higher throughput and lower average latency. However, applications also require predictable storage latency. Existing Flash devices fail to provide low tail read latency in the presence of write operations. We propose two novel techniques to address SSD read tail latency, including Redundant Array of Independent LUNs (RAIL) which avoids serialization of reads behind user writes as well as latency-aware hot-cold separation (HC) which improves write throughput while maintaining low tail latency. RAIL leverages the internal parallelism of modern Flash devices and allocates data and parity pages to avoid reads getting stuck behind writes. We implement RAIL in the Linux Kernel as part of the LightNVM Flash translation layer and show that it can reduce read tail latency by 7\u00d7 at the 99.99th percentile, while reducing relative bandwidth by only 33%.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:pS0ncopqnHgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "STAMP: Stanford transactional applications for multi-processing",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4636089/",
            "Abstract": "Transactional Memory (TM) is emerging as a promising technology to simplify parallel programming. While several TM systems have been proposed in the research literature, we are still missing the tools and workloads necessary to analyze and compare the proposals. Most TM systems have been evaluated using microbenchmarks, which may not be representative of any real-world behavior, or individual applications, which do not stress a wide range of execution scenarios. We introduce the Stanford Transactional Application for Multi-Processing (STAMP), a comprehensive benchmark suite for evaluating TM systems. STAMP includes eight applications and thirty variants of input parameters and data sets in order to represent several application domains and cover a wide range of transactional execution cases (frequent or rare use of transactions, large or small transactions, high or low contention, etc.). Moreover \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Ased: availability, security, and debugging support usingtransactional memory",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1378533.1378599",
            "Abstract": "We propose ASeD that uses the hardware resources of transactional memory systems for non transactional memory purpose. We show that the hardware components for register checkpointing, data versioning, and conflict detection can be reused as basic building blocks for reliability, security, and debugging support.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Pocket: Elastic ephemeral storage for serverless analytics",
            "Publication year": 2018,
            "Publication url": "https://www.usenix.org/conference/osdi18/presentation/klimovic",
            "Abstract": "Serverless computing is becoming increasingly popular, enabling users to quickly launch thousands of shortlived tasks in the cloud with high elasticity and fine-grain billing. These properties make serverless computing appealing for interactive data analytics. However exchanging intermediate data between execution stages in an analytics job is a key challenge as direct communication between serverless tasks is difficult. The natural approach is to store such ephemeral data in a remote data store. However, existing storage systems are not designed to meet the demands of serverless applications in terms of elasticity, performance, and cost. We present Pocket, an elastic, distributed data store that automatically scales to provide applications with desired performance at low cost. Pocket dynamically rightsizes resources across multiple dimensions (CPU cores, network bandwidth, storage capacity) and leverages multiple storage technologies to minimize cost while ensuring applications are not bottlenecked on I/O. We show that Pocket achieves similar performance to ElastiCache Redis for serverless analytics applications while reducing cost by almost 60%.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:PaBasH6fAo0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cross-examination of datacenter workload modeling techniques",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5961496/",
            "Abstract": "Data center workload modeling has become a necessity in recent years due to the emergence of large-scale applications and cloud data-stores, whose implementation remains largely unknown. Detailed knowledge of target workloads is critical in order to correctly provision performance, power and cost-optimized systems. In this work we aggregate previous work on data center workload modeling and perform a qualitative comparison based on the representativeness, accuracy and completeness of these designs. We categorize modeling techniques in two main approaches, in-breadth and in-depth, based on the way they address the modeling of the workload. The former models the behavior of a workload in specific system parts, while the latter traces a user request throughout its execution. Furthermore, we propose the early concept of a new design, which bridges the gap between these two approaches by \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:1qzjygNMrQYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Intelligent Memory Systems: Second International Workshop, IMS 2000, Cambridge, MA, USA, November 12, 2000. Revised Papers",
            "Publication year": 2001,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=DFpxqHMGyBwC&oi=fnd&pg=PR3&dq=info:UTsgPm3_H4MJ:scholar.google.com&ots=LAQ_NMCMkK&sig=EMKMLXSYgnbhfGh6DjfYHp_yaQQ",
            "Abstract": "We are pleased to present this collection of papers from the Second Workshop on Intelligent Memory Systems. Increasing die densities and inter chip communication costs continue to fuel interest in intelligent memory systems. Since the First Workshop on Mixing Logic and DRAM in 1997, technologies and systems for computation in memory have developed quickly. The focus of this workshop was to bring together researchers from academia and industry to discuss recent progress and future goals. The program committee selected 8 papers and 6 poster session abstracts from 29 submissions for inclusion in the workshop. Four to five members of the program committee reviewed each submission and their reviews were used to numerically rank them and guide the selection process. We believe that the resulting program is of the highest quality and interest possible. The selected papers cover a wide range of research topics such as circuit technology, processor and memory system architecture, compilers, operating systems, and applications. They also present a mix of mature projects, work in progress, and new research ideas. The workshop also included two invited talks. Dr. Subramanian Iyer (IBM Microelectronics) provided an overview of embedded memory technology and its potential. Dr. Mark Snir (IBM Research) presented the Blue Gene, an aggressive supercomputer system based on intelligent memory technology.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:dhFuZR0502QC",
            "Publisher": "Springer Science & Business Media"
        },
        {
            "Title": "Tradeoffs in transactional memory virtualization",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1168857.1168903",
            "Abstract": "For transactional memory (TM) to achieve widespread acceptance, transactions should not be limited to the physical resources of any specific hardware implementation. TM systems should guarantee correct execution even when transactions exceed scheduling quanta, overflow the capacity of hardware caches and physical memory, or include more independent nesting levels than what is supported in hardware. Existing proposals for TM virtualization are either incomplete or rely on complex hardware implementations, which are an overkill if virtualization is invoked infrequently in the common case. We present eXtended Transactional Memory (XTM), the first TM virtualization system that virtualizes all aspects of transactional execution (time, space, and nesting depth). XTM is implemented in software using virtual memory support. It operates at page granularity, using private copies of overflowed pages to buffer \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:M3NEmzRMIkIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Practical near-data processing for in-memory analytics frameworks",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7429299/",
            "Abstract": "The end of Dennard scaling has made all systemsenergy-constrained. For data-intensive applications with limitedtemporal locality, the major energy bottleneck is data movementbetween processor chips and main memory modules. For such workloads, the best way to optimize energy is to place processing near the datain main memory. Advances in 3D integrationprovide an opportunity to implement near-data processing (NDP) withoutthe technology problems that similar efforts had in the past. This paper develops the hardware and software of an NDP architecturefor in-memory analytics frameworks, including MapReduce, graphprocessing, and deep neural networks. We develop simple but scalablehardware support for coherence, communication, and synchronization, anda runtime system that is sufficient to support analytics frameworks withcomplex data patterns while hiding all thedetails of the NDP hardware \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:ClCfbGk0d_YC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Generating configurable hardware from parallel patterns",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2954679.2872415",
            "Abstract": "In recent years the computing landscape has seen an increasing shift towards specialized accelerators. Field programmable gate arrays (FPGAs) are particularly promising for the implementation of these accelerators, as they offer significant performance and energy improvements over CPUs for a wide class of applications and are far more flexible than fixed-function ASICs. However, FPGAs are difficult to program. Traditional programming models for reconfigurable logic use low-level hardware description languages like Verilog and VHDL, which have none of the productivity features of modern software languages but produce very efficient designs, and low-level software languages like C and OpenCL coupled with high-level synthesis (HLS) tools that typically produce designs that are far less efficient. Functional languages with parallel patterns are a better fit for hardware generation because they provide high \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:hCrLmN-GePgC",
            "Publisher": "ACM"
        },
        {
            "Title": "Dynamic management of TurboMode in modern multi-core chips",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6835969/",
            "Abstract": "Dynamic overclocking of CPUs, or TurboMode, is a feature recently introduced on all x86 multi-core chips. It leverages thermal and power headroom from idle execution resources to overclock active cores to increase performance. TurboMode can accelerate CPU-bound applications at the cost of additional power consumption. Nevertheless, naive use of TurboMode can significantly increase power consumption without increasing performance. Thus far, there is no strategy for managing TurboMode to optimize its use across all workloads and efficiency metrics. This paper analyzes the impact of TurboMode on a wide range of efficiency metrics (performance, power, cost, and combined metrics such as QPS/W and ED2) for representative server workloads on various hardware configurations. We determine that TurboMode is generally beneficial for performance (up to +24%), cost efficiency (QPS/$ up to +8%), energy \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:hMsQuOkrut0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Vantage: Scalable and efficient fine-grain cache partitioning",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2000064.2000073",
            "Abstract": "Cache partitioning has a wide range of uses in CMPs, from guaranteeing quality of service and controlled sharing to security-related techniques. However, existing cache partitioning schemes (such as way-partitioning) are limited to coarse-grain allocations, can only support few partitions, and reduce cache associativity, hurting performance. Hence, these techniques can only be applied to CMPs with 2-4 cores, but fail to scale to tens of cores.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:eflP2zaiRacC",
            "Publisher": "Unknown"
        },
        {
            "Title": "3D nanosystems enable embedded abundant-data computing: special session paper",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3125502.3125531",
            "Abstract": "The world's appetite for abundant-data computing, where a massive amount of structured and unstructured data is analyzed, has increased dramatically. The computational demands of these applications, such as deep learning, far exceed the capabilities of today's systems, especially for energy-constrained embedded systems (eg, mobile systems with limited battery capacity). These demands are unlikely to be met by isolated improvements in transistor or memory technologies, or integrated circuit (IC) architectures alone. Transformative nanosystems, which leverage the unique properties of emerging nanotechnologies to create new IC architectures, are required to deliver unprecedented functionality, performance, and energy efficiency. We show that the projected energy efficiency benefits of domain-specific 3D nanosystems is in the range of 1,000 x (quantified using the product of system-level energy \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:lvd772isFD0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A memory system design framework: creating smart memories",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1555754.1555805",
            "Abstract": "As CPU cores become building blocks, we see a great expansion in the types of on-chip memory systems proposed for CMPs. Unfortunately, designing the cache and protocol controllers to support these memory systems is complex, and their concurrency and latency characteristics significantly affect the performance of any CMP. To address this problem, this paper presents a microarchitecture framework for cache and protocol controllers, which can aid in generating the RTL for new memory systems. The framework consists of three pipelined engines' request-tracking, state-manipulation, and data movement'which are programmed to implement a higher-level memory model. This approach simplifies the design and verification of CMP systems by decomposing the memory model into sequences of state and data manipulations. Moreover, implementing the framework itself produces a polymorphic memory system.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:D03iK_w7-QYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A practical FPGA-based framework for novel CMP research",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1216919.1216936",
            "Abstract": "Chip-multiprocessors are quickly gaining momentum in all segments of computing. However, the practical success of CMPs strongly depends on addressing the difficulty of multithreaded application development. To address this challenge, it is necessary to co-develop new CMP architecture with novel programming models. Currently, architecture research relies on software simulators which are too slow to facilitate interesting experiments with CMP software without using small datasets or significantly reducing the level of detail in the simulated models. An alternative to simulation is to exploit the rich capabilities of modern FPGAs to create FPGA-based platforms for novel CMP research. This paper presents ATLAS, the first prototype for CMPs with hardware support for Transactional Memory (TM), a technology aiming to simplify parallel programming. ATLAS uses the BEE2 multi-FPGA board to provide a system with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:RGFaLdJalmkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Potential show-stoppers for transactional synchronization",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1229428.1229439",
            "Abstract": "Potential show-stoppers for transactional synchronization | Proceedings of the 12th ACM \nSIGPLAN symposium on Principles and practice of parallel programming ACM Digital \nLibrary home ACM home Google, Inc. (search) Advanced Search Browse About Sign in \nRegister Advanced Search Journals Magazines Proceedings Books SIGs Conferences \nPeople More Search ACM Digital Library SearchSearch Advanced Search ppopp \nConference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesPPOPPProceedingsPPoPP \n'07Potential show-stoppers for transactional synchronization Article Potential show-stoppers \nfor transactional synchronization Share on Authors: Ali-Reza Adl-Tabatabai Intel Corporation \nIntel Corporation View Profile , David Dice Sun Microsystems Sun Microsystems View \nProfile , Maurice Herlihy Brown University Brown University View Profile , Nir Shavit Sun \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:u9iWguZQMMsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Locality-aware task management for unstructured parallelism: A quantitative limit study",
            "Publication year": 2013,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2486159.2486175",
            "Abstract": "As we increase the number of cores on a processor die, the on-chip cache hierarchies that support these cores are getting larger, deeper, and more complex. As a result, non-uniform memory access effects are now prevalent even on a single chip. To reduce execution time and energy consumption, data access locality should be exploited. This is especially important for task-based programming systems, where a scheduler decides when and where on the chip the code segments, ie, tasks, should execute. Capturing locality for structured task parallelism has been done effectively, but the more difficult case, unstructured parallelism, remains largely unsolved-little quantitative analysis exists to demonstrate the potential of locality-aware scheduling, and to guide future scheduler implementations in the most fruitful direction.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:AvfA0Oy_GE0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Transactional coherence and consistency: Simplifying parallel hardware and software",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1388164/",
            "Abstract": "Transactional coherence and consistency (TCC) simplifies parallel hardware and software design by eliminating the need for conventional cache coherence and consistency models and letting programmers parallelize a wide range of applications with a simple, lock-free transactional model. TCC eases both parallel programming and parallel architecture design by relying on programmer-defined transactions as the basic unit of parallel work, communication, memory coherence, and memory consistency",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:W7OEmFMy1HYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Dune: Safe user-level access to privileged {CPU} features",
            "Publication year": 2012,
            "Publication url": "https://www.usenix.org/conference/osdi12/technical-sessions/presentation/belay",
            "Abstract": "Dune is a system that provides applications with direct but safe access to hardware features such as ring protection, page tables, and tagged TLBs, while preserving the existing OS interfaces for processes. Dune uses the virtualization hardware in modern processors to provide a process, rather than a machine abstraction. It consists of a small kernel module that initializes virtualization hardware and mediates interactions with the kernel, and a user-level library that helps applications manage privileged hardware features. We present the implementation of Dune for 64bit x86 Linux. We use Dune to implement three userlevel applications that can benefit from access to privileged hardware: a sandbox for untrusted code, a privilege separation facility, and a garbage collector. The use of Dune greatly simplifies the implementation of these applications and provides significant performance advantages.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:NJ774b8OgUMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Shinjuku: Preemptive scheduling for \u03bcsecond-scale tail latency",
            "Publication year": 2019,
            "Publication url": "https://www.usenix.org/conference/nsdi19/presentation/kaffes",
            "Abstract": "The recently proposed dataplanes for microsecond scale applications, such as IX and ZygOS, use non-preemptive policies to schedule requests to cores. For the many real-world scenarios where request service times follow distributions with high dispersion or a heavy tail, they allow short requests to be blocked behind long requests, which leads to poor tail latency.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:jU7OWUQzBzMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Block-aware instruction set architecture",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1162690.1162694",
            "Abstract": "Instruction delivery is a critical component for wide-issue, high-frequency processors since its bandwidth and accuracy place an upper limit on performance. The processor front-end accuracy and bandwidth are limited by instruction-cache misses, multicycle instruction-cache accesses, and target or direction mispredictions for control-flow operations. This paper presents a block-aware instruction set (BLISS) that allows software to assist with front-end challenges. BLISS defines basic block descriptors that are stored separately from the actual instructions in a program. We show that BLISS allows for a decoupled front-end that tolerates instruction-cache latency, facilitates instruction prefetching, and leads to higher prediction accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:tOudhMTPpwUC",
            "Publisher": "ACM"
        },
        {
            "Title": "Heuristics for profile-driven method-level speculative parallelization",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1488610/",
            "Abstract": "Thread level speculation (TLS) is an effective technique for extracting parallelism from sequential code. Method calls provide good templates for the boundaries of speculative threads as they often describe independent tasks. However, selecting the most profitable methods to speculate on is difficult as it involves complicated trade-offs between speculation violations, thread overheads, and resource utilization. This paper presents a first analysis of heuristics for automatic selection of speculative threads across method boundaries using a dynamic or profile-driven compiler. We study the potential of three classes of heuristics that involve increasing amounts of profiling information and runtime complexity. Several of the heuristics allow for speculation to start at internal method points, nested speculation, and speculative thread preemption. Using a set of Java benchmarks, we demonstrate that careful thread selection at \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:ZHo1McVdvXMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Implementing and evaluating a model checker for transactional memory systems",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5628620/",
            "Abstract": "Transactional Memory (TM) is a promising technique that addresses the difficulty of parallel programming. Since TM takes responsibility for all concurrency control, TM systems are highly vulnerable to subtle correctness errors. Due to the difficulty of fully proving the correctness of TM systems, many of them are used without any formal correctness guarantees. This paper presents ChkTM, a flexible model checking environment to verify the correctness of various TM systems. ChkTM aims to model TM systems close to the implementation level to reveal as many potential bugs as possible. For example, ChkTM accurately models the version control mechanism in timestamp-based software TMs (STMs). In addition, ChkTM can flexibly model TM systems that use additional hardware components or support nested parallelism. Using ChkTM, we model several TM systems including a widely-used industrial STM (TL2), a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:B3FOqHPlNUQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Racksched: A microsecond-scale scheduler for rack-scale computers",
            "Publication year": 2020,
            "Publication url": "https://www.usenix.org/conference/osdi20/presentation/zhu",
            "Abstract": "Low-latency online services have strict Service Level Objectives (SLOs) that require datacenter systems to support high throughput at microsecond-scale tail latency. Dataplane operating systems have been designed to scale up multi-core servers with minimal overhead for such SLOs. However, as application demands continue to increase, scaling up is not enough, and serving larger demands requires these systems to scale out to multiple servers in a rack. We present RackSched, the first rack-level microsecond-scale scheduler that provides the abstraction of a rack-scale computer (ie, a huge server with hundreds to thousands of cores) to an external service with network-system co-design. The core of RackSched is a two-layer scheduling framework that integrates inter-server scheduling in the top-of-rack (ToR) switch with intra-server scheduling in each server. We use a combination of analytical results and simulations to show that it provides near-optimal performance as centralized scheduling policies, and is robust for both low-dispersion and high-dispersion workloads. We design a custom switch data plane for the inter-server scheduler, which realizes power-of-k-choices, ensures request affinity, and tracks server loads accurately and efficiently. We implement a RackSched prototype on a cluster of commodity servers connected by a Barefoot Tofino switch. End-to-end experiments on a twelve-server testbed show that RackSched improves the throughput by up to 1.44 x, and scales out the throughput near linearly, while maintaining the same tail latency as one server until the system is saturated.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:HhcuHIWmDEUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Flexible architectural support for fine-grain scheduling",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1735970.1736055",
            "Abstract": "To make efficient use of CMPs with tens to hundreds of cores, it is often necessary to exploit fine-grain parallelism. However, managing tasks of a few thousand instructions is particularly challenging, as the runtime must ensure load balance without compromising locality and introducing small overheads. Software-only schedulers can implement various scheduling algorithms that match the characteristics of different applications and programming models, but suffer significant overheads as they synchronize and communicate task information over the deep cache hierarchy of a large-scale CMP. To reduce these costs, hardware-only schedulers like Carbon, which implement task queuing and scheduling in hardware, have been proposed. However, a hardware-only solution fixes the scheduling algorithm and leaves no room for other uses of the custom hardware.This paper presents a combined hardware-software \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "ACM"
        },
        {
            "Title": "Dnn dataflow choice is overrated",
            "Publication year": 2018,
            "Publication url": "https://qdata.github.io/deep2Read/talks2019/19sCourse/20190328-Derrick-DNN-Dataflow-Overrated.pdf",
            "Abstract": "6 ConclusionPresenter: Derrick Blakely (University of Virginia) DNN Dataflow Choice Is Overrated Xuan Yang, Mingyu Gao, Jing Pu, Ankita Naya https://qdata. github. io/deep2Read 2/42",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:7BrZ7Jt4UNcC",
            "Publisher": "Sep"
        },
        {
            "Title": "ghOSt: Fast & Flexible User-Space Delegation of Linux Scheduling",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3477132.3483542",
            "Abstract": "We present ghOSt, our infrastructure for delegating kernel scheduling decisions to userspace code. ghOSt is designed to support the rapidly evolving needs of our data center workloads and platforms.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:QUX0mv85b1cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "GraphP: Reducing communication for PIM-based graph processing with efficient data partition",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8327036/",
            "Abstract": "Processing-In-Memory (PIM) is an effective technique that reduces data movements by integrating processing units within memory. The recent advance of \u201cbig data\u201d and 3D stacking technology make PIM a practical and viable solution for the modern data processing workloads. It is exemplified by the recent research interests on PIM-based acceleration. Among them, TESSERACT is a PIM-enabled parallel graph processing architecture based on Micron's Hybrid Memory Cube (HMC), one of the most prominent 3D-stacked memory technologies. It implements a Pregel-like vertex-centric programming model, so that users could develop programs in the familiar interface while taking advantage of PIM. Despite the orders of magnitude speedup compared to DRAM-based systems, TESSERACT generates excessive crosscube communications through SerDes links, whose bandwidth is much less than the aggregated \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:Bg7qf7VwUHIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Storage I/O generation and replay for datacenter applications",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5762724/",
            "Abstract": "With the advent of social networking and cloud data-stores, user data is increasingly being stored in large capacity and high performance storage systems, which account for a significant portion of the total cost of ownership of a datacenter (DC) [3]. One of the main challenges when trying to evaluate storage system options is the difficulty in replaying the entire application in all possible system configurations. Furthermore, code and datasets of DC applications are rarely available to storage system designers. This makes the development of a representative model that captures key aspects of the workload's storage profile, even more appealing. Once such a model is available, the next step is to create a tool that convincingly reproduces the application's storage behavior via a synthetic I/O access pattern.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:u-x6o8ySG0sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Vector IRAM: A media-oriented vector processor with embedded DRAM",
            "Publication year": 2000,
            "Publication url": "https://scholar.google.com/scholar?cluster=341817067507603515&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:kNdYIx-mwKoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A comparison of high-level full-system power models.",
            "Publication year": 2008,
            "Publication url": "https://www.usenix.org/event/hotpower08/tech/full_papers/rivoire/rivoire_html",
            "Abstract": "Dynamic power management in enterprise environments requires an understanding of the relationship between resource utilization and system-level power consumption. Power models based on resource utilization have been proposed in the context of enabling specific energy-efficiency optimizations on specific machines, but the accuracy and portability of different approaches to modeling have not been systematically compared. In this work, we use a common infrastructure to fit a family of high-level full-system power models, and we compare these models over a wide variation of workloads and machines, from a laptop to a server. This analysis shows that a model based on OS utilization metrics and CPU performance counters is generally most accurate across the machines and workloads tested. It is particularly useful for machines whose dynamic power consumption is not dominated by the CPU, as well as machines with aggressively power-managed CPUs, two classes of systems that are increasingly prevalent.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:ns9cj8rnVeAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Quality-of-service-aware scheduling in heterogeneous data centers with paragon",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6756704/",
            "Abstract": "Large-scale datacenters host tens of thousands of diverse applications each day. However, performance is degraded by interference between colocated workloads and the difficulty of matching applications to one of the many hardware platforms available, violating the quality of service (QoS) guarantees that many cloud workloads require. Thus, the authors present Paragon, an online and scalable datacenter scheduler that is aware of heterogeneity and interference. Paragon is derived from robust analytical methods. Instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown, incoming workload with respect to heterogeneity and interference by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:k8Z6L05lTy4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Testing implementations of transactional memory",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1152154.1152177",
            "Abstract": "Transactional memory is an attractive design concept for scalable multiprocessors because it offers efficient lock-free synchronization and greatly simplifies parallel software. Given the subtle issues involved with concurrency and atomicity, however, it is important that transactional memory systems be carefully designed and aggressively tested to ensure their correctness. In this paper, we propose an axiomatic framework to model the formal specification of a realistic transactional memory system which may contain a mix of transactional and non-transactional operations. Using this framework and extensions to analysis algorithms originally developed for checking traditional memory consistency, we show that the widely practiced pseudo-random testing methodology can be effectively applied to transactional memory systems. Our testing methodology was successful in finding previously unknown bugs in the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:HoB7MX3m0LUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "From chaos to QoS: case studies in CMP resource management",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1241601.1241608",
            "Abstract": "As more and more cores are enabled on the die of future CMP platforms, we expect that several diverse workloads will run simultaneously on the platform. A key example of this trend is the growth of virtualization usage models. When multiple virtual machines or applications or threads run simultaneously, the quality of service (QoS) that the platform provides to each individual thread is non-deterministic today. This occurs because the simultaneously running threads place very different demands on the shared resources (cache space, memory bandwidth, etc) in the platform and in most cases contend with each other. In this paper, we first present case studies that show how this results in non-deterministic performance. Unlike the compute resources managed through scheduling, platform resource allocation to individual threads cannot be controlled today. In order to provide better determinism and QoS, we then \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:rO6llkc54NcC",
            "Publisher": "ACM"
        },
        {
            "Title": "Memory-based apparatus and method",
            "Publication year": 2013,
            "Publication url": "https://patents.google.com/patent/US20130097387A1/en",
            "Abstract": "Aspects of various embodiments are directed to memory circuits, such as cache memory circuits. In accordance with one or more embodiments, cache-access to data blocks in memory is controlled as follows. In response to a cache miss for a data block having an associated address on a memory access path, data is fetched for storage in the cache (and serving the request), while one or more additional lookups are executed to identify candidate locations to store data. An existing set of data is moved from a target location in the cache to one of the candidate locations, and the address of the one of the candidate locations is associated with the existing set of data. Data in this candidate location may, for example, thus be evicted. The fetched data is stored in the target location and the address of the target location is associated with the fetched data.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:9Nmd_mFXekcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving resource efficiency at scale with Heracles",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2882783",
            "Abstract": "User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low traffic. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy efficiency of large-scale datacenters. With the slowdown in technology scaling caused by the sunsetting of Moore\u2019s law, it becomes important to address this opportunity.We present Heracles, a feedback-based controller that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:umqufdRvDiIC",
            "Publisher": "ACM"
        },
        {
            "Title": "From the program co-chairs of hot chips 20",
            "Publication year": 2008,
            "Publication url": "https://www.computer.org/csdl/proceedings-article/hcs/2008/07476527/12OmNxXl5Et",
            "Abstract": "On behalf of the Program Committee, we are pleased to welcome you to the 20th Annual Hot Chips Symposium.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:OTTXONDVkokC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "Transactional execution of Java programs",
            "Publication year": 2005,
            "Publication url": "https://urresearch.rochester.edu/institutionalPublicationPublicView.action?institutionalItemId=1897",
            "Abstract": "Parallel programming is difficult due to the complexity of dealing with conventional lock-based synchronization. To simplify parallel programming, there have been a number of proposals to support transactions directly in hardware and eliminate locks completely. Although hardware support for transactions has the potential to completely change the way parallel programs are written, initially transactions will be used to execute existing parallel programs. In this paper we investigate the implications of using transactions to execute existing parallel Java programs. Our results show that transactions can be used to support all aspects of Java multithreaded programs. Moreover, the conversion of a lock-based application into transactions is largely straightforward. The performance that these converted applications achieve is equal to or sometimes better than the original lock-based implementation.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:2P1L_qKh6hAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Feedback-directed barrier optimization in a strongly isolated STM",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1480881.1480909",
            "Abstract": "Speed improvements in today's processors have largely been delivered in the form of multiple cores, increasing the importance of abstractions that ease parallel programming. Software transactional memory (STM) addresses many of the complications of concurrency by providing a simple and composable model for safe access to shared data structures. Software transactions extend a language with an atomic primitive that declares that the effects of a block of code should not be interleaved with actions executing concurrently on other threads. Adding barriers to shared memory accesses provides atomicity, consistency and isolation.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:u_35RYKgDlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A thunk to remember: make-j1000 (and other jobs) on functions-as-a-service infrastructure",
            "Publication year": 2017,
            "Publication url": "https://sadjad.org/papers/fouladi-ggdraft-2017.pdf",
            "Abstract": "We present gg: a system for executing interdependent software workflows across thousands of short-lived \u201clambdas\u201d that run in parallel on public cloud infrastructure. The system includes three major contributions:(a) an interchange format for representing \u201cthunks\u201d\u2014programs and their complete data dependencies\u2014that can be executed anywhere;(b) a system to automatically infer the dependency tree of a software build system and synthesize it as a directed acyclic graph of thunks, by replacing stages of the system with \u201cmodels\u201d that capture dependencies with fine granularity; and (c) an execution engine that resolves thunks recursively on public functions-as-a-service infrastructure with thousand-way parallelism. We found that gg outperforms existing schemes for accelerating compilation\u2014with large projects such as inkscape and LLVM, gg was 3.7\u00d7 to 4\u00d7 as fast as outsourcing compilation to a remote 64-core machine, and 1.2\u00d7 to 2.2\u00d7 as fast as running make-j64 locally on the 64-core machine itself\u2014and that the thunk abstraction is applicable to a broad range of tasks.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:7H_MAutzIkAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Energy-efficient abundant-data computing: The N3XT 1,000 x",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7368008/",
            "Abstract": "Next-generation information technologies will process unprecedented amounts of loosely structured data that overwhelm existing computing systems. N3XT improves the energy efficiency of abundant-data applications 1,000-fold by using new logic and memory technologies, 3D integration with fine-grained connectivity, and new architectures for computation immersed in memory.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:86PQX7AUzd4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Phoenix++ modular mapreduce for shared-memory systems",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1996092.1996095",
            "Abstract": "This paper describes our rewrite of Phoenix, a MapReduce framework for shared-memory CMPs and SMPs. Despite successfully demonstrating the applicability of a MapReduce-style pipeline to shared-memory machines, Phoenix has a number of limitations; its uniform intermediate storage of key-value pairs, inefficient combiner implementation, and poor task overhead amortization fail to efficiently support a wide range of MapReduce applications, encouraging users to manually circumvent the framework. We describe an alternative implementation, Phoenix++, that provides a modular, flexible pipeline that can be easily adapted by the user to the characteristics of a particular workload. Compared to Phoenix, this new approach achieves a 4.7-fold performance improvement and increased scalability, while allowing users to write simple, strict MapReduce code.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:fQNAKQ3IYiAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Raksha: A flexible architecture for software security",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7482506/",
            "Abstract": "This article consists of a collection of slides from the author's conference presentation on Raksha, a flexible architecture for software security. Some of the specific topics discussed include: the special features, system specifications, and system design of Raksha; system architecture; applications for use; security features supported; and targeted markets.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:9c2xU6iGI7YC",
            "Publisher": "IEEE"
        },
        {
            "Title": "SCD: A scalable coherence directory with flexible sharer set encoding",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6168950/",
            "Abstract": "Large-scale CMPs with hundreds of cores require a directory-based protocol to maintain cache coherence. However, previously proposed coherence directories are hard to scale beyond tens of cores, requiring either excessive area or energy, complex hierarchical protocols, or inexact representations of sharer sets that increase coherence traffic and degrade performance. We present SCD, a scalable coherence directory that relies on efficient highly-associative caches (such as zcaches) to implement a single-level directory that scales to thousands of cores, tracks sharer sets exactly, and incurs negligible directory-induced invalidations. SCD scales because, unlike conventional directories, it uses a variable number of directory tags to represent sharer sets: lines with one or few sharers use a single tag, while widely shared lines use additional tags, so tags remain small as the system scales up. We show that, thanks \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:tkaPQYYpVKoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The stream virtual machine",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1342560/",
            "Abstract": "Stream programming is currently being pushed as a way to expose concurrency and separate communication from computation. Since there are many stream languages and potential stream execution engines, we propose an abstract machine model that captures the essential characteristics of stream architectures, the stream virtual machine (SVM). The goal of the SVM is to improve interoperability, allow development of common compilation tools and reason about stream program performance. The SVM contains control processors, slave kernel processors, and slave DMA units. The compilation process takes a stream program down to the SVM and finally down to machine binary. To extract the parameters for our SVM model, we use micro-kernels to characterize two graphics processors and a stream engine, Imagine. The results are encouraging; the model estimates the performance of the target machines with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:NMxIlDl6LWMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Evaluating impact of manageability features on device performance",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5691253/",
            "Abstract": "Manageability is a key design constraint for IT solutions, defined as the range of operations required to maintain and administer system resources through their lifecycle phases. Emerging complex and powerful management platforms and automation software expose new tensions-between the host and management applications, between costs and performance, and between costs and complexity. In this paper, we take a systematic approach to the evaluation of manageability workloads and define metrics for evaluating manageability efficiency. We propose the Manageability Quotient (MaQ) as a holistic measure of a system's ability to deliver guarantees on both host and management application performance while minimizing cost. We evaluate a range of host and management workloads on various manageability platforms using the metrics. Our results, based on more than 4000 experiments, provides insights on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:D_sINldO8mEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Leveraging application classes to save power in highly-utilized data centers",
            "Publication year": 2020,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3419111.3421274",
            "Abstract": "Data center energy consumption has become an increasingly significant contributor both to greenhouse emissions and costs. To increase utilization of individual hosts and improve efficiency, most modern data centers co-locate workloads belonging to different application classes, some being latency-sensitive (LS) and others best-effort (BE) which are more tolerant to performance variation. It is therefore necessary to design mechanisms that reduce power consumption even in the resulting high-utilization environment, while preserving LS task performance. Moreover, the abundance of different workloads and the security implications of public cloud make mechanisms that rely on extensive knowledge of workload characteristics or on application-exported metrics challenging to deploy.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:KNjnJ3z-R6IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Memory management beyond free ()",
            "Publication year": 2011,
            "Publication url": "http://csl.stanford.edu/~christos/publications/2011.ismm.keynote.pdf",
            "Abstract": "Timing (CAS, RAS, RC) 12, 40, 55ns 15, 38, 50ns 12, 40, 54ns 15, 42, 57ns Active Current (read, write) 160, 160mA 250, 250mA 130, 130mA 137, 154ns Energy per bit (peak, typical) 111, 266mW/Gbps 70, 160 mW/Gbps 110, 140 mW/Gbps 40, 50 mW/Gbps Static",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:tKAzc9rXhukC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Computer Architecture",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/0-387-27705-6_9.pdf",
            "Abstract": "Originally proposed in 1945 by John von Neumann, the von Neumann architecture has become the foundation for virtually all commercial processors. von Neumann machines have three distinguishing characteristics: 1) the storedprogram concept, 2) the partitioning of the processor into different functional components, 3) and the fetch-execute cycle. The key idea behind the stored-program concept is that the series of instructions that form the program are stored in processor-accessible memory. By contrast, for processors that do not utilize the stored-program concept, the instructions of the program need to be fed into the processor as the program is running or the program needs to be hard coded into the processor. Storing the program in memory where the processor can easily access it is obviously more efficient than feeding in each instruction while the program is running. Also, reprogramming a stored-program \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:fbc8zXXH2BUC",
            "Publisher": "Springer, Boston, MA"
        },
        {
            "Title": "Tangram: Optimized coarse-grained dataflow for scalable nn accelerators",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3297858.3304014",
            "Abstract": "The use of increasingly larger and more complex neural networks (NNs) makes it critical to scale the capabilities and efficiency of NN accelerators. Tiled architectures provide an intuitive scaling solution that supports both coarse-grained parallelism in NNs: intra-layer parallelism, where all tiles process a single layer, and inter-layer pipelining, where multiple layers execute across tiles in a pipelined manner. This work proposes dataflow optimizations to address the shortcomings of existing parallel dataflow techniques for tiled NN accelerators. For intra-layer parallelism, we develop buffer sharing dataflow that turns the distributed buffers into an idealized shared buffer, eliminating excessive data duplication and the memory access overheads. For inter-layer pipelining, we develop alternate layer loop ordering that forwards the intermediate data in a more fine-grained and timely manner, reducing the buffer \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:XUvXOeBm_78C",
            "Publisher": "Unknown"
        },
        {
            "Title": "An analysis of on-chip interconnection networks for large-scale chip multiprocessors",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1736065.1736069",
            "Abstract": "With the number of cores of chip multiprocessors (CMPs) rapidly growing as technology scales down, connecting the different components of a CMP in a scalable and efficient way becomes increasingly challenging. In this article, we explore the architectural-level implications of interconnection network design for CMPs with up to 128 fine-grain multithreaded cores. We evaluate and compare different network topologies using accurate simulation of the full chip, including the memory hierarchy and interconnect, and using a diverse set of scientific and engineering workloads.We find that the interconnect has a large impact on performance, as it is responsible for 60% to 75% of the miss latency. Latency, and not bandwidth, is the primary performance constraint, since, even with many threads per core and workloads with high miss rates, networks with enough bandwidth can be efficiently implemented for the system \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:KxtntwgDAa4C",
            "Publisher": "ACM"
        },
        {
            "Title": "Understanding and Co-designing the Data Ingestion Pipeline for Industry-Scale RecSys Training",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2108.09373",
            "Abstract": "The data ingestion pipeline, responsible for storing and preprocessing training data, is an important component of any machine learning training job. At Facebook, we use recommendation models extensively across our services. The data ingestion requirements to train these models are substantial. In this paper, we present an extensive characterization of the data ingestion challenges for industry-scale recommendation model training. First, dataset storage requirements are massive and variable; exceeding local storage capacities. Secondly, reading and preprocessing data is computationally expensive, requiring substantially more compute, memory, and network resources than are available on trainers themselves. These demands result in drastically reduced training throughput, and thus wasted GPU resources, when current on-trainer preprocessing solutions are used. To address these challenges, we present a disaggregated data ingestion pipeline. It includes a central data warehouse built on distributed storage nodes. We introduce Data PreProcessing Service (DPP), a fully disaggregated preprocessing service that scales to hundreds of nodes, eliminating data stalls that can reduce training throughput by 56%. We implement important optimizations across storage and DPP, increasing storage and preprocessing throughput by 1.9x and 2.3x, respectively, addressing the substantial power requirements of data ingestion. We close with lessons learned and cover the important remaining challenges and opportunities surrounding data ingestion at scale.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:mWEH9CqjF64C",
            "Publisher": "Unknown"
        },
        {
            "Title": "QuMan Profile-based Improvement of Cluster Utilization",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3210560",
            "Abstract": "Modern data centers consolidate workloads to increase server utilization and reduce total cost of ownership, and cope with scaling limitations. However, server resource sharing introduces performance interference across applications and, consequently, increases performance volatility, which negatively affects user experience. Thus, a challenging problem is to increase server utilization while maintaining application QoS.In this article, we present QuMan, a server resource manager that uses application isolation and profiling to increase server utilization while controlling degradation of application QoS. Previous solutions, either estimate interference across applications and then restrict colocation to \u201ccompatible\u201d applications, or assume that application requirements are known. Instead, QuMan estimates the required resources of applications. It uses an isolation mechanism to create properly-sized resource slices \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:nRpfm8aw39MC",
            "Publisher": "ACM"
        },
        {
            "Title": "Atlas: A chip-multiprocessor with transactional memory support",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4211763/",
            "Abstract": "Chip-multiprocessors are quickly becoming popular in embedded systems. However, the practical success of CMPs strongly depends on addressing the difficulty of multithreaded application development for such systems. Transactional memory (TM) promises to simplify concurrency management in multithreaded applications by allowing programmers to specify coarse-grain parallel tasks, while achieving performance comparable to fine-grain lock-based applications. This paper presents ATLAS, the first prototype of a CMP with hardware support for transactional memory. ATLAS includes 8 embedded PowerPC cores that access coherent shared memory in a transactional manner. The data cache for each core is modified to support the speculative buffering and conflict detection necessary for transactional execution. The authors have mapped ATLAS to the BEE2 multi-FPGA board to create a full-system prototype \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:NaGl4SEjCO4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "High performance hardware-accelerated flash key-value store",
            "Publication year": 2014,
            "Publication url": "http://csl.stanford.edu/~christos/publications/2014.hwkvs.nvmw.slides.pdf",
            "Abstract": "High Performance Hardware-Accelerated Flash Key-Value Store Page 1 \u00a9 2014 Toshiba \nCorporation High Performance Hardware-Accelerated Flash Key-Value Store Shingo Tanaka \nCorporate R&D Center Toshiba Corporation Christos Kozyrakis EE & CS Department Stanford \nUniversity Page 2 \u00a9 2014 Toshiba Corporation 2 Agenda \u2022 Key-value store \u2013 Memcache \u2013 \nInefficiencies with modern processors \u2022 Hardware-accelerated architecture \u2013 Key designs \u2013 \nMicro benchmarks Page 3 \u00a9 2014 Toshiba Corporation 3 Key-Value Store \u2022 Provides associative \narray \u2013 Uses a hash table for the association \u2013 Values can be accessed by specifying its \nassociated key \u2013 GET(key), SET(key, value), DELETE(key), etc \u2022 Memcache \u2013 DRAM based data \ncache server in large scale web 2.0 systems which relieves the burden of back-end database \n\u2013 Thousands of memcache servers are used to meet the requirement of: \u2022 A significant ,\u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:YohjEiUPhakC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning memory access patterns",
            "Publication year": 2018,
            "Publication url": "http://proceedings.mlr.press/v80/hashemi18a.html",
            "Abstract": "The explosion in workload complexity and the recent slow-down in Moore\u2019s law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations; augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:LgRImbQfgY4C",
            "Publisher": "PMLR"
        },
        {
            "Title": "How to solve the current memory access and data transfer bottlenecks: at the processor architecture or at the compiler level",
            "Publication year": 2000,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/343647.343813",
            "Abstract": "Current processor architectures, both in the programmable and custom case, become more and more dominated by the data access bottlenecks in the cache, system bus and main memory subsystems. In order to provide sufficiently high data throughput in the emerging era of highly parallel processors where many arithmetic resources can work concurrently, novel solutions for the memory access and data transfer will have to be introduced. The crucial question we want to address in this hot topic session is where one can expect these novel solutions to rely on: will they be mainly innovative processor architecture ideas, or novel approaches in the application compiler/synthesis technology, or a mix.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:ULOm3_A8WrAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Resource efficient computing for warehouse-scale datacenters",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6513724/",
            "Abstract": "An increasing amount of information technology services and data are now hosted in the cloud, primarily due to the cost and scalability benefits for both the end-users and the operators of the warehouse-scale datacenters (DCs) that host cloud services. Hence, it is vital to continuously improve the capabilities and efficiency of these large-scale systems. Over the past ten years, capability has improved by increasing the number of servers in a DC and the bandwidth of the network that connects them. Cost and energy efficiency have improved by eliminating the high overheads of the power delivery and cooling infrastructure. To achieve further improvements, we must now examine how well we are utilizing the servers themselves, which are the primary determinant for DC performance, cost, and energy efficiency. This is particularly important since the semiconductor chips used in servers are now energy limited and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:z_wVstp3MssC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Transactional memory",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1364782.1364800",
            "Abstract": "Is TM the answer for improving parallel programming?",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:YFjsv_pBGBYC",
            "Publisher": "ACM"
        },
        {
            "Title": "Tarcil: Reconciling scheduling speed and quality in large shared clusters",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2806777.2806779",
            "Abstract": "Scheduling diverse applications in large, shared clusters is particularly challenging. Recent research on cluster scheduling focuses either on scheduling speed, using sampling to quickly assign resources to tasks, or on scheduling quality, using centralized algorithms that search for the resources that improve both task performance and cluster utilization.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:PoWvk5oyLR8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Early release: Friend or foe",
            "Publication year": 2006,
            "Publication url": "http://csl.stanford.edu/~christos/publications/2006.er.wtw.pdf",
            "Abstract": "Transactional Memory (TM)[9] has the potential to simplify concurrency management by supporting parallel tasks (transactions) that appear to execute atomically and in isolation. There is already a significant body of work on programming language constructs for transactional memory [7, 6, 5, 3, 1, 2]. Nevertheless, there still exists little consensus on several constructs, particularly those motivated by performance optimizations. In this paper, we study a set of data structure algorithms to evaluate the ease-of-use and performance benefits of the early release (ER) construct [8, 5]. Early release allows a transaction to remove a data address from its transactional read-set long before it commits. Once an address has been released, other transactions can write to this address without generating a conflict with the releasing transaction. The programmer or a compiler must guarantee that early release of an address is safe: removing the address from the read-set should not violate the overall application atomicity and consistency. The tradeoff with early release is obvious: on one hand, removing addresses from the read-set reduces the probability of conflicts that incur expensive long stalls or rollbacks. On the other hand, there is an additional burden to guarantee that early release is safe for a particular address, regardless of any other code that may be executing in parallel. Therefore, if a programmer manually applies early release, she must be extremely careful about when, where, and with which address early release is used. Previous studies have used a single data structure algorithm (linked list) to conclude that various forms of early release can be a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:70eg2SAEIzsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A scalable, non-blocking approach to transactional memory",
            "Publication year": 2007,
            "Publication url": "https://scholarworks.unist.ac.kr/handle/201301/46869",
            "Abstract": "Transactional Memory (TM) provides mechanisms that promise to simplify parallel programming by eliminating the need for locks and their associated problems (deadlock, livelock, priority inversion, convoying). For TM to be adopted in the long term, not only does it need to deliver on these promises, but it needs to scale to a high number of processors. To date, proposals for scalable TM have relegated livelock issues to user-level contention managers. This paper presents the first scalable TM implementation for directory-based distributed shared memory systems that is livelock free without the need for user-level intervention. The design is a scalable implementation of optimistic concurrency control that supports parallel commits with a two-phase commit protocol, uses write-back caches, and filters coherence messages. The scalable design is based on Transactional Coherence and Consistency (TCC), which supports continuous transactions and fault isolation. A performance evaluation of the design using both scientific and enterprise benchmarks demonstrates that the directory-based TCC design scales efficiently for NUMA systems up to 64 processors. \u00a9 2007 IEEE",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:1DsIQWDZLl8C",
            "Publisher": "2007 IEEE 13th International Symposium on High Performance Computer Architecture, HPCA-13"
        },
        {
            "Title": "ECHO: Recreating network traffic maps for datacenters with tens of thousands of servers",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6402896/",
            "Abstract": "Large-scale datacenters now host a large part of the world's data and computation, which makes their design a crucial architectural challenge. Datacenter (DC) applications, unlike traditional workloads, are dominated by user patterns that only emerge in the large-scale. This creates the need for concise, accurate and scalable analytical models that capture both their temporal and spatial features and can be used to create representative activity patterns. Unfortunately, previous work lacks the ability to track the complex patterns that are present in these applications, or scales poorly with the size of the system. In this work, we focus on the network aspect of datacenter workloads. We present ECHO, a scalable and accurate modeling scheme that uses hierarchical Markov Chains to capture the network activity of large-scale applications in time and space. ECHO can also use these models to re-create representative \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:2KloaMYe4IUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The IX operating system: Combining low latency, high throughput, and efficiency in a protected dataplane",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2997641",
            "Abstract": "The conventional wisdom is that aggressive networking requirements, such as high packet rates for small messages and \u03bcs-scale tail latency, are best addressed outside the kernel, in a user-level networking stack. We present ix, a dataplane operating system that provides high I/O performance and high resource efficiency while maintaining the protection and isolation benefits of existing kernels. ix uses hardware virtualization to separate management and scheduling functions of the kernel (control plane) from network processing (dataplane). The dataplane architecture builds upon a native, zero-copy API and optimizes for both bandwidth and latency by dedicating hardware threads and networking queues to dataplane instances, processing bounded batches of packets to completion, and eliminating coherence traffic and multicore synchronization. The control plane dynamically adjusts core allocations and voltage \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:bKqednn6t2AC",
            "Publisher": "ACM"
        },
        {
            "Title": "TAPE: A transactional application profiling environment",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1088149.1088176",
            "Abstract": "Transactional Coherence and Consistency (TCC) provides a new parallel programming model that uses transactions as the basic unit of parallel work and communication. TCC simplifies the development of correct parallel code because hardware provides transaction atomicity and ordering. Nevertheless, the programmer or a dynamic compiler must still optimize the parallel code for performance. This paper presents TAPE, a hardware and software infrastructure for profiling in TCC systems. TAPE extends the hardware for transactional execution to identify performance impediments such as dependence violations, buffer overflows, and work imbalance. It filters infrequent events to reduce resource requirements and allows the programmer to focus on the most important bottlenecks. We demonstrate that TAPE introduces minimal die area and performance overhead and can be used continuously, even for production \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:pqnbT2bcN3wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The case for RAMCloud",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1965724.1965751",
            "Abstract": "With scalable high-performance storage entirely in DRAM, RAMCloud will enable a new breed of data-intensive applications.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:geHnlv5EZngC",
            "Publisher": "ACM"
        },
        {
            "Title": "Transactional memory implementation overview",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7477740/",
            "Abstract": "This article consists of a collection of slides from the author's conference presentation on transactional memory (TM). Some of the specific topics discussed include: TM implementation that provide atomicity and isolation; basic TM implementation requirements; and implementation options that incorporate hardware TM, software TM and hybrid TM.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:IRz6iEL74y4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Smart memories polymorphic chip multiprocessor",
            "Publication year": 2009,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.708.1992&rep=rep1&type=pdf",
            "Abstract": "The Stanford Smart Memories polymorphic chip-multiprocessor architecture was conceived as a unified multipurpose hardware architecture base, capable of supporting a variety of programming models and per-application optimizations [17]. Backing the architectural claims, our team of PhD students set out to implement this challenging design in silicon, targeting 90nm technology. Now, with 55M transistors covering 61mm2, this is one of the most complex chips ever fabricated in academia.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:738O_yMBCRsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Evaluating mapreduce for multi-core and multiprocessor systems",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4147644/",
            "Abstract": "This paper evaluates the suitability of the MapReduce model for multi-core and multi-processor systems. MapReduce was created by Google for application development on data-centers with thousands of servers. It allows programmers to write functional-style code that is automatically parallelized and scheduled in a distributed system. We describe Phoenix, an implementation of MapReduce for shared-memory systems that includes a programming API and an efficient runtime system. The Phoenix runtime automatically manages thread creation, dynamic task scheduling, data partitioning, and fault tolerance across processor nodes. We study Phoenix with multi-core and symmetric multiprocessor systems and evaluate its performance potential and error recovery features. We also compare MapReduce code to code written in lower-level APIs such as P-threads. Overall, we establish that, given a careful \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Ieee"
        },
        {
            "Title": "e case for RAMClouds: Scalable high-performance storage entirely",
            "Publication year": 2010,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.215.1286",
            "Abstract": "Disk-oriented approaches to online storage are becoming increasingly problematic: they do not scale gracefully to meet the needs of large-scale Web applications, and improvements in disk capacity have far outstripped improvements in access latency and bandwidth. This paper argues for a new approach to datacenter storage called RAMCloud, where information is kept entirely in DRAM and large-scale systems are created by aggregating the main memories of thousands of commodity servers. We believe that RAMClouds can provide durable and available storage with 100-1000x the throughput of disk-based systems and 100-1000x lower access latency. The combination of low latency and large scale will enable a new breed of dataintensive applications. 1",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:F2UWTTQJPOcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Special session paper 3D nanosystems enable embedded abundant-data computing",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8101297/",
            "Abstract": "The world's appetite for abundant-data computing, where a massive amount of structured and unstructured data is analyzed, has increased dramatically. The computational demands of these applications, such as deep learning, far exceed the capabilities of today's systems, especially for energy-constrained embedded systems (e.g., mobile systems with limited battery capacity). These demands are unlikely to be met by isolated improvements in transistor or memory technologies, or integrated circuit (IC) architectures alone. Transformative nanosystems, which leverage the unique properties of emerging nanotechnologies to create new IC architectures, are required to deliver unprecedented functionality, performance, and energy efficiency. We show that the projected energy efficiency benefits of domain-specific 3D nanosystems is in the range of 1,000x (quantified using the product of system-level energy \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:nVrZBo8bIpAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Asmdb: understanding and mitigating front-end stalls in warehouse-scale computers",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3307650.3322234",
            "Abstract": "The large instruction working sets of private and public cloud workloads lead to frequent instruction cache misses and costs in the millions of dollars. While prior work has identified the growing importance of this problem, to date, there has been little analysis of where the misses come from, and what the opportunities are to improve them. To address this challenge, this paper makes three contributions. First, we present the design and deployment of a new, always-on, fleet-wide monitoring system, AsmDB, that tracks front-end bottlenecks. AsmDB uses hardware support to collect bursty execution traces, fleet-wide temporal and spatial sampling, and sophisticated offline post-processing to construct full-program dynamic control-flow graphs. Second, based on a longitudinal analysis of AsmDB data from real-world online services, we present two detailed insights on the sources of front-end stalls:(1) cold code that is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:1yWc8FF-_SYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dbos: A proposal for a data-centric operating system",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2007.11112",
            "Abstract": "Current operating systems are complex systems that were designed before today's computing environments. This makes it difficult for them to meet the scalability, heterogeneity, availability, and security challenges in current cloud and parallel computing environments. To address these problems, we propose a radically new OS design based on data-centric architecture: all operating system state should be represented uniformly as database tables, and operations on this state should be made via queries from otherwise stateless tasks. This design makes it easy to scale and evolve the OS without whole-system refactoring, inspect and debug system state, upgrade components without downtime, manage decisions using machine learning, and implement sophisticated security features. We discuss how a database OS (DBOS) can improve the programmability and performance of many of today's most important applications and propose a plan for the development of a DBOS proof of concept.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:kWvqk_afx_IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Viram1: A media-oriented vector processor with embedded dram",
            "Publication year": 2004,
            "Publication url": "https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/Papers/2004/dac04iram.pdf",
            "Abstract": "Processors for mobile multimedia devices must be low power while having excellent performance on media applications. Our processor, VIRAM1, accomplishes this by combining vector processing with embedded DRAM. VIRAM1 includes a scalar core, 13 megabytes (104 megabits) of DRAM, and four vector datapaths. It consumes 2 watts at 200 MHz and executes up to 9.6 giga-ops (16 bit) per second.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:xtRiw3GOFMkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Evaluating bufferless flow control for on-chip networks",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5507566/",
            "Abstract": "With the emergence of on-chip networks, the power consumed by router buffers has become a primary concern. Bufferless flow control addresses this issue by removing router buffers, and handles contention by dropping or deflecting flits. This work compares virtual-channel (buffered) and deflection (packet-switched bufferless) flow control. Our evaluation includes optimizations for both schemes: buffered networks use custom SRAM-based buffers and empty buffer bypassing for energy efficiency, while bufferless networks feature a novel routing scheme that reduces average latency by 5%. Results show that unless process constraints lead to excessively costly buffers, the performance, cost and increased complexity of deflection flow control outweigh its potential gains: bufferless designs are only marginally (up to 1.5%) more energy efficient at very light loads, and buffered networks provide lower latency and higher \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:fPk4N6BV_jEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Trevor: Automatic configuration and scaling of stream processing pipelines",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1812.09442",
            "Abstract": "Operating a distributed data stream processing workload efficiently at scale is hard. The operator of the workload must parallelize and lay out tasks of the workload with resources that match the requirement of target data rate. The challenge is that neither the operator nor the programmer is typically aware of the scaling behavior of the workload as a function of resources. An operator manually searches for a safe operating point that can handle predicted peak load and deploys with ample headroom for absorbing unpredictable spikes. Such empirical, static over-provisioning is wasteful of both compute and human resources. We show that precise performance models can be automatically learned for distributed stream processing systems that can predict the execution performance of a job even before deployment. Further, those models can be used to optimally schedule logically specified jobs onto available physical hardware. Finally, those models and the derived execution schedules can be refined online to dynamically adapt to unpredictable changes in the runtime environment or auto-scale with variations in job load.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:kw52XkFRtyQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A case against (most) context switches",
            "Publication year": 2021,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3458336.3465274",
            "Abstract": "Multiplexing software threads onto hardware threads and serving interrupts, VM-exits, and system calls require frequent context switches, causing high overheads and significant kernel and application complexity. We argue that context switching is an idea whose time has come and gone, and propose eliminating it through a radically different hardware threading model targeted to solve software rather than hardware problems. The new model adds a large number of hardware threads to each physical core-making thread multiplexing unnecessary-and lets software manage them. The only state change directly triggered in hardware by system calls, exceptions, and asynchronous hardware events will be blocking and unblocking hardware threads. We also present ISA extensions to allow kernel and user software to exploit this new threading model. Developers can use these extensions to eliminate interrupts and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:eO3_k5sD8BwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Energy proportionality and workload consolidation for latency-critical applications",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2806777.2806848",
            "Abstract": "Energy proportionality and workload consolidation are important objectives towards increasing efficiency in large-scale datacenters. Our work focuses on achieving these goals in the presence of applications with \u03bcs-scale tail latency requirements. Such applications represent a growing subset of datacenter workloads and are typically deployed on dedicated servers, which is the simplest way to ensure low tail latency across all loads. Unfortunately, it also leads to low energy efficiency and low resource utilization during the frequent periods of medium or low load.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:FPJr55Dyh1AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exploiting on-chip memory bandwidth in the VIRAM compiler",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-44570-6_8",
            "Abstract": "Many architectural ideas that appear to be useful from a hardware standpoint fail to achieve wide acceptance due to lack of compiler support. In this paper we explore the design of the VIRAM architecture from the perspective of compiler writers, describing some of the code generation problems that arise in VIRAM and their solutions in the VIRAM compiler. VIRAM is a single chip system designed primarily for multimedia. It combines vector processing with mixed logic and DRAM to achieve high performance with relatively low energy, area, and design complexity. The paper focuses on two aspects of the VIRAM compiler and architecture. The first problem is to take advantage of the on-chip bandwidth for memory-intensive applications, including those with non-contiguous or unpredictable memory access patterns. The second problem is to support that kinds of narrow data types that arise in media processing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:YOwf2qJgpHMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Vector lane threading",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1690605/",
            "Abstract": "Multi-lane vector processors achieve excellent computational throughput for programs with high data-level parallelism (DLP). However, application phases without significant DLP are unable to fully utilize the datapaths in the vector lanes. In this paper, we propose vector lane threading (VLT), an architectural enhancement that allows idle vector lanes to run short-vector or scalar threads. VLT-enhanced vector hardware can exploit both data-level and thread-level parallelism to achieve higher performance. We investigate implementation alternatives for VLT, focusing mostly on the instruction issue bandwidth requirements. We demonstrate that VLT's area overhead is small. For applications with short vectors, VLT leads to additional speedup of IA to 23 over the base vector design. For scalar threads, VLT outperforms a 2-way CMP design by a factor of two. Overall, VLT allows vector processors to reach high \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:a0OBvERweLwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improving system energy efficiency with memory rank subsetting",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2133382.2133386",
            "Abstract": "VLSI process technology scaling has enabled dramatic improvements in the capacity and peak bandwidth of DRAM devices. However, current standard DDRx DIMM memory interfaces are not well tailored to achieve high energy efficiency and performance in modern chip-multiprocessor-based computer systems. Their suboptimal performance and energy inefficiency can have a significant impact on system-wide efficiency since much of the system power dissipation is due to memory power. New memory interfaces, better suited for future many-core systems, are needed. In response, there are recent proposals to enhance the energy efficiency of main-memory systems by dividing a memory rank into subsets, and making a subset rather than a whole rank serve a memory request.We holistically assess the effectiveness of rank subsetting from system-wide performance, energy-efficiency, and reliability perspectives \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:eq2jaN3J8jMC",
            "Publisher": "ACM"
        },
        {
            "Title": "Dynamic fine-grain scheduling of pipeline parallelism",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6113785/",
            "Abstract": "Scheduling pipeline-parallel programs, defined as a graph of stages that communicate explicitly through queues, is challenging. When the application is regular and the underlying architecture can guarantee predictable execution times, several techniques exist to compute highly optimized static schedules. However, these schedules do not admit run-time load balancing, so variability introduced by the application or the underlying hardware causes load imbalance, hindering performance. On the other hand, existing schemes for dynamic fine-grain load balancing (such as task-stealing) do not work well on pipeline-parallel programs: they cannot guarantee memory footprint bounds, and do not adequately schedule complex graphs or graphs with ordered queues. We present a scheduler implementation for pipeline-parallel programs that performs fine-grain dynamic load balancing efficiently. Specifically, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:qjMakFHDy7sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "ATLAS: A Scalable Emulator for Transactional Parallel Systems",
            "Publication year": 2005,
            "Publication url": "http://csl.stanford.edu/~christos/publications/2005.atlas.warfp.pdf",
            "Abstract": "With uniprocessor systems running into instruction-level parallelism (ILP) limits and fundamental VLSI constraints, multiprocessor architectures provide a realistic path towards scalable performance. Nevertheless, the key factors limiting the potential of multiprocessor systems are the difficulty of parallel application development and the hardware complexity of large-scale systems. Several groups have recently proposed parallel software and hardware based on the concept of transactions as a novel approach for addressing the problems of multiprocessor systems [1-6]. Transactions have the potential to simplify parallel programming by eliminating the need for manual orchestration of parallel tasks using locks and messages [5]. Transactions have the potential to improve parallel hardware by allowing for speculative parallelism and increasing the granularity of the coherence and consistency protocols [4].To realize the potential of transactional systems, researchers must address challenges that span across the fields of architecture, programming models, compilers, and operating systems. It is particularly important to prove that transactional software and hardware work well with large enterprise, scientific, and cognitive workloads, which rely on the performance potential of multiprocessor chips. While architectural studies of small-scale systems with reduced datasets are certainly possible through simulation, slowdowns of 5 orders of magnitude are typical when simulating large-scale parallel systems with large enterprise or scientific applications and modern operating systems [7]. The prohibitive slowdowns deter aggressive experiments and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:f2IySw72cVMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Hardware Enforcement of Application Security Policies Using Tagged Memory.",
            "Publication year": 2008,
            "Publication url": "https://www.usenix.org/legacy/event/osdi08/tech/full_papers/zeldovich/zeldovich.pdf",
            "Abstract": "Computers are notoriously insecure, in part because application security policies do not map well onto traditional protection mechanisms such as Unix user accounts or hardware page tables. Recent work has shown that application policies can be expressed in terms of information flow restrictions and enforced in an OS kernel, providing a strong assurance of security. This paper shows that enforcement of these policies can be pushed largely into the processor itself, by using tagged memory support, which can provide stronger security guarantees by enforcing application security even if the OS kernel is compromised.We present the Loki tagged memory architecture, along with a novel operating system structure that takes advantage of tagged memory to enforce application security policies in hardware. We built a full-system prototype of Loki by modifying a synthesizable SPARC core, mapping it to an FPGA board, and porting HiStar, a Unix-like operating system, to run on it. One result is that Loki allows HiStar, an OS already designed to have a small trusted kernel, to further reduce the amount of trusted code by a factor of two, and to enforce security despite kernel compromises. Using various workloads, we also demonstrate that HiStar running on Loki incurs a low performance overhead.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Practical FPGA-based Framework for Novel CMP",
            "Publication year": 2007,
            "Publication url": "https://scholar.google.com/scholar?cluster=12666445131872052235&hl=en&oi=scholarr",
            "Abstract": "Chip-multiprocessors are quickly gaining momentum in all segments of computing. However, the practical success of CMPs strongly depends on addressing the difficulty of mul-tithreaded application development. To address this chal-lenge, it is necessary to co-develop new CMP architecture with novel programming models. Currently, architecture research relies on software simulators which are too slow to facilitate interesting experiments with CMP software without using small datasets or significantly reducing the level of detail in the simulated models. An alternative to simulation is to exploit the rich capabilities of modern FPGAs to create FPGA-based platforms for novel CMP research. This paper presents ATLAS, the first prototype for CMPs with hardware support for Transactional Memory (TM), a technology aiming to simplify parallel programming. ATLAS uses the BEE2 multi-FPGA board to provide a system with 8 PowerPC cores that run at 100MHz and runs Linux. ATLAS provides significant benefits for CMP research such as 100x performance improvement over a software simulator and good visibility that helps with software tuning and architectural improvements. In addition to presenting and evaluating ATLAS, we share our observations about building a FPGA-based framework for CMP research. Specifically, we address issues such as overall performance, challenges of mapping ASIC-style CMP RTL on to FPGAs, software support, the selection criteria for the base processor, and the challenges of using pre-designed IP libraries.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:YsrPvlHIBpEC",
            "Publisher": "Association for Computing Machinery"
        },
        {
            "Title": "Centralized core-granular scheduling for serverless functions",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3357223.3362709",
            "Abstract": "In recent years, many applications have started using serverless computing platforms primarily due to the ease of deployment and cost efficiency they offer. However, the existing scheduling mechanisms of serverless platforms fall short in catering to the unique characteristics of such applications: burstiness, short and variable execution times, statelessness and use of a single core. Specifically, the existing mechanisms fall short in meeting the requirements generated due to the combined effect of these characteristics: scheduling at a scale of millions of function invocations per second while achieving predictable performance.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:AXkvAH5U_nMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Overcoming the limitations of conventional vector processors",
            "Publication year": 2003,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/859618.859664",
            "Abstract": "Despite their superior performance for multimedia applications, vector processors have three limitations that hinder their widespread acceptance. First, the complexity and size of the centralized vector register file limits the number of functional units. Second, precise exceptions for vector instructions are difficult to implement. Third, vector processors require an expensive on-chip memory system that supports high bandwidth at low access latency. This paper introduces CODE, a scalable vector microarchitecture that addresses these three shortcomings. It is designed around a clustered vector register file and uses a separate network for operand transfers across functional units. With extensive use of decoupling, it can hide the latency of communication across functional units and provides 26% performance improvement over a centralized organization. CODE scales efficiently to 8 functional units without requiring wide \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:JV2RwH3_ST0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "DRAF: A low-power DRAM-based reconfigurable acceleration fabric",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7948664/",
            "Abstract": "The DRAM-Based Reconfigurable Acceleration Fabric (DRAF) uses commodity DRAM technology to implement a bit-level, reconfigurable fabric that improves area density by 10 times and power consumption by more than 3 times over conventional field-programmable gate arrays. Latency overlapping and multicontext support allow DRAF to meet the performance and density requirements of demanding applications in data center and mobile environments.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:gKiMpY-AVTkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An effective hybrid transactional memory system with strong isolation guarantees",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1250662.1250673",
            "Abstract": "We propose signature-accelerated transactional memory (SigTM), ahybrid TM system that reduces the overhead of software transactions. SigTM uses hardware signatures to track the read-set and write-set forpending transactions and perform conflict detection between concurrent threads. All other transactional functionality, including dataversioning, is implemented in software. Unlike previously proposed hybrid TM systems, SigTM requires no modifications to the hardware caches, which reduces hardware cost and simplifies support for nested transactions and multithreaded processor cores. SigTM is also the first hybrid TM system to provide strong isolation guarantees between transactional blocks and non-transactional accesses without additional read and write barriers in non-transactional code.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:RHpTSmoSYBkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Plasticine: a reconfigurable accelerator for parallel patterns",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8357989/",
            "Abstract": "Plasticine is a new spatially reconfigurable architecture designed to efficiently execute applications composed of high-level parallel patterns. With an area footprint of 113 mm 2  in a 28-nm process and a 1-GHz clock, Plasticine has a peak floating-point performance of 12.3 single-precision Tflops and a total on-chip memory capacity of 16 MB, consuming a maximum power of 49 W. Plasticine provides an improvement of up to 76.9X in performance-per-watt over a conventional FPGA over a wide range of dense and sparse applications.",
            "Abstract entirety": 1,
            "Author pub id": "G2EJz5kAAAAJ:buQ7SEKw-1sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Explicitly parallel architectures for memory performance enhancement",
            "Publication year": 2000,
            "Publication url": "http://csl.stanford.edu/~christos/publications/2000.explicit_DATE2K.slides.pdf",
            "Abstract": "Explicitly Parallel Architectures for Memory Performance Enhancement Page 1 Explicitly \nParallel Architectures for Memory Performance Enhancement Christoforos E. Kozyrakis \nComputer Science Division University of California at Berkeley kozyraki cs.berkeley.edu http://iram.cs.berkeley.edu \nPage 2 DATE 2000 Conference, 3/29/00 CE ozyrakis, UC Berkeley 2 Thesis \u220e Memory \nperformance enhancement requires the synergy of hardware and software techniques \u220e This \ntalk: focus on hardware Explicitly parallel instruction sets for efficient communication of \nparallelism between hardware and software Exploit memory bandwidth to hide or tolerate high \nmemory latency Page 3 DATE 2000 Conference, 3/29/00 CE ozyrakis, UC Berkeley 3 Software \nHardware Strengths \u220e Software (compiler/run!time system) Better view and understanding \nof both application requirements and system resources Ability to apply complex of \"(\"\u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Raksha: a flexible information flow architecture for software security",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1273440.1250722",
            "Abstract": "High-level semantic vulnerabilities such as SQL injection and crosssite scripting have surpassed buffer overflows as the most prevalent security exploits. The breadth and diversity of software vulnerabilities demand new security solutions that combine the speed and practicality of hardware approaches with the flexibility and robustness of software systems.This paper proposes Raksha, an architecture for software security based on dynamic information flow tracking (DIFT). Raksha provides three novel features that allow for a flexible hardware/software approach to security. First, it supports flexible and programmable security policies that enable software to direct hardware analysis towards a wide range of high-level and low-level attacks. Second, it supports multiple active security policies that can protect the system against concurrent attacks. Third, it supports low-overhead security handlers that allow software to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "G2EJz5kAAAAJ:r0BpntZqJG4C",
            "Publisher": "ACM"
        }
    ]
}]