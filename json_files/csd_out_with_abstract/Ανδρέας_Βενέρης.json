[{
    "name": "\u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 \u0392\u03b5\u03bd\u03ad\u03c1\u03b7\u03c2",
    "romanize name": "Andreas Veneris",
    "School-Department": "ECE ",
    "University": "University of Toronto, Canada",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 7637,
    "Scholar name": "Andreas Veneris",
    "Scholar id": "ItQhxCkAAAAJ",
    "Affiliation": "Connaught Scholar and Professor of Electrical and Computer Engineering, cross-appointed with",
    "Citedby": 2698,
    "Interests": [
        "Blockchain technology",
        "crypto economic mechanism design",
        "CBDCs",
        "CAD for debug/verification/synthesis of smart contracts and syst"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=ItQhxCkAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "V. CONCLUSION",
            "Publication year": 2011,
            "Publication url": "https://scholar.google.com/scholar?cluster=15077709733770286005&hl=en&oi=scholarr",
            "Abstract": "Register renaming is a performance-critical component of modern, dynamically-scheduled processors. Register renaming latency increases as a function of several architectural parameters (eg, processor issue width, processor window size, and processor checkpoint count). Pipelining of the register renaming logic can help avoid restricting the processor clock frequency. This work presents a full-custom, two-stage register renaming implementation in a 130-nm fabrication technology. The latency of non-pipelined and two-stage, pipelined renaming is compared, and the underlying performance and complexity tradeoffs are discussed. The two-stage pipelined design reduces the renaming logic depth from 23 fan-out-of-four (FO4) down to 9.5 FO4.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:PELIpwtuRlgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Method, system and computer program for automated hardware design debugging",
            "Publication year": 2008,
            "Publication url": "https://patents.google.com/patent/US20080127009A1/en",
            "Abstract": "The present invention provides a method, system and computer program for automated debugging for pre-fabricated digital synchronous hardware designs implemented in Hardware Description Language (HDL). Required information is captured by interacting with the verification environment after verification fails. This capture information is used to build a diagnosis problem where the solution is a set of logic level error sources. Using the HDL information, the error at the logic level is translated to gates, modules, statements, and signals in the HDL description. The diagnosis problem can be solved efficiently formulating a Quantified Boolean Formula (QBF) problem and also by using the hierarchical and modular nature of the HDL design during diagnosis.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:bEWYMUwI8FkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Privacy and transparency in cbdcs: A regulation-by-design aml/cft scheme",
            "Publication year": 2021,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3759144",
            "Abstract": "Central banks and governments all over the world are increasingly exploring digital versions of fiat money, known as retail Central Bank Digital Currencies (CBDCs). Most initiatives rely on Distributed Ledger Technologies and are presented as alternatives to physical cash. Consequently, anonymity-related regulatory questions arise in terms of Anti-Money Laundering and Counter-Terrorist Financing compliance. Against this backdrop, this paper provides a techno-legal taxonomy of approaches to balance privacy and transparency in CBDCs without thwarting accountability, but it also underlines cross-sectoral impacts. The contribution heeds regulation-by-design as its core methodological foundation, with Privacy-Enhancing Technologies as the relevant use case. Thus, it highlights that not only technology aids legal purposes, but also that some regulatory requirements ought to be designed into technology for one to reach agreed upon results and/or standards.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:9vf0nzSNQJEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Baosheng Wang Barbara Jobstmann Bart Vermeulen Bashir M. Al-Hashimi",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5671551/",
            "Abstract": "Lists the reviewers who contributed to the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:2P1L_qKh6hAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Managing verification error traces with bounded model debugging",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5419816/",
            "Abstract": "Managing long verification error traces is one of the key challenges of automated debugging engines. Today, debuggers rely on the iterative logic array to model sequential behavior which drastically limits their application. This work presents bounded model debugging, an iterative, systematic and practical methodology to allow debuggers to tackle larger problems than previously possible. Based on the empirical observation that errors are excited in temporal proximity of the observed failures, we present a framework that improves performance by up to two orders of magnitude and solve 2.7x more problems than a conventional debugger.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:dhFuZR0502QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fault diagnosis and logic debugging using Boolean satisfiability",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1250264/",
            "Abstract": "Recent advances in Boolean satisfiability have made it attractive to solve many digital VLSI design problems such as verification and test generation. Fault diagnosis and logic debugging have not been addressed by existing satisfiability-based solutions. We attempt to bridge this gap by proposing a model-free satisfiability-based solution to these problems. The proposed formulation is intuitive and easy to implement. It shows that satisfiability captures significant problem characteristics and it offers different trade-offs. It also provides new opportunities for satisfiability-based diagnosis tools and diagnosis-specific satisfiability algorithms. Theory and experiments validate the claims and demonstrate its potential.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:pyW8ca7W8N0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Managing complexity in design debugging with sequential abstraction and refinement",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5722237/",
            "Abstract": "Design debugging is becoming an increasingly difficult task in the VLSI design flow with the growing size of modern designs and their error traces. In this work, a novel abstraction and refinement technique for design debugging is presented that addresses two key components of the debugging complexity, the design size and the error trace length. The abstraction technique works by under-approximating the debugging problem by removing modules of the original design and replacing them with simulated values of the erroneous circuit. After each abstract problem is solved, the refinement strategy uses the resulting UNSAT core to direct which modules should be refined. This refinement strategy is extended by allowing refinement of across time-frames in addition to modules. Experimental results show that the proposed algorithm is able to return solutions for all instances compared to only 41% without the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:dshw04ExmUIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "FudgeFactor: Syntax-guided synthesis for accurate RTL error localization and correction",
            "Publication year": 2015,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-26287-1_16",
            "Abstract": "Functional verification occupies a significant amount of the digital circuit design cycle. In this paper, we present a novel approach to improve circuit debugging which not only localizes errors with high confidence, but can also provide semantically-meaningful source code corrections. Our method, which we call FudgeFactor, starts with a buggy design, at least one failing and several correct test vectors, and a list of suspect bug locations. We obtain the suspect location from a state-of-the-art debugging tool that includes a significant number of false positives. Using this list and a library of rules empirically characterizing typical source-code mistakes, we instrument the buggy design to allow each potential error location to either be left unchanged, or replaced with a set of possible corrections. FudgeFactor then combines the instrumented design with the test vectors and solves a 2QBF-SAT problem to find the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:3fE2CSJIrl8C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Design diagnosis using Boolean satisfiability",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1337569/",
            "Abstract": "Recent advances in Boolean satisfiability have made it an attractive engine for solving many digital VLSI design problems such as verification, model checking, optimization and test generation. Fault diagnosis and logic debugging have not been addressed by existing satisfiability-based solutions. We attempt to bridge this gap by proposing a satisfiability-based solution to these problems. The proposed formulation is intuitive and easy to implement. It shows that satisfiability captures significant problem characteristics and it offers different trade-offs. It also provides new opportunities for satisfiability-based diagnosis tools and diagnosis-specific satisfiability algorithms. Theory and experiments validate the claims and demonstrate its potential.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:ns9cj8rnVeAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An extensible perceptron framework for revision rtl debug automation",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7858329/",
            "Abstract": "Automated debugging techniques can significantly reduce the manual effort required to localize RTL errors. These techniques return to the user a set of RTL locations where a change can correct erroneous behavior. However, each location must be manually investigated. This problem is exacerbated by the increasing amount of failures in the modern regression verification cycle. Recent work in clustering-based revision debugging mitigates this cost by ranking revisions based on their likelihood of having introduced an error. This work presents a perceptron based approach to revision debugging that can be extended to leverage the revision history of a design directly. Perceptrons are trained using labeled revisions from the design history. They are then used to predict the probability that a revision has introduced an error. The proposed methodology performs competitively with the state-of-the-art, but can be \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimal trace compaction with property preservation",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5235932/",
            "Abstract": "Debugging design errors is a challenging manual task which requires the analysis of long simulation traces. Trace compaction techniques help engineers analyze the cause of the problem by reducing the length of the trace. This work presents an optimal error trace compaction technique based on incremental SAT. The approach builds a SAT instance from the Iterative Logic Array representation of the circuit and performs a binary search to find the minimum trace length. Since failing properties in the original trace must be maintained in the compacted trace, we enrich our formulation with constraints to guarantee property preservation. Extensive experiments show the effectiveness out SAT based approach as it preserves failing properties with little overhead to the SAT problem while demonstrating on average an order of magnitude in performance improvement.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:IWHjjKOFINEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Method and system of data processor design by sensitizing logical difference",
            "Publication year": 2006,
            "Publication url": "https://patents.google.com/patent/US7003743B2/en",
            "Abstract": "A method of optimizing a design is disclosed, wherein a target element contributing to an undesirable characteristic in an original netlist is modified to create a modified netlist. A set of test vectors identifying differences between the original netlist and the modified netlist is identified and used to identify a set of corrections. In one disclosed embodiment, the set of corrections is identified by using an error correction algorithm. Each correction of the set of corrections, when applied to the modified netlist, results in a corrected netlist logically the same as the original netlist. One of the corrections is selected that improves the error characteristic of the original netlist. A final equivalency verification is performed as necessary.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:GnPB-g6toBAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Path-directed abstraction and refinement for sat-based design debugging",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6601029/",
            "Abstract": "Functional verification has become one of the most time-consuming tasks in the very large scale integration design flow accounting for up to 57% of the total project time. The largest component of this task is that of design debugging due to its resource-intensive manual nature. With the ever growing size of modern designs and their error traces, the complexity of the debugging problem poses a great challenge to automated debugging techniques. To overcome this challenge, this paper introduces a novel path-directed abstraction and refinement algorithm for design debugging to manage excessive error trace lengths. A sliding window of the error trace is iteratively analyzed in a time-windowing framework, which is made possible by the use of the path-directed abstraction. This abstraction forms a concise approximation of nonmodeled parts of the error trace while simultaneously providing an efficient representation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:d1gkVwhDpl0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Abstraction and refinement techniques in automated design debugging",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4211965/",
            "Abstract": "Verification is a major bottleneck in the VLSI design flow with the tasks of error detection, error localization, and error correction consuming up to 70% of the overall design effort. This work proposes a departure from conventional debugging techniques by introducing abstraction and refinement during error localization. Under this new framework, existing debugging techniques can handle large designs with long counter-examples yet remain run time and memory efficient. Experiments on benchmark and industrial designs confirm the effectiveness of the proposed framework and encourage further development of abstraction and refinement methodologies for existing debugging techniques",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A physical level study and optimization of CAM-based checkpointed register alias table",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5529049/",
            "Abstract": "Using full-custom layouts in 130 nm technology, this work studies how the latency and energy of a checkpointed, CAM-based Register Alias Table (cRAT) vary as a function of the window size, the issue width, and the number of embedded global checkpoints (GCs). These results are compared to those of the SRAM-based RAT (sRAT). Understanding these variations is useful during the early stages of architectural exploration where physical level information is not yet available. It is found that compared to sRAT, cRAT is more sensitive to the number of physical registers and issue width, however, it is less sensitive to the number of GCs. In addition, beyond a certain number of GCs, cRAT becomes faster than its equivalent sRAT. For instance, this is true when a RAT for 64 architectural and 128 physical registers has at least 20 GCs. This work also proposes an energy optimization for the cRAT; this optimization \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:UxriW0iASnsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Maximum circuit activity estimation using pseudo-boolean satisfiability",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6132653/",
            "Abstract": "With lower supply voltages, increased integration densities and higher operating frequencies, power grid verification has become a crucial step in the very large-scale integration design cycle. The accurate estimation of maximum instantaneous power dissipation aims at finding the worst-case scenario where excessive simultaneous switching could impose extreme current demands on the power grid. This problem is highly input-pattern dependent and is proven to be NP-hard. In this paper, we capitalize on the compelling advancements in satisfiability (SAT) solvers to propose a pseudo-Boolean SAT-based framework that reports the input patterns maximizing circuit activity, and consequently peak dynamic power, in combinational and sequential circuits. The proposed framework is enhanced to handle unit gate delays and output glitches. In order to disallow unrealistic input transitions, we show how to integrate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:zA6iFVUQeVQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Engineering economics in the conflux network",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9223310/",
            "Abstract": "Proof-of-work blockchains need to be carefully designed so as to create the proper incentives for miners to faithfully maintain the network in a sustainable way. This paper describes how the economic engineering of the Conflux Network, a high throughput proof-of-work blockchain, leads to sound economic incentives that support desirable and sustainable mining behavior. In detail, this paper parameterizes the level of income, and thus network security, that Conflux can generate, and it describes how this depends on user behavior and \u201cpolicy variables\u201d such as block and interest inflation. It also discusses how the underlying economic engineering design makes the Conflux Network resilient against double spending and selfish mining attacks.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:p2g8aNsByqUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Clustering-based failure triage for rtl regression debugging",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7035339/",
            "Abstract": "Regression verification at the pre-silicon stage has experienced a dramatic boost in capabilities over the past years. With the aid of assertions, improved simulation coverage and formal verification tools, a vast amount of trace data and myriads of failures are often generated after each regression run. Along these lines, modern flows face an emerging need to appropriately categorize, prioritize and distribute these failures to the engineer(s) best-suited for detailed debugging of each failure. This task is known as failure triage. Despite its resource-intensive nature, triage remains a predominantly manual process. In this work, an automated data-mining failure triage framework is introduced that mines simulation and SAT-based design debugging data, uncovers relations among verification failures and automatically groups the related ones together. The core characteristic of the framework is a novel feature-based \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:roLk4NBRz8UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Efficient suspect selection in unreachable state diagnosis",
            "Publication year": 2018,
            "Publication url": "http://www.eecg.toronto.edu/~veneris/18amai.pdf",
            "Abstract": "In the modern hardware design cycle, correcting the design when verification reveals a state to be erroneously unreachable can be a time-consuming manual process. Recently-developed algorithms aid the engineer in finding the root cause of the failure in these cases. However, they exhaustively examine every design location to determine a set of possible root causes, potentially requiring substantial runtime. This work develops a novel approach that is applicable to practical diagnosis problems. In contrast to previous approaches, it considers only a portion of the design locations but still finds the complete solution set to the problem. The presented approach proceeds through a series of iterations, each considering a strategically-chosen subset of the design locations (a suspect set) to determine if they are root causes. The results of each iteration inform the choice of suspect set for the next iteration. By choosing the first iteration\u2019s suspect set appropriately, the algorithm is able to find the complete solution set to the problem. Empirical results on industrial designs and standard benchmark designs demonstrate a 15x speedup compared to the previous approach, while considering only 18.7% of the design locations as suspects.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:t6usbXjVLHcC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "Chasing Minimal Inductive Validity Cores in Hardware Model Checking",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8894268/",
            "Abstract": "Model checking of safety properties is fundamental in formal verification. When a safety property is found to hold, the model checker provides (at best) a machine-checkable certificate that gives limited insight to users and little confidence that the check passes for the \u201cright\u201d reasons, rather than due to e.g., vacuity or unjustified assumptions. Recently, inductive validity cores (IVCs) have been developed to address this issue. In this paper, we lift several algorithms from the field of UNSAT core extraction in order to compute minimal IVCs of hardware safety checking problems. The MARCO algorithm extracts all minimal cores of an UNSAT formula by efficiently exploring the formula's power set, and has already been applied to compute IVCs in software safety checking. The CAMUS algorithm for UNSAT core extraction exploits a duality between minimal correction subsets (MCSes) of a formula and minimal UNSAT cores \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:4DMP91E08xMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Design rewiring using ATPG",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1097866/",
            "Abstract": "Logic optimization is the step of the very large scale integration (VLSI) design cycle where the designer performs modifications on a design to satisfy different constraints such as area, power, or delay. Recently, automated test pattern generation (ATPG)-based design rewiring techniques for technology-dependent logic optimization have gained increasing popularity. In this paper, the authors propose a new operational framework to design rewiring that uses ATPG and diagnosis algorithms. They also examine its complexity requirements and discuss different implementation tradeoffs. To perform this study, the authors reduce the problem of design rewiring to the process of injecting a redundant set of multiple pattern faults. This formulation arrives at a new set of results with theoretical and practical applications. Experiments demonstrate the competitiveness of the approach and motivate future work in the area.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:mVmsd5A6BfQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On simulation-based metrics that characterize the behavior of RTL errors.",
            "Publication year": 2016,
            "Publication url": "https://www.eecg.utoronto.ca/~veneris/scsm16.pdf",
            "Abstract": "Recent advances in automated debugging offer significant reductions in the manual effort required to localize RTL errors. These tools return relatively compact sets of RTL locations that can be potential error sources. However, once these locations are returned, the engineer still has to perform detailed analysis to discard irrelevant locations and identify the culprit. This process happens without any further guidance from the debugger. In this work, we perform a statistical analysis that exposes a significant discrepancy between RTL errors and other unrelated locations returned by these tools. The analysis is conducted on industrial designs and is based on metrics extracted from simulation. Our methodology determines that specific continuous distributions can effectively characterize the behavior of RTL errors. Using these welldefined metrics one can automate the process of further pruning the RTL locations returned by debuggers, effectively accelerating the localization of error sources.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:9yKSN-GCB0IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Functional fault equivalence and diagnostic test generation in combinational logic circuits using conventional ATPG",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/article/10.1007/s10836-005-1543-z",
            "Abstract": "Fault equivalence is an essential concept in digital design with significance in fault diagnosis, diagnostic test generation, testability analysis and logic synthesis. In this paper, an efficient algorithm to check whether two faults are equivalent is presented. If they are not equivalent, the algorithm returns a test vector that distinguishes them. The proposed approach is complete since for every pair of faults it either proves equivalence or it returns a distinguishing vector. The advantage of the approach lies in its practicality since it uses conventional ATPG and it automatically benefits from advances in the field. Experiments on ISCAS\u201985 and full-scan ISCAS\u201989 circuits demonstrate the competitiveness of the method and measure the performance of simulation for fault equivalence.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:tS2w5q8j5-wC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Automated data analysis solutions to silicon debug",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5090807/",
            "Abstract": "Since pre-silicon functional verification is insufficient to detect all design errors, re-spins are often needed due to malfunctions that escape into the silicon. This paper presents an automated software solution to analyze the data collected during silicon debug. The proposed methodology analyzes the test sequences to detect suspects in both the spatial and the temporal domain. A set of software debug techniques are proposed to analyze the acquired data from the hardware testing and provide suggestions for the setup of the test environment in the next debug session. A comprehensive set of experiments demonstrate its effectiveness in terms of run-time and resolution.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:4JMBOYKVnBMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automated data analysis techniques for a modern silicon debug environment",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6164963/",
            "Abstract": "With the growing size of modern designs and more strict time-to-market constraints, design errors unavoidably escape pre-silicon verification and reside in silicon prototypes. As a result, silicon debug has become a necessary step in the digital integrated circuit design flow. Although embedded hardware blocks, such as scan chains and trace buffers, provide a means to acquire data of internal signals in real time for debugging, there is a relative shortage in methodologies to efficiently analyze this vast data to identify root-causes. This paper presents an automated software solution that attempts to fill-in the gap. The presented techniques automate the configuration process for trace-buffer based hardware in order to acquire helpful information for debugging the failure, and detect suspects of the failure in both the spatial and temporal domain.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:8k81kl-MbHgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Variational autoencoders: A hands-off approach to volatility",
            "Publication year": 2021,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3827447",
            "Abstract": "A volatility surface is an important tool for pricing and hedging derivatives. The surface shows the volatility that is implied by the market price of an option on an asset as a function of the option's strike price and maturity. Often, market data is incomplete and it is necessary to estimate missing points on partially observed surfaces. In this paper, we show how variational autoencoders can be used for this task. The first step is to derive latent variables that can be used to construct synthetic volatility surfaces that are indistinguishable from those observed historically. The second step is to determine the synthetic surface generated by our latent variables that fits available data as closely as possible. As a dividend of our first step, the synthetic surfaces produced can also be used in stress testing, in market simulators for developing quantitative investment strategies, and for the valuation of exotic options. We illustrate our procedure and demonstrate its power using foreign exchange market data.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:5awf1xo2G04C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fault diagnosis using quantified boolean formulas",
            "Publication year": 2007,
            "Publication url": "http://www.eecg.utoronto.ca/~veneris/sdd07.pdf",
            "Abstract": "Automatic debugging of sequential circuits has been considered a practically intractable task due to the excessive memory and run-time requirements associated with tackling industrial-size problems. This paper proposes a novel Quantified Boolean Formula (QBF) based approach for fault diagnosis in sequential circuits. A performance-driven succinct QBF encoding of the problem, coupled with the tremendous present-day advances in QBF solvers make this strategy a successful one. Extensive experiments on industrial circuits confirm the memory advantage and demonstrate the outstanding performance of the proposed framework.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:YFjsv_pBGBYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automated Logic Restructuring with aSPFDs",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-1-4419-7518-8_15",
            "Abstract": "This chapter presents a comprehensive methodology to automate logic restructuring in combinational and sequential circuits. This technique algorithmically constructs the required transformation by utilizing a functional flexibility representation called Set of Pairs of Function to be Distinguished (SPFD). SPFDs can express more functional flexibility than the traditional don\u2019t cares and have proved to provide additional degrees of flexibility during logic synthesis [21, 27]. Computing SPFDs may suffer from memory or runtime problems [16]. Therefore, a simulation-based approach to approximate SPFDs is presented to alleviate those issues. The result is called Approximate SPFDs (aSPFDs).                                                                                               aSPFDs approximate the information contained in SPFDs using the results of test vector simulation. With the use of aSPFDs as a guideline, the algorithm searches for the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:hqOjcs7Dif8C",
            "Publisher": "Springer, New York, NY"
        },
        {
            "Title": "Blockchain for v2x: A taxonomy of design use cases and system requirements",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9569796/",
            "Abstract": "Vehicles today contain a multitude of sensors creating vast amounts of data. For many applications, these data need to be shared with other entities so that they can also utilize it. Vehicle-to-everything (V2X) is the amalgamation of all potential vehicle communication systems. V2X technologies are enabling many smart-vehicle applications, such as autonomous vehicles. However, in utilizing these data from external entities, vehicles rely on the availability and trustworthiness of centralized entities who may be able to delete, forge, leak, or otherwise tamper with the underlying data. Blockchain technology provides a decentralized mechanism to allow vehicles to validate data they receive in a trustless manner. This paper explores potential applications of blockchain technology in the V2X space, categorizing and analyzing use cases based on their underlying blockchain requirements. It then uses this analysis to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:ye4kPcJQO24C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automated silicon debug data analysis techniques for a hardware data acquisition environment",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5450506/",
            "Abstract": "Silicon debug poses a unique challenge to the engineer because of the limited access to internal signals of the chip. Embedded hardware such as trace buffers helps overcome this challenge by acquiring data in real time. However, trace buffers only provide access to a limited subset of pre-selected signals. In order to effectively debug, it is essential to configure the trace-buffer to trace the relevant signals selected from the pre-defined set. This can be a labor-intensive and time-consuming process. This paper introduces a set of techniques to automate the configuring process for trace buffer-based hardware. First, the proposed approach utilizes UNSAT cores to identify signals that can provide valuable information for localizing the error. Next, it finds alternatives for signals not part of the traceable set so that it can imply the corresponding values. Integrating the proposed techniques with a debugging methodology \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:_kc_bZDykSQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Logic verification based on diagnosis techniques",
            "Publication year": 2003,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1119772.1119791",
            "Abstract": "We present a formal logic verification methodology for combinational circuits. The method uses simulation, logic diagnosis and ATPG to identify circuit lines that implement equivalent logic functions efficiently. One advantage of the proposed technique is that it identifies line equivalences under controllability and observability don't care conditions, while not suffering from false negatives. The method is easy to implement, and, due to its general nature, existing techniques can benefit from ideas described here. We also give implementation details and present experiments to confirm its potential.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:5Ul4iDaHHb8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Debugging with dominance: On-the-fly RTL debug solution implications",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6105390/",
            "Abstract": "Design debugging has become a resource-intensive bottleneck in modern VLSI CAD flows, consuming as much as 60% of the total verification effort. With typical design sizes exceeding the half-million synthesized gates mark, the growing number of blocks to be examined dramatically slows down the debugging process. The aim of this work is to prune the number of debugging iterations for finding all potential bugs, without affecting the debugging resolution. This is achieved by using structural dominance relationships between circuit components. More specifically, an iterative fixpoint algorithm is presented for finding dominance relationships between multiple-output blocks of the design. These relationships are then leveraged for the early discovery of potential bugs, along with their corrections, resulting in significant debugging speed-ups. Extensive experiments on real industrial designs show that 66% of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:j3f4tGmQtD8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Diagnosing unreachable states using property directed reachability",
            "Publication year": 2015,
            "Publication url": "http://www.eecg.toronto.edu/~veneris/2cfv15.pdf",
            "Abstract": "In the modern design cycle, substantial manual effort is required to correct errors found when verification reveals an unreachable state. This work introduces two methodologies to automate this task. Given an unreachable target state, both methodologies return a set of design locations where changes can be implemented to make the target state reachable (ie, solutions). The first methodology uses intertwined steps of reachable state-space approximation, property checking, and traditional debugging to compute a subset of the solutions that make the target state reachable in some fixed number of clock cycles. The second methodology uses property-directed reachability with an enhanced version of the circuit\u2019s transition relation to compute the complete set of solutions to the problem. As an additional benefit, it returns an inductive invariant proving that no further solution (s) exist. The completeness of the approach comes at the cost of increased runtime when compared to the first methodology. Empirical results on industrial designs confirm the effectiveness of both approaches.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:l7t_Zn2s7bgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the minimization of potential transient errors and SER in logic circuits using SPFD",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4567073/",
            "Abstract": "Sets of Pairs of Functions to be Distinguished (SPFD) is a functional flexibility representation method that was recently introduced in the logic synthesis domain, and promises superiority in exploring the flexibility offered by a design over all previous representation methods. In this work, we illustrate how the SPFD of a particular wire reveals information regarding the number of potential transient errors that may occur on that wire and may affect the output of the circuit. Using an SPFD-based rewiring method, we then demonstrate how to evolve a logic circuit in order to minimize the total number of potential transient errors in the circuit and, consequently, reduce its Soft Error Rate (SER) while controlling the effect on the rest of the design parameters, such as area, power, delay, and testability. Experimental results on ISCAS'89 and ITC'99 benchmark circuits indicate that the SER can be reduced at no additional \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:-f6ydRqryjwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automating logic rectification by approximate SPFDs",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4196065/",
            "Abstract": "In the digital VLSI cycle, a netlist is often modified to correct design errors, perform small specification changes or implement incremental rewiring-based optimization operations. Most existing automated logic rectification tools use a small set of predefined logic transformations when they perform such modifications. This paper first shows that a small set of predefined transformations may not allow rectification to exploit the full potential of the design. Then, it proposes an automated simulation-based methodology to \"approximate\" sets of pairs of functions to be distinguished (SPFDs) and avoid the memory/time explosion problem. This representation is used by a SAT-based algorithm that devises appropriate logic transformations to fix a design. The SAT method is later complemented by a greedy one that improves on runtime performance. An extensive suite of experiments documents the added potential of the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:f2IySw72cVMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The day Sherlock Holmes decided to do EDA",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1629911.1630078",
            "Abstract": "Semiconductor design companies are in a continuous search for design tools that address the ever increasing chip design complexity coupled with strict time-to-market schedules and budgetary constraints. A fundamental aspect of the design process that remains primitive is that of debugging. It takes months to close, it introduces costs and it may jeopardize the release date of the chip. This paper reviews the debugging problem and the research behind it over the past 20 years. The case for automated RTL debug tools and methodologies is also made to help ease the manual burden and complement current industrial verification practices.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:OU6Ihb5iCvQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Leveraging Software Configuration Management in Automated RTL Design Debug",
            "Publication year": 2017,
            "Publication url": "https://www.eecg.utoronto.ca/~veneris/dandt2017.pdf",
            "Abstract": "\u25a0 FUNCTIONAL VERIFICATION OF HARDWARE designs is the major bottleneck in the design cycle today, accounting for up to 70% of the total time [1]. The majority of this effort is taken by debugging, a process that identifies errors once verification demonstrates a failure. As debug remains a semiautomated task, new tools and methodologies are needed to contain the pain and improve the modern verification cycle. Traditionally, given a set of counterexamples where the design fails, debug returns suspect lines that may contain the error source (s). In the modern verification cycle, these counterexamples are generated by simulation and formal tools that exercise different parts of the design\u2019s functional behavior. Failures are usually recorded by monitors, checkers, scoreboards, or assertions. When a failure is exposed, the counterexample (s) are used by the engineer to manually trace through the design with the help of waveform viewers to discover the error, or by debugging tools that automatically return suspect locations. These debug tools predominantly use a combination of simulation and formal techniques, such as path-trace, Boolean satisfiability (SAT) and binary decision diagrams, to prune the solution space and look for suspects [2]\u2013[4].Waveform-based debug tools allow users to backtrace through the circuit using information obtained from the counterexamples. They can also perform what-if analysis and resimulation on the fly, allowing users to inspect how changes to the circuit will propagate forward. This aids the debug process, but still requires a significant amount of manual effort to decide where changes should be made \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:O3NaXMp0MMsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Incremental diagnosis of multiple open-interconnects",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1041865/",
            "Abstract": "With increasing chip interconnect distances, open-interconnect is becoming an important defect. The main challenge with open-interconnects stems from its non-deterministic real-life behavior In this work, we present an efficient diagnostic technique for multiple open-interconnects. The algorithm proceeds in two phases. During the first phase, potential solution sets are identified following a model-free incremental diagnosis methodology. Heuristics are devised to speed up this step and screen the solution space efficiently. In the second phase, a generalized fault simulation scheme enumerates all possible faulty behaviors for each solution from the first phase. We conduct experiments on combinational and full-scan sequential circuits with one, two and three open faults. The results are very encouraging.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:8AbLer7MMksC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Message from the ICBC2020 General and Technical Program Chairs",
            "Publication year": 2020,
            "Publication url": "https://experts.illinois.edu/en/publications/message-from-the-icbc2020-general-and-technical-program-chairs",
            "Abstract": "Message from the ICBC2020 General and Technical Program Chairs \u2014 University of Illinois \nUrbana-Champaign Skip to main navigation Skip to search Skip to main content University of \nIllinois Urbana-Champaign Logo Help & FAQ Home Profiles Research Units Research & \nScholarship Datasets Activities Press / Media Honors Search by expertise, name or affiliation \nMessage from the ICBC2020 General and Technical Program Chairs Kostas Plataniotis, Andreas \nVeneris, Salil Kanhere, Grigore Rosu, Knottenbelt William Computer Science Information \nTrust Institute Research output: Contribution to journal \u203a Editorial \u203a peer-review Overview \nOriginal language English (US) Article number 9169409 Pages (from-to) 5-6 Number of pages \n2 Journal IEEE International Conference on Blockchain and Cryptocurrency, ICBC 2020 DOIs \nhttps://doi.org/10.1109/ICBC48266.State Published - May 2020 Event 2nd IEEE Conference \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:KlAtU1dfN6UC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning support sets in IC3 and Quip: The good, the bad, and the ugly",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8102252/",
            "Abstract": "In recent years, IC3 has enjoyed wide adoption by academia and industry as an unbounded model checking engine. The core algorithm works by learning lemmas that, given a safe property, eventually converge to an inductive proof. As such, its runtime performance is heavily dependent upon \u201cpushing\u201d (or \u201cpromoting\u201d) important lemmas, possibly by discovering additional supporting lemmas. More recently, Quip has emerged to be a complementary extension behind the reasoning capabilities of IC3 as it allows it to target particular lemmas for pushing. This also raises the following question: which lemmas should be promoted? To that end, this paper extends the reasoning capabilities of IC3 and Quip using special SAT queries to find support sets that represent fine-grained information on which lemmas are required to push other lemmas. Further, this paper presents an IC3-based algorithm called Truss (Testing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:TFP_iSt0sucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Debugging RTL using structural dominance",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6685866/",
            "Abstract": "Register-transfer level (RTL) debug has become a resource-intensive bottleneck in modern very large scale integration computer-aided design flows, consuming as much as 32% of the total verification effort. This paper aims to advance the state-of-the-art in automated RTL debuggers, which return all potential bugs in the RTL, called solutions, along with corresponding corrections. First, an iterative algorithm is presented to compute the dominance relationships between RTL blocks. These relationships are leveraged to discover implied solutions with every new solution, thus significantly reducing the number of formal engine calls. Furthermore, a modern Boolean satisfiability (SAT) solver is tailored to detect debugging nonsolutions, sets of RTL blocks guaranteed to be bug-free, and to imply other nonsolutions using the precomputed RTL dominance relationships. Extensive experiments on industrial designs show a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:ZHo1McVdvXMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Utilizing don't care states in SAT-based bounded sequential problems",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1057661.1057725",
            "Abstract": "Boolean Satisfiability (SAT) solvers are popular engines used throughout the verification world. Bounded sequential problems such as bounded model checking and bounded sequential equivalence checking rely on fast and robust SAT solvers. In this work, we introduce a technique that improves the performance of the underlying SAT solver for bounded sequential problems by taking advantage of a design's don't care states. We develop cost effective methods of filtering, replicating and applying the don't care states to the original problem thus reducing the search space. Experiments demonstrate the effectiveness of the proposed method on ISCAS'89 benchmarks.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:u_35RYKgDlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Suspect2vec: A suspect prediction model for directed RTL debugging",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3287624.3287661",
            "Abstract": "Automated debugging tools based on Boolean Satisfiability (SAT) have greatly alleviated the time and effort required to diagnose and rectify a failing design. Practical experience shows that long-running debugging instances can often be resolved faster using partial results that are available before the SAT solver completes its search. In such cases it is preferable for the tool to maximize the number of suspects it returns during the early stages of its deployment. To capitalize on this observation, this paper proposes a directed SAT-based debugging algorithm which prioritizes examining design locations that are more likely to be suspects. This prioritization is determined by suspect2vec---a model which learns from historical debug data to predict the suspect locations that will be found. Experiments show that this algorithm is expected to find 16% more suspects than the baseline algorithm if terminated prematurely \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Efficient Selection of Suspect Sets in Unreachable State Diagnosis.",
            "Publication year": 2016,
            "Publication url": "https://www.eecg.utoronto.ca/~veneris/isaim16.pdf",
            "Abstract": "In the modern hardware design cycle, correcting the design when functional verification reveals an erroneously unreachable state can be a time-consuming manual process. In order to mitigate this growing cost, this paper presents an automated methodology that, given an unreachable target state, returns all design locations where a change can be implemented to make it reachable. In contrast to previous automated unreachability debugging techniques, our approach avoids exhaustively examining all design locations, resulting in significant run-time savings. The presented approach proceeds through a series of iterations, each of which examines a subset of the design locations (suspects) to determine whether or not they are solutions to the problem. Based on the results of each iteration, a new set of suspects is considered in the subsequent iteration. However, in practice a small portion of the locations in the design are ever examined. Results are presented to prove that when the initial suspect set is chosen appropriately, the solution set returned by the methodology is complete. Empirical results on industrial designs confirm the theoretical and practical gains of this approach, demonstrating an impressive 33.7 x speedup over the previous approach while avoiding examining 76% of the design locations on average.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:aqlVkmm33-oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Lazy suspect-set computation: Fault diagnosis for deep electrical bugs",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2206781.2206827",
            "Abstract": "Current silicon test methods are highly effective at sensitizing and propagating most electrical faults. Unfortunately, with ever increasing chip complexity and shorter time-to-market windows, an increasing number of faults escape undetected. To address this problem, we propose a novel technique to help identify hard-to-find electrical faults that are not detected using conventional test methods, but manifest themselves as observable functional errors during functional test, system test, or during actual use in the field. These faults are too sequentially deep to be diagnosed using simulation, ATPG, or formal tools. Our technique relies on repeated full-speed chip runs that witness the functional bug, combined with some additional on-chip functional debug support and off-line analysis, to compute a possible set of suspected faults. The technique quickly prunes the suspect set, and for each suspect, it can provide a short \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:mB3voiENLucC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spatial and temporal design debug using partial MaxSAT",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1531542.1531621",
            "Abstract": "Design debug remains one of the major bottlenecks in the VLSI design cycle today. Existing automated solutions strive to aid engineers in reducing the debug effort by identifying possible error sources in the design. Unfortunately, these techniques do not provide any information regarding the time at which the bug is active during an error trace or counter-example. This work introduces an automated debug technique that provides the user with both spatial and temporal information about the source of error. The proposed method is based on a Partial MaxSAT formulation which models errors at the CNF clause level instead of the traditional gate or module level. Thus, error sites are identified based on erroneous implications that correspond to locations both in the design and in the error trace. Experiments demonstrate that we can provide this additional information at no extra cost in run time and are able to prune \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:uWQEDVKXjbEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cost-effective blockchain-based iot data marketplaces with a credit invariant",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9461127/",
            "Abstract": "Billions of Internet of Things (IoT) devices deployed today collect massive amounts of potentially valuable data. To efficiently utilize this data, markets must be developed where data can be traded in real time. Blockchain technology offers a potential platform for these types of markets. However, previous proposals using blockchain technology either require trusted third parties such as data brokers, or necessitate a large number of on-chain transactions to operate, incurring excessive overhead costs. This paper proposes a trustless data trading system that minimizes both the risk of fraud and the number of transactions performed on chain. In this system, data producers and consumers come to binding agreements while trading data off chain and they only settle on chain when a deposit or withdrawal of funds is required. A credit mechanism is also developed to further reduce the incurred fees. Additionally, the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:eq2jaN3J8jMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Truthful Decentralized Blockchain Oracles",
            "Publication year": 2021,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/nem.2179",
            "Abstract": "Blockchain systems rely on oracles to bridge external information to the decentralized applications residing in the systems. Astraea protocols are decentralized oracle designs utilizing majority\u2010voting mechanism to determine the oracle outcomes and/or rewards to voters. However, the voters are indifferent between voting through a single or multiple identities, as the potential rewards by the decentralized oracles grow linearly with the voters stakes. Additionally, the majority\u2010voting mechanism may facilitate herd behaviors among the voters, as the voters are rewarded only if they are in agreement with the majority outcomes. In this paper, a novel oracle protocol is introduced by proposing a peer prediction\u2010based scoring scheme along with non\u2010linear staking rules, aiming at extracting subjective data truthfully. Specifically, an incentive compatible scoring scheme is designed so that voters uniquely maximize their \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:VL0QpB8kHFEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Two-stage, pipelined register renaming",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5599894/",
            "Abstract": "Register renaming is a performance-critical component of modern, dynamically-scheduled processors. Register renaming latency increases as a function of several architectural parameters (e.g., processor issue width, processor window size, and processor checkpoint count). Pipelining of the register renaming logic can help avoid restricting the processor clock frequency. This work presents a full-custom, two-stage register renaming implementation in a 130-nm fabrication technology. The latency of non-pipelined and two-stage, pipelined renaming is compared, and the underlying performance and complexity tradeoffs are discussed. The two-stage pipelined design reduces the renaming logic depth from 23 fan-out-of-four (FO4) down to 9.5 FO4.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:4TOpqqG69KYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automated design debugging with maximum satisfiability",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5605301/",
            "Abstract": "As contemporary very large scale integration designs grow in complexity, design debugging has rapidly established itself as one of the largest bottlenecks in the design cycle today. Automated debug solutions such as those based on Boolean satisfiability (SAT) enable engineers to reduce the debug effort by localizing possible error sources in the design. Unfortunately, adaptation of these techniques to industrial designs is still limited by the performance and capacity of the underlying engines. This paper presents a novel formulation of the debugging problem using MaxSAT to improve the performance and applicability of automated debuggers. Our technique not only identifies errors in the design but also indicates when the bug is excited in the error trace. MaxSAT allows for a simpler formulation of the debugging problem, reducing the problem size by 80% compared to a conventional SAT-based technique \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:hFOr9nPyWt4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Extraction error diagnosis and correction in high-performance designs",
            "Publication year": 2003,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.125.1663&rep=rep1&type=pdf",
            "Abstract": "Test model generation is crucial in the test generation process of a high-performance design targeted for large volume production. A key process in test model generation requires the extraction of a gate-level (logic) model from the transistor level representation of the circuit under test. Logic extraction is an error prone process due to extraction tool limitations and due to the human interference. Errors introduced by extraction require manual debugging, a resource intensive and time consuming task. This paper presents a set of extraction errors typical in an industrial environment. It also proposes an automated solution to extraction error diagnosis and correction. Experiments on circuits with similar architecture to that of high speed custom-made industrial blocks are conducted to confirm the fitness of the approach.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:RHpTSmoSYBkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Constructing stability-based clock gating with hierarchical clustering",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7347593/",
            "Abstract": "In modern designs, a complex clock distribution network is employed to distribute the clock signal(s) to all the sequential elements. As the functionality of these sequential elements depends heavily on usage scenarios, it is vital that the clock network is optimized for these scenarios. This paper introduces a clock network power optimization methodology based on design usage patterns and stability based clock gating. Specifically, whenever a register retains its value from the previous cycle, a clock gating implementation shuts off its clock and disables data loading to enable power reduction. We first introduce the notion of a stability pattern and its correlation with clock gating efficiency. Next, we introduce a methodology to identify efficient clock gating implementations. In this framework, a clustering algorithm leveraging stability patterns iteratively computes more effective gating implementations. Each implementation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:Mojj43d5GZwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A succinct memory model for automated design debugging",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4681564/",
            "Abstract": "In todaypsilas complex SoC designs, verification and debugging are becoming ever more crucial and increasingly time-consuming tasks. The prevalence of embedded memories adds to the difficulty of the problem by exponentially increasing the state-space of the design. In this work, a novel memory model for design debugging is presented. It models memory succinctly by avoiding an explicit representation for each memory bit. The method uses the simulation of the erroneous design to guide the debugging process. This results in a parameterizable formal encoding that grows linearly with the erroneous trace length, significantly reducing the memory requirements of the debugging problem. In addition, the proposed model is extended to handle an arbitrary initial memory configuration, as well as non-cycle accurate output traces where only a final expected memory state is available for comparison. Experiments on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:BqipwSGYUEgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automating data analysis and acquisition setup in a silicon debug environment",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5771587/",
            "Abstract": "With the growing size of modern designs and more strict time-to-market constraints, design errors can unavoidably escape pre-silicon verification and reside in silicon prototypes. Due to those errors and faults in the fabrication process, silicon debug has become a necessary step in the digital integrated circuit design flow. Embedded hardware blocks, such as scan chains and trace buffers, provide a means to acquire data of internal signals in real time for debugging. However, the amount of the data is limited compared to pre-silicon debugging. This paper presents an automated software solution to analyze this sparse data to detect suspects of the failure in both the spatial and temporal domain. It also introduces a technique to automate the configuration process for trace-buffer-based hardware in order to acquire helpful information for debugging the failure. The technique takes the hardware constraints into account \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:fPk4N6BV_jEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Exemplar-based failure triage for regression design debugging",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s10836-016-5577-1.pdf",
            "Abstract": "Modern regression verification often exposes myriads of failures at the pre-silicon stage. Typically, these failures need to be properly grouped into bins, which then have to be distributed to engineers for detailed analysis. The above process is coined as failure triage, and is nowadays increasing in complexity, as the size of both design logic and verification environment continues to grow. However, it remains a predominantly manual process that can prolong the debug cycle and jeopardize time-sensitive design milestones. In this paper, we propose an exemplar-based data-mining formulation of failure triage that efficiently automates both failure grouping and bin distribution. The proposed framework maps failures as data points, applies an affinity-propagation (AP) clustering algorithm, and operates in both metric and non-metric spaces, offering complete flexibility and significant user control over the process \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:maZDTaKrznsC",
            "Publisher": "Springer US"
        },
        {
            "Title": "A physical-level study of the compacted matrix instruction scheduler for dynamically-scheduled superscalar processors",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5289240/",
            "Abstract": "This work studies physical-level characteristics of the recently proposed compacted matrix instruction scheduler for dynamically-scheduled, superscalar processors. Previous work focused on the matrix scheduler's architecture and argued in support of its speed and scalability advantages. However, no physical-level implementation or models were reported for it. Using full-custom layouts in a commercial 90 nm fabrication technology, this work investigates the latency and energy variations of the compacted matrix and its accompanying logic as a function of the issue width, the window size, and the number of global recovery checkpoints. This work also proposes an energy optimization that throttles unnecessary pre-charges and evaluations. This optimization reduces energy by 10% and 18% depending on the scheduler size.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:geHnlv5EZngC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Failure triage in RTL regression verification",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8194890/",
            "Abstract": "We propose an automated failure triage framework for register transfer level debugging in functional verification regression flows which unifies three critical aspects of the problem: the approximation of the general location of root-cause(s) in the design under verification, the binning of all related failures generated by regression runs, and the distribution of these binned failures to the proper engineer(s) for detailed analysis. The proposed triage engine entails two novel methodologies. The first is a classification framework that mines information from SAT-based debugging and simulation to probabilistically reason about the relation of root-causes with their respective failing verification traces. This enables the construction of a priority ranking for these root-causes, and can effectively guide debugging by focusing resources on high-priority root-causes. Second, we propose a formulation of failure binning as exemplar \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:u-x6o8ySG0sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A Performance-Driven QBF-Based ILA Representation with Applications to Verification, Debug and Test",
            "Publication year": 2007,
            "Publication url": "https://hal.archives-ouvertes.fr/hal-00466757/",
            "Abstract": "Archive ouverte HAL - A Performance-Driven QBF-based ILA Representation with \nApplications to Verification, Debug and Test Acc\u00e9der directement au contenu Acc\u00e9der \ndirectement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL \nListe des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org \nRevues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les \nderniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par \nstructure de recherche Les portails de l'archive Recherche Documentation hal-00466757, \nversion 1 Communication dans un congr\u00e8s A Performance-Driven QBF-based ILA \nRepresentation with Applications to Verification, Debug and Test Hratch Mangassarian 1 \nAndreas Veneris 2, 3 Sean Safarpour 1 Marco Benedetti 4 Duncan Smith 1 D\u00e9tails 1 ECE - \nDepartment of Electrical and Computer Engineering [] 2 ['\u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:ldfaerwXgEUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Design rewiring for power minimization",
            "Publication year": 2002,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.8026&rep=rep1&type=pdf",
            "Abstract": "A new ATPG-based approach to multi-level combinational logic circuit power optimization is presented. This method borrows from existing simulation-based design error diagnosis and correction techniques. At every step of the optimization procedure, a design error is introduced by removing logic with high switching activity and it is corrected by modifying the design in a less critical area. The experiments presented here indicate the added flexibility and potential of this approach.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:Y0pCki6q_DkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Searching for Bugs Using Probabilistic Suspect Implications",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8957691/",
            "Abstract": "Due to the excessive cost associated with manual RTL design debugging, automated tools are often employed to identify a set of suspect bug locations. To further accelerate the process, one observes that the anytime behavior of these tools allows partial results to be analyzed before the suspect search is complete. Thus, it is preferable for the tool to maximize the number of suspects that are found in the early stages of its search. Toward this end, this article proposes a new SAT-based debugging algorithm which predicts where solutions are most likely to be found and prioritize examining these locations. Two techniques are proposed to predict solution locations by learning from historical debug data. The first technique does so using belief propagation on a probabilistic graph, while the second trains a neural network to classify candidate suspects as solutions or nonsolutions. Intensive empirical evaluation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:AXPGKjj_ei8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Debugging sequential circuits using Boolean satisfiability",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1563072/",
            "Abstract": "Logic debugging of today's complex sequential circuits is an important problem. In this paper, a logic debugging methodology for multiple errors in sequential circuits with no state equivalence is developed. The proposed approach reduces the problem of debugging to an instance of Boolean satisfiability. This formulation takes advantage of modern Boolean satisfiability solvers that handle large circuits in a computationally efficient manner. An extensive suite of experiments with large sequential circuits confirm the robustness and efficiency of the proposed approach. The results further suggest that Boolean satisfiability provides an effective platform for sequential logic debugging.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:yD5IFk8b50cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Diagnosing multiple transition faults in the absence of timing information",
            "Publication year": 2005,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1057661.1057708",
            "Abstract": "As timing requirements in today's advanced VLSI designs become more aggressive, the need for automated tools to diagnose timing failures increases. This work presents two such algorithms capable of diagnosing multiple delay faults. One method uses multiple transition fault models and the other reasons with ternary logic values, thus achieving model independent diagnosis. Experiments are conducted on IS-CAS'85 combinational and full-scan version of ISCAS'89 se-quential circuits corrupted with multiple transition faults. The performance of both algorithms are evaluated and compared. The results show good efficiency and diagnostic resolution.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:ZeXyd9-uunAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Non-solution implications using reverse domination in a modern SAT-based debugging environment",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6176548/",
            "Abstract": "With the growing complexity of VLSI designs, functional debugging has become a bottleneck in modern CAD flows. To alleviate this cost, various SAT-based techniques have been developed to automate bug localization in the RTL. In this context, dominance relationships between circuit blocks have been recently shown to reduce the number of SAT solver calls, using the concept of solution implications. This paper first introduces the dual concepts of reverse domination and non-solution implications. A SAT solver is tailored to leverage reverse dominators for the early on-the-fly detection of bug-free components. These are non-solution areas and their early pruning significantly reduces the the debugging search-space. This process is expedited by branching on error-select variables first. Extensive experiments on tough real-life industrial debugging cases show an average speedup of 1.7x in SAT solving time over \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:hC7cP41nSMkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Maximum circuit activity estimation using pseudo-boolean satisfiability",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4212029/",
            "Abstract": "Disproportionate instantaneous power dissipation may result in unexpected power supply voltage fluctuations and permanent circuit damage. Therefore, estimation of maximum instantaneous power is crucial for the reliability assessment of VLSI chips. Circuit activity and consequently power dissipation in CMOS circuits are highly input-pattern dependent, making the problem of maximum power estimation computationally hard. This work proposes a novel pseudo-Boolean satisfiability based method that reports the exact input sequence maximizing circuit activity in combinational and sequential circuits. The method is also extended to take multiple gate transitions into account by integrating delay information into the pseudo-Boolean optimization problem. An extensive suite of experiments on ISCAS85 and ISCAS89 circuits confirms the efficiency and robustness of the approach compared to simulation based \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:D03iK_w7-QYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Method, system and computer program for hardware design debugging",
            "Publication year": 2014,
            "Publication url": "https://patents.google.com/patent/US8751984B2/en",
            "Abstract": "A plurality of diagnosis methods are provided for enabling hardware debugging. A first diagnosis method enables hardware debugging by means of time abstraction. A second diagnosis method enables hardware debugging by means of abstraction and refinement. A third diagnosis method enables hardware debugging by means of QBF-formulation for replicated functions. A fourth diagnosis method enables hardware debugging by means of a max-sat debugging formulation. A system and computer program for implementing the diagnosis methods is also provide.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:g5m5HwL7SMYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Astraea: A decentralized blockchain oracle",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8726819/",
            "Abstract": "The public blockchain was originally conceived to process monetary transactions in a peer-to-peer network while preventing double-spending. It has since been extended to numerous other applications including execution of programs that exist on the blockchain called \u201csmart contracts.\u201d Smart contracts have a major limitation, namely they only operate on data that is on the blockchain. Trusted entities called oracles attest to external data in order to bring it onto the blockchain but they do so without the robust security guarantees that blockchains generally provide. This has the potential to turn oracles into centralized points-of-failure. To address this concern, this paper introduces Astraea, a decentralized oracle based on a voting game that decides the truth or falsity of propositions. Players fall into two roles: voters and certifiers. Voters play a low-risk/low-reward role that is resistant to adversarial manipulation while \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:2osOgNQ5qMEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Extraction error modeling and automated model debugging in high-performance custom designs",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1661625/",
            "Abstract": "In the design cycle of high-performance integrated circuits, it is common that certain components are designed directly at the transistor level. This level of design representation may not be appropriate for test generation tools that usually require a model expressed at the gate level. Logic extraction is a key step in test model generation to produce a gate-level netlist from the transistor-level representation. This is a semi-automated process which is error-prone. Once a test model is found to be erroneous, manual debugging is required, which is a resource-intensive and time-consuming process. This paper presents an in-depth analysis of typical sets of extraction errors found in the test model representations of the pipelines in high-performance designs today. It also develops an automated debugging solution for single extraction errors for pipelines with no state equivalence information. A suite of experiments on \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:1sJd4Hv_s6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Reviving erroneous stability-based clock-gating using partial max-SAT",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6509685/",
            "Abstract": "The conflicting yet increasing demand for high performance and low power in multi-functional chips has pushed techniques for power reduction to the forefront of VLSI design. Although recent developments have automated most of the low power implementations, designers often manually modify the circuit in order to achieve further power savings. This human intervention is often paved with many errors that are bound to typical logic functional failures. Debugging these errors can be a resource intensive process that requires considerable manual effort. This discourages engineers and achieving power savings at the micro level of the design sometimes remains unrealized. This paper proposes a novel debugging methodology to rectify erroneous clock-gating implementations. With the use of Partial Max-SAT, the method localizes and rectifies the design error introduced in the circuit during a clock-gating \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Root-cause analysis for memory-locked errors",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7459465/",
            "Abstract": "Half of the time in the design cycle today is spent on verifying and debugging the correctness of a design. Although some debugging tasks have been automated, determining the root-cause of errors that have been locked in memory for a number of clock cycles before they propagate to an observation point remains a time consuming effort. This is because the error traces exposing such behavior can be excessively long, a fact that requires modeling the circuit for many time-frames. This paper introduces a performance-driven debugging methodology for pinpointing the root-cause of memory-locked errors. The technique models only a sliding time window and a final time window explicitly at any one time, while interstitial time-frames are linked with a lightweight memory model. This technique is later extended to a complete methodology that diagnoses errors that may be missed. Experiments on industrial designs with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:HE397vMXCloC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multi-objective voltage island floorplanning using sequence pair representation",
            "Publication year": 2012,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S2210537912000224",
            "Abstract": "In the nanometer era of VLSI design, high power consumption is considered to be a \u201cshow-stopper\u201d for many applications. Voltage island design has emerged as a popular method for addressing this issue. This technique requires multiple supply voltages on the same chip with blocks assigned to different supply voltages. Implementation challenges force blocks with similar supply voltages to be placed contiguous to one another, thereby creating \u201cislands\u201d. Classical floorplanners assume a single supply voltage in the entire chip and thus require additional design steps to realize voltage islands. In this paper we present a new multi-objective floorplanning algorithm based on the sequence pair representation that can floorplan blocks in the form of islands. Given the possible supply voltage choices for each block, the floorplanner simultaneously attempts to reduce power and area of the chip. Our floorplanner integrates \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:XiVPGOgt02cC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "SigVM: Toward Fully Autonomous Smart Contracts",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2102.10784",
            "Abstract": "This paper presents SigVM, a novel blockchain virtual machine that supports an event-driven execution model, enabling developers to build fully autonomous smart contracts. SigVM introduces another way for a contract to interact with another. Contracts in SigVM can emit signal events, on which other contracts can listen. Once an event is triggered, corresponding handler functions are automatically executed as signal transactions. We built an end-to-end blockchain platform SigChain and a contract language compiler SigSolid to realize the potential of SigVM. Experimental results show that SigVM enables contracts in our benchmark applications to be reimplemented in a fully autonomous way, eliminating the dependency on unreliable mechanisms like off-chain relay servers. SigVM can significantly simplify the execution flow of our benchmark applications, and can avoid security risks such as front-run attacks.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:SdhP9T11ey4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Central Bank Digital Loonie: Canadian Cash for a New Global Economy",
            "Publication year": 2021,
            "Publication url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3770024",
            "Abstract": "In February 2020, the Bank of Canada announced the conditions under which it would consider issuing a Central Bank-issued Digital Currency (CBDC), namely a significant decline in the use of banknotes to the point where Canadians no longer can use them for a wide range of transactions; and/or a situation where one or more alternative private-sector digital currencies start to become widely used as an alternative to the Canadian dollar as a method of payment, store of value and unit of account. Having determined the\" when\", the Bank then sought input from the university sector about the\" how\" via a design competition. This paper is our proposal (one of the three selected ones).",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:dTyEYWd-f8wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automated design debugging with abstraction and refinement",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5247156/",
            "Abstract": "Design debugging is one of the major remaining manual processes in the semiconductor design cycle. Despite recent advances in the area of automated design debugging, more effort is required to cope with the size and complexity of today's designs. This paper introduces an abstraction and refinement methodology to enable current debuggers to operate on designs that are orders of magnitude larger than otherwise possible. Two abstraction techniques are developed with the goals of improving debugger performance for different circuit structures: State abstraction is aimed at reducing the problem size for circuits consisting purely of primitive gates, while function abstraction focuses on designs that also contain modular and hierarchical information. In both methods, after an initial abstracted model is created, the problem can be solved by an existing automated debugger. If an error site is abstracted, refinement is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:XiSMed-E-HIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Leveraging dominators for preprocessing QBF",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5457088/",
            "Abstract": "Many CAD for VLSI problems can be naturally encoded as Quantified Boolean Formulas (QBFs) and solved with QBF solvers. Furthermore, such problems often contain circuit-based information that is lost during the translation to Conjunctive Normal Form (CNF), the format accepted by most modern solvers. In this work, a novel preprocessing framework for circuit-based QBF problems is presented. It leverages structural circuit dominators to reduce the problem size and expedite the solving process. Our circuit-based QBF preprocessor PReDom recursively reduces dominated subcircuits to return a simpler but equisatisfiable QBF instance. A rigorous proof is given for eliminating subcircuits dominated by single outputs, irrespective of input quantifiers. Experimental results are presented for circuit diameter computation problems. With preprocessing times of at most five seconds using PReDom, three state-of-the-art \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robust QBF encodings for sequential circuits with applications to verification, debug, and test",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5441288/",
            "Abstract": "Formal CAD tools operate on mathematical models describing the sequential behavior of a VLSI design. With the growing size and state-space of modern digital hardware designs, the conciseness of this mathematical model is of paramount importance in extending the scalability of those tools, provided that the compression does not come at the cost of reduced performance. Quantified Boolean Formula satisfiability (QBF) is a powerful generalization of Boolean satisfiability (SAT). It also belongs to the same complexity class as many CAD problems dealing with sequential circuits, which makes it a natural candidate for encoding such problems. This work proposes a succinct QBF encoding for modeling sequential circuit behavior. The encoding is parametrized and further compression is achieved using time-frame windowing. Comprehensive hardware constructions are used to illustrate the proposed encodings \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:u9iWguZQMMsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A complete approach to unreachable state diagnosability via property directed reachability",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7428000/",
            "Abstract": "In modern hardware design, substantial manual effort is required to fix a design when verification discovers a state unreachable. This paper addresses this growing pain where given an unreachable target state, a methodology is presented to return all design locations where a change can be implemented to make the target state reachable. In contrast to previous state reachability rectification techniques that use bounded model checking, our approach addresses the issue using unbounded model checking. It first enhances the circuit transition relation by inserting a novel error model construction at each suspect location. An unbounded model checking algorithm is then applied to the enhanced transition relation to find which of the suspect locations can be changed to make the target state reachable. The use of unbounded model checking allows it to identify the complete problem solution set. As an added benefit, it \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:D_sINldO8mEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improved design debugging using maximum satisfiability",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4401977/",
            "Abstract": "In today's SoC design cycles, debugging is one of the most time consuming manual tasks. CAD solutions strive to reduce the inefficiency of debugging by identifying error sources in designs automatically. Unfortunately, the capacity and performance of such automated techniques must be considerably extended for industrial applicability. This work aims to improve the performance of current state-of-the-art debugging techniques, thus making them more practical. More specifically, this work proposes a novel design debugging formulation based on maximum satisfiability (max-sat) and approximate max-sat. The developed technique can quickly discard many potential error sources in designs, thus drastically reducing the size of the problem passed to an existing debugger. The max-sat formulation is used as a pre-processing step to construct a highly optimized debugging framework. Empirical results demonstrate the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:70eg2SAEIzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Mining simulation metrics for failure triage in regression testing",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7229856/",
            "Abstract": "Design debugging poses a major bottleneck in modern VLSI CAD flows, consuming up to 60% of the verification cycle. The debug pain, however, worsens in regression verification flows at the pre-silicon stage where myriads of failures can be exposed. These failures need to be properly grouped and distributed among engineers for further analysis before the next regression run commences. This high-level and complex debug problem is referred to as failure triage and largely remains a manual task in the industry. In this paper, we propose an automated failure triage flow that mines information from both failing and passing tests during regression, and automatically performs a coarse-grain partitioning of the failures. The proposed framework combines formal tools and novel statistical metrics to quantify the likelihood of specific design components being the root-cause of the observed failures. These components \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:pqnbT2bcN3wC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sequential logic rectifications with approximate SPFDs",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5090936/",
            "Abstract": "In the digital VLSI cycle, logic transformations are often required to modify the design to meet different synthesis and optimization goals. Logic transformations on sequential circuits are hard to perform due to the vast underlying solution space. This paper proposes an SPFD-based sequential logic transformation methodology to tackle the problem with no sacrifice on performance. It first presents an efficient approach to construct approximate SPFDs (aSPFDs) for sequential circuits. Then, it demonstrates an algorithm using aSPFDs to perform the desirable sequential logic transformations using both combinational and sequential don't cares. Experimental results show the effectiveness and robustness of the approach.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:kNdYIx-mwKoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Exact Functional Fault Collapsing in Combinational Logic Circuits",
            "Publication year": 2003,
            "Publication url": "http://www.eecg.toronto.edu/~veneris/3latw.pdf",
            "Abstract": "Fault equivalence is an essential concept in digital VLSI design with significance in many different areas such as diagnosis, diagnostic ATPG, testability analysis and synthesis. In this paper, an efficient procedure to compute exact fault equivalence classes of combinational circuits is described. The procedure consists of two steps. The first step performs structural fault collapsing and uses fault simulation to return an approximation of the fault equivalent classes. The second step refines these classes with ATPG. Experiments on ISCAS\u201985 and fullscan ISCAS\u201989 circuits demonstrate its efficiency.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:0EnyYjriUFMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automating logic transformations with approximate spfds",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5752413/",
            "Abstract": "During the very large scale integration design process, a synthesized design is often required to be modified in order to accommodate different goals. To preserve the engineering effort already invested, designers seek small logic structural transformations to achieve these logic restructuring goals. This paper proposes a systematic methodology to devise such transformations automatically. It first presents a simulation-based formulation to approximate sets of pairs of functions to be distinguished and avoid the memory/time explosion issue inherent with the original representation. Then, it uses this new data structure to devise the required transformations dynamically without the need of a static dictionary model. The methodology is applied to both combinational and sequential designs with transformations at a single or multiple locations. An extensive suite of experiments documents the benefits of the proposed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:vRqMK49ujn8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fault equivalence and diagnostic test generation using ATPG",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1329502/",
            "Abstract": "Fault equivalence is an essential concept in digital design with significance in fault diagnosis, diagnostic test generation, testability analysis and logic synthesis. In this paper, an efficient algorithm to check whether two faults are equivalent is presented. If they are not equivalent, the algorithm returns a test vector that distinguishes them. The proposed approach is complete since for every pair of faults it either proves equivalence or it returns a distinguishing vector. This is performed with a simple hardware construction and a sequence of simulation/ATPG-based steps. Experiments on benchmark circuits demonstrate the competitiveness of the proposed method.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:IjCSPb-OGe4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "On public decentralized ledger oracles via a paired-question protocol",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8751484/",
            "Abstract": "Blockchain technology enables the operation of fully decentralized applications without the need for a central authority to manage the execution of the underlying process. However, a critical limitation in the technology today is the inability for such applications to query information external to the blockchain. Applications must make use of a decentralized oracle, i.e. a trusted source of external information. In this work we propose the paired-question decentralized oracle protocol, designed to extract true answers from the public. When querying the oracle, a user submits pairs of antithetic questions and voting users answer them for the chance to receive rewards. This new protocol lends itself to a simple formal analysis, and it is shown to strongly incentivize a Nash equilibrium of truthful reporting. This paper also discusses a number of extensions to the base protocol to improve its cost-effectiveness, security, and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:J_g5lzvAfSwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Efficient and exact diagnosis of multiple stuck-at faults",
            "Publication year": 2002,
            "Publication url": "https://www.researchgate.net/profile/Magdy-Abadir/publication/228964338_Efficient_and_Exact_Diagnosis_of_Multiple_Stuck-At_Faults/links/00b495213a16745f2a000000/Efficient-and-Exact-Diagnosis-of-Multiple-Stuck-At-Faults.pdf",
            "Abstract": "This paper presents a simulation-based approach to multiple stuck-at fault diagnosis. The algorithm accepts a digital implementation corrupted with stuck-at faults and its simulatable gate-level netlist. It then iteratively identifies and fault models suspect locations one at a time until the set of faults modeled can functionally fully account for the faulty behavior. The method uses a theoretical result along with a number of heuristics to help avoid the exponential complexity inherent to the problem. Experiments on up to four multiple faults confirm its effectiveness and accuracy which scales well with increasing number of faults.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:UeHWp8X0CEIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Simulation and satisfiability guided counter-example triage for rtl design debugging",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6783384/",
            "Abstract": "Regression verification flows in modern integrated circuit development environments expose a plethora of counterexamples during simulation. Sorting these counter-examples today is a tedious and time-consuming process. High level design debugging aims to triage these counter-examples into groups that will be assigned to the appropriate verification and/or design engineers for detailed root cause analysis. In this work, we present an automated triage process that leverages knowledge extracted from simulation and SAT-based debugging. We introduce novel metrics that correlate counter-examples based on the likelihood of sharing the same root cause. Triage is formulated as a pattern recognition problem and solved by hierarchical clustering techniques to generate groups of related counter-examples. Experimental results demonstrate an overall accuracy of 94% for the proposed automated triage framework \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:QIV2ME_5wuYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Efficient SAT-based Boolean matching for FPGA technology mapping",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1146909.1147034",
            "Abstract": "Most FPGA technology mapping approaches either target Lookup Tables (LUTs) or relatively simple Programmable Logic Blocks (PLBs). Considering networks of PLBs during technology mapping has the potential of providing unique optimizations unavailable through other techniques. This paper proposes a Boolean matching approach for FPGA technology mapping targeting networks of PLBs. To overcome the demanding memory requirements of previous approaches, the Boolean matching problem is formulated as a Boolean Satisfiability (SAT) problem. Since the SAT formulation provides a trade-off between space and time, the primary objective is to increase the efficiency of the SAT-based approach. To do this, the original SAT problem is decomposed into two easier SAT problems. To reduce the problem search space, a theorem is introduced to allow conflict clauses to be shared across problems and extra \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:isC4tDSrTZIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Methodologies for diagnosis of unreachable states via property directed reachability",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8023860/",
            "Abstract": "In the modern design cycle, substantial manual effort is required to correct failed liveness properties due to the limited availability of automated tools. To address this limitation, this paper introduces two techniques to diagnose register transfer level errors that manifest in the form of erroneously unreachable states, which represent a common form of liveness property failure. The first uses steps of reachable state-space over-approximation and traditional debugging to compute a subset of the solutions that make a target state reachable. The second solves a series of unbounded model checking problems using an enhanced model of the circuit's transition relation to compute the complete solution set to the problem. The proposed techniques are complementary to each other and present the user with a configurable tradeoff between runtime and resolution of the returned solution set. Empirical results on OpenCores and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:nb7KW1ujOQ8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Suspect set prediction in RTL bug hunting",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8342261/",
            "Abstract": "We propose a framework for predicting erroneous design components from partially observed solution sets that are found through automated debugging tools. The proposed method involves learning design component dependencies by using historical debugging data and representing these dependencies by means of a probabilistic graph. Using this representation, one can run a debugging tool non-exhaustively, obtain a partial set of potentially erroneous components and then predict the remaining by applying a cost-effective belief propagation pass. The method can reduce debugging runtime when it comes to multiple debugging sessions by 15x on the average while achieving a 91% average prediction accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Incremental design debugging in a logic synthesis environment",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/article/10.1007/s10836-005-0335-9",
            "Abstract": "In today\u2019s complex and challenging VLSI design process, multiple logic errors may occur due to the human factor and bugs in CAD tools. The designer often faces the challenge of correcting an erroneous design implementation. This study describes a simulation-based logic debugging solution for combinational circuits corrupted with multiple design errors. Unlike other simulation-based techniques that identify all errors at once, the proposed method works incrementally. At each iteration of incremental debugging, a single candidate location is rectified with linear time algorithms. This is done so that the functionality of the erroneous design gradually matches the correct one. A number of theorems, heuristics and data structures help identify a single candidate solution at each iteration and they also guide the search in the large solution space. Experiments on benchmark circuits confirm the effectiveness of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:_FxGoFyzp5QC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Session Abstract",
            "Publication year": 2006,
            "Publication url": "https://www.computer.org/csdl/proceedings-article/vts/2006/25140290/12OmNviZldf",
            "Abstract": "The aim of the TTTC Doctoral Thesis Award is to promote and strengthen the interaction between doctoral students who are about to graduate and the industrial community. It also serves as a process that allows their work to be exposed to and tested under real life industrial needs by experts in the field. This is achieved with student presentations in a dedicated VTS session in front of an industrial panel that evaluates, comments and contributes to their work in terms of novelty and advance of industrial practice and this of theoretical methodology.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "Automated debugging of SystemVerilog assertions",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5763057/",
            "Abstract": "In the last decade, functional verification has become a major bottleneck in the design flow. To relieve this growing burden, assertion-based verification has gained popularity as a means to increase the quality and efficiency of verification. Although robust, the adoption of assertion-based verification poses new challenges to debugging due to presence of errors in the assertions. These unique challenges necessitate a departure from past automated circuit debugging techniques which are shown to be ineffective. In this work, we present a methodology, mutation model and additional techniques to debug errors in SystemVerilog assertions. The methodology uses the failing assertion, counterexample and mutation model to produce alternative properties that are verified against the design. These properties serve as a basis for possible corrections. They also provide insight into the design behavior and the failing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:u5HHmVD_uO8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Leveraging reconfigurability to raise productivity in FPGA functional debug",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6176481/",
            "Abstract": "We propose new hardware and software techniques for FPGA functional debug that leverage the inherent reconfigurability of the FPGA fabric to reduce functional debugging time. The functionality of an FPGA circuit is represented by a programming bitstream that specifies the configuration of the FPGA's internal logic and routing. The proposed methodology allows different sets of design internal signals to be traced solely by changes to the programming bitstream followed by device reconfiguration and hardware execution. Evidently, the advantage of this new methodology vs. existing debug techniques is that it operates without the need of iterative executions of the computationally-intensive design re-synthesis, placement and routing tools. In essence, with a single execution of the synthesis flow, the new approach permits a large number of internal signals to be traced for an arbitrary number of clock cycles using a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:YsMSGLbcyi4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Trace compaction using SAT-based reachability analysis",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4196155/",
            "Abstract": "In today's designs, when functional verification fails, engineers perform debugging using the provided error traces. Reducing the length of error traces can help the debugging task by decreasing the number of variables and clock cycles that must be considered. We propose a novel trace length compaction approach based on SAT-based reachability analysis. We develop procedures and algorithms using pre-image computation to efficiently traverse the state space and reduce the trace lengths. We further introduce a data structure used to store the visited states which is critical to the performance of the proposed approach. Experiments demonstrate the effectiveness of the reachability approach as approximately 75% of the traces are reduced by one or two orders of magnitudes.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:5ugPr518TE4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Bounded model debugging",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5605322/",
            "Abstract": "Design debugging is a major bottleneck in modern very large scale integration design flows as both the design size and the length of the error trace contribute to its inherent complexity. With typical design blocks exceeding half a million synthesized logic gates and error traces in the thousands of clock cycles, the complexity of the debugging problem poses a great challenge to automated debugging techniques. This paper aims to address this daunting challenge by introducing the bounded model debugging methodology that iteratively analyzes bounded sequences of the error trace. Two techniques are introduced in this methodology to solve this growing problem. The first technique iteratively analyzes bounded subsequences of the error trace of increasing size until the error is found or the entire trace is analyzed. The second technique partitions the error trace into non-overlapping bounded sequences of clock \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:LPZeul_q3PIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An automated framework for correction and debug of PSL assertions",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5976210/",
            "Abstract": "Functional verification is becoming a major bottleneck in modern VLSI design flows. To manage this growing problem, assertion-based verification has been adopted as one of the key technologies to increase the quality and efficiency of verification. However, this technology also poses new challenges in the form of debugging and correcting errors in the assertions. In this work, we present a framework for correcting and debugging Property Specification Language assertions. The methodology uses the failing assertion, counter-example and mutation model to produce alternative properties that are verified against the design. Each one of these properties serve as a basis for possible corrections. They also provide insight into the design behavior and the failing assertion that can be used for debugging. Preliminary experimental results show that this process is effective in finding alternative assertions for all empirical \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:WbkHhVStYXYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Smart Contracts Refinement for Gas Optimization",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9569819/",
            "Abstract": "Smart contracts facilitate the execution of programmable code on a blockchain. The cost for executing smart contract code is metered using gas - the exact amount of which is based on the computational complexity of the underlying smart contract. Hence, it is imperative to optimize smart contract code to reduce gas consumption and, in some instances, to even avoid malicious attacks. In this paper, we propose an approach to optimize the gas consumption of smart contracts, specifically loop control structures. We present a prototype implementation of our approach using off-the-shelf tools for Solidity smart contracts. We experimentally evaluate our technique using 72 Solidity smart contracts. Our evaluation demonstrates the average gas cost savings per transaction to be around 23,943 gas units, or an equivalent 21% decrease in gas costs. Although the approach causes a slight increase in deployment costs due to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:N5tVd3kTz84C",
            "Publisher": "IEEE"
        },
        {
            "Title": "A failure triage engine based on error trace signature extraction",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6604054/",
            "Abstract": "The ever growing demand for functionally robust and error-free industrial electronics necessitates the development of techniques that will prohibit the propagation of functional errors to the final tape-out stage. This paramount requirement in the semiconductor world is imposed by the equivocal observation that functional errors slipping to silicon production introduce immense amounts of cost and jeopardize chip release dates. Functional verification and debugging are burdened with the tedious task of guaranteeing logic functionality early in the design cycle. In this paper, we present an automated method for the very first stage of functional debugging, called failure triage. Failure triage is the task of analyzing large sets of failures, grouping together those that are likely to be caused by the same design error, and then allocating those groups to the appropriate engineers for fixing. The introduced framework instruments \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:r0BpntZqJG4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Clustering-based revision debug in regression verification",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7357081/",
            "Abstract": "Modern digital systems are growing in size and complexity, introducing significant organizational and verification challenges in the design cycle. Verification today takes as much as 70% of the design time with debugging being responsible for half of this effort. Automation has mitigated part of the resource-intensive nature of rectifying erroneous designs. Nevertheless, most tools target failures in isolation. Since regression verification can discover myriads of failures in one run, automation is also required to guide an engineer to rank them and expedite debugging. To address this growing regression pain, this paper presents a framework that utilizes traditional machine learning techniques along with historical data in version control systems and the results of functional debugging. Its aim is to rank revisions based on their likelihood of being responsible for a particular failure. Ranking prioritizes revisions that ought to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:SP6oXDckpogC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A truth-inducing sybil resistant decentralized blockchain oracle",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9223272/",
            "Abstract": "Many blockchain applications use decentralized oracles to trustlessly retrieve external information as those platforms are agnostic to real-world information. Some existing decentralized oracle protocols make use of majority-voting schemes to determine the outcomes and/or rewards to participants. In these cases, the awards (or penalties) grow linearly to the participant stakes, therefore voters are indifferent between voting through a single or multiple identities. Furthermore, the voters receive a reward only when they agree with the majority outcome, a tactic that may lead to herd behavior. This paper proposes an oracle protocol based on peer prediction mechanisms with non-linear staking rules. In the proposed approach, instead of being rewarded when agreeing with a majority outcome, a voter receives awards when their report achieves a relatively high score based on a peer prediction scoring scheme. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:k_IJM867U9cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improved SAT-based reachability analysis with observability don\u2019t cares",
            "Publication year": 2009,
            "Publication url": "https://content.iospress.com/articles/journal-on-satisfiability-boolean-modeling-and-computation/sat190050",
            "Abstract": "The dramatic performance improvements of SAT solvers over the past decade have increased their deployment in hardware verification applications. Many problems that were previously too large and complex for SAT techniques can now be handled in an efficient manner. One such problem is reachability analysis, whose instances are found throughout verification applications such as unbounded model checking and trace reduction. In circuit-based reachability analysis important circuit information is often lost during the circuit-to-SAT translation process. Observability Don\u2019t Cares (ODCs) are an example of such information that can potentially help achieve better and faster results for the SAT solver. This work proposes to use the ODCs to improve the quality and performance of SAT-based reachability analysis frameworks. Since ODCs represent variables whose values do not a ect the outcome of a problem, it is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:YOwf2qJgpHMC",
            "Publisher": "IOS Press"
        },
        {
            "Title": "A performance-driven QBF-based iterative logic array representation with applications to verification, debug and test",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4397272/",
            "Abstract": "Many CAD for VLSI techniques use time-frame expansion, also known as the iterative logic array representation, to model the sequential behavior of a system. Replicating industrial-size designs for many time-frames may impose impractically excessive memory requirements. This work proposes a performance-driven, succinct and parametrizable quantified Boolean formula (QBF) satisfiability encoding and its hardware implementation for modeling sequential circuit behavior. This encoding is then applied to three notable CAD problems, namely bounded model checking (BMC), sequential test generation and design debugging. Extensive experiments on industrial circuits confirm outstanding run-time and memory gains compared to state-of-the-art techniques, promoting the use of QBF in CAD for VLSI.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:K3LRdlH-MEoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Efficient Debugging of Multiple Design Errors",
            "Publication year": 2011,
            "Publication url": "http://www.eecg.toronto.edu/~veneris/11mtv.pdf",
            "Abstract": "After functional verification detects a failure, design debugging aims to find all locations in the design that could be responsible for the observed error. The task of debugging becomes more difficult in modern designs because of the presence of multiple design errors. Multiple design errors exponentially increase the solution space of the debugging problem, leading to an intractable problem. This work aims to manage the complexity of multiple design errors within existing automated design debugging frameworks by using unsatisfiable cores to reduce the solution space. It builds upon previous work to generalize the generation and application of unsatisfiable cores for this purpose. An iterative debugging algorithm is presented in which unsatisfiable cores are generated as a by-product of the solving process to aid in reducing the search space for multiple design errors. Experiments on large designs for multiple errors show an average reduction in run-time of 22% with minimal impact to peak memory.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:MXK_kJrjxJIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Revision debug with non-linear version history in regression verification",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7566604/",
            "Abstract": "Modern digital designs are relentlessly growing in complexity, making their verification a daunting task. Verification and debugging are the bottleneck, accounting for up to 70% of the design cycle. Most automated debugging tools target failures in isolation and rely solely on the current version of a design's RTL. A recently developed methodology targets multiple failures simultaneously while leveraging the revision history present in a version control system. It finds revisions likely to be responsible for the failures and ranks them such that higher ranked revisions are more likely to contain bugs. However, this technique treats the version history as a simple linear list of revisions rather than a graph structure. To address this limitation, this paper presents a technique that properly leverages the branching information in version control systems. It offers two-stage ranking with improved performance, allowing both branches \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:hMod-77fHWUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Design rewiring based on diagnosis techniques",
            "Publication year": 2001,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/370155.370516",
            "Abstract": "Logic optimization is the step of the VLSI design cycle where the designer performs modifications on a design to satisfy different constraints such as area, power or delay. Recently, ATPG-based design rewiring techniques for logic optimization have gained increasing popularity. In this paper we propose a novel ATPG-based design rewiring methodology that borrows from previous design error diagnosis and correction techniques. We also present examples and experiments that indicate the added potential of our approach which is expected to provide a\" powerful\" route to design optimization.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:bFI3QPDXJZMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Seamless integration of SER in rewiring-based design space exploration",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4079360/",
            "Abstract": "Rewiring has been used extensively for optimizing the area, the power consumption, the delay, and the testability of a circuit. In this work, we demonstrate how rewiring can also be used for reducing the soft error rate (SER). We employ an ATPG-based rewiring method to generate functionally-equivalent yet structurally-different implementations of a logic circuit based on simple transformation rules. This rewiring capability, along with an off-the-shelf method for assessing the SER of a circuit, enable the integration of the SER in a unified search algorithm that iteratively evolves the design in order to satisfy a given set of objectives. Experimental results on ISCAS'89 and ITC'99 benchmark circuits verify that rewiring can indeed be successfully used to reduce the SER of a circuit and, thus, it facilitates a design-space exploration framework for trading off area, power consumption, delay, testability, and SER",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:M05iB0D1s5AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Quantified Bounded Model Checking for Rectangular Hybrid Automata",
            "Publication year": 2015,
            "Publication url": "http://taylortjohnson.com/research/nguyen2015cfv.pdf",
            "Abstract": "Satisfiability Modulo Theories (SMT) solvers have been successfully applied to solve many problems in formal verification such as bounded model checking (BMC) for many classes of systems from integrated circuits to cyber-physical systems (CPS). Typically, BMC is performed by checking satisfiability of a possibly long, but quantifier-free formula. However, BMC problems can naturally be encoded as quantified formulas over the number of BMC steps. In this approach, we then use decision procedures supporting quantifiers to check satisfiability of these quantified formulas. This approach has previously been applied to perform BMC using a Quantified Boolean Formula (QBF) encoding for purely discrete systems, and then discharges the QBF checks using QBF solvers. In this paper, we present a new quantified encoding of BMC for rectangular hybrid automata (RHA), which requires using more general logics due to the real (dense) time and real-valued state variables modeling continuous states. We have implemented a preliminary experimental prototype of the method using the HyST model transformation tool to generate the quantified BMC (QBMC) queries for the Z3 SMT solver. We describe experimental results on several timed and hybrid automata benchmarks, such as the Fischer and Lynch-Shavit mutual exclusion algorithms. We compare our approach to quantifier-free BMC approaches, such as those in the dReach tool that uses the dReal SMT solver, and the HyComp tool built on top of nuXmv that uses the MathSAT SMT solver. Based on our promising experimental results, QBMC may in the future be an effective analysis approach for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:NaGl4SEjCO4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised Embedding Enhancements of Knowledge Graphs using Textual Associations.",
            "Publication year": 2019,
            "Publication url": "https://www.ijcai.org/proceedings/2019/0725.pdf",
            "Abstract": "Knowledge graph embeddings are instrumental for representing and learning from multi-relational data, with recent embedding models showing high effectiveness for inferring new facts from existing databases. However, such precisely structured data is usually limited in quantity and in scope. Therefore, to fully optimize the embeddings it is important to also consider more widely available sources of information such as text. This paper describes an unsupervised approach to incorporate textual information by augmenting entity embeddings with embeddings of associated words. The approach does not modify the optimization objective for the knowledge graph embedding, which allows it to be integrated with existing embedding models. Two distinct forms of textual data are considered, with different embedding enhancements proposed for each case. In the first case, each entity has an associated text document that describes it. In the second case, a text document is not available, and instead entities occur as words or phrases in an unstructured corpus of text fragments. Experiments show that both methods can offer improvement on the link prediction task when applied to many different knowledge graph embedding models.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:kRWSkSYxWN8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Extraction error analysis, diagnosis and correction in custom-made high-performance designs",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1250263/",
            "Abstract": "Test model generation is crucial in the test generation process of a high-performance design. A key process in test model generation extracts a gate-level (logic) model from the transistor level representation of the circuit under test. Due to the limitation of the extraction tools and human interference, logic extraction may introduce errors. Such errors require a resource intensive and time consuming manual process to debug. We present a set of extraction errors typical in an industrial environment. It also proposes an automated solution to extraction error diagnosis and correction. Experiments on circuits with architecture similar to high speed custom-made industrial blocks confirm the fitness of the approach.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:W7OEmFMy1HYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Managing don't cares in Boolean satisfiability",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1268858/",
            "Abstract": "Advances in Boolean satisfiability solvers have popularized their use in many of today's CAD VLSI challenges. Existing satisfiability solvers operate on a circuit representation that does not capture all of the structural circuit characteristics and properties. This work proposes algorithms that take into account the circuit don't care conditions thus enhancing the performance of these tools. Don't care sets are addressed in this work both statically and dynamically to reduce the search space and guide the decision making process. Experiments demonstrate performance gains.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:NMxIlDl6LWMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automated debugging of missing assumptions",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6742977/",
            "Abstract": "Formal verification has increased efficiency by detecting corner case design bugs but it has also introduced new challenges when failures are detected. Once a counter-example is returned by a formal tool, the user typically does not know if the failure is caused by a design bug, an incorrectly written assertion, or a missing assumption. Previous work in debug automation has focused on the former two cases. This paper introduces a novel methodology to automatically debug missing assumptions. It begins by generating multiple formal counter-examples for the error. Next, a function is extracted from these counter-examples that encodes the input combinations that cause the assertion to fail. This function is later used to generate a list of fixed cycle assumptions that prevent failures similar to the generated counter-examples. These filtered assumptions can then be used as hints for the actual missing assumption \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:sSrBHYA8nusC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automated debugging with high level abstraction and refinement",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5340178/",
            "Abstract": "Design debugging is a manual and time consuming task which takes as much as 60% of the verification effort. To alleviate the debugging pain automated debuggers must tackle industrial problems by increasing their capacity and improving their performance. This work introduces an abstraction and refinement methodology for debugging that leverages the high level information inherent to RTL designs. Function abstraction uses the modular nature of designs to simplify the debugging problem. If required, refinement re-introduces the necessary circuitry back into the design in order to find all error locations. The abstraction and refinement process is applied throughout the design's hierarchy allowing for a divide and conquer methodology. The proposed technique is shown to reduce the memory requirement by as much as 27 x and reduce the run-time by two orders of magnitude over a conventional debugger.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:eQOLeE2rZwMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Finding all minimal safe inductive sets",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-94144-8_21",
            "Abstract": "Computing minimal (or even just small) certificates is a central problem in automated reasoning and, in particular, in automated formal verification. For unsatisfiable formulas in CNF such certificates take the form of Minimal Unsatisfiable Subsets (MUSes) and have a wide range of applications. As a formula can have multiple MUSes that each provide different insights on unsatisfiability, commonly studied problems include computing a smallest MUS (SMUS) or computing all MUSes (AllMUS) of a given unsatisfiable formula. In this paper, we consider certificates to safety properties in the form of Minimal Safe Inductive Sets (MSISes), and we develop algorithms for exploring such certificates by computing a smallest MSIS (SMSIS) or computing all MSISes (AllMSIS) of a given safe inductive invariant. More precisely, we show how the well-known MUS enumeration algorithms CAMUS and MARCO can be \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:L8Ckcad2t8MC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Extraction error modeling and automated model debugging in high-performance low power custom designs",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1395719/",
            "Abstract": "Test model generation is common in the design cycle of custom made high performance low power designs targeted for high volume production. Logic extraction is a key step in test model generation to produce a logic level netlist from the transistor level representation. This is a semi-automated process which is error prone. The paper analyzes typical extraction errors applicable to clocking schemes seen in high-performance designs today. An automated debugging solution for these errors in designs with no state equivalence information is also presented. A suite of experiments on circuits with similar architectures to those found in the industry confirm the fitness and practicality of the solution.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fast GPU-based influence maximization within finite deadlines via node-level parallelism",
            "Publication year": 2017,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-62701-4_12",
            "Abstract": "Influence maximization in the continuous-time domain is a prevalent topic in social media analytics. It relates to the problem of identifying those individuals in a social network, whose endorsement of an opinion will maximize the number of expected follow-ups within a finite time window. This work presents a novel GPU-accelerated algorithm that enables node-parallel estimation of influence spread in the continuous-time domain. Given a finite time window, the method involves decomposing a social graph into multiple local regions within which influence spread can be estimated in parallel to allow for fast and low-cost computations. Experiments show that the proposed method achieves up to x85 speed-up vs. the state-of-the-art on real-world social graphs with up to 100K nodes and 2.5M edges. In addition, our optimization solutions are within 98.9% of the influence spread achieved by current state-of-the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:NhqRSupF_l8C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "From RTL to silicon: The case for automated debug",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5722204/",
            "Abstract": "Computer-aided design tools are continuously improving their scalability and efficiency to mitigate the high cost associated with designing and fabricating modern VLSI systems. A key step in the design process is the root-cause analysis of detected errors. Debugging may take months to close, introduce high cost and uncertainty ultimately jeopardizing the chip release date. This study makes the case for debug automation in each part of the design flow (RTL to silicon) to bridge the gap. Contemporary research, challenges and future directions motivate for the urgent need in automation to relieve the pain from this highly manual task.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On error tolerance and engineering change with partially programmable circuits",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6165045/",
            "Abstract": "The growing size, density and complexity of modern VLSI chips are contributing to an increase in hardware faults and design errors in the silicon, decreasing manufacturing yield and increasing the design cycle. The use of Partially Programmable Circuits (PPCs) has been recently proposed for yield enhancement with very small overhead. This new circuit structure is obtained from conventional logic by replacing some subcircuits with programmable LUTs. The present paper lays the theoretical groundwork for evaluating PPCs with Quantified Boolean Formula (QBF) satisfiability. First, QBF models are constructed to calculate the fault tolerance and design error tolerance of a PPC, namely the percentages of faults and design errors that can be masked using LUT reconfigurations. Next, zero-cost Engineering Change Order (ECO) in PPCs is investigated. QBF formulations are given for performing ECOs, and for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:wbdj-CoPYUoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the latency, energy and area of checkpointed, superscalar register alias tables",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5514293/",
            "Abstract": "We present two full-custom implementations of the Register Alias Table (RAT) for a 4-way superscalar dynamically-scheduled processor in a commercial 130nm CMOS technology. The implementations differ in the way they organize the embedded global checkpoints (GCs) which support speculative execution. In the first implementation, representative of early designs, the GCs are organized as shift registers. In the second implementation, representative of more recent proposals, the GCs are organized as random access buffers. We measure the impact of increasing the number of GCs on the latency, energy, and area of the RAT. The results support the importance of recent techniques that reduce the number of GCs while maintaining performance.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:VOx2b1Wkg3QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fault diagnosis and logic debugging using Boolean satisfiability",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1512377/",
            "Abstract": "Recent advances in Boolean satisfiability have made it an attractive engine for solving many digital very-large-scale-integration design problems. Although useful in many stages of the design cycle, fault diagnosis and logic debugging have not been addressed within a satisfiability-based framework. This work proposes a novel Boolean satisfiability-based method for multiple-fault diagnosis and multiple-design-error diagnosis in combinational and sequential circuits. A number of heuristics are presented that keep the method memory and run-time efficient. An extensive suite of experiments on large circuits corrupted with different types of faults and errors confirm its robustness and practicality. They also suggest that satisfiability captures significant characteristics of the problem of diagnosis and encourage novel research in satisfiability-based diagnosis as a complementary process to design verification.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:35N4QoGY0k4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Verified development and deployment of multiple interacting smart contracts with VeriSolid",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9169428/",
            "Abstract": "Smart contracts enable the creation of decentralized applications which often handle assets of large value. These decentralized applications are frequently built on multiple interacting contracts. While the underlying platform ensures the correctness of smart contract execution, today developers continue struggling to create functionally correct contracts, as evidenced by a number of security incidents in the recent past. Even though these incidents often exploit contract interaction, prior work on smart contract verification, vulnerability discovery, and secure development typically considers only individual contracts. This paper proposes an approach for the correct-by-design development and deployment of multiple interacting smart contracts by introducing a graphical notation (called deployment diagrams) for specifying possible interactions between contract types. Based on this notation, it later presents a framework for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:blknAaTinKkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Software solutions to automating data analysis and acquisition setup in silicon debug",
            "Publication year": 2010,
            "Publication url": "http://www.eecg.toronto.edu/~veneris/sdd10.pdf",
            "Abstract": "With the growing size of modern designs and more strict time-to-market constraints, design errors can unavoidably escape pre-silicon verification and pass in the silicon prototype. As a result, silicon debug has become a necessary step in the implementation flow of digital integrated circuits. Embedded hardware blocks such as scan chains and trace buffers provide a means to acquire data of internal signals in real time for debugging. However, there is a lack of techniques to automate the process of analyzing those data. This paper presents an automated software solution to analyze this sparse data to detect suspects of the failure. Furthermore, a set of techniques that help to configure the data acquisition environment is presented. The experiments demonstrate the effectiveness of the proposed software solution in terms of run-time and resolution.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:cFHS6HbyZ2cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the latency and energy of checkpointed superscalar register alias tables",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5169866/",
            "Abstract": "This paper investigates how the latency and energy of register alias tables (RATs) vary as a function of the number of global checkpoints (GCs), processor issue width, and window size. It improves upon previous RAT checkpointing work that ignored the actual latency and energy tradeoffs and focused solely on evaluating performance in terms of instructions per cycle (IPC). This work utilizes measurements from the full-custom checkpointed RAT implementations developed in a commercial 130-nm fabrication technology. Using physical- and architectural-level evaluations together, this paper demonstrates the tradeoffs among the aggressiveness of the RAT checkpointing, performance, and energy. This paper also shows that, as expected, focusing on IPC alone incorrectly predicts performance. The results of this study justify checkpointing techniques that use very few GCs (e.g., four). Additionally, based on full \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:V3AGJWp-ZtQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Accelerating post silicon debug of deep electrical faults",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6604052/",
            "Abstract": "With the growing complexity of current designs and shrinking time-to-market, traditional ATPG methods fail to detect all electrical faults in the design. Debug teams have to spend considerable amount of time and effort to identify these faults during post silicon debug. This work proposes off-chip analysis to speed-up the effort of identifying hard-to-find electrical faults that are not detected using conventional test methods, but cause the chip to crash during functional testing or silicon-bring-up. With the goal of reducing the search space for reconstructing the failure trace path, formal methodology is used to analyze the reachable states along the path. Isolating the root cause of failure is also accelerated. Moreover, we propose a forward traversal technique on selected few possible faults to generate a complete failure trace starting from the initial state to the crash state. Experimental results show that the proposed \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:J-pR_7NvFogC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automated rectification methodologies to functional state-space unreachability",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7092610/",
            "Abstract": "In the modern design cycle, significant manual resources are dedicated to fix a design when verification shows that a state is not reachable. Today there is little automation to aid an engineer in understanding why a state is not reachable and how to correct it. This paper presents a novel methodology that automates this task. In detail, a process that involves intertwined steps of state approximation, reachability analysis and traditional debugging is developed to identify design locations where fixes can be applied so the target state becomes reachable. An initial formulation identifies such error locations that, when corrected, can make the target state reachable directly from the existing reachable set of states. This is later extended for the cases where more than one state transition is required to reach an unreachable state from the existing reachable set. Empirical results on industrial level designs show a performance \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:iH-uZ7U-co4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Logic rewiring for delay and power minimization",
            "Publication year": 2004,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.67.3950",
            "Abstract": "An application of the ATPG-based method by Veneris et al.[11] to multi-level combinational logic circuit delay and power optimization is presented. A number of theoretical results and various heuristics are described to allow for an efficient implementation of the algorithm. Experiments confirm the robustness of the approach.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:BrmTIyaxlBUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Integrating observability don't cares in all-solution SAT solvers",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1692903/",
            "Abstract": "All-solution Boolean satisfiability (SAT) solvers are engines employed to find all the possible solutions to a SAT problem. Their applications are found throughout the EDA industry in fields such as formal verification, circuit synthesis and automatic test pattern generation. Typically, these engines iteratively find each solution by calling a standard SAT solving procedure. Each solution is minimized using different post processing techniques and the problem is constrained to prevent recurring solutions. In this work, instead of applying post processing techniques, the objective is to minimize the size of the solution \"on the fly\" during the all-solution SAT solving process. This is achieved by allowing the solver to exploit the structural circuit observability don't cares (ODC) arising from the problem. The solver makes decisions such that the number of ODCs is maximized in each solution thus leading to an overall smaller \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Method, system and computer program for hardware design debugging",
            "Publication year": 2014,
            "Publication url": "https://patents.google.com/patent/US8881077B2/en",
            "Abstract": "A plurality of diagnosis methods are provided for enabling hardware debugging. A first diagnosis method enables hardware debugging by means of time abstraction. A second-diagnosis method enables hardware debugging by means of abstraction and refinement. A third diagnosis method enables hardware debugging by means of QBF-formulation for replicated functions. A fourth diagnosis method enables hardware debugging by means of a max-sat debugging formulation. A system and computer program for implementing the diagnosis methods is also provide.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:CHSYGLWDkRkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Method, system, and computer program for identifying design revisions in hardware design debugging",
            "Publication year": 2016,
            "Publication url": "https://patents.google.com/patent/US20160342720A1/en",
            "Abstract": "The present invention provides a method, system and computer program for ranking suspect components in a hardware design that fails verification, based on their likelihood of being actual error sources, and identifying design revisions or branches that are likely to contain actual error sources. The method is implemented as a suspect and revision ranking engine. The ranking engine involves the input of an engineer to provide an initial set of suspects or it can use the application of at least one automated debugging tool for each failure exposed by verification, and collects suspect sets returned by these tools. These tools can be based on simulation, path tracing, ATPG, BDDs, SAT, and QBF techniques. The engine applies either an analytical or statistical process on the suspects that are collected, to identify suspect components that are likely responsible for a large number of design failures. The process to rank \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:ULOm3_A8WrAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ASTRAEA: A decentralized blockchain oracle",
            "Publication year": 2019,
            "Publication url": "https://blockchain.ieee.org/technicalbriefs/march-2019/astraea-a-decentralized-blockchain-oracle",
            "Abstract": "The idea of a public blockchain was first conceived to prevent double-spending while processing monetary transactions in a peer-to-peer network [1]. In its original application, it allows a set of quasi-anonymous and mutually-distrusting parties to reach consensus on the ordering of monetary transactions. It has since been extended to other applications including execution of state machines (ie, software programs) called \u201csmart contracts\u201d[2] in a decentralized fashion. Smart contracts have a major inherent limitation, as they can only operate on data that is on the blockchain. As a result, trusted entities called oracles attest to external data so as to bring it onto the blockchain. Existing oracles typically do not provide robust guarantees on the accuracy of this data. Trusted oracles are therefore at risk of becoming centralized points-of-failure for decentralized applications that depend on them. This isn't a major issue when \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:qUcmZB5y_30C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Design Optimization based on Diagnosis Techniques",
            "Publication year": 2000,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.2660&rep=rep1&type=pdf",
            "Abstract": "Logic optimization is the step of the VLSI design cycle where the designer performs modifications on the design obtained to satisfy different constraints such as area, power or delay. In this paper we propose a novel ATPG-based optimization methodology that borrows from previous design error diagnosis and correction techniques. We also present examples and experiments that indicate that our approach has additional potential when compared to previous ATPG/simulation-based optimization methods.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:WF5omc3nYNoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "FudgeFactor: Syntax-Guided Synthesis for Accurate RTL Error Localization and Correction",
            "Publication year": 2015,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=2yfUCgAAQBAJ&oi=fnd&pg=PA257&dq=info:UUxL_h4UhhkJ:scholar.google.com&ots=3X7kq8k5oN&sig=3Rv2yuMIZJMRslebJN1lrgEdcZg",
            "Abstract": "Functional verification occupies a significant amount of the digital circuit design cycle. In this paper, we present a novel approach to improve circuit debugging which not only localizes errors with high confidence, but can also provide semantically-meaningful source code corrections. Our method, which we call FudgeFactor, starts with a buggy design, at least one failing and several correct test vectors, and a list of suspect bug locations. We obtain the suspect location from a state-of-the-art debugging tool that includes a significant number of false positives. Using this list and a library of rules empirically characterizing typical source-code mistakes, we instrument the buggy design to allow each potential error location to either be left unchanged, or replaced with a set of possible corrections. FudgeFactor then combines the instrumented design with the test vectors and solves a 2QBF-SAT problem to find the minimum number of source-level changes from the original code which correct the bug. Our 13 benchmarks demonstrate that our method is able to correct a sizable portion of realistic bugs within a reasonable computational time. With the aid of available golden reference designs, we show that those corrections are, at least on these benchmarks, always valid and non-trivial fixes. We believe that our technique significantly improves over other debugging tools in two respects: When we succeed, we obtain a much more precise bug localization with no false positives and little or no ambiguity. Additionally, we offer bug corrections that are inherently meaningful to the designers and enable designers to quickly recognize and understand the root cause \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:LkGwnXOMwfcC",
            "Publisher": "Springer"
        },
        {
            "Title": "L-CBF: a low-power, fast counting Bloom filter architecture",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4509488/",
            "Abstract": "An increasing number of architectural techniques have relied on hardware counting bloom filters (CBFs) to improve upon the energy, delay, and complexity of various processor structures. CBFs improve the energy and speed of membership tests by maintaining an imprecise and compact representation of a large set to be searched. This paper studies the energy, delay, and area characteristics of two implementations for CBFs using full custom layouts in a commercial 0.13-mum fabrication technology. One implementation, S-CBF, uses an SRAM array of counts and a shared up/down counter. Our proposed implementation, L-CBF, utilizes an array of up/down linear feedback shift registers and local zero detectors. Circuit simulations show that for a 1 K-entry CBF with a 15-bit count per entry, L-CBF compared to S-CBF is 3.7times or 1.6times faster and requires 2.3times or 1.4times less energy depending on the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:bnK-pcrLprsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Incremental diagnosis and correction of multiple faults and errors",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/998378/",
            "Abstract": "An incremental simulation-based approach to fault diagnosis and logic debugging is presented. During each iteration of the algorithm, a single suspicious location is identified and fault modeled such that the functionality of the new design becomes \"closer\" to its specification. The method is based on a simple and, at a first glance, counter-intuitive theoretical result along with a number of heuristics which help avoid the exponential complexity inherent to the problems. Experiments on multiple design errors and multiple stuck-at faults confirm its effectiveness and accuracy, which scales well with increasing number of errors.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:lSLTfruPkqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "9D. 3 Post-Verification Debugging of Hierarchical Designs",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=15952077192771616224&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:JoZmwDi-zQgC",
            "Publisher": "ICCAD PUBLICATIONS"
        },
        {
            "Title": "Special Drawing Rights on a New Decentralized Century",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1907.11057",
            "Abstract": "Unfulfilled expectations from macro-economic initiatives during the Great Recession and the massive shift into globalization echo today with political upheaval, anti-establishment propaganda, and looming trade/currency wars that threaten domestic and international value chains. Once stable entities like the EU now look fragile and political instability in the US presents unprecedented challenges to an International Monetary System (IMS) that predominantly relies on the USD and EUR as reserve currencies. In this environment, it is critical for an international organization mandated to ensure stability to plan and act ahead. This paper argues that Decentralized Ledger-based technology (DLT) is key for the International Monetary Fund (IMF) to mitigate some of those risks, promote stability and safeguard world prosperity. Over the last two years, DLT has made headline news globally and created a worldwide excitement not seen since the internet entered the mainstream. The rapid adoption and open-to-all philosophy of DLT has already redefined global socioeconomics, promises to shake up the world of commerce/finance and challenges the workings of central governments/regulators. This paper examines DLT core premises and proposes a two-step approach for the IMF to expand Special Drawing Rights (SDR) into that sphere so as to become the originally envisioned numeraire and reserve currency for cross-border transactions in this new decentralized century.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:Y5dfb0dijaUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sequence pair based voltage island floorplanning",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6008601/",
            "Abstract": "In the nanometer era of VLSI design, high power consumption is considered to be a \u201cshow-stopper\u201d for many applications. Voltage Island design has emerged as a popular method for addressing this issue. This technique requires multiple supply voltages on the same chip with blocks assigned to different supply voltages. Implementation challenges force blocks with similar supply voltages to be placed contiguous to one another, thereby creating \u201cislands\u201d. Classical floorplanners assume a single supply voltage in the entire SoC and thus require additional design steps to realize voltage islands. In this paper we present a new floorplanning algorithm based on the sequence pair representation that can floorplan blocks in the form of islands. Given the possible supply voltage choices for each block, the floorplanner simultaneously attempts to reduce power and area of the chip. Our floorplanner integrates the tasks of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:HDshCWvjkbEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Learning lemma support graphs in quip and IC3",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8031554/",
            "Abstract": "Formal verification is one of the fastest growing fields in verification. The Boolean satisfiability-based unbounded model checking algorithm of IC3 has become widely applied in industry and is frequently used as a subroutine in other formal verification algorithms, such as FAIR and IICTL. Any improvement to IC3 can therefore yield substantial benefits in many areas of formal verification. Towards that end, this paper introduces the notion of a support graph, which is applied in IC3. Techniques are presented to compute the support graph by modifying the satisfiability queries used in IC3 at the cost of a modest increase in runtime. It is used to increase the re-use of information across runs of the model checker, thereby improving runtime performance in incremental model checking. It can also be applied within a single run of the model checker to avoid unnecessary queries to the satisfiability solver and accelerate the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:Se3iqnhoufwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automated debugging of missing input constraints in a formal verification environment",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6462561/",
            "Abstract": "In the past decade, formal tools have increased functional verification efficiency by exhaustively searching for hard to find bugs. Often the counter-examples returned are not due to design bugs but due to missing constraints that are needed to model the surrounding environment. These types of false positives have become a great concern in the industry today. To address this issue, input constraints are typically added by the engineer to restrict the input space a formal tool is allowed to explore. These constraints are difficult to generate as they are usually implicit in the documentation or implementation of adjacent design blocks. As a consequence, this process reduces the efficiency of formal methodologies because missing input constraints must be determined before deep design bugs can actually be detected. In this work, we present an algorithm to automatically generate missing input constraints given a failing \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:P5F9QuxV20EC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Post-verification debugging of hierarchical designs",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1560184/",
            "Abstract": "As VLSI designs grow in complexity and size, errors become more frequent and difficult to track. Recent developments have automated most of the verification tasks but debugging still remains a resource-intensive, manually conducted procedure. This paper bridges this gap as it develops robust automated debugging methodologies that complement verification processes. Unlike prior debugging techniques, the proposed one exploits the hierarchical nature of modern designs to improve the performance and quality of debugging. It also formulates the problem in terms of Quantified Boolean Formula Satisfiability to obtain dramatic reduction in memory requirements, which allows for debugging of large designs. Extensive experiments conducted on industrial and benchmark designs confirm the efficiency and practicality of the proposed approach.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:Tiz5es2fbqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "On public crowdsource-based mechanisms for a decentralized blockchain oracle",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9113449/",
            "Abstract": "Blockchain technology has created an excitement that was last seen two decades ago when the Internet was entering the mainstream. An appealing feature of blockchain technology is smart contracts. A smart contract is an executable code. It runs on top of the blockchain facilitating an agreement between untrusted parties. These smart contracts have a major limitation, namely they cannot operate on information external to the blockchain. The inability to query such information has paved the need for trusted entities called \u201coracles.\u201d These oracles attest to facts without the robust security guarantees that blockchains generally provide. This can potentially harm the integrity of the network and lead to centralized points-of-failure. To address this concern, this article proposes a decentralized oracle which is based on a voting-based game that decides the truth or falsity of queries. In the context of this article, we are only interested in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:mvPsJ3kp5DgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Early detection of current hot spots in power gated designs",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6629265/",
            "Abstract": "With the growing popularity of hand-held battery-powered devices, leakage power is a major concern in the nanometer CMOS era. Power gating technique is an effective and widely adopted solution to this problem. The challenge of implementing power gating is the sizing and placement of the sleep transistors that are used to gate the power supply. In a placed design, due to non-uniform current demand of logic cells, some regions of the chip can have sleep transistors with very high current demand, causing power grid noise violations. Identifying these regions early in the design cycle is critical to the success of power gating implementation. This paper presents a novel methodology to calculate the current demand of each sleep transistor and locate regions in the chip where multiple sleep transistors experience very high current demand. In this paper, we model the spatial locality of the current drawn by each logic \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:M3NEmzRMIkIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Scaling VLSI design debugging with interpolation",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5351130/",
            "Abstract": "Given an erroneous design, functional verification returns an error trace exhibiting a mismatch between the specification and the implementation of a design. Automated design debugging uses these error traces to identify potentially erroneous modules causing the error. With the increasing size and complexity of modern VLSI designs, error traces have become longer and harder to analyze. At the same time, design debugging has become one of the most resource-intensive steps in the chip design cycle. This work proposes a scalable SAT-based design debugging algorithm that uses interpolants to over-approximate sets of constraints that model the erroneous behavior. The algorithm partitions the original problem into a sequence of smaller subproblems by using subsections of the error trace that are examined iteratively. This is made possible by using interpolants to properly constrain the erroneous behavior for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:rO6llkc54NcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multiple clock domain synchronization in a QBF-based verification environment",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7001426/",
            "Abstract": "Modern designs are growing in size and complexity, becoming increasingly harder to verify. Today, they are architected to include multiple clock domains as a measure to reduce power consumption. Verifying them proves to be a computationally intensive and challenging task as it requires their clocks to be synchronized. To achieve synchronization, existing Boolean satisfiability-based methodologies add hardware to combine the clock domains before transforming them into their iterative logic array representation (ILA). As a consequence, this results in the addition of redundant time-frames adding overhead during verification. This paper introduces a novel framework to verify designs with multiple clocks using Quantified Boolean Formula satisfiability (QBF). We first present a formulation that models an ILA representation with symbolic universal quantification to achieve synchronization. This is later extended with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:4fKUyHm3Qg0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the relation between simulation-based and SAT-based diagnosis",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1657064/",
            "Abstract": "The problem of diagnosis - or locating the source of an error or fault $occurs in several areas of computer aided design, such as dynamic verification, property checking, equivalence checking and production test. Manually locating errors can be a time consuming and resource-intensive process. Several automated approaches for diagnosis have been presented, among them are simulation-based and SAT-based techniques. These two approaches are found to be robust even for large circuits as well as being applicable to a broad range of diagnosis problems. An in-depth comparison of both approaches necessary to augment our knowledge of diagnosis procedures has not been addressed by previous work. This paper provides a thorough analysis of the similarities and differences between simulation-based and SAT-based procedures for diagnosis. The relation between the basic approaches is theoretically \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:HoB7MX3m0LUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Short Paper_",
            "Publication year": 2004,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.103.463&rep=rep1&type=pdf",
            "Abstract": "An application of the ATPG-based method by Veneris et al.[11] to multi-level combinational logic circuit delay and power optimization is presented. A number of theoretical results and various heuristics are described to allow for an efficient implementation of the algorithm. Experiments confirm the robustness of the approach.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:1qzjygNMrQYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Incremental fault diagnosis",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1386379/",
            "Abstract": "Fault diagnosis is important in improving the circuit-design process and the manufacturing yield. Diagnosis of today's complex defects is a challenging problem due to the explosion of the underlying solution space with the increasing number of fault locations and fault models. To tackle this complexity, an incremental diagnosis method is proposed. This method captures faulty lines one at a time using the novel linear-time single-fault diagnosis algorithms. To capture complex fault effects, a model-free incremental diagnosis algorithm is outlined, which alleviates the need for an explicit fault model. To demonstrate the applicability of the proposed method, experiments on multiple stuck-at faults, open-interconnects and bridging faults are performed. Extensive results on combinational and full-scan sequential benchmark circuits confirm its resolution and performance.",
            "Abstract entirety": 1,
            "Author pub id": "ItQhxCkAAAAJ:xtRiw3GOFMkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "System, method and computer program for determining fixed value, fixed time, and stimulus hardware diagnosis",
            "Publication year": 2012,
            "Publication url": "https://patents.google.com/patent/US20120198399A1/en",
            "Abstract": "The present invention provides a system, method and computer program for determining constraint errors in hardware design debugging. The invention may be included as part of a complete verification solution. The method involves applying a diagnostic technique such that under-constrained problems are identified by adding a model-free error suspect or error candidate on the primary input signals (or other signals where constraints or stimuli are usually added). The present invention also provides a system, method and computer program that enables hardware design correction, consisting of the use of generating correction waveforms for identifying one or more corrections at the gate level and/or logic level of the hardware design. A number of different diagnostic techniques can be used in this way for example, include simulation-based techniques, BDD-based techniques, SAT-based techniques and path \u2026",
            "Abstract entirety": 0,
            "Author pub id": "ItQhxCkAAAAJ:fQNAKQ3IYiAC",
            "Publisher": "Unknown"
        }
    ]
}]