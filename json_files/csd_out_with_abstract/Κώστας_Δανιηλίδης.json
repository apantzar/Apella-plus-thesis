[{
    "name": "\u039a\u03ce\u03c3\u03c4\u03b1\u03c2 \u0394\u03b1\u03bd\u03b9\u03b7\u03bb\u03af\u03b4\u03b7\u03c2",
    "romanize name": "Kostas Daniilidis",
    "School-Department": " Computer and Information Science\u00a0",
    "University": "University of Pennsylvania",
    "Rank": "\u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 4678,
    "Scholar name": "Kostas Daniilidis",
    "Scholar id": "dGs2BcIAAAAJ",
    "Affiliation": "Ruth Yalom Stone Professor of Computer and Information Science, University of Pennsylvania",
    "Citedby": 17961,
    "Interests": [
        "Computer Vision",
        "Robotics"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=dGs2BcIAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Learning Portrait Style Representations",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2012.04153",
            "Abstract": "Style analysis of artwork in computer vision predominantly focuses on achieving results in target image generation through optimizing understanding of low level style characteristics such as brush strokes. However, fundamentally different techniques are required to computationally understand and control qualities of art which incorporate higher level style characteristics. We study style representations learned by neural network architectures incorporating these higher level characteristics. We find variation in learned style features from incorporating triplets annotated by art historians as supervision for style similarity. Networks leveraging statistical priors or pretrained on photo collections such as ImageNet can also derive useful visual representations of artwork. We align the impact of these expert human knowledge, statistical, and photo realism priors on style representations with art historical research and use these representations to perform zero-shot classification of artists. To facilitate this work, we also present the first large-scale dataset of portraits prepared for computational analysis.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:L2Pn6qttGKUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction",
            "Publication year": 2019,
            "Publication url": "https://openreview.net/forum?id=BklfR3EYDH",
            "Abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them. We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:lRnoeYR1YAAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Constructing topological maps using markov random fields and loop-closure detection",
            "Publication year": 2009,
            "Publication url": "https://proceedings.neurips.cc/paper/2009/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html",
            "Abstract": "We present a system which constructs a topological map of an environment given a sequence of images. This system includes a novel image similarity score which uses dynamic programming to match images using both the appearance and relative positions of local features simultaneously. Additionally an MRF is constructed to model the probability of loop-closures. A locally optimal labeling is found using Loopy-BP. Finally we outline a method to generate a topological map from loop closure data. Results are presented on four urban sequences and one indoor sequence.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:S16KYo8Pm5AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Discovering and Achieving Goals with World Models",
            "Publication year": 2021,
            "Publication url": "https://openreview.net/forum?id=Qf1C1ThepPs",
            "Abstract": "How can an artificial agent learn to solve a wide range of tasks in a complex visual environment in the absence of external supervision? We decompose this question into two problems, global exploration of the environment and learning to reliably reach situations found during exploration. We introduce the Explore Achieve Network (ExaNet), a unified solution to these by learning a world model from the high-dimensional images and using it to train an explorer and an achiever policy from imagined trajectories. Unlike prior methods that explore by reaching previously visited states, our explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever. After the unsupervised phase, ExaNet solves tasks specified by goal images without any additional learning. We introduce a challenging benchmark spanning across four standard robotic manipulation and locomotion domains with a total of over 40 test tasks. Our agent substantially outperforms previous approaches to unsupervised goal reaching and achieves goals that require interacting with multiple objects in sequence. Finally, to demonstrate the scalability and generality of our approach, we train a single general agent across four distinct environments. For videos, see https://sites. google. com/view/exanet/home.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:FSl0EHHYj-kC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reinforcement learning with videos: Combining offline observations with interaction",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2011.06507",
            "Abstract": "Reinforcement learning is a powerful framework for robots to acquire skills from experience, but often requires a substantial amount of online data collection. As a result, it is difficult to collect sufficiently diverse experiences that are needed for robots to generalize broadly. Videos of humans, on the other hand, are a readily available source of broad and interesting experiences. In this paper, we consider the question: can we perform reinforcement learning directly on experience collected by humans? This problem is particularly difficult, as such videos are not annotated with actions and exhibit substantial visual domain shift relative to the robot's embodiment. To address these challenges, we propose a framework for reinforcement learning with videos (RLV). RLV learns a policy and value function using experience collected by humans in combination with data collected by robots. In our experiments, we find that RLV is able to leverage such videos to learn challenging vision-based skills with less than half as many samples as RL methods that learn from scratch.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:J2Md-p1DcKkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fast, autonomous flight in GPS\u2010denied and cluttered environments",
            "Publication year": 2018,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21774",
            "Abstract": "One of the most challenging tasks for a flying robot is to autonomously navigate between target locations quickly and reliably while avoiding obstacles in its path, and with little to no a priori knowledge of the operating environment. This challenge is addressed in the present paper. We describe the system design and software architecture of our proposed solution and showcase how all the distinct components can be integrated to enable smooth robot operation. We provide critical insight on hardware and software component selection and development and present results from extensive experimental testing in real\u2010world warehouse environments. Experimental testing reveals that our proposed solution can deliver fast and robust aerial robot autonomous navigation in cluttered, GPS\u2010denied environments.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:J2VLEJC5QowC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Labeling Panoramas with Spherical Hourglass Networks",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1809.02123",
            "Abstract": "With the recent proliferation of consumer-grade 360{\\deg} cameras, it is worth revisiting visual perception challenges with spherical cameras given the potential benefit of their global field of view. To this end we introduce a spherical convolutional hourglass network (SCHN) for the dense labeling on the sphere. The SCHN is invariant to camera orientation (lifting the usual requirement for `upright' panoramic images), and its design is scalable for larger practical datasets. Initial experiments show promising results on a spherical semantic segmentation task.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:7YMrAF6eRCIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Rotation recovery from spherical images without correspondences",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1634347/",
            "Abstract": "This paper addresses the problem of rotation estimation directly from images defined on the sphere and without correspondence. The method is particularly useful for the alignment of large rotations and has potential impact on 3D shape alignment. The foundation of the method lies in the fact that the spherical harmonic coefficients undergo a unitary mapping when the original image is rotated. The correlation between two images is a function of rotations and we show that it has an SO(3)-Fourier transform equal to the pointwise product of spherical harmonic coefficients of the original images. The resolution of the rotation space depends on the bandwidth we choose for the harmonic expansion and the rotation estimate is found through a direct search in this 3D discretized space. A refinement of the rotation estimate can be obtained from the conservation of harmonic coefficients in the rotational shift theorem. A novel \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "On the quotient representation for the essential manifold",
            "Publication year": 2014,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2014/html/Tron_On_the_Quotient_2014_CVPR_paper.html",
            "Abstract": "The essential matrix, which encodes the epipolar constraint between points in two projective views, is a cornerstone of modern computer vision. Previous works have proposed different characterizations of the space of essential matrices as a Riemannian manifold. However, they either do not consider the symmetric role played by the two views, or do not fully take into account the geometric peculiarities of the epipolar constraint. We address these limitations with a characterization as a quotient manifold which can be easily interpreted in terms of camera poses. While our main focus in on theoretical aspects, we include experiments in pose averaging, and show that the proposed formulation produces a meaningful distance between essential matrices.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Ncwx4PHgTB8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Performance evaluation of stereo for tele-presence",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/937675/",
            "Abstract": "In an immersive tele-presence environment a 3D remote real scene is projected from the viewpoint of the local user. This 3D world is acquired through stereo reconstruction at the remote site. In this paper we start a performance analysis of stereo algorithms with respect to the task of immersive visualization. As opposed to usual monocular image based rendering, we are also interested in the depth error in novel views because our rendering is stereoscopic. We describe an evaluation test-bed which provides a world-wide first available set of registered dense \"ground-truth\" laser data and image data from multiple views. We establish metrics for novel depth views that reflect discrepancies both in the image and in 3D-space. It is well known that stereo performance is affected by both erroneous matching as well as incorrect depth triangulation. We experimentally study the effects of occlusion and low texture on the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:LkGwnXOMwfcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Simple and effective VAE training with calibrated decoders",
            "Publication year": 2021,
            "Publication url": "https://proceedings.mlr.press/v139/rybkin21a.html",
            "Abstract": "Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:41wR57jXX04C",
            "Publisher": "PMLR"
        },
        {
            "Title": "A dynamical systems approach to distributed eigenvector computation",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8263972/",
            "Abstract": "We study the problem of distributedly estimating the k largest/smallest eigenvalues and the associated eigenvectors of a (possibly weighted) graph. In this work, we propose a dynamical systems approach that is fully decentralized and has global convergence guarantees. We demonstrate the validity of our approach through rigorous theoretical analysis and experimental evaluation.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:hL6j_VNiN9gC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Using skew Gabor filter in source signal separation and local spectral orientation analysis",
            "Publication year": 2005,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0262885604001969",
            "Abstract": "Responses of Gabor wavelets in the mid-frequency space build a local spectral representation scheme with optimal properties regarding the time-frequency uncertainty principle. However, when using Gabor wavelets we observe a skewness in the mid-frequency space caused by the unsymmetrically spreading effect of Gabor wavelets. Though in most current applications the skewness does not obstruct the sampling of the spectral domain, it affects the identification and separation of source Signals from the filter response in the mid-frequency space. In this paper, we present a modification of the original Gabor filter, the skew Gabor filter, which corrects skewness so that the filter response can be described with a sum-of-Gaussians model in the mid-frequency space. The correction further enables us to use higher order moment information to analytically separate different source signal components. This provides us \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:2P1L_qKh6hAC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Tagslam: Robust slam with fiducial markers",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1910.00679",
            "Abstract": "TagSLAM provides a convenient, flexible, and robust way of performing Simultaneous Localization and Mapping (SLAM) with AprilTag fiducial markers. By leveraging a few simple abstractions (bodies, tags, cameras), TagSLAM provides a front end to the GTSAM factor graph optimizer that makes it possible to rapidly design a range of experiments that are based on tags: full SLAM, extrinsic camera calibration with non-overlapping views, visual localization for ground truth, loop closure for odometry, pose estimation etc. We discuss in detail how TagSLAM initializes the factor graph in a robust way, and present loop closure as an application example. TagSLAM is a ROS based open source package and can be found at https://berndpfrommer.github.io/tagslam_web.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:oLQGhPHTrpcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Mesh Representation Driven by Variance Normalized Neighborhood in Scale Space",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4547875/",
            "Abstract": "In this paper, we extend our previous work on regular mesh simplification using scale-space filtering (SSF), to arbitrary meshes. Additional challenges in using arbitrary meshes involve accurately defining a geodesic distance and a structure attribute. Since our SSF method is based on Laplacian smoothing, which filters finer local details before larger global structures, it preserves perceptual quality that is consistent with how the human visual system looks at 3D objects moving from close to farther away. By keeping the simplified mesh vertices as a subset of the denser meshes, versus using the vertex split operation, bandwidth is utilized more efficiently during refinement. Our contribution lies in applying the shortest path normalized by variance, instead of using an un-normalized geodesic distance to weight the influence of a neighborhood. Variance normalization ensures that the global smoothing property is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:1sJd4Hv_s6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Initialization techniques for 3D SLAM: a survey on rotation estimation and its use in pose graph optimization",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7139836/",
            "Abstract": "Pose graph optimization is the non-convex optimization problem underlying pose-based Simultaneous Localization and Mapping (SLAM). If robot orientations were known, pose graph optimization would be a linear least-squares problem, whose solution can be computed efficiently and reliably. Since rotations are the actual reason why SLAM is a difficult problem, in this work we survey techniques for 3D rotation estimation. Rotation estimation has a rich history in three scientific communities: robotics, computer vision, and control theory. We review relevant contributions across these communities, assess their practical use in the SLAM domain, and benchmark their performance on representative SLAM problems (Fig. 1). We show that the use of rotation estimation to bootstrap iterative pose graph solvers entails significant boost in convergence speed and robustness.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:54MofcL-yxcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "View-independent scene acquisition for tele-presence",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/880933/",
            "Abstract": "Tele-immersion is a new medium that enables a user to share a virtual space with remote participants. The user is immersed in a rendered 3D-world that is transmitted from a remote site. To acquire this 3D description we apply bi- and trinocular stereo techniques. The challenge is to compute dense stereo range data at high frame rates, since participants cannot easily communicate if the processing cycle or network latencies are long. Moreover, new views of the received 3D-world must be as accurate as possible. We address both issues of speed and accuracy and we propose a method for combining motion and stereo in order to increase speed and robustness.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Integrated intelligence for human-robot teams",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-50115-4_28",
            "Abstract": "With recent advances in robotics technologies and autonomous systems, the idea of human-robot teams is gaining ever-increasing attention. In this context, our research focuses on developing an intelligent robot that can autonomously perform non-trivial, but specific tasks conveyed through natural language. Toward this goal, a consortium of researchers develop and integrate various types of intelligence into mobile robot platforms, including cognitive abilities to reason about high-level missions, perception to classify regions and detect relevant objects in an environment, and linguistic abilities to associate instructions with the robot\u2019s world model and to communicate with human teammates in a natural way. This paper describes the resulting system with integrated intelligence and reports on the latest assessment.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:MOFHY6MwG3AC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Pose and Shape Estimation with Discriminatively Learned Parts",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1502.00192",
            "Abstract": "We introduce a new approach for estimating the 3D pose and the 3D shape of an object from a single image. Given a training set of view exemplars, we learn and select appearance-based discriminative parts which are mapped onto the 3D model from the training set through a facil- ity location optimization. The training set of 3D models is summarized into a sparse set of shapes from which we can generalize by linear combination. Given a test picture, we detect hypotheses for each part. The main challenge is to select from these hypotheses and compute the 3D pose and shape coefficients at the same time. To achieve this, we optimize a function that minimizes simultaneously the geometric reprojection error as well as the appearance matching of the parts. We apply the alternating direction method of multipliers (ADMM) to minimize the resulting convex function. We evaluate our approach on the Fine Grained 3D Car dataset with superior performance in shape and pose errors. Our main and novel contribution is the simultaneous solution for part localization, 3D pose and shape by maximizing both geometric and appearance compatibility.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:yuCoZvLJRl8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multiple motion analysis: in spatial or in spectral domain?",
            "Publication year": 2003,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1077314203000110",
            "Abstract": "In this paper, we compare the effects of multiple motions in spatial and spectral representations of an image sequence. We describe multiple motions in both domains and establish a comparison regarding their inherent properties when discretized. Though the spectral model provides us with an explicit description of both occlusion and transparency, it turns out that its resolution is very limited. We show that the spatial domain represented by the spatio-temporal derivatives has superior resolution properties and is thus more appropriate for the treatment of occlusion. We present an algorithm which based on an initial estimate of the number of motions uses the shift-and-subtract technique to localize occlusion boundaries and to track their movement in occlusion sequences. The same technique is used to distinguish occlusion from transparency and to decompose transparency scenes into multi-layers.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:kMrClmKSQGwC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Harvesting multiple views for marker-less 3d human pose annotations",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2017/html/Pavlakos_Harvesting_Multiple_Views_CVPR_2017_paper.html",
            "Abstract": "Recent advances with Convolutional Networks (ConvNets) have shifted the bottleneck for many computer vision tasks to annotated data collection. In this paper, we present a geometry-driven approach to automatically collect annotations for human pose prediction tasks. Starting from a generic ConvNet for 2D human pose, and assuming a multi-view setup, we describe an automatic way to collect accurate 3D human pose annotations. We capitalize on constraints offered by the 3D geometry of the camera setup and the 3D structure of the human body to probabilistically combine per view 2D ConvNet predictions into a globally optimal 3D pose. This 3D pose is used as the basis for harvesting annotations. The benefit of the annotations produced automatically with our approach is demonstrated in two challenging settings:(i) fine-tuning a generic ConvNet-based 2D pose predictor to capture the discriminative aspects of a subject's appearance (ie,\" personalization\"), and (ii) training a ConvNet from scratch for single view 3D human pose prediction without leveraging 3D pose groundtruth. The proposed multi-view pose estimator achieves state-of-the-art results on standard benchmarks, demonstrating the effectiveness of our method in exploiting the available multi-view information.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Id9pRtCSqO0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Trinocular stereo: A real-time algorithm and its evaluation",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/article/10.1023/A:1014525320885",
            "Abstract": "In telepresence applications each user is immersed in a rendered 3D-world composed from representations transmitted from remote sites. The challenge is to compute dense range data at high frame rates, since participants cannot easily communicate if the processing cycle or network latencies are long. Moreover, errors in new stereoscopic views of the remote 3D-world should be hardly perceptible. To achieve the required speed and accuracy, we use trinocular stereo, a matching algorithm based on the sum of modified normalized cross-correlations, and subpixel disparity interpolation. To increase speed we use Intel IPL functions in the pre-processing steps of background subtraction and image rectification as well as a four-processor parallelization. To evaluate our system we have developed a test-bed which provides a set of registered dense \u201cground-truth\u201d laser data and image data from multiple views.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Planar motion of a parabolic catadioptric camera",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1333707/",
            "Abstract": "Camera motion is said to be planar if the direction of translation is perpendicular to the axis of rotation. A parabolic catadioptric camera is a camera realizing the orthogonal projection of rays reflected on a parabolic mirror. We consider the planar motion of a parabolic catadioptric camera, especially the motion restricted to a plane perpendicular to the optical axis, a common case in mobile robots working in urban environments. We begin by deriving the catadioptric fundamental matrix for such a motion and the intrinsic degrees of freedom in this matrix, which turn out to be 8. We show that the camera intrinsics and the 3D motion can be recovered from the fundamental matrix. We derive the necessary and sufficient condition for a fundamental matrix to be induced by a planar motion. Based on the additional constraint for a planar motion, we present an algorithm to compute epipolar geometry and recover the camera \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:70eg2SAEIzsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Exploiting motion priors in visual odometry for vehicle-mounted cameras with non-holonomic constraints",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6095123/",
            "Abstract": "This paper presents a new method to estimate the relative motion of a vehicle from images of a single camera. The biggest problem in visual motion estimation is data association; matched points contain many outliers that must be detected and removed so that the motion can be estimated accurately. A very established method for robust motion estimation in the presence of outliers is the five-point RANSAC algorithm. Five-point RANSAC operates by generating motion hypotheses from randomly-sampled minimal sets of five-point correspondences. These hypotheses are then tested against all data points and the motion hypothesis that after a given number of iterations returns the largest number of inliers is taken as the solution to the problem. A typical drawback of RANSAC is that the number of iterations required to find a suitable solution grows exponentially with the number of outliers, often requiring thousands of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:p2g8aNsByqUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Shape-based object detection via boundary structure segmentation",
            "Publication year": 2012,
            "Publication url": "https://link.springer.com/article/10.1007/s11263-012-0521-z",
            "Abstract": "We address the problem of object detection and segmentation using global holistic properties of object shape. Global shape representations are highly susceptible to clutter inevitably present in realistic images, and thus can be applied robustly only using a precise segmentation of the object. To this end, we propose a figure/ground segmentation method for extraction of image regions that resemble the global properties of a model boundary structure and are perceptually salient. Our shape representation, called the chordiogram, is based on geometric relationships of object boundary edges, while the perceptual saliency cues we use favor coherent regions distinct from the background. We formulate the segmentation problem as an integer quadratic program and use a semidefinite programming relaxation to solve it. The obtained solutions provide a segmentation of the object as well as a detection score \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:2KloaMYe4IUC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Live Demonstration: Unsupervised Event-based Learning of Optical Flow, Depth and Egomotion",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9025423/",
            "Abstract": "We propose a demo of our work, Unsupervised Event-based Learning of Optical Flow, Depth and Egomotion, which will also appear at CVPR 2019. Our demo consists of a CNN which takes as input events from a DAVIS-346b event camera, represented as a discretized event volume, and predicts optical flow for each pixel in the image. Due to the generalization abilities of our network, we are able to predict accurate optical flow for a very wide range of scenes, including for very fast motions and challenging lighting.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Zrzg8MEyHc4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Wine Cellar: An OCR Based Wine Recommendation Application For the Android Mobile OS",
            "Publication year": 2009,
            "Publication url": "https://scholar.google.com/scholar?cluster=3177227245810335552&hl=en&oi=scholarr",
            "Abstract": "Wine Cellar is a search engine application developed for the Android Operating System\u2019s Donut (1.6) build. The program features a search system focused on the collective display of results, rather than individual queries. This allows for a user to analyze all his options at the same time, in order to make a more informed decision. In addition, the application has the ability to perform the fore mentioned queries from an image input from the mobile device, as a more comprehensive and faster input method. On a basic level, Wine Cellar features a standard text-based search engine to perform its collective queries, but the main feature lies within the application\u2019s ability to use the mobile device\u2019s built-in camera to capture an image and use it as an input for the query, by applying an OCR algorithm to the captured image. Existing wine review applications for mobile phones provide functionality ranging from simple lookup of wine brands and types to recommendations to the user and management of wine collections. The offered services, sizes of the databases and sources of information change across developers and mobile operating systems, but all these programs use text entry as the input method for the query. The intent is to provide the user with a more advanced interface to the information related to the beverages offered at a certain location. This is accomplished through the integration of a text-recognition algorithm applied to images from the phone\u2019s camera. Once an image of the wine menu is obtained, the program extracts the text from it and use it as the input for the search. This query will be specialized on the choices at the location and will \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:noJvBAPgP0YC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Joint Estimation of Image Representations and their Lie Invariants",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2012.02903",
            "Abstract": "Images encode both the state of the world and its content. The former is useful for tasks such as planning and control, and the latter for classification. The automatic extraction of this information is challenging because of the high-dimensionality and entangled encoding inherent to the image representation. This article introduces two theoretical approaches aimed at the resolution of these challenges. The approaches allow for the interpolation and extrapolation of images from an image sequence by joint estimation of the image representation and the generators of the sequence dynamics. In the first approach, the image representations are learned using probabilistic PCA \\cite{tipping1999probabilistic}. The linear-Gaussian conditional distributions allow for a closed form analytical description of the latent distributions but assumes the underlying image manifold is a linear subspace. In the second approach, the image representations are learned using probabilistic nonlinear PCA which relieves the linear manifold assumption at the cost of requiring a variational approximation of the latent distributions. In both approaches, the underlying dynamics of the image sequence are modelled explicitly to disentangle them from the image representations. The dynamics themselves are modelled with Lie group structure which enforces the desirable properties of smoothness and composability of inter-image transformations.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:aKos2Y7kUz0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Keyframing the future: Keyframe discovery for visual prediction and planning",
            "Publication year": 2020,
            "Publication url": "http://proceedings.mlr.press/v120/pertsch20a.html",
            "Abstract": "To flexibly and efficiently reason about dynamics of temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them. We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:dONrx3-W1TkC",
            "Publisher": "PMLR"
        },
        {
            "Title": "Visual and haptic collaborative tele-presence",
            "Publication year": 2001,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0097849301001212",
            "Abstract": "The core of a successful sense of presence is a visually, aurally, and haptically compelling experience. In this paper, we introduce the integration of vision and haptics for the purposes of remote collaboration. A remote station acquires a 3D-model of an object of interest which is transmitted to a local station. A user in the local station manipulates a virtual and the remote object as if he/she is haptically and visually at the remote station. This tele-presence feeling is achieved by visually registering the head-mounted display of the local user to the remote world and by dynamically registering the local object both visually and haptically with respect to the remote world. This can be achieved by adequate modeling and feedforward compensation including gravity compensation for the robotic manipulator with which the operator interacts. We present multiple scenarios where such a capability will be useful. One is remote \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Digitizing archaeological excavations from multiple views",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1443231/",
            "Abstract": "We present a novel approach on digitizing large scale unstructured environments like archaeological excavations using off-the-shelf digital still cameras. The cameras are calibrated with respect to few markers captured by a theodolite system. Having all cameras registered in the same coordinate system enables a volumetric approach. Our new algorithm has as input multiple calibrated images and outputs an occupancy voxel space where occupied pixels have a local orientation and a confidence value. Both, orientation and confidence facilitate an efficient rendering and texture mapping of the resulting point cloud. Our algorithm combines the following new features: Images are back-projected to hypothesized local patches in the world and correlated on these patches yielding the best orientation. Adjacent cameras build tuples which yield a product of pair-wise correlations, called strength. Multiple camera tuples \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:YFjsv_pBGBYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Realtime time synchronized event-based stereo",
            "Publication year": 2018,
            "Publication url": "http://openaccess.thecvf.com/content_ECCV_2018/html/Alex_Zhu_Realtime_Time_Synchronized_ECCV_2018_paper.html",
            "Abstract": "In this work, we propose a novel event based stereo method which addresses the problem of motion blur for a moving event camera. Our method uses the velocity of the camera and a range of disparities, to synchronize the positions of the events, as if they were captured at a single point in time. We represent these events using a pair of novel time synchronized event disparity volumes, which we show remove motion blur for pixels at the correct disparity in the volume, while further blurring pixels at the wrong disparity. We then apply a novel matching cost over these time synchronized event disparity volumes, which both rewards similarity between the volumes while penalizing blurriness. We show that our method outperforms more expensive, smoothing based event stereo methods, by evaluating on the Multi Vehicle Stereo Event Camera dataset.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:_E0j-SBEHDwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tactile-visual integration for task-aware grasping",
            "Publication year": 2018,
            "Publication url": "http://renaud-detry.net/publications/Zhang-RSSW-2018.pdf",
            "Abstract": "A second approach is independent of object identity. Given a scene, the grasp detector is simply given the raw camera input and predicts grasp candidates by geometry only [26, 17, 27, 22, 29, 21, 9, 34, 30]. The advantage is that it does not depend on correct identity and pose estimation, eliminating the risk of error propagation. With that, however, comes the disadvantage that the grasp detection is completely unaware of object semantics, and is thus only useful for pick and place tasks such as emptying a basket. A common disadvantage for both approaches is that neither take object functionality into account. In cases of common tool use, such as hammer, pliers, or key, the object must be picked up in a certain orientation in order to execute its functionality. On the other hand, when the task is simply transportation, the object can be picked up in any orientation. 1) Task-driven grasping: This shortcoming has been addressed in several ways. A direct extension to the first approach is to add constraints to the grasp candidates based on the given task [31]. A more direct alternative is to compute grasps by simultaneously taking into account object identity and functionality. To this end, affordance estimation and taskdriven grasping have been studied [20, 10, 28, 1]. More recently, deep learning has enabled grasp detection that takes object identity into account without explicit recognition [18]. 2) Touch-based grasping: So far, all cases above are visionbased grasping. Recently, improvement in tactile sensing brought touch back into the light for perception [25] and grasping [5, 3, 13, 15, 23, 6, 8, 16, 7]. Other than exclusively touch-based grasping, touch is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:VGxY11nYJJYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Decentralized active information acquisition: Theory and application to multi-robot SLAM",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7139863/",
            "Abstract": "This paper addresses the problem of controlling mobile sensing systems to improve the accuracy and efficiency of gathering information autonomously. It applies to scenarios such as environmental monitoring, search and rescue, surveillance and reconnaissance, and simultaneous localization and mapping (SLAM). A multi-sensor active information acquisition problem, capturing the common characteristics of these scenarios, is formulated. The goal is to design sensor control policies which minimize the entropy of the estimation task, conditioned on the future measurements. First, we provide a non-greedy centralized solution, which is computationally fast, since it exploits linearized sensing models, and memory efficient, since it exploits sparsity in the environment model. Next, we decentralize the control task to obtain linear complexity in the number of sensors and provide suboptimality guarantees. Finally, our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:5OQWvpknaCIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Eventgan: Leveraging large scale image datasets for event cameras",
            "Publication year": 2021,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9466265/",
            "Abstract": "Event cameras provide a number of benefits over traditional cameras, such as the ability to track incredibly fast motions, high dynamic range, and low power consumption. However, their application into computer vision problems, many of which are primarily dominated by deep learning solutions, has been limited by the lack of labeled training data for events. In this work, we propose a method which leverages the existing labeled data for images by simulating events from a pair of temporal image frames, using a convolutional neural network. We train this network on pairs of images and events, using an adversarial discriminator loss and a pair of cycle consistency losses. The cycle consistency losses utilize a pair of pre-trained self-supervised networks which perform optical flow estimation and image reconstruction from events, and constrain our network to generate events which result in accurate outputs from both \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:udEBkMWtBUQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Event-based visual inertial odometry",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_Event-Based_Visual_Inertial_CVPR_2017_paper.html",
            "Abstract": "Event-based cameras provide a new visual sensing model by detecting changes in image intensity asynchronously across all pixels on the camera. By providing these events at extremely high rates (up to 1MHz), they allow for sensing in both high speed and high dynamic range situations where traditional cameras may fail. In this paper, we present the first algorithm to fuse a purely event-based tracking algorithm with an inertial measurement unit, to provide accurate metric tracking of a camera's full 6dof pose. Our algorithm is asynchronous, and provides measurement updates at a rate proportional to the camera velocity. The algorithm selects features in the image plane, and tracks spatiotemporal windows around these features within the event stream. An Extended Kalman Filter with a structureless measurement model then fuses the feature tracks with the output of the IMU. The camera poses from the filter are then used to initialize the next step of the tracker and reject failed tracks. We show that our method successfully tracks camera motion on the Event-Camera Dataset in a number of challenging situations.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:0xcHesCNKywC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Articulated motion estimation from a monocular image sequence using spherical tangent bundles",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7487183/",
            "Abstract": "We propose a second order stochastic dynamical model for generic articulated objects whose state space is a Riemannian manifold naturally suggested by the articulation constraints. We derive the equations of a Riemannian Extended Kalman Filter to perform the structure estimation from an image sequence captured by a perspective camera. In order to theoretically validate our approach, we prove that the proposed model is locally weakly observable. Finally, we report quantitative results on both synthetic data and on real sequences from the CMU Mocap dataset.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:DQQjGlBKAuwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Tactile-vision integration for task-compatible fine-part manipulation",
            "Publication year": 2017,
            "Publication url": "http://35.196.119.3/publications/Zhang-RSSW-2017.pdf",
            "Abstract": "We propose to integrate tactile and visual sensing to predict task-compatible grasp regions for manipulation. We address the problem of fine-part assembly, by leveraging vision to observe scene-level information, then touch to perceive fine local details necessary for task completion. We directly target the end goal of task success, by predicting grasp regions that are simultaneously geometrically stable and task-compatible. We represent grasp regions with 2D probabilistic maps, which we first coarsely estimate with vision, and then refine by making contacts with the scene. We show preliminary results of probabilistic grasp regions generated by vision, and demonstrate the impact of tactile sensors for disambiguating task-compatible regions.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:KG521SxztIwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Epipolar geometry of central projection systems using veronese maps",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1640894/",
            "Abstract": "We study the epipolar geometry between views acquired by mixtures of central projection systems including catadioptric sensors and cameras with lens distortion. Since the projection models are in general non-linear, a new representation for the geometry of central images is proposed. This representation is the lifting through Veronese maps of the image plane to the 5D projective space. It is shown that, for most sensor combinations, there is a bilinear form relating the lifted coordinates of corresponding image points. We analyze the properties of the embedding and explicitly construct the lifted fundamental matrices in order to understand their structure. The usefulness of the framework is illustrated by estimating the epipolar geometry between images acquired by a paracatadioptric system and a camera with radial distortion.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:_kc_bZDykSQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Object-centric Video Prediction without Annotation",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2105.02799",
            "Abstract": "In order to interact with the world, agents must be able to predict the results of the world's dynamics. A natural approach to learn about these dynamics is through video prediction, as cameras are ubiquitous and powerful sensors. Direct pixel-to-pixel video prediction is difficult, does not take advantage of known priors, and does not provide an easy interface to utilize the learned dynamics. Object-centric video prediction offers a solution to these problems by taking advantage of the simple prior that the world is made of objects and by providing a more natural interface for control. However, existing object-centric video prediction pipelines require dense object annotations in training video sequences. In this work, we present Object-centric Prediction without Annotation (OPA), an object-centric video prediction method that takes advantage of priors from powerful computer vision models. We validate our method on a dataset comprised of video sequences of stacked objects falling, and demonstrate how to adapt a perception model in an environment through end-to-end video prediction training.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:GwaQhVSQhKEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Single image 3D object detection and pose estimation for grasping",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6907430/",
            "Abstract": "We present a novel approach for detecting objects and estimating their 3D pose in single images of cluttered scenes. Objects are given in terms of 3D models without accompanying texture cues. A deformable parts-based model is trained on clusters of silhouettes of similar poses and produces hypotheses about possible object locations at test time. Objects are simultaneously segmented and verified inside each hypothesis bounding region by selecting the set of superpixels whose collective shape matches the model silhouette. A final iteration on the 6-DOF object pose minimizes the distance between the selected image contours and the actual projection of the 3D model. We demonstrate successful grasps using our detection and pose estimate with a PR2 robot. Extensive evaluation with a novel ground truth dataset shows the considerable benefit of using shape-driven cues for detecting objects in heavily \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:vq7B84E5p90C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Linear pose estimation from points or lines",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-47979-1_19",
            "Abstract": "Estimation of camera pose from an image of n points or lines with known correspondence is a thoroughly studied problem in computer vision. Most solutions are iterative and depend on nonlinear optimization of some geometric constraint, either on the world coordinates or on the projections to the image plane. For real-time applications we are interested in linear or closed-form solutions free of initialization. We present a general framework which allows for a novel set of linear solutions to the pose estimation problem for both n points and n lines. We present a number of simulations which compare our results to two other recent linear algorithm as well as to iterative approaches. We conclude with tests on real imagery in an augmented reality setup. We also present an analysis of the sensitivity of our algorithms to image noise.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:CqsiOOvXZmUC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Local exploration: online algorithms and a probabilistic framework",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1241874/",
            "Abstract": "Mapping an environment with an imaging sensor becomes very challenging if the environment to be mapped is unknown and has to be explored. Exploration involves the planning of views so that the entire environment is covered. The majority of implemented mapping systems use a heuristic planning while theoretical approaches regard only the traveled distance as cost. However, practical range acquisition systems spend a considerable amount of time for acquisition. In this paper, we address the problem of minimizing the cost of looking around a corner, involving the time spent in traveling as well as the time spent for reconstruction. Such a local exploration can be used as a subroutine for global algorithms. We prove competitive ratios for two online algorithms. Then, we provide two representations of local exploration as a Markov Decision Process and apply a known policy iteration algorithm. Simulation results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:TFP_iSt0sucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Bearing-Only Control Laws For Balanced Circular",
            "Publication year": 2008,
            "Publication url": "https://repository.upenn.edu/cgi/viewcontent.cgi?article=1007&context=grasp_papers",
            "Abstract": "For a group of constant-speed ground robots, a simple control law is designed to stabilize the motion of the group into a balanced circular formation using a consensus approach. It is shown that the measurements of the bearing angles between the robots are sufficient for reaching a balanced circular formation. We consider two different scenarios that the connectivity graph of the system is either a complete graph or a ring. Collision avoidance capabilities are added to the team members and the effectiveness of the control laws are demonstrated on a group of mobile robots.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:bnK-pcrLprsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Statistical pose averaging with non-isotropic and incomplete relative measurements",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-10602-1_52",
            "Abstract": "In the last few years there has been a growing interest in optimization methods for averaging pose measurements between a set of cameras or objects (obtained, for instance, using epipolar geometry or pose estimation). Alas, existing approaches do not take into consideration that measurements might have different uncertainties (i.e., the noise might not be isotropically distributed), or that they might be incomplete (e.g., they might be known only up to a rotation around a fixed axis). We propose a Riemannian optimization framework which addresses these cases by using covariance matrices, and test it on synthetic and real data.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:RtDPZMhf-s8C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Omnidirectional vision: Theory and algorithms",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/905282/",
            "Abstract": "Surround perception is crucial for an immersive sense of presence in communication and for efficient navigation and surveillance in robotics. To enable surround perception, new omnidirectional systems were designed which gave a new impetus for rethinking the way images are acquired and analyzed. Based on insights gained from such designs, we formulate a novel unifying theory of imaging. We prove that all single viewpoint mirror-lens devices are equivalent to projective mappings from the sphere to the plane. These mappings are paired with a duality principle which relates points to line projections. The commonly used parabolic mirror projection is shown to be equivalent to the stereographic projection, providing therefore the invariants of a conformal mapping. It turns out that conventional cameras, which are only a special case in our theory, provide the barest minimum of information about the environment \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:dJ-sGqsME_YC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Special issue on panoramic robots",
            "Publication year": 2004,
            "Publication url": "https://experts.umn.edu/en/publications/special-issue-on-panoramic-robots",
            "Abstract": "Special issue on panoramic robots \u2014 Experts@Minnesota Skip to main navigation Skip to \nsearch Skip to main content Experts@Minnesota Logo Home Profiles Research Units University \nAssets Projects and Grants Research Output Press / Media Datasets Activities Fellowships, \nHonors, and Prizes Search by expertise, name or affiliation Special issue on panoramic robots \nKostas Daniilidis, N. Papanikolopoulos Computer Science and Engineering Research output: \nContribution to journal \u203a Editorial \u203a peer-review 1 Scopus citations Overview Fingerprint \nOriginal language English (US) Pages (from-to) 4-5 Number of pages 2 Journal IEEE Robotics \nand Automation Magazine Volume 11 Issue number 4 DOIs https://doi.org/10.1109/MRA.State \nPublished - Dec 1 2004 Access 10.1109/MRA.Link to publication in Scopus Link to citation \nlist in Scopus Cite this APA Standard Harvard Vancouver Author BIBTEX RIS Daniilidis, K.:\u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:XoXfffV-tXoC",
            "Publisher": "Institute of Electrical and Electronics Engineers Inc."
        },
        {
            "Title": "University of pennsylvania magic 2010 final report",
            "Publication year": 2011,
            "Publication url": "https://apps.dtic.mil/sti/citations/ADA535266",
            "Abstract": "In this report, we describe the technical approach and algorithms that have been used by the Univ. of Pennsylvania in the MAGIC 2010 competition. We have constructed and deployed a multivehicle robot team, consisting of intelligent sensor and disrupter UGVs, that can survey, map, recognize, and respond to threats in a dynamic urban environment with minimal human guidance. The custom hardware systems consist of robust and complementary sensors, integrated electronics, computation, and highly capable propulsion and actuation. The mapping, navigation, and planning software is organized hierarchically, allowing autonomous decisions to be made by the robots while enabling human operators to interact with the robot team in an efficient and strategic manner. The ground control station interfaces integrate information coming from the robots as well as metadata feeds to focus the operator attention and rapidly respond to emerging threats. These systems were developed and tested by the team to complete two phases of the MAGIC 2010 challenge in a safe and timely manner.Descriptors:",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:WbkHhVStYXYC",
            "Publisher": "MOORE SCHOOL OF ELECTRICAL ENGINEERING PHILADELPHIA PA GRASP LAB"
        },
        {
            "Title": "Autonomous precision pouring from unknown containers",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8653969/",
            "Abstract": "We autonomously pour from unknown symmetric containers found in a typical wet laboratory for the development of a robot-assisted, rapid experiment preparation system. The robot estimates the pouring container symmetric geometry, then leverages simulated pours as priors for a given fluid to pour precisely and quickly in a single attempt. The fluid is detected in the transparent receiving container by combining weight and vision. The change of volume in the receiver is a function of the geometry of the pouring container, the pouring angle, and rate. To determine the volumetric flow rate, the profile for maximum containable volume for a given angle is estimated along with the time delay of the fluid exiting the container. A trapezoidal trajectory generation algorithm prescribes the desired volumetric flow rate as a function of the estimation accuracy. A hybrid control strategy is then used to attenuate volumetric error \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:rywEMSoAiS0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Motion Equivariant Networks for Event Cameras with the Temporal Normalization Transform",
            "Publication year": 2019,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2019arXiv190206820Z/abstract",
            "Abstract": "In this work, we propose a novel transformation for events from an event camera that is equivariant to optical flow under convolutions in the 3-D spatiotemporal domain. Events are generated by changes in the image, which are typically due to motion, either of the camera or the scene. As a result, different motions result in a different set of events. For learning based tasks based on a static scene such as classification which directly use the events, we must either rely on the learning method to learn the underlying object distinct from the motion, or to memorize all possible motions for each object with extensive data augmentation. Instead, we propose a novel transformation of the input event data which normalizes the  and  positions by the timestamp of each event. We show that this transformation generates a representation of the events that is equivariant to this motion when the optical flow is constant, allowing a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:IA09g522ZoUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Planar ego-motion without correspondences",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4129600/",
            "Abstract": "General structure-from-motion methods are not adept at dealing with constrained camera motions, even though such motions greatly simplify vision tasks like mobile robot localization. Typical ego-motion techniques designed for such a purpose require locating feature correspondences between images. However, there are many cases where features cannot be matched robustly. For example, images from panoramic sensors are limited by nonuniform angular sampling, which can complicate the feature matching process under wide baseline motions. In this paper we compute the planar ego-motion of a spherical sensor without correspondences. We propose a generalized Hough transform on the space of planar motions. Our transform directly processes the information contained within all the possible feature pair combinations between two images, thereby circumventing the need to isolate the best corresponding \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:hMod-77fHWUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multi-Image Matching via Fast Alternating Minimization",
            "Publication year": 2015,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhou_Multi-Image_Matching_via_ICCV_2015_paper.html",
            "Abstract": "In this paper we propose a global optimization-based approach to jointly matching a set of images. The estimated correspondences simultaneously maximize pairwise feature affinities and cycle consistency across multiple images. Unlike previous convex methods relying on semidefinite programming, we formulate the problem as a low-rank matrix recovery problem and show that the desired semidefiniteness of a solution can be spontaneously fulfilled. The low-rank formulation enables us to derive a fast alternating minimization algorithm in order to handle practical problems with thousands of features. Both simulation and real experiments demonstrate that the proposed algorithm can achieve a competitive performance with an order of magnitude speedup compared to the state-of-the-art algorithm. In the end, we demonstrate the applicability of the proposed method to match the images of different object instances and as a result the potential to reconstruct category-specific object models from those images.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:q7hqJx8pYzEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Robot localization using soft object detection",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6225216/",
            "Abstract": "In this paper, we give a new double twist to the robot localization problem. We solve the problem for the case of prior maps which are semantically annotated perhaps even sketched by hand. Data association is achieved not through the detection of visual features but the detection of object classes used in the annotation of the prior maps. To avoid the caveats of general object recognition, we propose a new representation of the query images that consists of a vector of the detection scores for each object class. Given such soft object detections we are able to create hypotheses about pose and to refine them through particle filtering. As opposed to small confined office and kitchen spaces, our experiment takes place in a large open urban rail station with multiple semantically ambiguous places. The success of our approach shows that our new representation is a robust way to exploit the plethora of existing prior maps \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:-_dYPAW6P2MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Monocular visual odometry in urban environments using an omnidirectional camera",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4651205/",
            "Abstract": "We present a system for monocular simultaneous localization and mapping (mono-SLAM) relying solely on video input. Our algorithm makes it possible to precisely estimate the camera trajectory without relying on any motion model. The estimation is completely incremental: at a given time frame, only the current location is estimated while the previous camera positions are never modified. In particular, we do not perform any simultaneous iterative optimization of the camera positions and estimated 3D structure (local bundle adjustment). The key aspect of the system is a fast and simple pose estimation algorithm that uses information not only from the estimated 3D map, but also from the epipolar constraint. We show that the latter leads to a much more stable estimation of the camera trajectory than the conventional approach. We perform high precision camera trajectory estimation in urban scenes with a large amount \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:eQOLeE2rZwMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Reactive navigation in partially familiar planar environments using semantic perceptual feedback",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2002.08946",
            "Abstract": "This paper solves the planar navigation problem by recourse to an online reactive scheme that exploits recent advances in SLAM and visual object recognition to recast prior geometric knowledge in terms of an offline catalogue of familiar objects. The resulting vector field planner guarantees convergence to an arbitrarily specified goal, avoiding collisions along the way with fixed but arbitrarily placed instances from the catalogue as well as completely unknown fixed obstacles so long as they are strongly convex and well separated. We illustrate the generic robustness properties of such deterministic reactive planners as well as the relatively modest computational cost of this algorithm by supplementing an extensive numerical study with physical implementation on both a wheeled and legged platform in different settings.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:27qsyVibG6YC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Surface representations using spherical harmonics and gabor wavelets on the sphere",
            "Publication year": 2001,
            "Publication url": "https://repository.upenn.edu/cgi/viewcontent.cgi?article=1094&context=cis_reports",
            "Abstract": "In this paper we present a new scheme for the representation of object surfaces. The purpose is to model a surface efficiently in a coarse to fine hierarchy. Our scheme is based on the combination of spherical harmonic functions and wavelet networks on the sphere. The coefficients can be estimated from scattered data sampled from a star-shaped object\u2019s surface. Spherical harmonic functions are used to model the coarse structure of the surface, while spherical Gabor wavelets are used for the representation of fine scale detail. Theoretical background on wavelets on the sphere is provided as well as a discussion of implementation issues concerning convolutions on the sphere. Results are presented which show the efficiency of the proposed representation.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:-f6ydRqryjwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Active tactile object recognition by Monte Carlo tree search",
            "Publication year": 2017,
            "Publication url": "https://128.84.4.15/abs/1703.00095v1",
            "Abstract": "This paper considers the problem of object recognition using only tactile information. The focus is on designing a sequence of robot hand grasps that achieves accurate recognition after few enclosure contacts. It seeks to maximize the recognition probability and minimize the number of touches required. The actions are formulated as hand poses relative to each other, making the algorithm independent of small object movements and absolute workspace coordinates. The optimal sequence of actions is approximated by Monte Carlo tree search. We demonstrate active tactile recognition results in physics simulation and on a real robot. In simulation, most object instances were recognized within 16 moves. On a real robot, our method correctly recognized objects in 2--9 grasps and outperformed a greedy baseline.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:siTy-4AL0AwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Nonmyopic View Planning for Active Object Classification and Pose Estimation",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6822578/",
            "Abstract": "One of the central problems in computer vision is the detection of semantically important objects and the estimation of their pose. Most of the work in object detection has been based on single image processing, and its performance is limited by occlusions and ambiguity in appearance and geometry. This paper proposes an active approach to object detection in which the point of view of a mobile depth camera is controlled. When an initial static detection phase identifies an object of interest, several hypotheses are made about its class and orientation. Then, a sequence of views, which balances the amount of energy used to move the sensor with the chance of identifying the correct hypothesis, is planned. We formulate an active hypothesis testing problem, which includes sensor mobility, and solve it using a point-based approximate partially observable Markov decision process algorithm. The validity of our \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:aAWV-AKBBEQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Towards statistically provable geometric 3d human pose recovery",
            "Publication year": 2021,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/19M1299955",
            "Abstract": "Recovering three-dimensional (3D) structures such as object poses from limited two-dimensional (2D) information is an important research problem in computer vision, graphics, and robotics. The estimation of object pose from single images or multiple casual images could be ill-conditioned math problems. There is a popular family of algorithms of geometric sparse representation for 3D pose recovery (GSR-3D) that  pretrains an overcomplete dictionary of 3D basis poses , and then matches the detected 2D object pose  by jointly estimating the transformation , projection  and combination coefficients , assuming . In this paper, we make the first step of analyzing to which extent could we solve this ill-conditioned problem, and of understanding how the recovery error is affected by fundamental factors, e.g., dictionary size, observation noise, and running time.  As these factors are implicit in objective functions, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:M6kHaddf_34C",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Multi-image semantic matching by mining consistent features",
            "Publication year": 2018,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Multi-Image_Semantic_Matching_CVPR_2018_paper.html",
            "Abstract": "This work proposes a multi-image matching method to estimate semantic correspondences across multiple images. In contrast to the previous methods that optimize all pairwise correspondences, the proposed method identifies and matches only a sparse set of reliable features in the image collection. In this way, the proposed method is able to prune nonrepeatable features and also highly scalable to handle thousands of images. We additionally propose a low-rank constraint to ensure the geometric consistency of feature correspondences over the whole image collection. Besides the competitive performance on multi-graph matching and semantic flow benchmarks, we also demonstrate the applicability of the proposed method for reconstructing object-class models and discovering object-class landmarks from images without using any annotation.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:LimhlhUO2s4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Robustness Meets Deep Learning: An End-to-End Hybrid Pipeline for Unsupervised Learning of Egomotion",
            "Publication year": 2018,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2018arXiv181208351Z/abstract",
            "Abstract": "In this work, we propose a method that combines unsupervised deep learning predictions for optical flow and monocular disparity with a model based optimization procedure for instantaneous camera pose. Given the flow and disparity predictions from the network, we apply a RANSAC outlier rejection scheme to find an inlier set of flows and disparities, which we use to solve for the relative camera pose in a least squares fashion. We show that this pipeline is fully differentiable, allowing us to combine the pose with the network outputs as an additional unsupervised training loss to further refine the predicted flows and disparities. This method not only allows us to directly regress relative pose from the network outputs, but also automatically segments away pixels that do not fit the rigid scene assumptions that many unsupervised structure from motion methods apply, such as on independently moving objects. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:K8XpiWYAYk8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Vision-based control laws for distributed flocking of nonholonomic agents",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1642120/",
            "Abstract": "We study the problem of vision-based flocking and coordination of a group of kinematic agents in 2 and 3 dimensions. It is shown that in the absence of communication among agents, and by using only visual information, a group of mobile agents can align their velocity vectors and move in a formation. A coordinate-free control law is used to develop a vision-based input for each nonholonomic agent. The vision-based input does not rely on heading measurements, but only requires measurements of bearing, optical flow and time-to-collision, all of which can be efficiently measured",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:uXirmJe02n4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Model-Based Reinforcement Learning via Latent-Space Collocation",
            "Publication year": 2020,
            "Publication url": "https://openreview.net/forum?id=ku4sJKvnbwV",
            "Abstract": "The ability to construct and execute long-term plans enables intelligent agents to solve complex multi-step tasks and prevents myopic behavior only seeking the short-term reward. Recent work has achieved significant progress on building agents that can predict and plan from raw visual observations. However, existing visual planning methods still require a densely shaped reward that provides the algorithm with a short-term signal that is always easy to optimize. These algorithms fail when the shaped reward is not available as they use simplistic planning methods such as sampling-based random shooting and are unable to plan for a distant goal. Instead, to achieve long-horizon visual control, we propose to use collocation-based planning, a powerful optimal control technique that plans forward a sequence of states while constraining the transitions to be physical. We propose a planning algorithm that adapts collocation to visual planning by leveraging probabilistic latent variable models. A model-based reinforcement learning agent equipped with our planning algorithm significantly outperforms prior model-based agents on challenging visual control tasks with sparse rewards and long-term goals.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:T-Bu49-jKQgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Technical report on optimization-based bearing-only visual homing with applications to a 2-d unicycle model",
            "Publication year": 2014,
            "Publication url": "https://arxiv.org/abs/1402.3584",
            "Abstract": "We consider the problem of bearing-based visual homing: Given a mobile robot which can measure bearing directions with respect to known landmarks, the goal is to guide the robot toward a desired \"home\" location. We propose a control law based on the gradient field of a Lyapunov function, and give sufficient conditions for global convergence. We show that the well-known Average Landmark Vector method (for which no convergence proof was known) can be obtained as a particular case of our framework. We then derive a sliding mode control law for a unicycle model which follows this gradient field. Both controllers do not depend on range information. Finally, we also show how our framework can be used to characterize the sensitivity of a home location with respect to noise in the specified bearings. This is an extended version of the conference paper [1].",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:ui-gComCE0IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automatic mapping of store layout using soft object recognition",
            "Publication year": 2021,
            "Publication url": "https://patents.google.com/patent/US10991036B1/en",
            "Abstract": "A method for automatically mapping a store layout includes identifying a path for traversing a retail area and capturing images of the retail area at various points along the path. The images may be analyzed to identify visual characteristics which may be compared to a template of retail products in a template library. When an object depicted in the image matches with a retail product in the template library, the object may correspond to the retail product. Additionally, a retail department for the object may also be identified. The retail department may be compared to the retail product corresponding to the object, and when the retail product is not associated with the retail department, another retail product which is associated with the retail department may be identified as corresponding to the object. A map of the store layout may be generated based on the identified retail products.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:f408LfHov48C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Three dimensional orientation signatures with conic kernel filtering for multiple motion analysis",
            "Publication year": 2003,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S026288560300012X",
            "Abstract": "We propose a new 3D kernel for the recovery of 3D orientation signatures. In the Cartesian coordinates, the kernel has a shape of a truncated cone with its axis in the radial direction and very small angular support. In the local spherical coordinates, the angular part of the kernel is a 2D Gaussian function. A set of such kernels is obtained by uniformly sampling the 2D space of azimuth and elevation angles. The projection of a local neighborhood on such a kernel set produces a local 3D orientation signature. In case of spatio-temporal analysis, such a kernel set can be applied either on the derivative space of a local neighborhood or on the local Fourier transform. The well known planes arising from one or multiple motions produce maxima in the orientation signature. The kernel's local support enables the resulting spatio-temporal signatures to possess higher orientation resolution than 3D steerable filters \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:_TIfIljC7OAC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Correspondence-free structure from motion",
            "Publication year": 2007,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s11263-007-0035-2.pdf",
            "Abstract": "We present a novel approach for the estimation of 3D-motion directly from two images using the Radon transform. The feasibility of any camera motion is computed by integrating over all feature pairs that satisfy the epipolar constraint. This integration is equivalent to taking the inner product of a similarity function on feature pairs with a Dirac function embedding the epipolar constraint. The maxima in this five dimensional motion space will correspond to compatible rigid motions. The main novelty is in the realization that the Radon transform is a filtering operator: If we assume that the similarity and Dirac functions are defined on spheres and the epipolar constraint is a group action of rotations on spheres, then the Radon transform is a correlation integral. We propose a new algorithm to compute this integral from the spherical Fourier transform of the similarity and Dirac functions. Generating the similarity \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:HDshCWvjkbEC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Oriented structure of the occlusion distortion: Is it reliable?",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1033220/",
            "Abstract": "In the energy spectrum of an occlusion sequence, the distortion term has the same orientation as the velocity of the occluding signal. Other works claimed that this oriented structure can be used to distinguish the occluding velocity from the occluded one. We argue that the orientation structure of the distortion cannot always work as a reliable feature due to the rapidly decreasing energy contribution. This already weak orientation structure is further blurred by a superposition of distinct distortion components. We also indicate that the superposition principle of Shizawa and Mase (1991) for multiple motion estimation needs to be adjusted.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:N5tVd3kTz84C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Shape-based Object Detection via Boundary Structure",
            "Publication year": 2011,
            "Publication url": "https://repository.upenn.edu/cis_papers/544/",
            "Abstract": "We address the problem of object detection and segmentation using global holistic properties of object shape. Global shape representations are highly susceptible to clutter inevitably present in realistic images, and can be applied robustly only using a precise segmentation of the object. To this end, we propose a figure/ground segmentation method for extraction of image regions that resemble the global properties of a model boundary structure and are perceptually salient. Our shape representation, called the chordiogram, is based on geometric relationships of object boundary edges, while the perceptual saliency cues we use favor coherent regions distinct from the background. We formulate the segmentation problem as an integer quadratic program and use a semdefinite programming relaxation to solve it. Obtained solutions provide the segmentation of an object as well as a detection score used for object recognition. Our single-step approach achieves state-of-the-art performance on several object detection and segmentation benchmarks.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:M7yex6snE4oC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Event-based feature tracking with probabilistic data association",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7989517/",
            "Abstract": "Asynchronous event-based sensors present new challenges in basic robot vision problems like feature tracking. The few existing approaches rely on grouping events into models and computing optical flow after assigning future events to those models. Such a hard commitment in data association attenuates the optical flow quality and causes shorter flow tracks. In this paper, we introduce a novel soft data association modeled with probabilities. The association probabilities are computed in an intertwined EM scheme with the optical flow computation that maximizes the expectation (marginalization) over all associations. In addition, to enable longer tracks we compute the affine deformation with respect to the initial point and use the resulting residual as a measure of persistence. The computed optical flow enables a varying temporal integration different for every feature and sized inversely proportional to the length of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:k4O5U3vRA4YC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Bearing-only formation control with auxiliary distance measurements, leaders, and collision avoidance",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7798527/",
            "Abstract": "We address the controller synthesis problem for distributed formation control. Our solution requires only relative bearing measurements (as opposed to full translations), and is based on the exact gradient of a Lyapunov function with only global minimizers (independently from the formation topology). These properties allow a simple proof of global asymptotic convergence, and extensions for including distance measurements, leaders and collision avoidance. We validate our approach through simulations and comparison with other stateof-the-art algorithms.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:RXC-vbXDMdwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Image registration using mutual information",
            "Publication year": 2000,
            "Publication url": "https://repository.upenn.edu/cgi/viewcontent.cgi?article=1119&context=cis_reports",
            "Abstract": "Almost all imaging systems require some form of registration. A few examples are aligning medical images for diagnosis, matching stereo images to recover shape, and comparing facial images in a database to recognize people. Given the difficulty of registering images taken at different times, using different sensors, from different positions, registration algorithms come in different shapes and sizes.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:qUcmZB5y_30C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Autonomous vehicle for mapping store layout",
            "Publication year": 2019,
            "Publication url": "https://patents.google.com/patent/US10318907B1/en",
            "Abstract": "A method for automatically mapping a store layout includes identifying a path for traversing a retail area and capturing images of the retail area at various points along the path. The images may be analyzed to identify visual characteristics which may be compared to a template of retail products in a template library. When an object depicted in the image matches with a retail product in the template library, the object may correspond to the retail product. Additionally, a retail department for the object may also be identified. The retail department may be compared to the retail product corresponding to the object, and when the retail product is not associated with the retail department, another retail product which is associated with the retail department may be identified as corresponding to the object. A map of the store layout may be generated based on the identified retail products.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:HklM7qHXWrUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Visual Planning with Semi-Supervised Stochastic Action Representations",
            "Publication year": 2019,
            "Publication url": "https://openreview.net/pdf?id=B1gAxoTLwH",
            "Abstract": "Planning with learned visual predictive models allows solving robotic problems where ground truth dynamics are intractable or do not have analytic models. In visual planning, an agent constructs a visual plan of how to solve a task, and then executes this plan with a corresponding sequence of motor actions. However, to construct a visual plan, understanding how the agent can influence the environment is necessary. We propose a method to learn a representation of possible effects of an agents\u2019s actions on the environment that can be predicted either from the motor action or from past and future sensory input. In contrast to prior work, which only used motor input to train forward models, we can train our representation using richer sensory input, and even on data for which motor input is not available.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:lX_RDcPAamoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "3D scanning using spatiotemporal orientation",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1333991/",
            "Abstract": "We present a new approach to volumetric scene reconstruction which can produce accurate models from turntable image sequences. Instead of an epipolar plane image (EPI) volume, we consider a function on the 4D spatiotemporal volume valued with the intensity back projection of the camera (time) to the particular voxel. Using an optical flow technique we compute the local orientation of this spatiotemporal image and decide on occupancy based on the relative orientation between the viewing ray of the voxel at the particular time and the local image structure. Our method does not require a background compensation like the silhouette-based methods and is comparable in performance with space carving.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:cFHS6HbyZ2cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sampling based sensor-network deployment",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1389654/",
            "Abstract": "In this paper, we consider the problem of placing networked sensors in a way that guarantees coverage and connectivity. We focus on sampling based deployment and present algorithms that guarantee coverage and connectivity with a small number of sensors. We consider two different scenarios based on the flexibility of deployment. If deployment has to be accomplished in one step, like airborne deployment, then the main question becomes how many sensors are needed. If deployment can be implemented in multiple steps, then awareness of coverage and connectivity can be updated. For this case, we present incremental deployment algorithms, which consider the current placement to adjust the sampling domain. The algorithms are simple, easy to implement, and require a small number of sensors. We believe the concepts and algorithms presented in this paper provide a unifying framework for existing and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Semantic Localization Via the Matrix Permanent.",
            "Publication year": 2014,
            "Publication url": "http://roboticsproceedings.org/rss10/p43.pdf",
            "Abstract": "Most approaches to robot localization rely on lowlevel geometric features such as points, lines, and planes. In this paper, we use object recognition to obtain semantic information from the robot\u2019s sensors and consider the task of localizing the robot within a prior map of landmarks, which are annotated with semantic labels. As object recognition algorithms miss detections and produce false alarms, correct data association between the detections and the landmarks on the map is central to the semantic localization problem. Instead of the traditional vectorbased representations, we use random finite sets to represent the object detections. This allows us to explicitly incorporate missed detections, false alarms, and data association in the sensor model. Our second contribution is to reduce the problem of computing the likelihood of a set-valued observation to the problem of computing a matrix permanent. It is this crucial transformation that enables us to solve the semantic localization problem with a polynomial-time approximation to the set-based Bayes filter. The performance of our approach is demonstrated in simulation and in a real environment using a deformable-part-model-based object detector. Comparisons are made with the traditional lidarbased geometric Monte-Carlo localization.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:1tvASLRm6poC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Probabilistic Modeling for Human Mesh Recovery",
            "Publication year": 2021,
            "Publication url": "http://openaccess.thecvf.com/content/ICCV2021/html/Kolotouros_Probabilistic_Modeling_for_Human_Mesh_Recovery_ICCV_2021_paper.html",
            "Abstract": "This paper focuses on the problem of 3D human reconstruction from 2D evidence. Although this is an inherently ambiguous problem, the majority of recent works avoid the uncertainty modeling and typically regress a single estimate for a given input. In contrast to that, in this work, we propose to embrace the reconstruction ambiguity and we recast the problem as learning a mapping from the input to a distribution of plausible 3D poses. Our approach is based on the normalizing flows model and offers a series of advantages. For conventional applications, where a single 3D estimate is required, our formulation allows for efficient mode computation. Using the mode leads to performance that is comparable with the state of the art among deterministic unimodal regression models. Simultaneously, since we have access to the likelihood of each sample, we demonstrate that our model is useful in a series of downstream tasks, where we leverage the probabilistic nature of the prediction as a tool for more accurate estimation. These tasks include reconstruction from multiple uncalibrated views, as well as human model fitting, where our model acts as a powerful image-based prior for mesh recovery. Our results validate the importance of probabilistic modeling, and indicate state-of-the-art performance across a variety of settings. Code and models are available at: https://www. seas. upenn. edu/nkolot/projects/prohmr.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:iRaVAuoZnuIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "3D bird reconstruction: a dataset, model, and shape recovery from a single view",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/978-3-030-58523-5_1.pdf",
            "Abstract": "Automated capture of animal pose is transforming how we study neuroscience and social behavior. Movements carry important social cues, but current methods are not able to robustly estimate pose and shape of animals, particularly for social animals such as birds, which are often occluded by each other and objects in the environment. To address this problem, we first introduce a model and multi-view optimization approach, which we use to capture the unique shape and pose space displayed by live birds. We then introduce a pipeline and experiments for keypoint, mask, pose, and shape regression that recovers accurate avian postures from single views. Finally, we provide extensive multi-view keypoint and mask annotations collected from a group of 15 social birds housed together in an outdoor aviary. The project website with videos, results, code, mesh model, and the Penn Aviary Dataset can be found at \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:VTkKiNFP83YC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "Geometric polynomial constraints in higher-order graph matching",
            "Publication year": 2014,
            "Publication url": "https://arxiv.org/abs/1405.6261",
            "Abstract": "Correspondence is a ubiquitous problem in computer vision and graph matching has been a natural way to formalize correspondence as an optimization problem. Recently, graph matching solvers have included higher-order terms representing affinities beyond the unary and pairwise level. Such higher-order terms have a particular appeal for geometric constraints that include three or more correspondences like the PnP 2D-3D pose problems. In this paper, we address the problem of finding correspondences in the absence of unary or pairwise constraints as it emerges in problems where unary appearance similarity like SIFT matches is not available. Current higher order matching approaches have targeted problems where higher order affinity can simply be formulated as a difference of invariances such as lengths, angles, or cross-ratios. In this paper, we present a method of how to apply geometric constraints modeled as polynomial equation systems. As opposed to RANSAC where such systems have to be solved and then tested for inlier hypotheses, our constraints are derived as a single affinity weight based on  hypothesized correspondences without solving the polynomial system. Since the result is directly a correspondence without a transformation model, our approach supports correspondence matching in the presence of multiple geometric transforms like articulated motions.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:H7WDvlwkmv8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Simple and Effective VAE Training with Calibrated Decoders",
            "Publication year": 2020,
            "Publication url": "https://openreview.net/forum?id=nkap3LV7t7O",
            "Abstract": "Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:kF4WlwDc9qcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using the algebra of dual quaternions for motion alignment",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-662-04621-0_20",
            "Abstract": "Whenever measurements have to be taken with respect to two different coordinate frames the problem arises how to relate these measurements to each other. When these measurements are rigid 3D-displacements we obtain descriptions of motions with respect to two different coordinate systems. These systems might for example be the motor coordinate system of a vehicle and the coordinate system of a sensor mounted on the vehicle. If we use conventional homogeneous coordinates notation we usually obtain the well known equation AX = XB where all the variables are 4x4 matrices representing rigid motions. On the other hand, we might have line measurements with respect to two coordinate frames in which case we usually have a problem of the form P = QX where P, Qare matrices containing the Pliicker coordinates of the lines and Xa matrix encoding the rigid motion which we will describe later. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:JV2RwH3_ST0C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Object detection from large-scale 3d datasets using bottom-up and top-down descriptors",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-88693-8_41",
            "Abstract": "We propose an approach for detecting objects in large-scale range datasets that combines bottom-up and top-down processes. In the bottom-up stage, fast-to-compute local descriptors are used to detect potential target objects. The object hypotheses are verified after alignment in a top-down stage using global descriptors that capture larger scale structure information. We have found that the combination of spin images and Extended Gaussian Images, as local and global descriptors respectively, provides a good trade-off between efficiency and accuracy. We present results on real outdoors scenes containing millions of scanned points and hundreds of targets. Our results compare favorably to the state of the art by being applicable to much larger scenes captured under less controlled conditions, by being able to detect object classes and not specific instances, and by being able to align the query with the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:M3NEmzRMIkIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Limiting the search range of correlation stereo using silhouettes",
            "Publication year": 2002,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.5157&rep=rep1&type=pdf",
            "Abstract": "We present a new approach to combine two approaches to three-dimensional reconstruction: silhouette-based and correspondence-based approaches. The two approaches have complementary costs and benefits. Silhouette-based approaches deliver volumetric descriptions which often have very few outliers, but they cannot reconstruct concave surfaces. Correspondence-based approaches give surface descriptions with sub-pixel accuracy, but their search range either allows outliers or falls short of the correct match. We show that a combination of the two can deliver fine-grained accuracy with few outliers. Our specific implementation uses the silhouette reconstruction as prior data to center and bound a stereo search process. We explore the different performance characteristics of the combination and its two component methods qualitatively and quantitatively using real imagery.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:yD5IFk8b50cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Human motion capture using a drone",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8462830/",
            "Abstract": "Current motion capture (MoCap) systems generally require markers and multiple calibrated cameras, which can be used only in constrained environments. In this work we introduce a drone-based system for 3D human MoCap. The system only needs an autonomously flying drone with an on-board RGB camera and is usable in various indoor and outdoor environments. A reconstruction algorithm is developed to recover full-body motion from the video recorded by the drone. We argue that, besides the capability of tracking a moving subject, a flying drone also provides fast varying viewpoints, which is beneficial for motion reconstruction. We evaluate the accuracy of the proposed system using our new DroCap dataset and also demonstrate its applicability for MoCap in the wild using a consumer drone.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:ToZsFq5dof0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Shape-based Object Detection via Boundary Structure",
            "Publication year": 2011,
            "Publication url": "https://repository.upenn.edu/hms/132/",
            "Abstract": "We address the problem of object detection and segmentation using global holistic properties of object shape. Global shape representations are highly susceptible to clutter inevitably present in realistic images, and can be applied robustly only using a precise segmentation of the object. To this end, we propose a figure/ground segmentation method for extraction of image regions that resemble the global properties of a model boundary structure and are perceptually salient. Our shape representation, called the chordiogram, is based on geometric relationships of object boundary edges, while the perceptual saliency cues we use favor coherent regions distinct from the background. We formulate the segmentation problem as an integer quadratic program and use a semidefinite programming relaxation to solve it. Obtained solutions provide the segmentation of an object as well as a detection score used for object recognition. Our single-step approach achieves state-of-the-art performance on several object detection and segmentation benchmarks.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:vDijr-p_gm4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "3D shape estimation from 2D landmarks: A convex relaxation approach",
            "Publication year": 2015,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2015/html/Zhou_3D_Shape_Estimation_2015_CVPR_paper.html",
            "Abstract": "We investigate the problem of estimating the 3D shape of an object, given a set of 2D landmarks in a single image. To alleviate the reconstruction ambiguity, a widely-used approach is to confine the unknown 3D shape within a shape space built upon existing shapes. While this approach has proven to be successful in various applications, a challenging issue remains, ie, the joint estimation of shape parameters and camera-pose parameters requires to solve a nonconvex optimization problem. The existing methods often adopt an alternating minimization scheme to locally update the parameters, and consequently the solution is sensitive to initialization. In this paper, we propose a convex formulation to address this problem and develop an efficient algorithm to solve the proposed convex program. We demonstrate the exact recovery property of the proposed method, its merits compared to alternative methods, and the applicability in human pose and car shape estimation.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:uK1dVpBkok0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "The University of Pennsylvania MAGIC 2010 multi\u2010robot unmanned vehicle system",
            "Publication year": 2012,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21437",
            "Abstract": "In this report, we describe the technical approach and algorithms that have been used by the University of Pennsylvania in the MAGIC 2010 competition. We have constructed and deployed a multi\u2010vehicle robot team, consisting of intelligent sensor and disrupter unmanned ground vehicles that can survey, map, recognize, and respond to threats in a dynamic urban environment with minimal human guidance. The custom hardware systems consist of robust and complementary sensors, integrated electronics, computation, and highly capable propulsion and actuation. The mapping, navigation, and planning software is organized hierarchically, allowing autonomous decisions to be made by the robots while enabling human operators to interact with the robot team in an efficient and strategic manner. The ground control station integrates information coming from the robots as well as metadata feeds to focus the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:URolC5Kub84C",
            "Publisher": "Wiley Subscription Services, Inc., A Wiley Company"
        },
        {
            "Title": "Volumetric multi-camera scene acquisition with partially metric calibration for wide-area tele-immersion",
            "Publication year": 2003,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.4113&rep=rep1&type=pdf",
            "Abstract": "Tele-immersion is a new medium that tries to create the illusion of virtual collocation among physically distant places. To create this sense of copresence, tele-immersion has to be visually compelling and run in real-time, putting, thus, high performance constraints in all three areas of computer vision, graphics, and networking. We describe our newest results in the scene acquisition component of tele-immersion: algorithms for volumetric scene reconstruction from multiple cameras and for calibration of camera clusters. The reconstruction algorithm is volumetric, makes no assumption on camera loci and is based on the detection of local correlation maxima in 3D. It outputs an occupancy voxel grid, with occupied voxels being accompanied by a surface normal; a fact that improves reconstruction quality. The calibration of cameras distributed in a wide area is a challenging task because it is impossible to use reference objects visible to all cameras and because wide field-of-view cameras suffer under radial distortion. Our calibration method uses a reference object to calibrate a minimum of two cameras in order to provide a euclidean world coordinate system. Then, we deliberately move an LED in front of all cameras to obtain correspondances between views. The projection matrices and radial distortion parameters of all cameras are computed using two-step factorization. The combination of the two algorithms alleviates \u201cmisregistration\u201d[8] artifacts, encountered when concatenating reconstructions/calibrations, obtainbed from independent stereos.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:u_35RYKgDlwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multispectral omnidirectional optical sensor and methods therefor",
            "Publication year": 2006,
            "Publication url": "https://patents.google.com/patent/US6982743B2/en",
            "Abstract": "Provided is a multispectral and omnidirectional imaging sensor and system that produces improved surround awareness, as well as methods of use therefor. The multispectral omnidirectional optical sensor device comprises a series of view and reflecting mirrors for splitting the electromagnetic spectrum into two or more electromagnetic wavelength bands; in combination with a plurality of cameras that are spaced relative to the reflecting mirrors. Each camera senses one of the wavelength bands or the resulting signal passing through the mirrors and creates an image thereof (focussed on the same view), so that each image is automatically registered.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:K3LRdlH-MEoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Model-based reinforcement learning via latent-space collocation",
            "Publication year": 2021,
            "Publication url": "https://proceedings.mlr.press/v139/rybkin21b.html",
            "Abstract": "The ability to plan into the future while utilizing only raw high-dimensional observations, such as images, can provide autonomous agents with broad and general capabilities. However, realistic tasks require performing temporally extended reasoning, and cannot be solved with only myopic, short-sighted planning. Recent work in model-based reinforcement learning (RL) has shown impressive results on tasks that require only short-horizon reasoning. In this work, we study how the long-horizon planning abilities can be improved with an algorithm that optimizes over sequences of states, rather than actions, which allows better credit assignment. To achieve this, we draw on the idea of collocation and adapt it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. Our latent collocation method (LatCo) provides a general and effective visual planning approach, and significantly outperforms prior model-based approaches on challenging visual control tasks with sparse rewards and long-term goals. See the videos on the supplementary website\\url {https://sites. google. com/view/latco-mbrl/.}",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:IMJZBBnUFLgC",
            "Publisher": "PMLR"
        },
        {
            "Title": "Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion",
            "Publication year": 2018,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2018arXiv181208156Z/abstract",
            "Abstract": "In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Qo9Q-PfIzZ0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Index of papers published in the IEEE Robotics and Automation Letters and presented at IEEE/RSJ Int. Conf. on Intelligent Robots and Systems 2017 (IROS\u201917)",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8202129/",
            "Abstract": "The following topics are dealt with: MAV; Android robot; augmented reality for kilobots; ARK; quadrotor; path planning; bipedal robots; adaptive depth control; micro diving agent; shape control; modular active-cell robots; mobile mixed-reality interaction; multi-robot systems; prestressed soft gripper; target-tracking game; time-delayed control; uncertain Euler-Lagrange systems; 7! Robots; recyclable robots; robot-integrated microfluidic chip; 3d-printed tactile gripper; humanoid robot; reinforcement learning; bioinspired continuum manipulator; musculoskeletal humanoids; feature-based matching; multimodal robotic skin; serial-link robots; feedback control; legged robots; ZMP constraints; state estimators; switching control; torque-controlled series-elastic actuators; RGB-D SLAM; reachability maps; co-safe temporal logic specifications; end effector; acceleration control; human-robot collaborative minimally invasive \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:N75c7piKpcAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Cross-domain 3d equivariant image embeddings",
            "Publication year": 2019,
            "Publication url": "http://proceedings.mlr.press/v97/esteves19a.html",
            "Abstract": "Spherical convolutional networks have been introduced recently as tools to learn powerful feature representations of 3D shapes. Spherical CNNs are equivariant to 3D rotations making them ideally suited to applications where 3D data may be observed in arbitrary orientations. In this paper we learn 2D image embeddings with a similar equivariant structure: embedding the image of a 3D object should commute with rotations of the object. We introduce a cross-domain embedding from 2D images into a spherical CNN latent space. This embedding encodes images with 3D shape properties and is equivariant to 3D rotations of the observed object. The model is supervised only by target embeddings obtained from a spherical CNN pretrained for 3D shape classification. We show that learning a rich embedding for images with appropriate geometric structure is sufficient for tackling varied applications, such as relative pose estimation and novel view synthesis, without requiring additional task-specific supervision.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:_9cyEV96HHsC",
            "Publisher": "PMLR"
        },
        {
            "Title": "Image matching via saliency region correspondences",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4269998/",
            "Abstract": "We introduce the notion of co-saliency for image matching. Our matching algorithm combines the discriminative power of feature correspondences with the descriptive power of matching segments. Co-saliency matching score favors correspondences that are consistent with 'soft' image segmentation as well as with local point feature matching. We express the matching model via a joint image graph (JIG) whose edge weights represent intra-as well as inter-image relations. The dominant spectral components of this graph lead to simultaneous pixel-wise alignment of the images and saliency-based synchronization of 'soft' image segmentation. The co-saliency score function, which characterizes these spectral components, can be directly used as a similarity metric as well as a positive feedback for updating and establishing new point correspondences. We present experiments showing the extraction of matching \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:M3ejUd6NZC8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Methods, systems, and computer readable media for visual odometry using rigid structures identified by antipodal transform",
            "Publication year": 2016,
            "Publication url": "https://patents.google.com/patent/US9280832B2/en",
            "Abstract": "The subject matter described herein includes methods for visual odometry using rigid structures identified by an antipodal transform. One exemplary method includes receiving a sequence of images captured by a camera. The method further includes identifying rigid structures in the images using an antipodal transform. The method further includes identifying correspondence between rigid structures in different image frames. The method further includes estimating motion of the camera based on motion of corresponding rigid structures among the different image frames.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:9KdEqzwCTsEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Penncosyvio: A challenging visual inertial odometry benchmark",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7989443/",
            "Abstract": "We present PennCOSYVIO, a new challenging Visual Inertial Odometry (VIO) benchmark with synchronized data from a VI-sensor (stereo camera and IMU), two Project Tango hand-held devices, and three GoPro Hero 4 cameras. Recorded at UPenn's Singh center, the 150m long path of the hand-held rig crosses from outdoors to indoors and includes rapid rotations, thereby testing the abilities of VIO and Simultaneous Localization and Mapping (SLAM) algorithms to handle changes in lighting, different textures, repetitive structures, and large glass surfaces. All sensors are synchronized and intrinsically and extrinsically calibrated. We demonstrate the accuracy with which ground-truth poses can be obtained via optic localization off of fiducial markers. The data set can be found at https://daniilidis-group.github.io/penncosyvio/.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:NAGhd4NKCV8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Robotics & Automation Magazine Vol. 11",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1371620/",
            "Abstract": "This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Lc9Ei6r3docC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Live Demonstration: Unsupervised Event-Based Learning of Optical Flow, Depth and Egomotion",
            "Publication year": 2019,
            "Publication url": "http://openaccess.thecvf.com/content_CVPRW_2019/html/EventVision/Zhu_Live_Demonstration_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_CVPRW_2019_paper.html",
            "Abstract": "We propose a demo of our work, Unsupervised Event-based Learning of Optical Flow, Depth and Egomotion, which will also appear at CVPR 2019. Our demo consists of a CNN which takes as input events from a DAVIS-346b event camera, represented as a discretized event volume, and predicts optical flow for each pixel in the image. Due to the generalization abilities of our network, we are able to predict accurate optical flow for a very wide range of scenes, including for very fast motions and challenging lighting.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:S_0nULq340kC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Equivalence of catadioptric projections and mappings of the sphere",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/853812/",
            "Abstract": "In this paper we demonstrate that all single viewpoint catadioptric projections are equivalent to the composition of central projection to the sphere followed by a point projection from the sphere to an image plane. Special cases of this equivalence are parabolic projection, for which the second map is a stereographic projection, and perspective projection, for which the second map is central projection. We also show that two projections are dual by the mapping which takes conics to their foci. The foci of line images are points of another, dual, catadiotpric projection; and vice versa, points in the image-are foci of lines in the dual projection. They are dual because the mapping preserves incidence relationships. Finally we show some applications of the theory presented above. We present a general algorithm for calibration of a catadioptric system with lines in a single view.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:0kxB6oEY0CcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Information acquisition with sensing robots: Algorithms and error bounds",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6907811/",
            "Abstract": "Utilizing the capabilities of configurable sensing systems requires addressing difficult information gathering problems. Near-optimal approaches exist for sensing systems without internal states. However, when it comes to optimizing the trajectories of mobile sensors the solutions are often greedy and rarely provide performance guarantees. Notably, under linear Gaussian assumptions, the problem becomes deterministic and can be solved off-line. Approaches based on submodularity have been applied by ignoring the sensor dynamics and greedily selecting informative locations in the environment. This paper presents a non-greedy algorithm with suboptimality guarantees, which relies on concavity instead of submodularity and takes the sensor dynamics into account. Coupled with linearization and model predictive control, the algorithm can be used to generate adaptive policies for mobile sensors with non-linear \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:ClCfbGk0d_YC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Wide area multiple camera calibration and estimation of radial distortion",
            "Publication year": 2004,
            "Publication url": "http://www.deec.uc.pt/~jpbar/Publication_Source/omni_a.pdf",
            "Abstract": "The calibration of cameras distributed in a wide area is a challenging task because it is impossible to use reference objects visible to all cameras and because wide field-of-view cameras suffer under radial distortion. The present work proposes the first algorithm in the literature for radial distortion estimation from multiple views without involving non-linear minimization. The correspondences between views are obtained by deliberately moving an LED in thousands of unknown positions in front of the cameras. Then both projection matrices and radial distortion parameters are simultaneously computed using a factorization approach. The algorithm is based on the application of two subspace approximation steps. At these steps, the estimated approximate solution for a matrix can be projected to the manifold of the parameter space by adjusting the singular values. It is remarkable, that our system does not involve a single non-linear minimization or outlier treatment and still produces accurate results which have been tested in a multi-camera reconstruction algorithm. In addition to real imagery results, we have analyzed the behavior of the algorithm in simulations.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:hqOjcs7Dif8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "3D-orientation signatures with conic kernel filtering for multiple motion analysis",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/990490/",
            "Abstract": "In this paper we propose a new 3D kernel for the recovery of 3D-orientation signatures. The kernel is a Gaussian function defined in local spherical coordinates and its Cartesian support has the shape of a truncated cone with its axis in the radial direction and very small angular support. A set of such kernels is obtained by uniformly sampling the 2D space of polar and azimuth angles. The projection of a local neighborhood on such a kernel set produces a local 3D-orientation signature. In the case of spatiotemporal analysis, such a kernel set can be applied either on the derivative space of a local neighborhood or on the local Fourier transform. The well known planes arising from single or multiple motion produce maxima in the orientation signature. Due to the kernel's local support spatiotemporal signatures possess higher orientation resolution than 3D steerable filters and motion maxima can be detected and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:uYElc5AnwZoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Event-based feature tracking",
            "Publication year": 2021,
            "Publication url": "https://patents.google.com/patent/US11138742B2/en",
            "Abstract": "A method for implementing a soft data association modeled with probabilities is provided. The association probabilities are computed in an intertwined expectation maximization (EM) scheme with an optical flow computation that maximizes the expectation (marginalization) over all associations. In addition, longer tracks can be enabled by computing the affine deformation with respect to the initial point and using the resulting residual as a measure of persistence. The computed optical flow enables a varying temporal integration that is different for every feature and sized inversely proportional to the length of the optical flow. The results can be seen in egomotion and very fast vehicle sequences.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:RELVpAr6_7wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Toward fieldable human-scale mobile manipulation using RoMan",
            "Publication year": 2020,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11413/2559995/Toward-fieldable-human-scale-mobile-manipulation-using-RoMan/10.1117/12.2559995.short",
            "Abstract": "Robots are ideal surrogates for performing tasks that are dull, dirty, and dangerous. To fully achieve this ideal, a robotic teammate should be able to autonomously perform human-level tasks in unstructured environments where we do not want humans to go. In this paper, we take a step toward realizing that vision by introducing the integration of state of the art advancements in intelligence, perception, and manipulation on the RoMan (Robotic Manipulation) platform. RoMan is comprised of two 7 degree of freedom (DoF) limbs connected to a 1 DoF torso and mounted on a tracked base. Multiple lidars are used for navigation, and a stereo depth camera visualizes point clouds for grasping. Each limb has a 6 DoF force-torque sensor at the wrist, with a dexterous 3-finger gripper on one limb and a stronger 4-finger claw-like hand on the other. Tasks begin with an operator specifying a mission type, a desired final \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:zDAX0LUT-dsC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "Identifying maximal rigid components in bearing-based localization",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6386132/",
            "Abstract": "We present an approach for sensor network localization when provided with a set of angular constraints. This problem arises in camera networks when angles between nearby points can be measured but depth measurements are not readily available. We provide contributions for two different variations on this problem. First, when each node is aware of a global coordinate frame, we present a novel method for identifying the components of the problem that are rigidly constrained. Second, in the more difficult case where only relative angles are known, we propose a novel spectral solution that achieves a globally-optimal embedding under transitively-triangular constraints, which we show encompass a wide range of real-world conditions. We demonstrate the utility of our algorithm on both synthetic data and data from quadrotor robot formations.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Z5m8FVwuT1cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A Unifying View of Geometry, Semantics, and Data Association in SLAM.",
            "Publication year": 2018,
            "Publication url": "https://georgejpappas.org/papers/Paper254.pdf",
            "Abstract": "Traditional approaches for simultaneous localization and mapping (SLAM) rely on geometric features such as points, lines, and planes to infer the environment structure. They make hard decisions about the (data) association between observed features and mapped landmarks to update the environment model. This paper makes two contributions to the state of the art in SLAM. First, it generalizes the purely geometric model by introducing semantically meaningful objects, represented as structured models of mid-level part features. Second, instead of making hard, potentially wrong associations between semantic features and objects, it shows that SLAM inference can be performed efficiently with probabilistic data association. The approach not only allows building meaningful maps (containing doors, chairs, cars, etc.) but also offers significant advantages in ambiguous environments.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:eKGuBlYFiu8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Variation in female songbird state determines signal strength needed to evoke copulation",
            "Publication year": 2021,
            "Publication url": "https://www.biorxiv.org/content/10.1101/2021.05.19.444794v3.abstract",
            "Abstract": "It is the female response to male signals that determines courtship success. In most songbirds, females control reproduction via the copulation solicitation display (CSD), an innate, stereotyped posture produced in direct response to male displays. Because CSD can be elicited in the absence of males by the presentation of recorded song, CSD production enables investigations into the effects of underlying signal features and behavioral state on female mating preferences. Using computer vision to quantify CSD trajectory in female brown-headed cowbirds (Molothrus ater), we show that both song quality and a female9s internal state predict CSD production, as well as the onset latency and duration of the display. We also show that CSD can be produced in a graded fashion based on both signal strength and internal state. These results emphasize the importance of underlying receiver state in determining behavioral responses and suggest that female responsiveness acts in conjunction with male signal strength to determine the efficacy of male courtship.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:aGxtzYPGyQEC",
            "Publisher": "Cold Spring Harbor Laboratory"
        },
        {
            "Title": "Learning what you can do before doing anything",
            "Publication year": 2018,
            "Publication url": "https://openreview.net/forum?id=SylPMnR9Ym",
            "Abstract": "Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. In this work, we address the problem of learning an agent\u2019s action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content. We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions. We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP). We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings. When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels. Project website: https://daniilidis-group. github. io/learned_action_spaces",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:QoN_6baHBqgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Structure from motion with known camera positions",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1640795/",
            "Abstract": "The wide availability of GPS sensors is changing the landscape in the applications of structure from motion techniques for localization. In this paper, we study the problem of estimating camera orientations from multiple views, given the positions of the viewpoints in a world coordinate system and a set of point correspondences across the views. Given three or more views, the above problem has a finite number of solutions for three or more point correspondences. Given six or more views, the problem has a finite number of solutions for just two or more points. In the three-view case, we show the necessary and sufficient conditions for the three essential matrices to be consistent with a set of known baselines. We also introduce a method to recover the absolute orientations of three views in world coordinates from their essential matrices. To refine these estimates we perform a least-squares minimization on the group \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:r0BpntZqJG4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The Tiercel: A novel autonomous micro aerial vehicle that can map the environment by flying into obstacles",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9197269/",
            "Abstract": "Autonomous flight through unknown environments in the presence of obstacles is a challenging problem for micro aerial vehicles (MAVs). A majority of the current state-of-art research assumes obstacles as opaque objects that can be easily sensed by optical sensors such as cameras or LiDARs. However in indoor environments with glass walls and windows, or scenarios with smoke and dust, robots (even birds) have a difficult time navigating through the unknown space.In this paper, we present the design of a new class of micro aerial vehicles that achieves autonomous navigation and are robust to collisions. In particular, we present the Tiercel MAV: a small, agile, light weight and collision-resilient robot powered by a cellphone grade CPU. Our design exploits contact to infer the presence of transparent or reflective obstacles like glass walls, integrating touch with visual perception for SLAM. The Tiercel is able to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:HV_RJt6Pqn0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Bearing-only control laws for balanced circular formations of ground robots",
            "Publication year": 2008,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=fvCaQfBQ7qEC&oi=fnd&pg=PA119&dq=info:-bqWWPrPU4UJ:scholar.google.com&ots=VPMwDKRCxg&sig=wMKjyjXp8a7q--D-tA4wp6qP_Zw",
            "Abstract": "For a group of constant-speed ground robots, a simple control law is designed to stabilize the motion of the group into a balanced circular formation using a consensus approach. It is shown that the measurements of the bearing angles between the robots are suf\ufb01cient for reaching a balanced circular formation. We consider two different scenarios that the connectivity graph of the system is either a complete graph or a ring. Collision avoidance capabilities are added to the team members and the effectiveness of the control laws are demonstrated on a group of mobile robots.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:RGFaLdJalmkC",
            "Publisher": "MIT Press"
        },
        {
            "Title": "Omnidirectional image processing",
            "Publication year": 2003,
            "Publication url": "https://www.cis.upenn.edu/~kostas/omni/kostas03tutorial.pdf",
            "Abstract": "The set of all gx in X for any g in G is called the orbit of x. If the group possesses an orbit, that means for any a, b in X, ga= b for ag in G, then the group action is called transitive. For example, there is always a rotation mapping one point on the sphere to another.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:g-FVFPYC6a8C",
            "Publisher": "ICCV"
        },
        {
            "Title": "Autonomous flight for detection, localization, and tracking of moving targets with a small quadrotor",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7921549/",
            "Abstract": "In this letter, we address the autonomous flight of a small quadrotor, enabling tracking of a moving object. The 15-cm diameter, 250-g robot relies only on onboard sensors (a single camera and an inertial measurement unit) and computers, and can detect, localize, and track moving objects. Our key contributions include the relative pose estimate of a spherical target as well as the planning algorithm, which considers the dynamics of the underactuated robot, the actuator limitations, and the field of view constraints. We show simulation and experimental results to demonstrate feasibility and performance, as well as robustness to abrupt variations in target motion.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:sWz8iI0ruhYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Calibration/Active and Real-Time and Robot Vision/Image and Video Indexing/Medical Image Understanding/Vision Systems/Engineering and Evaluations/Statistical Learning-Linear Pose",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=8400425171661934650&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:W5xh706n7nkC",
            "Publisher": "Berlin: Springer-Verlag, 1973-"
        },
        {
            "Title": "Temporally consistent segmentation of point clouds",
            "Publication year": 2014,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9084/1/Temporally-consistent-segmentation-of-point-clouds/10.1117/12.2050666.short",
            "Abstract": "We consider the problem of generating temporally consistent point cloud segmentations from streaming RGB-D data, where every incoming frame extends existing labels to new points or contributes new labels while maintaining the labels for pre-existing segments. Our approach generates an over-segmentation based on voxel cloud connectivity, where a modified k-means algorithm selects supervoxel seeds and associates similar neighboring voxels to form segments. Given the data stream from a potentially mobile sensor, we solve for the camera transformation between consecutive frames using a joint optimization over point correspondences and image appearance. The aligned point cloud may then be integrated into a consistent model coordinate frame. Previously labeled points are used to mask incoming points from the new frame, while new and previous boundary points extend the existing segmentation \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:VofzgsFG4o0C",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "All graphs lead to rome: Learning geometric and cycle-consistent representations with graph convolutional networks",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1901.02078",
            "Abstract": "Image feature matching is a fundamental part of many geometric computer vision applications, and using multiple images can improve performance. In this work, we formulate multi-image matching as a graph embedding problem then use a Graph Convolutional Network to learn an appropriate embedding function for aligning image features. We use cycle consistency to train our network in an unsupervised fashion, since ground truth correspondence is difficult or expensive to aquire. In addition, geometric consistency losses can be added at training time, even if the information is not available in the test set, unlike previous approaches that optimize cycle consistency directly. To the best of our knowledge, no other works have used learning for multi-image feature matching. Our experiments show that our method is competitive with other optimization based approaches.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:h-U6AArFrx8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using omnidirectional structure from motion for registration of range images of minimal overlap",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1335222/",
            "Abstract": "We propose a novel method of merging a series of range images with a minimal overlap between any two consecutive range images. We rigidly mount a parabolic catadioptric camera to the range scanner. Using two omniviews we are able to accurately estimate the relative displacement between two range views. The resultant motion is used for the registration of all range data to the same coordinate system. An additional perspective camera calibrated with respect to the scanner is used for texture mapping.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:olpn-zPbct0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Leader-follower formations: Uncalibrated vision-based localization and control",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4209443/",
            "Abstract": "This paper focuses on leader-follower formations of mobile robots equipped with panoramic cameras and extend earlier works in the literature addressing both the vision-based localization and control problems. First, a new sufficient analytical condition for localizability is proved and used to shed light on the geometrical meaning of formation localization using uncalibrated vision sensors, here performed with the unscented Kalman filter. Second, we design a feedback control law based on dynamic extension in order to extend the applicability of our control scheme also to the case of distant robots.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Object detection via boundary structure segmentation",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5540114/",
            "Abstract": "We address the problem of object detection and segmentation using holistic properties of object shape. Global shape representations are highly susceptible to clutter inevitably present in realistic images, and can be robustly recognized only using a precise segmentation of the object. To this end, we propose a figure/ground segmentation method for extraction of image regions that resemble the global properties of a model boundary structure and are perceptually salient. Our shape representation, called the chordiogram, is based on geometric relationships of object boundary edges, while the perceptual saliency cues we use favor coherent regions distinct from the background. We formulate the segmentation problem as an integer quadratic program and use a semidefinite programming relaxation to solve it. Obtained solutions provide the segmentation of an object as well as a detection score used for object \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:isC4tDSrTZIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "VAIS: A dataset for recognizing maritime imagery in the visible and infrared spectrums",
            "Publication year": 2015,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W05/html/Zhang_VAIS_A_Dataset_2015_CVPR_paper.html",
            "Abstract": "The development of fully autonomous seafaring vessels has enormous implications to the world's global supply chain and militaries. To obey international marine traffic regulations, these vessels must be equipped with machine vision systems that can classify other ships nearby during the day and night. In this paper, we address this problem by introducing VAIS, the world's first publicly available dataset of paired visible and infrared ship imagery. This dataset contains more than 1,000 paired RGB and infrared images among six ship categories-merchant, sailing, passenger, medium, tug, and small-which are salient for control and following maritime traffic regulations. We provide baseline results on this dataset using two off-the-shelf algorithms: gnostic fields and deep convolutional neural networks. Using these classifiers, we are able to achieve 87.4% mean per-class recognition accuracy during the day and 61.0% at night.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:lo0OIn9KAZgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Equivalence of catadioptric projections and mappings of the sphere",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/853812/",
            "Abstract": "In this paper we demonstrate that all single viewpoint catadioptric projections are equivalent to the composition of central projection to the sphere followed by a point projection from the sphere to an image plane. Special cases of this equivalence are parabolic projection, for which the second map is a stereographic projection, and perspective projection, for which the second map is central projection. We also show that two projections are dual by the mapping which takes conics to their foci. The foci of line images are points of another, dual, catadiotpric projection; and vice versa, points in the image-are foci of lines in the dual projection. They are dual because the mapping preserves incidence relationships. Finally we show some applications of the theory presented above. We present a general algorithm for calibration of a catadioptric system with lines in a single view.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:aqlVkmm33-oC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sparseness meets deepness: 3d human pose estimation from monocular video",
            "Publication year": 2016,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Sparseness_Meets_Deepness_CVPR_2016_paper.html",
            "Abstract": "This paper addresses the challenge of 3D full-body human pose estimation from a monocular image sequence. Here, two cases are considered:(i) the image locations of the human joints are provided and (ii) the image locations of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the image locations of the joints as latent variables in order to take into account considerable uncertainties in 2D joint locations. A deep fully convolutional network is trained to predict the uncertainty maps of the 2D joint locations. The 3D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human3. 6M dataset shows that the proposed approaches achieve greater 3D pose estimation accuracy over state-of-the-art baselines. Further, the proposed approach outperforms a publicly available 2D pose estimation baseline on the challenging PennAction dataset.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:NN0YOz9s3UgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Understanding image motion with group representations",
            "Publication year": 2016,
            "Publication url": "https://arxiv.org/abs/1612.00472",
            "Abstract": "Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:oHSet2Z0r48C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning to estimate 3D human pose and shape from a single color image",
            "Publication year": 2018,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Learning_to_Estimate_CVPR_2018_paper.html",
            "Abstract": "This work addresses the problem of estimating the full body 3D human pose and shape from a single color image. This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efficient and effective direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction. Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks. These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further refinement of the network, by optimizing for the consistency of the projection with 2D annotations (ie, 2D keypoints or masks). The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:zMY8q35v6VMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Linear augmented reality registration",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-44692-3_47",
            "Abstract": "Augmented reality requires the geometric registration of virtual or remote worlds with the visual stimulus of the user. This can be achieved by tracking the head pose of the user with respect to the reference coordinate system of virtual objects. If tracking is achieved with head-mounted cameras, registration is known in computer vision as pose estimation. Augmented reality is by definition a real-time problem, so we are interested only in bounded and short computational time. We propose a new linear algorithm for pose estimation. Our algorithm shows better performance than the linear algorithm of Quan and Lan [14] and is comparable to the non-predicted time iterative approach of Kumar and Hanson [8].",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:rO6llkc54NcC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Planning to explore via self-supervised world models",
            "Publication year": 2020,
            "Publication url": "http://proceedings.mlr.press/v119/sekar20a.html",
            "Abstract": "Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code: https://ramanans1. github. io/plan2explore/",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:tp0eXr8pwPYC",
            "Publisher": "PMLR"
        },
        {
            "Title": "Stereo-based environment scanning for immersive telepresence",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1273541/",
            "Abstract": "The processing power and network bandwidth required for true immersive telepresence applications are only now beginning to be available. We draw from our experience developing stereo based tele-immersion prototypes to present the main issues arising when building these systems. Tele-immersion is a new medium that enables a user to share a virtual space with remote participants. The user is immersed in a rendered three-dimensional (3-D) world that is transmitted from a remote site. To acquire this 3-D description, we apply binocular and trinocular stereo techniques which provide a view-independent scene description. Slow processing cycles or long network latencies interfere with the users' ability to communicate, so the dense stereo range data must be computed and transmitted at high frame rates. Moreover, reconstructed 3-D views of the remote scene must be as accurate as possible to achieve a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:_sLlIXX7spQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Deformable Linear Object Prediction Using Locally Linear Latent Dynamics",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2103.14184",
            "Abstract": "We propose a framework for deformable linear object prediction. Prediction of deformable objects (e.g., rope) is challenging due to their non-linear dynamics and infinite-dimensional configuration spaces. By mapping the dynamics from a non-linear space to a linear space, we can use the good properties of linear dynamics for easier learning and more efficient prediction. We learn a locally linear, action-conditioned dynamics model that can be used to predict future latent states. Then, we decode the predicted latent state into the predicted state. We also apply a sampling-based optimization algorithm to select the optimal control action. We empirically demonstrate that our approach can predict the rope state accurately up to ten steps into the future and that our algorithm can find the optimal action given an initial state and a goal state.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:rn2Io16i3IwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "From moments to months: Multi-timescale tracking and analysis of songbird social interactions in a smart aviary",
            "Publication year": 2020,
            "Publication url": "https://scholar.google.com/scholar?cluster=7260256415746462423&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:kPzzr9KoCG0C",
            "Publisher": "OXFORD UNIV PRESS INC"
        },
        {
            "Title": "Semantic perception for ground robotics",
            "Publication year": 2012,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8387/83870Y/Semantic-perception-for-ground-robotics/10.1117/12.918915.short",
            "Abstract": "Semantic perception involves naming objects and features in the scene, understanding the relations between them, and understanding the behaviors of agents, e.g., people, and their intent from sensor data. Semantic perception is a central component of future UGVs to provide representations which 1) can be used for higher-level reasoning and tactical behaviors, beyond the immediate needs of autonomous mobility, and 2) provide an intuitive description of the robot's environment in terms of semantic elements that can shared effectively with a human operator. In this paper, we summarize the main approaches that we are investigating in the RCTA as initial steps toward the development of perception systems for UGVs.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:zLWjf1WUPmwC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "Fast, autonomous flight in GPS-denied and cluttered environments (vol 35, pg 101, 2018)",
            "Publication year": 2018,
            "Publication url": "https://scholar.google.com/scholar?cluster=11890458096960086815&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:g57m_4BFVZsC",
            "Publisher": "WILEY"
        },
        {
            "Title": "Rigid components identification and rigidity control in bearing-only localization using the graph cycle basis",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7171940/",
            "Abstract": "Bearing-only localization can be formulated in terms of optimal graph embedding: for each node in the graph, find the coordinates that satisfy as close as possible all the bearing-only constraints on the edges. If the graph is parallel rigid, this can be done via spectral methods. When the graph is not rigid the solution is ambiguous, as different subsets of vertices can be scaled differently. It is then important to first partition the problem into maximal rigid components. In this paper we show that the cycle basis matrix of the graph can be used for this task, and that it can also be used to provide a more intuitive look at graph rigidity. We can explain, for instance, why triangulated graphs are rigid and why graphs with longer cycles may loose this property. Furthermore, we can obtain tools for enforcing rigidity by controlling the addition of new measurements.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:goOyc-W9OFwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fast multi-image matching via density-based clustering",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_iccv_2017/html/Tron_Fast_Multi-Image_Matching_ICCV_2017_paper.html",
            "Abstract": "We consider the problem of finding consistent matches across multiple images. Current state-of-the-art solutions use constraints on cycles of matches together with convex optimization, leading to computationally intensive iterative algorithms. In this paper, we instead propose a clustering-based formulation: we first rigorously show its equivalence with traditional approaches, and then propose QuickMatch, a novel algorithm that identifies multi-image matches from a density function in feature space. Specifically, QuickMatch uses the density estimate to order the points in a tree, and then extracts the matches by breaking this tree using feature distances and measures of distinctiveness. Our algorithm outperforms previous state-of-the-art methods (such as MatchALS) in accuracy, and it is significantly faster (up to 62 times faster on some benchmarks), and can scale to large datasets (with more than twenty thousands features).",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:E9jS3u5z5twC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Automatic mapping of store layout using soft object recognition",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US10733661B1/en",
            "Abstract": "A method for automatically mapping a store layout includes identifying a path for traversing a retail area and capturing images of the retail area at various points along the path. The images may be analyzed to identify visual characteristics which may be compared to a template of retail products in a template library. When an object depicted in the image matches with a retail product in the template library, the object may correspond to the retail product. Additionally, a retail department for the object may also be identified. The retail department may be compared to the retail product corresponding to the object, and when the retail product is not associated with the retail department, another retail product which is associated with the retail department may be identified as corresponding to the object. A map of the store layout may be generated based on the identified retail products.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:rzkGdFpNPO0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Vision-based control of a quadrotor for perching on lines",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6907309/",
            "Abstract": "We formulate the position-based visual servoing problem for a quadrotor equipped with a monocular camera and an IMU relying only on features on planes and lines in order to fly above and perch on arbitrarily oriented lines. We show that we are able to compute the orientation of an arbitrarily oriented line, the speed of the robot and its position with respect to the target line using two points at a known distance on the line. The direction of the velocity is derived from optical flow induced by features on a plane in the background Finally, we demonstrate fully autonomous flight and perching using a small 230 gram quadrotor with all the computations running on the robot.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:JTutsjMeBaAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Scale-invariant features on the sphere",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4408893/",
            "Abstract": "This paper considers an application of scale-invariant feature detection using scale-space analysis suitable for use with wide field of view cameras. Rather than obtain scale- space images via convolution with the Gaussian function on the image plane, we map the image to the sphere and obtain scale-space images as the solution to the heat (diffusion) equation on the sphere which is implemented in the frequency domain using spherical harmonics. The percentage correlation of scale-invariant features that may be matched between any two wide-angle images subject to change in camera pose is then compared using each of these methods. We also present a means by which the required sampling bandwidth may be determined and propose a suitable anti-aliasing filter which may be used when this bandwidth exceeds the maximum permissible due to computational requirements. The results show improved \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:hFOr9nPyWt4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Imaging Beyond the Pinhole Camera",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/978-1-4020-4894-4.pdf",
            "Abstract": "Imaging Beyond the Pinhole Camera Kostas Daniilidis\u00b7 Reinhard Klette Imaging beyond the \nPinhole Camera COMPUTATIONAL IMAGING 33 Page 2 Imaging Beyond the Pinhole \nCamera Page 3 Volume 33 Computational Imaging and Vision Managing Editor MAX \nVIERGEVER Utrecht University, The Netherlands Series Editors GUNILLA BORGEFORS, \nCentre for Image Analysis, SLU, Uppsala, Sweden RACHID DERICHE, INRIA, France \nTHOMAS S. HUANG, University of Illinois, Urbana, USA TIANZI JIANG, Institute of \nAutomation, CAS, Beijing REINHARD KLETTE, University of Auckland, New Zealand ALES \nLEONARDIS, ViCoS, University of Ljubljana, Slovenia HEINZ-OTTO PEITGEN, CeVis, \nBremen, Germany Imaging Systems and Image Processing Computer Vision and Image \nUnderstanding Visualization This comprehensive book series embraces state-of-the-art \nexpository works and advanced research on of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:RYcK_YlVTxYC",
            "Publisher": "Springer"
        },
        {
            "Title": "Spherical correlation of visual representations for 3D model retrieval",
            "Publication year": 2010,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s11263-009-0280-7.pdf",
            "Abstract": "In recent years we have seen a tremendous growth in the amount of freely available 3D content, in part due to breakthroughs for 3D model design and acquisition. For example, advances in range sensor technology and design software have dramatically reduced the manual labor required to construct 3D models. As collections of 3D content continue to grow rapidly, the ability to perform fast and accurate retrieval from a database of models has become a necessity.At the core of this retrieval task is the fundamental challenge of defining and evaluating similarity between 3D shapes. Some effective methods dealing with this challenge consider similarity measures based on the visual appearance of models. While collections of rendered images are discriminative for retrieval tasks, such representations come with a few inherent limitations such as restrictions in the image viewpoint sampling and high \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:pqnbT2bcN3wC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Perception-driven curiosity with bayesian surprise",
            "Publication year": 2019,
            "Publication url": "https://openreview.net/forum?id=rJlBQkrFvr",
            "Abstract": "Intrinsic rewards in reinforcement learning provide a powerful algorithmic capability for agents to learn how to interact with their environment in a task-generic way. However, increased incentives for motivation can come at the cost of increased fragility to stochasticity. We introduce a method for computing an intrinsic reward for curiosity using metrics derived from sampling a latent variable model used to estimate dynamics. Ultimately, an estimate of the conditional probability of observed states is used as our intrinsic reward for curiosity. In our experiments, a video game agent uses our model to autonomously learn how to play Atari games using our curiosity reward in combination with extrinsic rewards from the game to achieve improved performance on games with sparse extrinsic rewards. When stochasticity is introduced in the environment, our method still demonstrates improved performance over the baseline.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:H-nlc5mcmJQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Active Deformable Part Models",
            "Publication year": 2014,
            "Publication url": "https://arxiv.org/abs/1404.0334",
            "Abstract": "This paper presents an active approach for part-based object detection, which optimizes the order of part filter evaluations and the time at which to stop and make a prediction. Statistics, describing the part responses, are learned from training data and are used to formalize the part scheduling problem as an offline optimization. Dynamic programming is applied to obtain a policy, which balances the number of part evaluations with the classification accuracy. During inference, the policy is used as a look-up table to choose the part order and the stopping time based on the observed filter responses. The method is faster than cascade detection with deformable part models (which does not optimize the part order) with negligible loss in accuracy when evaluated on the PASCAL VOC 2007 and 2010 datasets.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:JzGFD3-rS6kC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Using skew Gabor filter in source signal separation and local spectral multi-orientation analysis",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1315068/",
            "Abstract": "Responses of Gabor wavelets in the mid-frequency space build a local spectral representation scheme with optimal properties regarding the time-frequency uncertainty principle. However, when using Gabor wavelets we observe a skewness in the mid-frequency space caused by the unsymmetrically spreading effect of Gabor wavelets. Though in most current applications the skewness does not obstruct the sampling of the spectral domain, it affects the identification and separation of source signals from the filter response in the mid-frequency space. In this paper, we present a modification of the original Gabor filter, the skew Gabor filter, to correct the skewness so that the filter responses can be described with a sum-of-Gaussians model. The correction enables us to use higher-order-moment information to analytically separate different source signal components. This provides us with an analytical framework to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:UxriW0iASnsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Experiments on visual loop closing using vocabulary trees",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4563140/",
            "Abstract": "In this paper we study the problem of visual loop closing for long trajectories in an urban environment. We use GPS positioning only to narrow down the search area and use pre-built vocabulary trees to find the best matching image in this search area. Geometric consistency is then used to prune out the bad matches. We compare several vocabulary trees on a sequence of 6.5 kilometers. We experiment with hierarchical k-means based trees as well as extremely randomized trees and compare results obtained using five different trees. We obtain the best results using extremely randomized trees. After enforcing geometric consistency the matched images look promising for structure from motion applications.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:O3NaXMp0MMsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sparse representation for 3D shape estimation: A convex relaxation approach",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7558185/",
            "Abstract": "We investigate the problem of estimating the 3D shape of an object defined by a set of 3D landmarks, given their 2D correspondences in a single image. A successful approach to alleviating the reconstruction ambiguity is the 3D deformable shape model and a sparse representation is often used to capture complex shape variability. But the model inference is still challenging due to the nonconvexity in the joint optimization of shape and viewpoint. In contrast to prior work that relies on an alternating scheme whose solution depends on initialization, we propose a convex approach to addressing this challenge and develop an efficient algorithm to solve the proposed convex program. We further propose a robust model to handle gross errors in the 2D correspondences. We demonstrate the exact recovery property of the proposed method, the advantage compared to several nonconvex baselines and the applicability to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:eb1hsBXB1ukC",
            "Publisher": "IEEE"
        },
        {
            "Title": "and Source Signal Separation",
            "Publication year": 2003,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=9XCqCAAAQBAJ&oi=fnd&pg=PA269&dq=info:RizBqrFw0jkJ:scholar.google.com&ots=AUBp4g69FY&sig=MC-vp3HoJpb7MRmscTQkXIsUTDc",
            "Abstract": "Responses of Gabor wavelets in the mid-frequency space build a local spectral representation scheme with optimal properties regarding the time\u2013frequency uncertainty principle. However, when using Gabor wavelets we observe a skewness in the mid-frequency space caused by the spreading effect of Gabor wavelets. Though in most current applications the skewness does not obstruct the sampling of the spectral domain, it affects the identification and separation of source signals from the filter response in the mid-frequency space. In this paper, we present a modification of the original Gabor filter, the skew Gabor filter, which corrects skewness so that the filter response can be described with a sumof-Gaussians model in the mid-frequency space. The correction further enables us to use higher-order moment information to separate different source signal components. This provides us with an elegant framework to deblur the filter response which is not characterized by the limited spectral resolution of other local spectral representations.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:2POsdo3tyOAC",
            "Publisher": "Springer"
        },
        {
            "Title": "Robustness meets deep learning: An end-to-end hybrid pipeline for unsupervised learning of egomotion",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1812.08351",
            "Abstract": "In this work, we propose a method that combines unsupervised deep learning predictions for optical flow and monocular disparity with a model based optimization procedure for instantaneous camera pose. Given the flow and disparity predictions from the network, we apply a RANSAC outlier rejection scheme to find an inlier set of flows and disparities, which we use to solve for the relative camera pose in a least squares fashion. We show that this pipeline is fully differentiable, allowing us to combine the pose with the network outputs as an additional unsupervised training loss to further refine the predicted flows and disparities. This method not only allows us to directly regress relative pose from the network outputs, but also automatically segments away pixels that do not fit the rigid scene assumptions that many unsupervised structure from motion methods apply, such as on independently moving objects. We evaluate our method on the KITTI dataset, and demonstrate state of the art results, even in the presence of challenging independently moving objects.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:n0_S8QYMK-AC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Monocular 3d tracking of deformable surfaces",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7487182/",
            "Abstract": "The problem of reconstructing deformable 3D surfaces has been studied in the non-rigid structure from motion context, where either tracked points over long sequences or an initial 3D shape are required, and also with piecewise methods, where the deformable surface is modeled as a triangulated mesh, which is fitted to an initial estimation of the 3D surface computed from correspondences in two views. In this paper we present a new scheme to reconstruct deformable surfaces by tracking relevant features that parametrize such deformation. Assuming that an initial 3D shape related to a reference frame is available, we initially match the reference and current frames using visual information. Then, these correspondences are clustered in patches with geometric characteristics in the image domain and 3D space. In order to reduce the number of parameters to be estimated, we explain each cluster using thin-plate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:D2lc7i8A8ooC",
            "Publisher": "IEEE"
        },
        {
            "Title": "UNCALIBRATED PARACATADIOPTRIC VISUAL SERVOING",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=7537135579601286661&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:_Ybze24A_UAC",
            "Publisher": "The University; 1998"
        },
        {
            "Title": "Predicting disparity windows for real-time stereo",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-45054-8_15",
            "Abstract": "New applications in fields such as augmented or virtualized reality have created a demand for dense, accurate real-time stereo reconstruction. Our goal is to reconstruct a user and her office environment for networked tele-immersion, which requires accurate depth values in a relatively large workspace. In order to cope with the combinatorics of stereo correspondence we can exploit the temporal coherence of image sequences by using coarse optical flow estimates to bound disparity search ranges at the next iteration. We use a simple flood fill segmentation method to cluster similar disparity values into overlapping windows and predict their motion over time using a single optical flow calculation per window. We assume that a contiguous region of disparity represents a single smooth surface which allows us to restrict our search to a narrow disparity range. The values in the range may vary over time as \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:R3hNpaxXUhUC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "B.: Tele-immersion Portal: Towards an Ultimate Systhesis",
            "Publication year": 2001,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.2405",
            "Abstract": "We describe a novel approach for unifying computer graphics and computer vision systems, and our initial results in building and using a prototype system. This approach has three significant characteristics: unification of the real and virtual worlds for both input and output, tele-collaboration between remote participants, and interaction between heterogeneous stand-alone 3-dimensional (3D) graphics applications [1]. The system is designed to run on the networks of the future, and it is capable of transmitting blends of dynamic computer graphics and computer vision data in real-time. In this text we concentrate on its visual part, in particular on synergy of computer graphics and computer vision systems as a new medium for collaboration and tele-presence. The preliminary steps of our research make us increasingly optimistic that in the future this technology will provide highly compelling, immersive environments for an increasing variety of tele-collaborative applications over high bandwidth networks",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:9aOYe38lPcwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Coherent reconstruction of multiple humans from a single image",
            "Publication year": 2020,
            "Publication url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Jiang_Coherent_Reconstruction_of_Multiple_Humans_From_a_Single_Image_CVPR_2020_paper.html",
            "Abstract": "In this work, we address the problem of multi-person 3D pose estimation from a single image. A typical regression approach in the top-down setting of this problem would first detect all humans and then reconstruct each one of them independently. However, this type of prediction suffers from incoherent results, eg, interpenetration and inconsistent depth ordering between the people in the scene. Our goal is to train a single network that learns to avoid these problems and generate a coherent 3D reconstruction of all the humans in the scene. To this end, a key design choice is the incorporation of the SMPL parametric body model in our top-down framework, which enables the use of two novel losses. First, a distance field-based collision loss penalizes interpenetration among the reconstructed people. Second, a depth ordering-aware loss reasons about occlusions and promotes a depth ordering of people that leads to a rendering which is consistent with the annotated instance segmentation. This provides depth supervision signals to the network, even if the image has no explicit 3D annotations. The experiments show that our approach outperforms previous methods on standard 3D pose benchmarks, while our proposed losses enable more coherent reconstruction in natural images. The project website with videos, results, and code can be found at: https://jiangwenpl. github. io/multiperson",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:t1niNHmIXQYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning so (3) equivariant representations with spherical cnns",
            "Publication year": 2018,
            "Publication url": "http://openaccess.thecvf.com/content_ECCV_2018/html/Carlos_Esteves_Learning_SO3_Equivariant_ECCV_2018_paper.html",
            "Abstract": "We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:YAnBoHO8NTMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Event-based vision: A survey",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1904.08405",
            "Abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:UmJFWc0aipQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Video-based localization without 3D mapping for the visually impaired",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5543581/",
            "Abstract": "In this paper, we present a system for indoor human localization that does not need 3D reconstruction of features or landmarks. We assume that a video sequence has been acquired and that keyframes have been registered with respect to 2D positions and orientations. In online mode, we use only a handheld monochrome fisheye camera and a synchronized IMU as sensory inputs. The query is not based on a single image but uses a HMM-based state estimator. Our image representation consists of initial global GIST vectors followed by local SURF features. We present a novel approach to localization by using search space reduction on global features, then HMM based position prediction and estimation on local features. Experimental results show that accurate localization is achieved and realtime performance is feasible. This work demonstrates that a working portable system could be designed for the visually \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:HE397vMXCloC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Vision-based Leader-Follower Formations: Nonlinear Observability and Control",
            "Publication year": 2006,
            "Publication url": "https://scholar.google.com/scholar?cluster=590813102807344653&hl=en&oi=scholarr",
            "Abstract": "In this paper we address both the vision-based localization and control problems for a leaderfollower formation of nonholonomic mobile robots equipped with panoramic cameras. The localization problem has been analytically studied using the Extended Output Jacobian. Two formation control strategies are also presented: in the first one we design a stable feedback linearizing input-state tracking control law, while in the second one, a feedback controller based on dynamic extension is used in order to deal with the case in which the on-board camera can only observe the centroid of the other robots (a convenient choice with distant robots). A comparison between nonlinear observers commonly used in robotics applications, first-order EKF, second-order EKF, IEKF and UKF, is finally introduced for the present application. Extensive simulation experiments show the pros and cons of each filter and the effectiveness of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:WA5NYHcadZ8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Birds of a Feather: Capturing Avian Shape Models from Images",
            "Publication year": 2021,
            "Publication url": "https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Birds_of_a_Feather_Capturing_Avian_Shape_Models_From_Images_CVPR_2021_paper.html",
            "Abstract": "Animals are diverse in shape, but building a deformable shape model for a new species is not always possible due to the lack of 3D data. We present a method to capture new species using an articulated template and images of that species. In this work, we focus mainly on birds. Although birds represent almost twice the number of species as mammals, no accurate shape model is available. To capture a novel species, we first fit the articulated template to each training sample. By disentangling pose and shape, we learn a shape space that captures variation both among species and within each species from image evidence. We learn models of multiple species from the CUB dataset, and contribute new species-specific and multi-species shape models that are useful for downstream reconstruction tasks. Using a low-dimensional embedding, we show that our learned 3D shape space better reflects the phylogenetic relationships among birds than learned perceptual features.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:4IpgxnMJogoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scale invariant feature matching with wide angle images",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4399266/",
            "Abstract": "Numerous scale-invariant feature matching algorithms using scale-space analysis have been proposed for use with perspective cameras, where scale-space is defined as convolution with a Gaussian. The contribution of this work is a method suitable for use with wide angle cameras. Given an input image, we map it to the unit sphere and obtain scale-space images by convolution with the solution of the spherical diffusion equation on the sphere which we implement in the spherical Fourier domain. Using such an approach, the scale-space response of a point in space is independent of its position on the image plane for a camera subject to pure rotation. Scale-invariant features are then found as local extrema in scale-space. Given this set of scale-invariant features, we then generate feature descriptors by considering a circular support region defined on the sphere whose size is selected relative to the feature scale \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:hC7cP41nSMkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An Optimization Approach to Bearing-Only Navigation with Applications to a 2-D Unicycle Model",
            "Publication year": 2014,
            "Publication url": "Unknown",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:h0mLeC6b6wcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Optimal pixel aspect ratio for enhanced 3D TV visualization",
            "Publication year": 2012,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1077314211001871",
            "Abstract": "In multiview 3D TV, a pair of corresponding pixels in adjacent 2D views contributes to the reconstruction of voxels (3D pixels) in the 3D scene. We analyze this reconstruction process and determine the optimal pixel aspect ratio based on which the estimated object position can be improved given specific imaging or viewing configurations and constraints. By applying mathematical modeling, we deduce the optimal solutions for two general stereo configurations: parallel and with vergence. We theoretically show that for a given total resolution a finer horizontal resolution, compared to the usual uniform pixel distribution, in general, provides a better 3D visual experience for both configurations. The optimal value may vary depending on different configuration parameter values. We validate our theoretical results by conducting subjective studies using a set of simulated non-square discretized red\u2013blue stereo pairs and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:f2IySw72cVMC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "3-D Vision for Navigation and Grasping",
            "Publication year": 2016,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-32552-1_32",
            "Abstract": "In this chapter, we describe algorithms for three-dimensional (3-D                                    ) vision that help robots accomplish navigation and grasping. To model cameras, we start with the basics of perspective projection and distortion due to lenses. This projection from a 3-D world to a two-dimensional (2-D                                    ) image can be inverted only by using information from the world or multiple 2-D views. If we know the 3-D model of an object or the location of 3-D landmarks, we can solve the pose estimation problem from one view. When two views are available, we can compute the 3-D motion and triangulate to reconstruct the world up to a scale factor. When multiple views are given either as sparse viewpoints or a continuous incoming video, then the robot path can be computer and point tracks can yield a sparse 3-D representation of the world. In order to grasp objects, we can estimate 3-D pose of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:Lr5Uwm59ZTwC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Distributed consistent data association via permutation synchronization",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7989308/",
            "Abstract": "Data association is one of the fundamental problems in multi-sensor systems. Most current techniques rely on pairwise data associations which can be spurious even after the employment of outlier rejection schemes. Considering multiple pairwise associations at once significantly increases accuracy and leads to consistency. In this work, we propose a fully decentralized method for globally consistent data association from pairwise data associations based on a distributed averaging scheme on the set of doubly stochastic matrices. We demonstrate the effectiveness of the proposed method using theoretical analysis and experimental evaluation.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:uSZH581ylUkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A distributed optimization framework for localization and formation control: Applications to vision-based measurements",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7515271/",
            "Abstract": "Multiagent systems have been a major area of research for the last 15 years. This interest has been motivated by tasks that can be executed more rapidly in a collaborative manner or that are nearly impossible to carry out otherwise. To be effective, the agents need to have the notion of a common goal shared by the entire network (for instance, a desired formation) and individual control laws to realize the goal. The common goal is typically centralized, in the sense that it involves the state of all the agents at the same time. On the other hand, it is often desirable to have individual control laws that are distributed, in the sense that the desired action of an agent depends only on the measurements and states available at the node and at a small number of neighbors. This is an attractive quality because it implies an overall system that is modular and intrinsically more robust to communication delays and node failures.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:ObAD8Md4PD8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Probabilistic data association for simultaneous localization and mapping",
            "Publication year": 2019,
            "Publication url": "https://patents.google.com/patent/US20190219401A1/en",
            "Abstract": "A method for simultaneous location and mapping (SLAM) includes receiving, by at least one processor, a set of sensor measurements from a movement sensor of a mobile robot and a set of images captured by a camera on the mobile robot as the mobile robot traverses an environment. The method includes, for each image of at least a subset of the set of images, extracting, by the at least one processor, a plurality of detected objects from the image. The method includes estimating, by the at least one processor, a trajectory of the mobile robot and a respective semantic label and position of each detected object within the environment using the sensor measurements and an expectation maximization (EM) algorithm.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:joW6AvqysxAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "3-D Vision and Recognition.",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=3020481049744399260&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:e_rmSamDkqQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras",
            "Publication year": 2018,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2018arXiv180206898Z/abstract",
            "Abstract": "Event-based cameras have shown great promise in a variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new class of hand crafted algorithms. Deep learning has shown great success in providing model free solutions to many problems in the vision community, but existing networks have been developed with frame based images in mind, and there does not exist the wealth of labeled data for events as there does for images for supervised training. To these points, we present EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras. In particular, we introduce an image based representation of a given event stream, which is fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:KbeHZ-DlqmcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning what you can do before doing anything",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1806.09655",
            "Abstract": "Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. In this work, we address the problem of learning an agent's action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content. We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions. We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP). We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings. When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels. Project website: https://daniilidis-group.github.io/learned_action_spaces",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:X5QHDg3V9EEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Adversarial Curiosity",
            "Publication year": 2020,
            "Publication url": "https://www.researchgate.net/profile/Nikolai-Matni/publication/339946331_Action_for_Better_Prediction/links/5f43dbd1299bf13404ed6361/Action-for-Better-Prediction.pdf",
            "Abstract": "Model-based curiosity combines active learning approaches to optimal sampling with the information gain based incentives for exploration presented in the curiosity literature. Existing model-based curiosity methods look to approximate prediction uncertainty with approaches which struggle to scale to many prediction-planning pipelines used in robotics tasks. We address these scalability issues with an adversarial curiosity method minimizing a score given by a discriminator network. This discriminator is optimized jointly with a prediction model and enables our active learning approach to sample sequences of observations and actions which result in predictions considered the least realistic by the discriminator. We demonstrate increased downstream task performance in simulated environments using our adversarial curiosity approach compared to other model-based and model-free exploration strategies. We further demonstrate the ability of our adversarial curiosity method to scale to a robotic manipulation prediction-planning pipeline where we improve sample efficiency and prediction performance for a domain transfer problem.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:tgXRM5oPLYQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Rotation estimation from spherical images",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1334598/",
            "Abstract": "Robotic navigation algorithms increasingly make use of the panoramic field of view provided by omnidirectional images to assist with localization tasks. Since the images taken by a particular class of omnidirectional sensors can be mapped to the sphere, the problem of attitude estimation arising from 3D rotations of the camera can be treated as a problem of estimating rotations between spherical images. Recently, it has been shown that direct signal processing techniques are effective tools in handling rotations of the sphere, but are limited when the signal is altered by larger rotations of omnidirectional cameras. We present an effective solution to the attitude estimation problem under large rotations. Our approach utilizes a shift theorem for the spherical Fourier transform to produce a solution in the spectral domain.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:8k81kl-MbHgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Motion estimation using a spherical camera",
            "Publication year": 2004,
            "Publication url": "https://repository.upenn.edu/cis_reports/3/",
            "Abstract": "Robotic navigation algorithms increasingly make use of the panoramic field of view provided by omnidirectional images to assist with localization tasks. Since the images taken by a particular class of omnidirectional sensors can be mapped to the sphere, the problem of attitude estimation arising from 3D motions of the camera can be treated as a problem of estimating the camera motion between spherical images. This problem has traditionally been solved by tracking points or features between images. However, there are many natural scenes where the features cannot be tracked with confidence. We present an algorithm that uses image features to estimate ego-motion without explicitly searching for correspondences. We formulate the problem as a correlation of functions defined on the product of spheres S 2\u00d7 S 2 which are acted upon by elements of the direct product group SO (3)\u00d7 SO (3). We efficiently compute this correlation and obtain our solution using the spectral information of functions in S 2\u00d7 S 2.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:XiSMed-E-HIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Geometric Urban Geo-Localization",
            "Publication year": 2014,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Bansal_Geometric_Urban_Geo-Localization_2014_CVPR_paper.html",
            "Abstract": "We propose a purely geometric correspondence-free approach to urban geo-localization using 3D point-ray features extracted from the Digital Elevation Map of an urban environment. We derive a novel formulation for estimating the camera pose locus using 3D-to-2D correspondence of a single point and a single direction alone. We show how this allows us to compute putative correspondences between building corners in the DEM and the query image by exhaustively combining pairs of point-ray features. Then, we employ the two-point method to estimate both the camera pose and compute correspondences between buildings in the DEM and the query image. Finally, we show that the computed camera poses can be efficiently ranked by a simple skyline projection step using building edges from the DEM. Our experimental evaluation illustrates the promise of a purely geometric approach to the urban geo-localization problem.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:lM7bPffmjyEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Online self-supervised monocular visual odometry for ground vehicles",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7139928/",
            "Abstract": "This paper presents an online self-supervised approach to monocular visual odometry and ground classification applied to ground vehicles. We solve the motion and structure problem based on a constrained kinematic model. The true scale of the monocular scene is recovered by estimating the ground surface. We consider a general parametric ground surface model and use the Random Sample Consensus (RANSAC) algorithm for robust fitting of the parameters. The estimated ground surface provides training samples to learn a probabilistic appearance-based ground classifier in an online and self-supervised manner. The appearance-based classifier is then used to bias the RANSAC sampling to generate better hypotheses for parameter estimation of the ground surface model. Thus, without relying on any prior information, we combine geometric estimates with appearance-based classification to achieve an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:CXI6bF9CpJ4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Ordinal depth supervision for 3d human pose estimation",
            "Publication year": 2018,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.html",
            "Abstract": "Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:adFjLtLvNj0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spatiotemporal Orientation Simplifies 3D Reconstruction",
            "Publication year": 2003,
            "Publication url": "https://scholar.google.com/scholar?cluster=11236913802854477665&hl=en&oi=scholarr",
            "Abstract": "We present a new approach to volumetric scene reconstruction which can produce accurate models from turntable image sequences. Instead of an epipolar plane image (EPI) volume, we consider a function on the 4D spatiotemporal volume valued with the intensity back projection of the camera (time) to the particular voxel. Using an optical flow technique we compute the local orientation of this spatiotemporal image and decide on occupancy based on the relative orientation between the viewing ray of the voxel at the particular time and the local image structure. Our method does not require a background compensation like the silhouette-based methods and is comparable in performance with space carving. The visual quality in rendering of the established model is dramatically increased by using the number of occupancy candidates for a voxel along all cameras as a transparency measure.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:wbdj-CoPYUoC",
            "Publisher": "Tech. Rep. MS-CIS-03-09, Dept. of Computer and Information Science"
        },
        {
            "Title": "A new 3D orientation steerable filter",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-59802-9_26",
            "Abstract": "In this paper we present a new filter based on Gaussian functions for the extraction of local 3D orientation information. Compared with current 3D steerability approaches our method achieves higher orientation resolution with lower complexity. This property enables us to solve challenging problems like complex surface analysis and multiple motion estimation. This new method decomposes a sphere with a set of overlapping basis filters which are isotropic in the feature space. We study the problem of non-uniform distribution of the spherical coordinates and discuss the application of a weighting compensation function in the computation of the 3D orientation signature. Comparisons show that our method is more efficient and robust than the SD Hough transform.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "TLIO: Tight learned inertial odometry",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9134860/",
            "Abstract": "In this letter we propose a tightly-coupled Extended Kalman Filter framework for IMU-only state estimation. Strap-down IMU measurements provide relative state estimates based on IMU kinematic motion model. However the integration of measurements is sensitive to sensor bias and noise, causing significant drift within seconds. Recent research by Yan  et al.  (RoNIN) and Chen  et al.  (IONet) showed the capability of using trained neural networks to obtain accurate 2D displacement estimates from segments of IMU data and obtained good position estimates from concatenating them. This letter demonstrates a network that regresses 3D displacement estimates and its uncertainty, giving us the ability to tightly fuse the relative state measurement into a stochastic cloning EKF to solve for pose, velocity and sensor biases. We show that our network, trained with pedestrian data from a headset, can produce statistically \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:2PBQaVm3t-0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Computer Vision--ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part V",
            "Publication year": 2010,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=-1xsCQAAQBAJ&oi=fnd&pg=PR3&dq=info:OFQbHZqRQJ8J:scholar.google.com&ots=E-QtJE8p5J&sig=4fKcSjyc_XRn7tFSxlDL20gJRxo",
            "Abstract": "The 2010 edition of the European Conference on Computer Vision was held in Heraklion, Crete. The call for papers attracted an absolute record of 1,174 submissions. We describe here the selection of the accepted papers: Thirty-eight area chairs were selected coming from Europe (18), USA and Canada (16), and Asia (4). Their selection was based on the following criteria:(1) Researchers who had served at least two times as Area Chairs within the past two years at major vision conferences were excluded;(2) Researchers who served as Area Chairs at the 2010 Computer Vision and Pattern Recognition were also excluded (exception: ECCV 2012 Program Chairs);(3) Minimization of overlap introduced by Area Chairs being former student and advisors;(4) 20% of the Area Chairs had never served before in a major conference;(5) The Area Chair selection process made all possible efforts to achieve a reasonable geographic distribution between countries, thematic areas and trends in computer vision. Each Area Chair was assigned by the Program Chairs between 28\u201332 papers. Based on paper content, the Area Chair recommended up to seven potential reviewers per paper. Such assignment was made using all reviewers in the database including the conflicting ones. The Program Chairs manually entered the missing conflict domains of approximately 300 reviewers. Based on the recommendation of the Area Chairs, three reviewers were selected per paper (with at least one being of the top three suggestions), with 99.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:OcT3jDimY5MC",
            "Publisher": "Springer"
        },
        {
            "Title": "Linear solutions for visual augmented reality registration",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/970533/",
            "Abstract": "Correct registration of virtual objects into real scenes requires robust estimation of camera pose. Since most augmented reality applications also require real-time performance in potentially restricted environments with no a priori motion model, we seek pose estimation algorithms which are fast, perform well with few reference objects and require no initialization. We present a pair of linear pose estimation algorithms for arbitrary point and line correspondences and demonstrate their suitability for augmented reality applications.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Signal Analysis and Geometry in Immersive Sensing.",
            "Publication year": 2002,
            "Publication url": "https://scholar.google.com/scholar?cluster=15022536266646211733&hl=en&oi=scholarr",
            "Abstract": "Immersive visualization is rapidly becoming very popular with the dissemination of platforms enabling switching among viewpoints and directions. While it might look as a pure graphics problem if the content is virtual, it is a problem of immersive visual sensing if we visualize a real world and in real time.Immersive sensing is best described by the notion of the plenoptic function. In this talk I will start with describing tele-immersion, a system that amplifies the sense of co-presence in the Internet. Then, I will provide ways to analyze samplings of the plenoptic function beyond the traditional perspective plane starting from omnidirectional systems with a single viewpoint.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:kRWSkSYxWN8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Vision-based localization of leader-follower formations",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1582227/",
            "Abstract": "This paper focuses on the localization problem for a mobile camera network. In particular, we consider the case of leader-follower formations of nonholonomic mobile vehicles equipped with vision sensors which provide only the bearing to the other robots. We prove a sufficient condition for observability and show that recursive estimation enables a leader-follower formation if the leader is not trapped in an unobservable configuration. We employ an Extended Kalman Filter for the estimation of each follower position and orientation with respect to the leader and we adopt a feedback linearizing control strategy to achieve a desired formation. Simulation results in a noisy environment are provided.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Reconstructing and analyzing periodic human motion from stationary monocular views",
            "Publication year": 2012,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1077314212000513",
            "Abstract": "We have shown previously that it is possible to accurately reconstruct periodic motions in 3D from a single camera view, using periodicity as a physical constraint from which to perform geometric inference. In this paper we explore the suitability of the reconstruction techniques for real human motion. We examine the degree of periodicity of human gait empirically, and develop algorithmic tools to address some of the challenges arising from this type of motion, including reconstructing motions that deviate from pure periodicity, properly handling the trajectories of multiple points on an articulated body, and proposing a distance function for measuring the difference between two reconstructions. Importantly, we illustrate the usefulness of these techniques by applying them to the tasks of view-invariant activity classification, clinical gait analysis and person identification.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:nrtMV_XWKgEC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Joint spectral correspondence for disparate image matching",
            "Publication year": 2013,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2013/html/Bansal_Joint_Spectral_Correspondence_2013_CVPR_paper.html",
            "Abstract": "We address the problem of matching images with disparate appearance arising from factors like dramatic illumination (day vs. night), age (historic vs. new) and rendering style differences. The lack of local intensity or gradient patterns in these images makes the application of pixellevel descriptors like SIFT infeasible. We propose a novel formulation for detecting and matching persistent features between such images by analyzing the eigen-spectrum of the joint image graph constructed from all the pixels in the two images. We show experimental results of our approach on a public dataset of challenging image pairs and demonstrate significant performance improvements over state-of-the-art.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:0N-VGjzr574C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning to reconstruct 3D human pose and shape via model-fitting in the loop",
            "Publication year": 2019,
            "Publication url": "http://openaccess.thecvf.com/content_ICCV_2019/html/Kolotouros_Learning_to_Reconstruct_3D_Human_Pose_and_Shape_via_Model-Fitting_ICCV_2019_paper.html",
            "Abstract": "Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:9cqGXoYeWaMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Motion equivariant networks for event cameras with the temporal normalization transform",
            "Publication year": 2019,
            "Publication url": "https://arxiv.org/abs/1902.06820",
            "Abstract": "In this work, we propose a novel transformation for events from an event camera that is equivariant to optical flow under convolutions in the 3-D spatiotemporal domain. Events are generated by changes in the image, which are typically due to motion, either of the camera or the scene. As a result, different motions result in a different set of events. For learning based tasks based on a static scene such as classification which directly use the events, we must either rely on the learning method to learn the underlying object distinct from the motion, or to memorize all possible motions for each object with extensive data augmentation. Instead, we propose a novel transformation of the input event data which normalizes the  and  positions by the timestamp of each event. We show that this transformation generates a representation of the events that is equivariant to this motion when the optical flow is constant, allowing a deep neural network to learn the classification task without the need for expensive data augmentation. We test our method on the event based N-MNIST dataset, as well as a novel dataset N-MOVING-MNIST, with significantly more variety in motion compared to the standard N-MNIST dataset. In all sequences, we demonstrate that our transformed network is able to achieve similar or better performance compared to a network with a standard volumetric event input, and performs significantly better when the test set has a larger set of motions than seen at training.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:gFcjaLVeiroC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Discovering and Achieving Goals via World Models",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2110.09514",
            "Abstract": "How can artificial agents learn to solve many diverse tasks in complex visual environments in the absence of any supervision? We decompose this question into two problems: discovering new goals and learning to reliably achieve them. We introduce Latent Explorer Achiever (LEXA), a unified solution to these that learns a world model from image inputs and uses it to train an explorer and an achiever policy from imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. After the unsupervised phase, LEXA solves tasks specified as goal images zero-shot without any additional learning. LEXA substantially outperforms previous approaches to unsupervised goal-reaching, both on prior benchmarks and on a new challenging benchmark with a total of 40 test tasks spanning across four standard robotic manipulation and locomotion domains. LEXA further achieves goals that require interacting with multiple objects in sequence. Finally, to demonstrate the scalability and generality of LEXA, we train a single general agent across four distinct environments. Code and videos at https://orybkin.github.io/lexa/",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:k6hhvAYhr9EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "The Multi Vehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
            "Publication year": 2018,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2018arXiv180110202Z/abstract",
            "Abstract": "Event based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of 3D perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this paper, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale images and IMU \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:C-GuzCveMkwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Visual Servoing of Quadrotors for Perching by Hanging from Cylindrical Objects",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7347389/",
            "Abstract": "This letter addresses vision-based localization and servoing for quadrotors to enable autonomous perching by hanging from cylindrical structures using only a monocular camera. We focus on the problems of relative pose estimation, control, and trajectory planning for maneuvering a robot relative to cylinders with unknown orientations. We first develop a geometric model that describes the pose of the robot relative to a cylinder. Then, we derive the dynamics of the system, expressed in terms of the image features. Based on the dynamics, we present a controller, which guarantees asymptotic convergence to the desired image space coordinates. Finally, we develop an effective method to plan dynamically feasible trajectories in the image space, and we provide experimental results to demonstrate the proposed method under different operating conditions such as hovering, trajectory tracking, and perching.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:7LPp1NROPxwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Distributed coordination of dynamic rigid bodies",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4434851/",
            "Abstract": "This paper provides a design methodology to construct a set of distributed control laws for a group of rigid bodies moving in 3D space. The motion of the rigid bodies is restricted by a nonholonomic constraint that prohibits the agents from spinning around their velocity vectors. The connections between two seemingly different flocking control laws presented by Tanner et al. [1] and by Justh and Krishnaprasad [2] are studied, and it is shown that they are actually the same laws expressed in different coordinate systems. The former is expressed in the fixed world frame, whereas the latter is expressed in the moving body frame.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:HoB7MX3m0LUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Single image pop-up from discriminatively learned parts",
            "Publication year": 2015,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Single_Image_Pop-Up_ICCV_2015_paper.html",
            "Abstract": "We introduce a new approach for estimating a fine grained 3D shape and continuous pose of an object from a single image. Given a training set of view exemplars, we learn and select appearance-based discriminative parts which are mapped onto the 3D model through a facility location optimization. The training set of 3D models is summarized into a set of basis shapes from which we can generalize by linear combination. Given a test image, we detect hypotheses for each part. The main challenge is to select from these hypotheses and compute the 3D pose and shape coefficients at the same time. To achieve this, we optimize a function that considers simultaneously the appearance matching of the parts as well as the geometric reprojection error. We apply the alternating direction method of multipliers (ADMM) to minimize the resulting convex function. Our main and novel contribution is the simultaneous solution for part localization and detailed 3D geometry estimation by maximizing both appearance and geometric compatibility with convex relaxation.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:hUq98zRk74IC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Robust and scalable transmission of arbitrary 3D models over wireless networks",
            "Publication year": 2008,
            "Publication url": "https://link.springer.com/content/pdf/10.1155/2008/890482.pdf",
            "Abstract": "We describe transmission of 3D objects represented by texture and mesh over unreliable networks, extending our earlier work for regular mesh structure to arbitrary meshes and considering linear versus cubic interpolation. Our approach to arbitrary meshes considers stripification of the mesh and distributing nearby vertices into different packets, combined with a strategy that does not need texture or mesh packets to be retransmitted. Only the valence (connectivity) packets need to be retransmitted; however, storage of valence information requires only 10% space compared to vertices and even less compared to photorealistic texture. Thus, less than 5% of the packets may need to be retransmitted in the worst case to allow our algorithm to successfully reconstruct an acceptable object under severe packet loss. Even though packet loss during transmission has received limited research attention in the past \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:fPk4N6BV_jEC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "The role of vision in perching and grasping for MAVs",
            "Publication year": 2016,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9836/98361S/The-role-of-vision-in-perching-and-grasping-for-MAVs/10.1117/12.2224056.short",
            "Abstract": "In this work, we provide an overview of vision-based control for perching and grasping for Micro Aerial Vehicles. We investigate perching on  at, inclined, or vertical surfaces as well as visual servoing techniques for quadrotors to enable autonomous perching by hanging from cylindrical structures using only a monocular camera and an appropriate gripper. The challenges of visual servoing are discussed, and we focus on the problems of relative pose estimation, control, and trajectory planning for maneuvering a robot with respect to an object of interest. Finally, we discuss future challenges to achieve fully autonomous perching and grasping in more realistic scenarios.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:22N0J9dj6kwC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "MEVO: Multi-environment stereo visual odometry",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6943270/",
            "Abstract": "The ego motion estimation from an image sequence, commonly known as visual odometry, has been thoroughly studied in recent years. Different solutions have been developed depending on the particular scenario the system interacts in. In highly textured environments point features are abundant and visual odometry approaches focus on complementary steps, such as sparse bundle adjustment or keyframe techniques, to improve the accuracy of the motion estimation. In textureless scenarios, the absence of point features motivates the use of different image features. Lines have proven to be an interesting alternative to points in man-made environments, but very few visual odometry approaches have been developed using these types of features. Moreover, the combination of point and line features has not been considered in the development of real-time visual odometry algorithms. In this paper, we explore the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:_Xy5tTOxz_oC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Technical report: Reactive semantic planning in unexplored semantic environments using deep perceptual feedback",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2002.12349",
            "Abstract": "This paper presents a reactive planning system that enriches the topological representation of an environment with a tightly integrated semantic representation, achieved by incorporating and exploiting advances in deep perceptual learning and probabilistic semantic reasoning. Our architecture combines object detection with semantic SLAM, affording robust, reactive logical as well as geometric planning in unexplored environments. Moreover, by incorporating a human mesh estimation algorithm, our system is capable of reacting and responding in real time to semantically labeled human motions and gestures. New formal results allow tracking of suitably non-adversarial moving targets, while maintaining the same collision avoidance guarantees. We suggest the empirical utility of the proposed control architecture with a numerical study including comparisons with a state-of-the-art dynamic replanning algorithm, and physical implementation on both a wheeled and legged platform in different settings with both geometric and semantic goals.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Z4TH09HQ3SoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Distal protection guidewire with nitinol core",
            "Publication year": 2004,
            "Publication url": "https://patents.google.com/patent/US20040116831A1/en",
            "Abstract": "A guidewire having a proximal section, a distal section, and a transition section is disclosed. In one exemplary embodiment of the present invention, the proximal section may be formed of a relatively stiff, inelastic material, whereas the distal section may be formed of a relatively flexible, elastic material having super-elastic properties. A coupling member may be placed adjacent to the transition section to secure the proximal and distal sections together.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Jxy3h8XkNu0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "EV-FlowNet: Self-supervised optical flow estimation for event-based cameras",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1802.06898",
            "Abstract": "Event-based cameras have shown great promise in a variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new class of hand crafted algorithms. Deep learning has shown great success in providing model free solutions to many problems in the vision community, but existing networks have been developed with frame based images in mind, and there does not exist the wealth of labeled data for events as there does for images for supervised training. To these points, we present EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras. In particular, we introduce an image based representation of a given event stream, which is fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured from the same camera at the same time as the events are then used as a supervisory signal to provide a loss function at training time, given the estimated flow from the network. We show that the resulting network is able to accurately predict optical flow from events only in a variety of different scenes, with performance competitive to image based networks. This method not only allows for accurate estimation of dense optical flow, but also provides a framework for the transfer of other self-supervised methods to the event-based domain.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:hcF2OqvMasEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ALGORITHMS FOR DISTRIBUTED AND MOBILE SENSING Ibrahim VolkanIs. ler",
            "Publication year": 2004,
            "Publication url": "https://www-users.cs.umn.edu/~isler/pub/thesis/dissertation.pdf",
            "Abstract": "The desire to extend one\u2019s presence and sense distant worlds dwells at the depths of the human spirit. Since their earliest days, humans have tried numerous methods to fulfill this desire. A common method to extend one\u2019s presence is to use an agent (eg another person) to sense a distant environment. This, of course, requires a medium through which one can communicate with this agent. So far, most of the effort in this design has concentrated on establishing a communication medium. Earlier methods include using pigeons to carry, or smoke signals to decode messages between two parties. In 1837, Samuel Morse invented the telegraph, marking the beginning of the modern information age. Today, using wireless communication, we can transfer enormous amounts of data through the air and even space.Even though the search for better communication media still continues, recently part of the scientific community has turned its attention to the other component\u2013the sensing agent. It is desirable to replace humans as sensing agents for various reasons. First of all, it is difficult to deploy them to many environments such as other planets, ocean bottoms, and volcanos. The second reason is best explained by an example. Consider the task of monitoring forest fires. This is a sensing task where we need to observe a very large area and the events we would like to detect happen",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:abG-DnoFyZgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised learning of sensorimotor affordances by stochastic future prediction",
            "Publication year": 2018,
            "Publication url": "https://scholar.google.com/scholar?cluster=9518797574983900220&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:0VGYH9MJNTkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Precise dispensing of liquids using visual feedback",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8202301/",
            "Abstract": "Robotic pouring is an important step in improving the safety, productivity and repeatability in the biotechnology industry and generally increasing the effectiveness of robotics in human based environments. In this work we present a method to autonomously dispense a precise amount of fluid using only visual feedback without using precision pouring instruments such as pipettes, syringes or pourers. We model circular and rectangular pouring container geometries. We prove that for square containers we can control the flow by only observing the fluid height in the receiving beaker. We show a systematic approach using a hybrid control scheme that is robust to the initial amount of fluid in the pouring container and inconsistent flow. Specifically we present (a) a model for pouring (b) a model based algorithm to drive a robot arm (c) visual feedback for regulating the pouring rate. We demonstrate this using the Rethink \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:IZcclEPD2KUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Skewness of Gabor wavelets and source signal separation",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-45333-4_34",
            "Abstract": "Responses of Gabor wavelets in the mid-frequency space build a local spectral representation scheme with optimal properties regarding the time-frequency uncertainty principle. However, when using Gabor wavelets we observe a skewness in the mid-frequency space caused by the spreading effect of Gabor wavelets. Though in most current applications the skewness does not obstruct the sampling of the spectral domain, it affects the identification and separation of source signals from the filter response in the mid-frequency space. In this paper, we present a modification of the original Gabor filter, the skew Gabor filter, which corrects skewness so that the filter response can be described with a sum-of-Gaussians model in the mid-frequency space. The correction further enables us to use higher-order moment information to separate different source signal components. This provides us with an elegant \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:vRqMK49ujn8C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Closed form solutions for reconstruction via complex analysis",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/article/10.1023/A:1008381724192",
            "Abstract": "We address the problem of control-based recovery of robot pose and environmental lay-out. Panoramic sensors provide a 1D projection of characteristic features of a 2D operation map. Trajectories of these projections contain information about the position of a priori unknown landmarks in the environment. We introduce the notion of spatiotemporal signatures of projection trajectories. These signatures are global measures, characterized by considerably higher robustness with respect to noise and outliers than the commonly applied point correspondence. By modeling the 2D motion plane as the complex plane we show that by means of complex analysis the reconstruction problem can be reduced to a quadratic\u2014or even linear in some cases\u2014equation. The algorithm is tested in simulations and in a real experiment.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:ldfaerwXgEUC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Omnidirectional Sensing for Robot Control",
            "Publication year": 2003,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-36224-X_12",
            "Abstract": "Most of today\u2019s mobile robots are equipped with some kind of omnidirectional camera. The advantages of such sensors in tasks like navigation, homing, appearance-based localization cannot be overlooked. In this paper, we address the basic questions of how to process omnidirectional signals, how to describe the intrinsic geometry of omnidirectional cameras with a single viewpoint, how to infer 3D motion, and how to place omnidirectional sensors efficiently to guarantee complete coverage.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:B3FOqHPlNUQC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Visual Navigation with Android Tablets",
            "Publication year": 2012,
            "Publication url": "https://rosap.ntl.bts.gov/view/dot/36536",
            "Abstract": "The goal of this project is to develop a visual navigation system for use in vehicles using Android tablets. Current vehicle navigation techniques rely solely on static road maps and noisy Global Positioning System (GPS) data. This approach is prone to errors where GPS information is not available, such as urban canyons. The author proposes to use a windshield mounted, Android tablet as a sensor platform for augmenting in-vehicle navigation. The author begins by detecting and tracking salient features over time from the color camera images. Combined with the accelerometer and gyroscope data, one can distinguish between static (background) and dynamic (foreground) features in the images. The static features are then used to compute the visual odometry (motion of the vehicle) and produce a 3D model of the environment. This information can then be used to augment the navigation system with local, real-time information and overlays. Ultimately the author aims to create an Android application that helps to make drivers more aware of their environments and promotes safety and focus while driving. To this end, the author is developing algorithms to carry out key operations which include: detecting the road and lane markings, identifying other vehicles on the road, recognizing pedestrians and cyclists, and identifying important road features such as stop lights, stop signs, and speed limits. Using moving object detection and visual odometry, the application will compute and monitor distances to other vehicles, pedestrians and cyclists nearby. This information can in turn be used to alert the driver of a potentially hazardous situation such as an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:m1cs02wJCiwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spin-weighted spherical cnns",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2006.10731",
            "Abstract": "Learning equivariant representations is a promising way to reduce sample and model complexity and improve the generalization performance of deep neural networks. The spherical CNNs are successful examples, producing SO(3)-equivariant representations of spherical inputs. There are two main types of spherical CNNs. The first type lifts the inputs to functions on the rotation group SO(3) and applies convolutions on the group, which are computationally expensive since SO(3) has one extra dimension. The second type applies convolutions directly on the sphere, which are limited to zonal (isotropic) filters, and thus have limited expressivity. In this paper, we present a new type of spherical CNN that allows anisotropic filters in an efficient way, without ever leaving the spherical domain. The key idea is to consider spin-weighted spherical functions, which were introduced in physics in the study of gravitational waves. These are complex-valued functions on the sphere whose phases change upon rotation. We define a convolution between spin-weighted functions and build a CNN based on it. The spin-weighted functions can also be interpreted as spherical vector fields, allowing applications to tasks where the inputs or outputs are vector fields. Experiments show that our method outperforms previous methods on tasks like classification of spherical images, classification of 3D shapes and semantic segmentation of spherical panoramas.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:dnWPDgH667kC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Topological map from only visual orientation information using omnidirectional cameras",
            "Publication year": 2010,
            "Publication url": "http://www.dei.unipd.it/~emg/omniRoboVis2010/OmniRoboVis2010/Program_files/omnirobovis2010_submission_5.pdf",
            "Abstract": "In this paper we present a new way to compute a topological map using only orientation information. We exploit the natural presence of lines in man-made environments in dominant directions. We extract all the image lines present in the scene acquired by an omnidirectional system composed of 6 aligned cameras. From the parallel lines we robustly compute the three dominant directions using vanishing points. With this information we are able to align the camera with respect to the scene and to identify the turns in the trajectory. Assuming a Manhattan world where the changes of heading in the navigation are related by multiples 90 degrees. We also use geometrical image-pair constraints as a tool to identify the visual traversable nodes that compose our topological map. Experiments with an indoor sequence have been performed to validate this approach.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:dshw04ExmUIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Catadioptric projective geometry",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/article/10.1023/A:1013610201135",
            "Abstract": "Catadioptric sensors are devices which utilize mirrors and lenses to form a projection onto the image plane of a camera. Central catadioptric sensors are the class of these devices having a single effective viewpoint. In this paper, we propose a unifying model for the projective geometry induced by these devices and we study its properties as well as its practical implications. We show that a central catadioptric projection is equivalent to a two-step mapping via the sphere. The second step is equivalent to a stereographic projection in the case of parabolic mirrors. Conventional lens-based perspective cameras are also central catadioptric devices with a virtual planar mirror and are, thus, covered by the unifying model. We prove that for each catadioptric projection there exists a dual catadioptric projection based on the duality between points and line images (conics). It turns out that planar and parabolic mirrors \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:d1gkVwhDpl0C",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Fundamental matrix for cameras with radial distortion",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1541312/",
            "Abstract": "When deploying a heterogeneous camera network or when we use cheap zoom cameras like in cell-phones, it is not practical, if not impossible to off-line calibrate the radial distortion of each camera using reference objects. It is rather desirable to have an automatic procedure without strong assumptions about the scene. In this paper, we present a new algorithm for estimating the epipolar geometry of two views where the two views can be radially distorted with different distortion factors. It is the first algorithm in the literature solving the case of different distortion in the left and right view linearly and without assuming the existence of lines in the scene. Points in the projective plane are lifted to a quadric in three-dimensional projective space. A radial distortion of the projective plane results to a matrix transformation in the space of lifted coordinates. The new epipolar constraint depends linearly on a 4 /spl times/ 4 radial \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:_FxGoFyzp5QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Novel Representations, Methods, and Algorithms in Computer Vision",
            "Publication year": 2013,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s11263-013-0628-x.pdf",
            "Abstract": "This special issue presents five articles presenting foundational contributions to computer vision covering a broad spectrum of the field: image compositing, segmentation, optimization of higher order MRFs and structure from motion. The articles are characterized by their clarity and rigor and offer an opportunity to the reader to really learn about new methodologies in the field. The three first papers operate at the pixel level while the two last papers are rather geometric using points as features.The first paper \u201cError-tolerant Image Compositing\u201d by Tao et al. addresses the problem of assembling more than one images through a gradient-based approach. The contributions of the authors lie on the definition of a new nearly integrable gradient domain where the integration is weighted such that significant residuals appear in non-visible regions. Computationally, the solution is similar to sparse least squares resulting from \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:i2xiXl-TujoC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Unsupervised Event-based Optical Flow using Motion Compensation",
            "Publication year": 2018,
            "Publication url": "http://openaccess.thecvf.com/content_eccv_2018_workshops/w36/html/Zhu_Unsupervised_Event-based_Optical_Flow_using_Motion_Compensation_ECCVW_2018_paper.html",
            "Abstract": "In this work, we propose a novel framework for unsupervised learning for event cameras that learns to predict optical flow from only the event stream. In particular, we propose an input representation of the events in the form of a discretized 3D volume, which we pass through a neural network to predict the optical flow for each event. This optical flow is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We evaluate this network on the Multi Vehicle Stereo Event Camera dataset (MVSEC), along with qualitative results from a variety of different scenes.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:lQh10hhnIEIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Optimal aspect ratio under vergence for 3D TV",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4547845/",
            "Abstract": "In this paper we extend our earlier work on determining optimal aspect ratio of 3D display devices considering the vergence of the eyes of a viewer. We show that a small vergence angle modifies earlier results, without vergence, by a scaling factor on one of the error terms and also affects the viewing volume. Experimental results showing how the vergence angle influences the aspect ratio are outlined.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:vV6vV6tmYwMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2109.13396",
            "Abstract": "Robot learning holds the promise of learning policies that generalize broadly. However, such generalization requires sufficiently diverse datasets of the task of interest, which can be prohibitively expensive to collect. In other fields, such as computer vision, it is common to utilize shared, reusable datasets, such as ImageNet, to overcome this challenge, but this has proven difficult in robotics. In this paper, we ask: what would it take to enable practical data reuse in robotics for end-to-end skill learning? We hypothesize that the key is to use datasets with multiple tasks and multiple domains, such that a new user that wants to train their robot to perform a new task in a new domain can include this dataset in their training process and benefit from cross-task and cross-domain generalization. To evaluate this hypothesis, we collect a large multi-domain and multi-task dataset, with 7,200 demonstrations constituting 71 tasks across 10 environments, and empirically study how this data can improve the learning of new tasks in new environments. We find that jointly training with the proposed dataset and 50 demonstrations of a never-before-seen task in a new domain on average leads to a 2x improvement in success rate compared to using target domain data alone. We also find that data for only a few tasks in a new domain can bridge the domain gap and make it possible for a robot to perform a variety of prior tasks that were only seen in other domains. These results suggest that reusing diverse multi-task and multi-domain datasets, including our open-source dataset, may pave the way for broader robot generalization, eliminating the need to re-collect data for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:m5Mwo8ouzesC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Correspondenceless structure from motion",
            "Publication year": 2007,
            "Publication url": "https://repository.upenn.edu/cis_papers/348/",
            "Abstract": "We present a novel approach for the estimation of 3D-motion directly from two images using the Radon transform. The feasibility of any camera motion is computed by integrating over all feature pairs that satisfy the epipolar constraint. This integration is equivalent to taking the inner product of a similarity function on feature pairs with a Dirac function embedding the epipolar constraint. The maxima in this five dimensional motion space will correspond to compatible rigid motions. The main novelty is in the realization that the Radon transform is a filtering operator: If we assume that the similarity and Dirac functions are defined on spheres and the epipolar constraint is a group action of rotations on spheres, then the Radon transform is a correlation integral. We propose a new algorithm to compute this integral from the spherical Fourier transform of the similarity and Dirac functions. Generating the similarity function now becomes a preprocessing step which reduces the complexity of the Radon computation by a factor equal to the number of feature pairs processed. The strength of the algorithm is in avoiding a commitment to correspondences, thus being robust to erroneous feature detection, outliers, and multiple motions.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:sYWh8IhQ1GMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Convolutional mesh regression for single-image human shape reconstruction",
            "Publication year": 2019,
            "Publication url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kolotouros_Convolutional_Mesh_Regression_for_Single-Image_Human_Shape_Reconstruction_CVPR_2019_paper.html",
            "Abstract": "This paper addresses the problem of 3D human pose and shape estimation from a single image. Previous approaches consider a parametric model of the human body, SMPL, and attempt to regress the model parameters that give rise to a mesh consistent with image evidence. This parameter regression has been a very challenging task, with model-based approaches underperforming compared to nonparametric solutions in terms of pose estimation. In our work, we propose to relax this heavy reliance on the model's parameter space. We still retain the topology of the SMPL template mesh, but instead of predicting model parameters, we directly regress the 3D location of the mesh vertices. This is a heavy task for a typical network, but our key insight is that the regression becomes significantly easier using a Graph-CNN. This architecture allows us to explicitly encode the template mesh structure within the network and leverage the spatial locality the mesh has to offer. Image-based features are attached to the mesh vertices and the Graph-CNN is responsible to process them on the mesh structure, while the regression target for each vertex is its 3D location. Having recovered the complete 3D geometry of the mesh, if we still require a specific model parametrization, this can be reliably regressed from the vertices locations. We demonstrate the flexibility and the effectiveness of our proposed graph-based mesh regression by attaching different types of features on the mesh vertices. In all cases, we outperform the comparable baselines relying on model parameter regression, while we also achieve state-of-the-art results among model-based pose \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:nFloTcPoiwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Learning event-based height from plane and parallax",
            "Publication year": 2019,
            "Publication url": "http://openaccess.thecvf.com/content_CVPRW_2019/html/EventVision/Chaney_Learning_Event-Based_Height_From_Plane_and_Parallax_CVPRW_2019_paper.html",
            "Abstract": "Event-based cameras are a novel asynchronous sensing modality that provides exciting benefits, such as the ability to track fast moving objects with no motion blur and low latency, high dynamic range, and low power consumption. Given the low latency of the cameras, as well as their ability to work in challenging lighting conditions, these cameras are a natural fit for reactive problems such as fast local structure estimation. In this work, we propose a fast method to perform structure estimation for vehicles traveling in a roughly 2D environment (eg in an environment with a ground plane). Our method transfers the method of plane and parallax to events, which, given the homography to a ground plane and the pose of the camera, generates a warping of the events which removes the optical flow for events on the ground plane, while inducing flow for events above the ground plane. We then estimate dense flow in this warped space using a self-supervised neural network, which provides the height of all points in the scene. We evaluate our method on the Multi Vehicle Stereo Event Camera dataset, and show its ability to rapidly estimate the scene structure both at high speeds and in low lighting conditions.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:B4wWq2ztVNgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Active end-effector pose selection for tactile object recognition through monte carlo tree search",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8206161/",
            "Abstract": "This paper considers the problem of active object recognition using touch only. The focus is on adaptively selecting a sequence of wrist poses that achieves accurate recognition by enclosure grasps. It seeks to minimize the number of touches and maximize recognition confidence. The actions are formulated as wrist poses relative to each other, making the algorithm independent of absolute workspace coordinates. The optimal sequence is approximated by Monte Carlo tree search. We demonstrate results in a physics engine and on a real robot. In the physics engine, most object instances were recognized in at most 16 grasps. On a real robot, our method recognized objects in 2-9 grasps and outperformed a greedy baseline.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:TXgqPU86QykC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A survey on rotation optimization in structure from motion",
            "Publication year": 2016,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Tron_A_Survey_on_CVPR_2016_paper.html",
            "Abstract": "We consider the problem of robust rotation optimization in Structure from Motion applications. A number of different approaches have been recently proposed, with solutions that are at times incompatible, and at times complementary. The goal of this paper is to survey and compare these ideas in a unified manner, and to benchmark their robustness against the presence of outliers. In all, we have tested more than forty variants of a these methods (including novel ones), and we find the best performing combination.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:n5u26LFhhPsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Compression of Stereo Disparity Streams Using Wavelets and Optical Flow",
            "Publication year": 2001,
            "Publication url": "https://repository.upenn.edu/cis_reports/71/",
            "Abstract": "Recent advances in computing have enabled fast reconstructions of dynamic scenes from multiple images. However, the efficient coding of changing 3D-data has hardly been addressed. Progressive geometric compression and streaming are based on static data sets which are mostly artificial or obtained from accurate range sensors. In this paper, we present a system for efficient coding of 3D-data which are given in forms of 2+ 1/2 disparity maps. Disparity maps are spatially coded using wavelets and temporally predicted by computing flow. The resulted representation of a 3D-stream consists then of spatial wavelet coefficients, optical flow vectors, and disparity differences between predicted and incoming image. The approach has also very useful by-products: disparity predictions can significantly reduce the disparity search range and if appropriately modeled increase the accuracy of depth estimation.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:D_sINldO8mEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Linear pose estimation from points or lines",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1195992/",
            "Abstract": "Estimation of camera pose from an image of n points or lines with known correspondence is a thoroughly studied problem in computer vision. Most solutions are iterative and depend on nonlinear optimization of some geometric constraint, either on the world coordinates or on the projections to the image plane. For real-time applications, we are interested in linear or closed-form solutions free of initialization. We present a general framework which allows for a novel set of linear solutions to the pose estimation problem for both n points and n lines. We then analyze the sensitivity of our solutions to image noise and show that the sensitivity analysis can be used as a conservative predictor of error for our algorithms. We present a number of simulations which compare our results to two other recent linear algorithms, as well as to iterative approaches. We conclude with tests on real imagery in an augmented reality setup.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:tkaPQYYpVKoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Geometric properties of central catadioptric projections",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/10722492_15",
            "Abstract": "In this paper we consider all imaging systems that consist of reflective and refractive components \u2013called catadioptric\u2013 and possessing a unique effective viewpoint. Conventional cameras are a special case of such systems if we imagine a planar mirror in front of them. We show that all unique viewpoint catadioptric systems can be modeled with a two-step projection: a central projection to the sphere followed by a projection from the sphere to an image plane. Special cases of this equivalence are parabolic projection, for which the second map is a stereographic projection, and perspective projection, for which the second map is central projection. Certain pairs of catadioptric projections are dual by the mapping which takes conics in the image plane to their foci. The foci of line images are points of another, dual, catadioptric projection; and vice versa, points in the image are foci of line images in the dual \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:bFI3QPDXJZMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Tutorials",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7785070/",
            "Abstract": "These tutorials discuss the following: 3D object geometry from single image; large-scale 3D modeling from crowdsourced data; semantic and structured 3D modeling; and understanding 3D and visuo-motor learning.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:CtYknXOfbFEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "MSG-cal: Multi-sensor graph-based calibration",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7353889/",
            "Abstract": "We present a system for determining a global solution for the relative poses between multiple sensors with different modalities and varying fields of view. The final calibration result produces a tree of transforms rooted at a single sensor that allows the fusion of the sensor streams into a shared coordinate frame. The method differs from other approaches by handling any number of sensors with only minimal constraints on their fields of view, producing a global solution that is better than any pairwise solution, and by simplifying the data collection process through automatic data association.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:R3JqVFXIqpYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Semi-dense visual-inertial odometry and mapping for quadrotors with swap constraints",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8463163/",
            "Abstract": "Micro Aerial Vehicles have the potential to assist humans in real life tasks involving applications such as smart homes, search and rescue, and architecture construction. To enhance autonomous navigation capabilities these vehicles need to be able to create dense 3D maps of the environment, while concurrently estimating their own motion. In this paper, we are particularly interested in small vehicles that can navigate cluttered indoor environments. We address the problem of visual inertial state estimation, control and 3D mapping on platforms with Size, Weight, And Power (SWAP) constraints. The proposed approach is validated through experimental results on a 250 g, 22 cm diameter quadrotor equipped only with a stereo camera and an IMU with a computationally-limited CPU showing the ability to autonomously navigate, while concurrently creating a 3D map of the environment.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:H1aCVKaixnMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Active deformable part models inference",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-10584-0_19",
            "Abstract": "This paper presents an active approach for part-based object detection, which optimizes the order of part filter evaluations and the time at which to stop and make a prediction. Statistics, describing the part responses, are learned from training data and are used to formalize the part scheduling problem as an offline optimization. Dynamic programming is applied to obtain a policy, which balances the number of part evaluations with the classification accuracy. During inference, the policy is used as a look-up table to choose the part order and the stopping time based on the observed filter responses. The method is faster than cascade detection with deformable part models (which does not optimize the part order) with negligible loss in accuracy when evaluated on the PASCAL VOC 2007 and 2010 datasets.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:viYOxJONeN0C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Methods, systems, and computer readable media for estimation of optical flow, depth, and egomotion using neural network trained using event-based learning",
            "Publication year": 2020,
            "Publication url": "https://patents.google.com/patent/US20200265590A1/en",
            "Abstract": "A method for prediction of an indication of motion using input from an event-based camera includes receiving events captured by an event-based camera, wherein each of the events represents a location of a change in pixel intensity, a polarity of the change, and a time. The method further includes discretizing the events into time discretized event volumes, each of which contain events that occur within a specified time range. The method further includes providing the time discretized event volumes as input to an encoder-decoder neural network trained to predict an indication of motion using a loss function that measures quality of image deblurring; generating, using the neural network, a prediction of the indication of motion. The method further includes using the prediction of the indication of motion in a machine vision application.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:5y95FQUaxGgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "VC-dimension of exterior visibility",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1273987/",
            "Abstract": "In this paper, we study the Vapnik-Chervonenkis (VC)-dimension of set systems arising in 2D polygonal and 3D polyhedral configurations where a subset consists of all points visible from one camera. In the past, it has been shown that the VC-dimension of planar visibility systems is bounded by 23 if the cameras are allowed to be anywhere inside a polygon without holes. Here, we consider the case of exterior visibility, where the cameras lie on a constrained area outside the polygon and have to observe the entire boundary. We present results for the cases of cameras lying on a circle containing a polygon (VC-dimension=2) or lying outside the convex hull of a polygon (VC-dimension=5). The main result of this paper concerns the 3D case: We prove that the VC-dimension is unbounded if the cameras lie on a sphere containing the polyhedron, hence the term exterior visibility.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "The multivehicle stereo event camera dataset: An event camera dataset for 3D perception",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8288670/",
            "Abstract": "Event-based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range, and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of three-dimensional perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this letter, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car, and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:Kaaf24wrr50C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Structure and motion from uncalibrated catadioptric views",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/990487/",
            "Abstract": "In this paper we present a new algorithm for structure from motion from point correspondences in images taken from uncalibrated catadioptric cameras with parabolic mirrors. We assume that the unknown intrinsic parameters are three: the combined focal length of the mirror and lens and the intersection of the optical axis with the image. We introduce a new representation for images of points and lines in catadioptric images which we call the circle space. This circle space includes imaginary circles, one of which is the image of the absolute conic. We formulate the epipolar constraint in this space and establish a new 4/spl times/4 catadioptric fundamental matrix. We show that the image of the absolute conic belongs to the kernel of this matrix. This enables us to prove that Euclidean reconstruction is feasible from two views with constant parameters and from three views with varying parameters. In both cases, it is one \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:qjMakFHDy7sC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Geo-localization of street views with aerial image databases",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2072298.2071954",
            "Abstract": "We study the feasibility of solving the challenging problem of geolocalizing ground level images in urban areas with respect to a database of images captured from the air such as satellite and oblique aerial images. We observe that comprehensive aerial image databases are widely available while complete coverage of urban areas from the ground is at best spotty. As a result, localization of ground level imagery with respect to aerial collections is a technically important and practically significant problem. We exploit two key insights:(1) satellite image to oblique aerial image correspondences are used to extract building facades, and (2) building facades are matched between oblique aerial and ground images for geo-localization. Key contributions include:(1) A novel method for extracting building facades using building outlines;(2) Correspondence of building facades between oblique aerial and ground images \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:35r97b3x0nAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Spike-flownet: event-based optical flow estimation with energy-efficient hybrid neural networks",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-58526-6_22",
            "Abstract": "Event-based cameras display great potential for a variety of tasks such as high-speed motion detection and navigation in low-light environments where conventional frame-based cameras suffer critically. This is attributed to their high temporal resolution, high dynamic range, and low-power consumption. However, conventional computer vision methods as well as deep Analog Neural Networks (ANNs) are not suited to work well with the asynchronous and discrete nature of event camera outputs. Spiking Neural Networks (SNNs) serve as ideal paradigms to handle event camera outputs, but deep SNNs suffer in terms of performance due to the spike vanishing phenomenon. To overcome these issues, we present Spike-FlowNet, a deep hybrid neural network architecture integrating SNNs and ANNs for efficiently estimating optical flow from sparse event camera outputs without sacrificing the performance. The \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:m1aD9PlKDecC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Paracatadioptric camera calibration",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1000241/",
            "Abstract": "Catadioptric sensors refer to the combination of lens-based devices and reflective surfaces. These systems are useful because they may have a field of view which is greater than hemispherical, providing the ability to simultaneously view in any direction. Configurations which have a unique effective viewpoint are of primary interest, among these is the case where the reflective surface is a parabolic mirror and the camera is such that it induces an orthographic projection and which we call paracatadioptric. We present an algorithm for the calibration of such a device using only the images of lines in space. In fact, we show that we may obtain all of the intrinsic parameters from the images of only three lines and that this is possible without any metric information. We propose a closed-form solution for focal length, image center, and aspect ratio for skewless cameras and a polynomial root solution in the presence of skew \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:_B80troHkn4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Modellfreie Bewegungsverfolgung durch",
            "Publication year": 2013,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=CjSmBgAAQBAJ&oi=fnd&pg=PA277&dq=info:ZUqloQ93tk0J:scholar.google.com&ots=WFEudcQtSo&sig=aS6R7ZrR78WzDJnLsPQIjKR01Cc",
            "Abstract": "Diese Arbeit beschreibt die aktive Verfolgung eines bewegten Objektes von beliebiger Form mittels Blicksteuerung. Die Verfolgung erfolgt modellfrei durch die Subtraktion des von der Kame-rabewegung induzierten optischen Normalflusses an den Stellen eines hohen Grauwertgradienten. Der von der Verarbeitungsverz\u00f6gerung ge-pr\u00e4gte Regelkreis basiert auf der Pr\u00e4diktion der Objektbewegung mittels eines station\u00e4ren Kalman-Filters und wird einem Achsenregler \u00fcbergeordnet. Die Verarbeitungszeit von 30ms erm\u00f6glicht eine Regelrate von",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:JZsVLox4iN8C",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Keyin: Discovering subgoal structure with keyframe-based video prediction",
            "Publication year": 2019,
            "Publication url": "https://www.seas.upenn.edu/~oleh/KeyIn_paper.pdf",
            "Abstract": "Real-world image sequences can often be naturally decomposed into a small number of frames depicting interesting, highly stochastic moments (its keyframes) and the low-variance frames in between them. In image sequences depicting trajectories to a goal, keyframes can be seen as capturing the subgoals of the sequence as they depict the high-variance moments of interest that ultimately led to the goal. In this paper, we introduce a video prediction model that discovers the keyframe structure of image sequences in an unsupervised fashion. We do so using a hierarchical Keyframe-Intermediate model (KEYIN) that stochastically predicts keyframes and their offsets in time and then uses these predictions to deterministically predict the intermediate frames. We propose a differentiable formulation of this problem that allows us to train the full hierarchical model using a sequence reconstruction loss. We show that our model is able to find meaningful keyframe structure in a simulated dataset of robotic demonstrations and that these keyframes can serve as subgoals for planning. Our model outperforms other hierarchical prediction approaches for planning on a simulated pushing task.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:qaiyjGHpP8sC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Curvature-Driven Probabilistic Strategy for Transmission of Arbitrary 3D Meshes over Unreliable Networks",
            "Publication year": 2008,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.506.4883&rep=rep1&type=pdf",
            "Abstract": "Packet loss affects the receiving quality of 3D meshes transmitted over unreliable networks. While some applications are able to tolerate higher loss, others may need to restrict the loss below a specified level. In this work we describe a curvature-driven probabilistic strategy to control the adverse impact of packet loss. Critical mesh features, with high curvature, like sharp edges and corners are allocated more bandwidth to increase the rate of their successful transmission. When the probability of visual degradation caused by lost critical features exceeds an acceptable level, a group of curvature indices is added to the transmission pipeline and stored in packets different from those containing the critical features. The size of the indices is governed by three parameters: The mesh resolution, the minimum required quality and the tolerance. We incorporate this new strategy with an earlier interleaved transmission approach. Experimental results show that the reconstructed meshes using the integrated strategy have higher visual quality.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:9Nmd_mFXekcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Vision-based distributed coordination and flocking of multi-agent systems",
            "Publication year": 2005,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6780&rep=rep1&type=pdf",
            "Abstract": "We propose a biologically inspired, distributed co-ordination scheme based on nearest-neighbor interactions for a set of mobile kinematic agents equipped with vision sensors. It is assumed that each agent is only capable of measuring the following three quantities relative to each of its nearest neighbors (as defined by a proximity graph): time-to-collision, a single optical flow vector and relative bearing. We prove that the proposed distributed control law results in alignment of headings and flocking, even when the topology of the proximity graph representing the interconnection changes with time. It is shown that when the proximity graph is\u201d jointly connected\u201d over time, flocking and velocity alignment will occur. Furthermore, the distributed control law can be extended to the case where the agents follow a leader. Under similar connectivity assumptions, we prove that the headings converge to that of the leader. Simulations are presented to demonstrate the effectiveness of this approach.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:5aszrCQfcYQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A unifying theory for central panoramic systems and practical implications",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-45053-X_29",
            "Abstract": "Omnidirectional vision systems can provide panoramic alertness in surveillance, improve navigational capabilities, and produce panoramic images for multimedia. Catadioptric realizations of omnidirectional vision combine reflective surfaces and lenses. A particular class of them, the central panoramic systems, preserve the uniqueness of the projection viewpoint. In fact, every central projection system including the well known perspective projection on a plane falls into this category.In this paper, we provide a unifying theory for all central catadioptric systems. We show that all of them are isomorphic to projective mappings from the sphere to a plane with a projection center on the perpendicular to the plane. Subcases are the stereographic projection equivalent to parabolic projection and the central planar projection equivalent to every conventional camera. We define a duality among projections of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:u-x6o8ySG0sC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Vision-based, distributed control laws for motion coordination of nonholonomic robots",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5071250/",
            "Abstract": "In this paper, we study the problem of distributed motion coordination among a group of nonholonomic ground robots. We develop vision-based control laws for parallel and balanced circular formations using a consensus approach. The proposed control laws are distributed in the sense that they require information only from neighboring robots. Furthermore, the control laws are coordinate-free and do not rely on measurement or communication of heading information among neighbors but instead require measurements of bearing, optical flow, and time to collision, all of which can be measured using visual sensors. Collision-avoidance capabilities are added to the team members, and the effectiveness of the control laws are demonstrated on a group of mobile robots.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Zph67rFs4hoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2109.02740",
            "Abstract": "We address the problem of estimating the shape of a person's head, defined as the geometry of the complete head surface, from a video taken with a single moving camera, and determining the alignment of the fitted 3D head for all video frames, irrespective of the person's pose. 3D head reconstructions commonly tend to focus on perfecting the face reconstruction, leaving the scalp to a statistical approximation. Our goal is to reconstruct the head model of each person to enable future mixed reality applications. To do this, we recover a dense 3D reconstruction and camera information via structure-from-motion and multi-view stereo. These are then used in a new two-stage fitting process to recover the 3D head shape by iteratively fitting a 3D morphable model of the head with the dense reconstruction in canonical space and fitting it to each person's head, using both traditional facial landmarks and scalp features extracted from the head's segmentation mask. Our approach recovers consistent geometry for varying head shapes, from videos taken by different people, with different smartphones, and in a variety of environments from living rooms to outdoor spaces.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:6E5OHDUOeTQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Mesh Optimization Guided by Just-Noticeable-Difference and Stereo Discretization",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4379483/",
            "Abstract": "Although network speed has been increased over the years, bandwidth limitation continues to pose challenges to online applications requiring dynamic visualization. This is because the demand for instantaneous multimedia content far surpasses the network capability. It is therefore beneficial to suppress redundant data which does not contribute to visual quality in order to optimize resource utilization. When modeling photorealistic objects in 3D space, joint transmission of texture and mesh is required. In this paper, we propose an optimization strategy to reduce mesh data by transmitting only data perceptible to the human visual system. This strategy is motivated by the experimental finding on the just-noticeable-difference approach and the concept of stereo vision.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:geHnlv5EZngC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Unsupervised event-based learning of optical flow, depth, and egomotion",
            "Publication year": 2019,
            "Publication url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_Egomotion_CVPR_2019_paper.html",
            "Abstract": "In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Ecsxi449JjsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Distributed Control and Estimation of Robotic Vehicle Networks, Part 2",
            "Publication year": 2016,
            "Publication url": "https://scholar.google.com/scholar?cluster=1206735201402562979&hl=en&oi=scholarr",
            "Abstract": "Han-Lim Choi in July 2014. One of the key motivations for or ganizing this workshop and the special issues was the observation that specialized research efforts on distributed control and estimation within the control and robotics communities have not been well co ordinated with each other in recent years. One goal of these spe cial issues is help close the knowledge gaps between specialists in these two communities with the aim of helping both researchers and practitioners better appreciate the funda mental connections between distributed control, planning, per ception, and decision making for multive hicle networks. The introduction, written by the special issue editors Nisar Ahmed from the University of Colorado, Boul der, Jorge Cortes from UC San D\u00edego, and Sonia Mart\u00ednez from UC San Di ego, provides a detailed analysis of these knowledge gaps and identifies key technical challenges that remain in the areas of distributed planning, estimation, and control. The first of the four articles is \u201cA Distributed Optimization Framework for Localization and Formation Con trol: Applications to VisionBased Mea surements,\u201d by Roberto Tron, Justin Thomas, Giuseppe Loianno, Kostas Daniilidis, and Vijay Kumar, which considers two central problems in multiagent systems: mutual localiza tion (estimating the pose of each static agent with respect to a common ref erence frame) and formation control (maneuvering the agents to achieve a",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:s9piBQ-TX4wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A game-theoretic approach to robust fusion and kalman filtering under unknown correlations",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7963339/",
            "Abstract": "This work addresses the problem of fusing two random vectors with unknown cross-correlations. We present a formulation and a numerical method for computing the optimal estimate in the minimax sense. We extend our formulation to linear measurement models that depend on two random vectors with unknown cross-correlations. As an application we consider the problem of decentralized state estimation. The proposed estimator takes cross-correlations into account while being less conservative than the widely used Covariance Intersection. We demonstrate the superiority of the proposed method compared to Covariance Intersection with numerical examples and simulations within the specific application of decentralized state estimation.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:fhMX52Pd6iQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automated system for semantic object labeling with soft-object recognition and dynamic programming segmentation",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7795205/",
            "Abstract": "This paper presents an automated robotic system for generating semantic maps of inventory in retail environments. In retail settings, semantic maps are labeled maps of stores where each discrete section of shelving is assigned a department label describing the types of products on that shelf. Starting from a metric map of the store, the robot autonomously extracts the shelf boundaries, generates a distance-optimal tour of the store to view every shelf, and follows the tour while avoiding unmapped clutter and moving people. The robot creates a point cloud of the store using the data collected from this tour. We introduce a novel soft-object assignment algorithm to create a virtual map and a dynamic programming algorithm to segment this map. These algorithms use a priori information about the products to boost data from laser and camera sensors in order to recognize and semantically label objects. The primary \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:ZgPQhQxLujAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A distributed optimization approach to consistent multiway matching",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8619511/",
            "Abstract": "Multiway matching refers to the problem of establishing correspondences among a set of images from noisy pairwise correspondences, typically by exploiting cycle-consistency. Existing approaches for multiway matching address the problem in a centralized setting. In this work, we propose a novel distributed optimization approach to multiway matching based on distributed projected gradient descent with constant step size. We rigorously analyze the convergence properties of our algorithm, specifically the range of the step size that guarantees convergence to a stationary point. We provide experimental evidence supporting that the proposed approach has performance comparable with the state of the art centralized approaches.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:sFUlmsclzkgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Grasping surfaces of revolution: Simultaneous pose and shape recovery from two views",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7139366/",
            "Abstract": "In many scenarios, robots encounter rotationally symmetric objects for which no known 3D model exists. To be able to grasp such objects using existing grasp point computation schemes, an estimate of their 3D-pose and shape is necessary. In this paper, we address the problem of recovering 3D-pose and shape of an unknown surface of revolution from two perspective views of known relative orientation. We propose a new algorithm for simultaneous estimation of pose and shape without making use of any cross-sections or bi-tangent points needed by other approaches. Our algorithm builds upon existing single-view SOR reconstruction approaches and couples the pose estimation and reconstruction process. Pose is optimized to minimize discrepancies between reconstructed shapes from two views. Our method works even in the presence of only one of the two apparent contours of a surface of revolution. We \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:QjNCP7ux8QYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Coarse-to-fine volumetric prediction for single-image 3D human pose",
            "Publication year": 2017,
            "Publication url": "http://openaccess.thecvf.com/content_cvpr_2017/html/Pavlakos_Coarse-To-Fine_Volumetric_Prediction_CVPR_2017_paper.html",
            "Abstract": "This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach outperforms all state-of-the-art methods on standard benchmarks achieving a relative error reduction greater than 30% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our end-to-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:-1RNHcZo4Y8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Mirrors in motion: Epipolar geometry and motion estimation",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1238426/",
            "Abstract": "In this paper we consider the images taken from pairs of parabolic catadioptric cameras separated by discrete motions. Despite the nonlinearity of the projection model, the epipolar geometry arising from such a system, like the perspective case, can be encoded in a bilinear form, the catadioptric fundamental matrix. We show that all such matrices have equal Lorentzian singular values, and they define a nine-dimensional manifold in the space of 4 /spl times/ 4 matrices. Furthermore, this manifold can be identified with a quotient of two Lie groups. We present a method to estimate a matrix in this space, so as to obtain an estimate of the motion. We show that the estimation procedures are robust to modest deviations from the ideal assumptions.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:W7OEmFMy1HYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "INSTITUT FUR INFORMATIK UND PRAKTISCHE MATHEMATIK",
            "Publication year": 2000,
            "Publication url": "https://www.informatik.uni-kiel.de/inf/Sommer/doc/TechnicalReports/2000_tr2008.pdf",
            "Abstract": "In this paper, we study the characterization of multiple motions from the standpoint of orientation in spatiotemporal volume. Using the fact that multiple motions are equivalent to multiple planes in the derivative space or in the spectral domain, we apply a new 3D steerable filter for motion estimation. This new method is based on the decomposition of the sphere with a set of overlapping basis filters in the feature space. It is superior to principal axis analysis based approaches and current 3D steerability approaches in achieving higher orientation resolution. Our approach is more efficient and robust than a similar spatiotemporal Hough transform and outperforms existing EM algorithms applied in the derivative space.In occlusion estimation, we use an eigenvalue analysis based multi-window strategy to detect and to eliminate outliers in the derivative space. This technique purifies input data and improves therefore the precision of the estimation results. Furthermore, based on the spatial coherence in image sequences we use the \u201cshift-and-subtract\u201d technique to localize occlusion boundaries and to track their movement in occlusion sequences. Our technique can be also used to distinguish occlusion from transparency and to decompose transparency scenes into multi-layers.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:xCS0c1RzPY8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multi-camera reconstruction based on surface normal estimation and best viewpoint selection",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1335388/",
            "Abstract": "We present a new algorithm for reconstructing an environment from images recorded by multiple calibrated cameras. Multiple camera systems challenge traditional stereo algorithms in many issues including view registration, selection of commonly visible image parts for matching, and the fact that surfaces are imaged differently from different viewpoints and poses. On the other hand, multiple cameras have the advantage of revealing surfaces at occluding contours and covering wide areas. The presented algorithm makes no assumption on camera loci and outputs an occupancy voxel grid, with occupied voxels being accompanied by a surface normal. It is correlation-based, however, outperforms the conventional correlation-based approach in reconstruction quality. It is highly parallelizable, and most importantly, is robust against artifacts due to camera registration errors that are typically encountered when using \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:L8Ckcad2t8MC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Multispectral skin color modeling",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/991023/",
            "Abstract": "The automated detection of humans in computer vision as well as the realistic rendering of people in computer graphics necessitates improved modeling of the human skin color. We describe the acquisition and modeling of skin reflectance data densely sampled over the entire visible spectrum. The data collected through a spectrograph allows us to explain skin color (and its variations) and to discriminate between human skin and dyes designed to mimic human skin. We study the approximation of these data using several sets of basis functions. Our study shows that skin reflectance data can best be approximated by a linear combination of Gaussians or their first derivatives. This result has a significant practical impact on optical acquisition devices: the entire visible spectrum of skin reflectance can now be captured with a few filters of optimally chosen central wavelengths and bandwidth.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:YsMSGLbcyi4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Motor algebra for 3D kinematics: The case of the hand-eye calibration",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/article/10.1023/A:1026567812984",
            "Abstract": "In this paper we apply the Clifford geometric algebra for solving problems of visually guided robotics. In particular, using the algebra of motors we model the 3D rigid motion transformation of points, lines and planes useful for computer vision and robotics. The effectiveness of the Clifford algebra representation is illustrated by the example of the hand-eye calibration. It is shown that the problem of the hand-eye calibration is equivalent to the estimation of motion of lines. The authors developed a new linear algorithm which estimates simultaneously translation and rotation as components of rigid motion.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Se3iqnhoufwC",
            "Publisher": "Kluwer Academic Publishers"
        },
        {
            "Title": "Learning predictive models from observation and interaction",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/978-3-030-58565-5_42.pdf",
            "Abstract": "Learning predictive models from interaction with the world allows an agent, such as a robot, to learn about how the world works, and then use this learned model to plan coordinated sequences of actions to bring about desired outcomes. However, learning a model that captures the dynamics of complex skills represents a major challenge: if the agent needs a good model to perform these skills, it might never be able to collect the experience on its own that is required to learn these delicate and complex behaviors. Instead, we can imagine augmenting the training set with observational data of other agents, such as humans. Such data is likely more plentiful, but cannot always be combined with data from the original agent. For example, videos of humans might show a robot how to use a tool, but (i) are not annotated with suitable robot actions, and (ii) contain a systematic distributional shift due to the embodiment \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:YW9K3tL-BTUC",
            "Publisher": "Springer International Publishing"
        },
        {
            "Title": "Vision-based formation control of aerial vehicles",
            "Publication year": 2014,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.671.9055&rep=rep1&type=pdf",
            "Abstract": "We propose a general solution for the problem of distributed, vision-based formation control of aerial vehicles. Our solution is based on pure bearing measurements, optionally augmented with the corresponding distances. As opposed to the state of the art, our control law does not require auxiliary distance measurements or estimators, it can be applied to leaderless or leaderbased formations with arbitrary topologies, and it has global convergence guarantees. We validate our approach through simulations and experiments on a platform of three quadrotors.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:4Q5OFK1iulkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Low-Rank Matrix Approximation Approach to Multiway Matching with Applications in Multi-Sensory Data Association",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9196583/",
            "Abstract": "Consider the case of multiple visual sensors perceiving the same scene from different viewpoints. In order to achieve consistent visual perception, the problem of data association, in this case establishing correspondences between observed features, must be first solved. In this work, we consider multiway matching which is a specific instance of multi-sensory data association. Multiway matching refers to the problem of establishing correspondences among a set of images from noisy pairwise correspondences, typically by exploiting cycle- consistency. We propose a novel optimization-based formulation of multiway matching problem as a nonconvex low-rank matrix approximation problem. We propose two novel algorithms for numerically solving the problem at hand. The first one is an algorithm based on the Alternating Direction Method of Multipliers (ADMM). The second one is a Riemannian trust- region method \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:WIzaTCs-0dQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Two efficient solutions for visual odometry using directional correspondence",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6095561/",
            "Abstract": "This paper presents two new, efficient solutions to the two-view, relative pose problem from three image point correspondences and one common reference direction. This three-plus-one problem can be used either as a substitute for the classic five-point algorithm, using a vanishing point for the reference direction, or to make use of an inertial measurement unit commonly available on robots and mobile devices where the gravity vector becomes the reference direction. We provide a simple, closed-form solution and a solution based on algebraic geometry which offers numerical advantages. In addition, we introduce a new method for computing visual odometry with RANSAC and four point correspondences per hypothesis. In a set of real experiments, we demonstrate the power of our approach by comparing it to the five-point method in a hypothesize-and-test visual odometry setting.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:eflP2zaiRacC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Vision-based localization and control of leader-follower formations",
            "Publication year": 2008,
            "Publication url": "https://www.cis.upenn.edu/~kostas/mypub.dir/mariottini08ro.pdf",
            "Abstract": "The paper deals with vision-based localization and control of leader-follower formations of unicycle robots. Each robot is equipped with a panoramic camera which only provides the view-angle to the other robots. As an original contribution, the localization problem is analytically studied using a new observability condition valid for general nonlinear systems and based on the Extended Output Jacobian matrix. The state of the leader-follower system, estimated via the extended Kalman filter, is used by an input-state feedback control law to stabilize the formation. Simulations as well as experimental results validate the theoretical results and show the effectiveness of the proposed design.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:FcH-RsB9iB0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Monocap: Monocular human motion capture using a cnn coupled with a geometric prior",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8316924/",
            "Abstract": "Recovering 3D full-body human pose is a challenging problem with many applications. It has been successfully addressed by motion capture systems with body worn markers and multiple cameras. In this paper, we address the more challenging case of not only using a single camera but also not leveraging markers: going directly from 2D appearance to 3D geometry. Deep learning approaches have shown remarkable abilities to discriminatively learn 2D appearance features. The missing piece is how to integrate 2D, 3D, and temporal information to recover 3D geometry and account for the uncertainties arising from the discriminative model. We introduce a novel approach that treats 2D joint locations as latent variables whose uncertainty distributions are given by a deep fully convolutional neural network. The unknown 3D poses are modeled by a sparse representation and the 3D parameter estimates are realized \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:JQG40wivBEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Distributed 3-D Bearing-Only Orientation Localization",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9030234/",
            "Abstract": "We propose a method to recover the orientation of a set of agents with respect to a global reference frame using local bearing measurements alone. Our method is distributed, does not require prior rotation information, and considers the full 3-D version of the problem. We identify sufficient localizability conditions on the directed graph of measurements, propose an algorithm based on distributed Riemannian gradient descent to recover a localization, and verify our theoretical results with simulations.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:mSu_8wZzne8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Image processing in catadioptric planes: Spatiotemporal derivatives and optical flow computation",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1044483/",
            "Abstract": "Images produced by catadioptric sensors contain a significant amount of radial distortion and variation in inherent scale. Blind application of conventional shift-invariant operators or optical flow estimators yields erroneous results. One could argue that given a calibration of such a sensor we would always be able to remove distortions and apply any operator in a local perspective plane. In addition to the inefficiency of such an approach, interpolation effects during warping have undesired results in filtering. In this paper, we propose to use the sphere as the underlying domain of image processing in central catadioptric systems. This does not mean that we will warp the catadioptric image into a spherical image. Instead, we will formulate all the operations on the sphere but use the samples from the original catadioptric plane. As an example, we study the convolution with the Gaussian and its derivatives and as well as \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:Y0pCki6q_DkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Vision-based localization for leader\u2013follower formation control",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5299228/",
            "Abstract": "This paper deals with vision-based localization for leader-follower formation control. Each unicycle robot is equipped with a panoramic camera that only provides the view angle to the other robots. The localization problem is studied using a new observability condition valid for general nonlinear systems and based on the extended output Jacobian. This allows us to identify those robot motions that preserve the system observability and those that render it nonobservable. The state of the leader-follower system is estimated via the extended Kalman filter, and an input-state feedback control law is designed to stabilize the formation. Simulations and real-data experiments confirm the theoretical results and show the effectiveness of the proposed formation control.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:RHpTSmoSYBkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Vision-based distributed coordination and flocking of multi-agent systems",
            "Publication year": 2005,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6780&rep=rep1&type=pdf",
            "Abstract": "We propose a biologically inspired, distributed co-ordination scheme based on nearest-neighbor interactions for a set of mobile kinematic agents equipped with vision sensors. It is assumed that each agent is only capable of measuring the following three quantities relative to each of its nearest neighbors (as defined by a proximity graph): time-to-collision, a single optical flow vector and relative bearing. We prove that the proposed distributed control law results in alignment of headings and flocking, even when the topology of the proximity graph representing the interconnection changes with time. It is shown that when the proximity graph is\u201d jointly connected\u201d over time, flocking and velocity alignment will occur. Furthermore, the distributed control law can be extended to the case where the agents follow a leader. Under similar connectivity assumptions, we prove that the headings converge to that of the leader. Simulations are presented to demonstrate the effectiveness of this approach.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:mVmsd5A6BfQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A Single-perspective Novel Panoramic View from Radially Distorted Non-central Images.",
            "Publication year": 2007,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.421.852&rep=rep1&type=pdf",
            "Abstract": "In this paper, we propose an image-based technique for panoramic novelview generation using three uncalibrated wide-angle images as its input. State of the art in novel view generation presumes the calibration and removal of radial distortion or any other deformation resulting from the geometry of a non-central camera. We propose a method which replaces this calibration with the assumption that the epipole corresponding to the novel viewpoint is at the center of radial distortion and that it is known.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:eJXPG6dFmWUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Multi-granularity representations for human inter-actions: Pose, motion and intention",
            "Publication year": 2013,
            "Publication url": "https://search.proquest.com/openview/16762b04a1d262270365218489ea5b0d/1?pq-origsite=gscholar&cbl=18750",
            "Abstract": "Tracking people and their body pose in videos is a central problem in computer vision. Standard tracking representations reason about temporal coherence of detected people and body parts. They have difficulty tracking targets under partial occlusions or rare body poses, where detectors often fail, since the number of training examples is often too small to deal with the exponential variability of such configurations.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:1zNUifcpCKoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Semi-dense visual-inertial odometry and mapping for computationally constrained platforms",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/article/10.1007/s10514-021-10002-z",
            "Abstract": "In this paper we present a direct semi-dense stereo Visual-Inertial Odometry (VIO) algorithm enabling autonomous flight for quadrotor systems with Size, Weight, and Power (SWaP) constraints. The proposed approach is validated through experiments on a 250 g, 22 cm diameter quadrotor equipped with a stereo camera and an IMU. Semi-dense methods have superior performance in low texture areas, which are often encountered in robotic tasks such as infrastructure inspection. However, due to the measurement size and iterative nonlinear optimization, these methods are computationally more expensive. As the scale of the platform shrinks down, the available computation of the on-board CPU becomes limited, making autonomous navigation using optimization-based semi-dense tracking a hard problem. We show that our direct semi-dense VIO performs comparably to other state-of-the-art methods, while taking \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:1lB6hEDIqXYC",
            "Publisher": "Springer US"
        },
        {
            "Title": "Probabilistic data association for semantic slam",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7989203/",
            "Abstract": "Traditional approaches to simultaneous localization and mapping (SLAM) rely on low-level geometric features such as points, lines, and planes. They are unable to assign semantic labels to landmarks observed in the environment. Furthermore, loop closure recognition based on low-level features is often viewpoint-dependent and subject to failure in ambiguous or repetitive environments. On the other hand, object recognition methods can infer landmark classes and scales, resulting in a small set of easily recognizable landmarks, ideal for view-independent unambiguous loop closure. In a map with several objects of the same class, however, a crucial data association problem exists. While data association and recognition are discrete problems usually solved using discrete inference, classical SLAM is a continuous optimization over metric information. In this paper, we formulate an optimization problem over \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:foJkpVhfThEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Texturepose: Supervising human mesh estimation with texture consistency",
            "Publication year": 2019,
            "Publication url": "http://openaccess.thecvf.com/content_ICCV_2019/html/Pavlakos_TexturePose_Supervising_Human_Mesh_Estimation_With_Texture_Consistency_ICCV_2019_paper.html",
            "Abstract": "This work addresses the problem of model-based human pose estimation. Recent approaches have made significant progress towards regressing the parameters of parametric human body models directly from images. Because of the absence of images with 3D shape ground truth, relevant approaches rely on 2D annotations or sophisticated architecture designs. In this work, we advocate that there are more cues we can leverage, which are available for free in natural images, ie, without getting more annotations, or modifying the network architecture. We propose a natural form of supervision, that capitalizes on the appearance constancy of a person among different frames (or viewpoints). This seemingly insignificant and often overlooked cue goes a long way for model-based pose estimation. The parametric model we employ allows us to compute a texture map for each frame. Assuming that the texture of the person does not change dramatically between frames, we can apply a novel texture consistency loss, which enforces that each point in the texture map has the same texture value across all frames. Since the texture is transferred in this common texture map space, no camera motion computation is necessary, or even an assumption of smoothness among frames. This makes our proposed supervision applicable in a variety of settings, ranging from monocular video, to multi-view images. We benchmark our approach against strong baselines that require the same or even more annotations that we do and we consistently outperform them. Simultaneously, we achieve state-of-the-art results among model-based pose estimation approaches in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:K9zgXSuleLYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Direct 3d-rotation estimation from spherical images via a generalized shift theorem",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1211473/",
            "Abstract": "Omnidirectional images arising from 3D-motion of a camera contain persistent structures over a large variation of motions because of their large field of view. This persistence made appearance-based methods attractive for robot localization given reference views. Assuming that central omnidirectional images can be mapped to the sphere, the question is what are the underlying mappings of the sphere that can reflect a rotational camera motion. Given such a mapping, we propose a systematic way for finding invariance and the mapping parameters themselves based on the generalization of the Fourier transform. Using results from representation theory, we can generalize the Fourier transform to any homogeneous space with a transitively acting group. Such a case is the sphere with rotation as the acting group. The spherical harmonics of an image pair are related to each other through a shift theorem involving the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:WF5omc3nYNoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "18 Distributed Control and Estimation of Robotic Vehicle Networks",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7515328/",
            "Abstract": "MISSION STATEMENT AND SCOPE: As the official means of communication for the IEEE Control Systems Society, IEEE Control Systems Magazine publishes interesting, useful, and informative material on all aspects of control system technology for the benefit of control educators, practitioners, and researchers. With this mission statement in mind, IEEE Control Systems Magazine encourages submissions, both feature articles and columns, on all aspects of control system technology. SUBMISSION OF MANUSCRIPTS: A feature article typically provides an in-depth treatment of either an application of control technology, a tutorial on some area of control theory, or an innovation in control education. IEEE Control Systems Magazine publishes a variety of columns.\u201cApplications of Control\u201d columns are industrially oriented summaries of innovations in control technology.\u201cFocus on Education\u201d typically describes some \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:Z8CpXElfgm4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A metric parametrization for trifocal tensors with non-colinear pinholes",
            "Publication year": 2015,
            "Publication url": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Leonardos_A_Metric_Parametrization_2015_CVPR_paper.html",
            "Abstract": "The trifocal tensor, which describes the relation between projections of points and lines in three views, is a fundamental entity of geometric computer vision. In this work, we investigate a new parametrization of the trifocal tensor for calibrated cameras with non-colinear pinholes obtained from a quotient Riemannian manifold. We incorporate this formulation into state-of-the art methods for optimization on manifolds, and show, through experiments in pose averaging, that it produces a meaningful way to measure distances between trifocal tensors.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:HFi42EZPDb4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Hybrid control for visibility-based pursuit-evasion games",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1389597/",
            "Abstract": "Pursuit-evasion games in complex environments have a rich but disconnected history. Continuous or differential pursuit-evasion games focus on optimal control methods, and rely on very intense computations in order to provide locally optimal controls. Discrete pursuit-evasion games on graphs are algorithmically much more appealing, but completely ignore the physical dynamics of the players, resulting in possibly infeasible motions. In this paper, we present a provable and algorithmically feasible solution for visibility-based pursuit-evasion games in simply-connected environments, for players with dynamic constraints. This is achieved by combining two recent but distant results.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:j3f4tGmQtD8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The space of essential matrices as a Riemannian quotient manifold",
            "Publication year": 2017,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/16M1091332",
            "Abstract": "The essential matrix, which encodes the epipolar constraint between points in two projective views, is a cornerstone of modern computer vision. Previous works have proposed different characterizations of the space of essential matrices as a Riemannian manifold. However, they either do not consider the symmetric role played by the two views or do not fully take into account the geometric peculiarities of the epipolar constraint. We address these limitations with a characterization as a quotient manifold that can be easily interpreted in terms of camera poses. While our main focus is on theoretical aspects, we include applications to optimization problems in computer vision.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:pqoCwmxD9dQC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Predicting the Future with Transformational States",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1803.09760",
            "Abstract": "An intelligent observer looks at the world and sees not only what is, but what is moving and what can be moved. In other words, the observer sees how the present state of the world can transform in the future. We propose a model that predicts future images by learning to represent the present state and its transformation given only a sequence of images. To do so, we introduce an architecture with a latent state composed of two components designed to capture (i) the present image state and (ii) the transformation between present and future states, respectively. We couple this latent state with a recurrent neural network (RNN) core that predicts future frames by transforming past states into future states by applying the accumulated state transformation with a learned operator. We describe how this model can be integrated into an encoder-decoder convolutional neural network (CNN) architecture that uses weighted residual connections to integrate representations of the past with representations of the future. Qualitatively, our approach generates image sequences that are stable and capture realistic motion over multiple predicted frames, without requiring adversarial training. Quantitatively, our method achieves prediction results comparable to state-of-the-art results on standard image prediction benchmarks (Moving MNIST, KTH, and UCF101).",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:jgTXuCzW-kcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Algorithmics of Motion",
            "Publication year": 2000,
            "Publication url": "https://apps.dtic.mil/sti/citations/ADA390170",
            "Abstract": "The main goal of our research program is to develop the mathematical concepts and algorithms for performing a wide range of operations on motions of physical objects, including capture, representation, synthesis, segmentation, editing, optimization, simulation, and execution. In addition, we have focused on the support tools that are critical for understanding and analyzing physical and synthetic agents interacting in a dynamic environment. These include the underlying data structures, the architecture, and the analysis and synthesis tools. The term Algorithmics of Motion refers to the overarching framework, the mathematical concepts, the algorithms and the tools for this work. This document presents an executive summary of our work over the last five years from October 1995 to September 2000.Descriptors:",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:XD-gHx7UXLsC",
            "Publisher": "MOORE SCHOOL OF ELECTRICAL ENGINEERING PHILADELPHIA PA GRASP LAB"
        },
        {
            "Title": "Seeing Glassware: from Edge Detection to Pose Estimation and Shape Recovery.",
            "Publication year": 2016,
            "Publication url": "https://scholar.archive.org/work/aghcyx2pgfa25ecnqwfkxsp2he/access/wayback/http://roboticsproceedings.org/rss12/p21.pdf",
            "Abstract": "Perception of transparent objects has been an open challenge in robotics despite advances in sensors and datadriven learning approaches. In this paper, we introduce a new approach that combines recent advances in learnt object detectors with perceptual grouping in 2D, and projective geometry of apparent contours in 3D. We train a state of the art structured edge detector on an annotated set of foreground glassware. We assume that we deal with surfaces of revolution (SOR) and apply perceptual symmetry grouping in a 2D spherical transformation of the image to obtain a 2D detection of the glassware object and a hypothesis about its 2D axis. Rather than stopping at a single view detection, we ultimately want to reconstruct the 3D shape of the object and its 3D pose to allow for a robot to grasp it. Using two views allows us to decouple the 3D axis localization from the shape estimation. We develop a parametrization that uniquely relates the shape reconstruction of SOR to given a set of contour points and tangents. Finally, we provide the first annotated dataset for 2D detection, 3D pose and 3D shape of glassware and we show results comparable to category-based detection and localization of opaque objects without any training on the object shape.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:seU1ZbiIO-YC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reactive semantic planning in unexplored semantic environments using deep perceptual feedback",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9113725/",
            "Abstract": "This letter presents a reactive planning system that enriches the topological representation of an environment with a tightly integrated semantic representation, achieved by incorporating and exploiting advances in deep perceptual learning and probabilistic semantic reasoning. Our architecture combines object detection with semantic SLAM, affording robust, reactive logical as well as geometric planning in unexplored environments. Moreover, by incorporating a human mesh estimation algorithm, our system is capable of reacting and responding in real time to semantically labeled human motions and gestures. New formal results allow tracking of suitably non-adversarial moving targets, while maintaining the same collision avoidance guarantees. We suggest the empirical utility of the proposed control architecture with a numerical study including comparisons with a state-of-the-art dynamic replanning algorithm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:KUgzLixe4cAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Shape-based object recognition in videos using 3D synthetic object models",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5206803/",
            "Abstract": "In this paper we address the problem of recognizing moving objects in videos by utilizing synthetic 3D models. We use only the silhouette space of the synthetic models making thus our approach independent of appearance. To deal with the decrease in discriminability in the absence of appearance, we align sequences of object masks from video frames to paths in silhouette space. We extract object silhouettes from video by an integration of feature tracking, motion grouping of tracks, and co-segmentation of successive frames. Subsequently, the object masks from the video are matched to 3D model silhouettes in a robust matching and alignment phase. The result is a matching score for every 3D model to the video, along with a pose alignment of the model to the video. Promising experimental results indicate that a purely shape-based matching scheme driven by synthetic 3D models can be successfully applied for \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:bEWYMUwI8FkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Polar Transformer Networks",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1709.01889",
            "Abstract": "Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-of-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:TLwS_1sUIYkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Scale Space for Camera Invariant Features",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6740835/",
            "Abstract": "In this paper we propose a new approach to compute the scale space of any central projection system, such as catadioptric, fisheye or conventional cameras. Since these systems can be explained using a unified model, the single parameter that defines each type of system is used to automatically compute the corresponding Riemannian metric. This metric, is combined with the partial differential equations framework on manifolds, allows us to compute the Laplace-Beltrami (LB) operator, enabling the computation of the scale space of any central projection system. Scale space is essential for the intrinsic scale selection and neighborhood description in features like SIFT. We perform experiments with synthetic and real images to validate the generalization of our approach to any central projection system. We compare our approach with the best-existing methods showing competitive results in all type of cameras \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:HWVPSj4JXeEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Distributed, bearing-only control laws for circular formations of ground robots",
            "Publication year": 2008,
            "Publication url": "https://scholar.google.com/scholar?cluster=5030064176400953863&hl=en&oi=scholarr",
            "Abstract": "For a group of ground robots, a set of control laws are designed that stabilize the the motion of the group into a balanced circular formation. The control law is distributed in the sense that each robot only needs to \u201csee\u201d its neighbors to compute its input. It is shown that the measurements of bearing angle between the neighboring robots are sufficient for reaching a balance formation. Experimental results with ground robots demonstrate the effectiveness of the bearing-only control laws.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:BqipwSGYUEgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Correspondenceless ego-motion estimation using an imu",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1570657/",
            "Abstract": "Mobile robots can be easily equipped with numerous sensors which can aid in the tasks of localization and ego-motion estimation. Two such examples are Inertial Measurement Units (IMU), which provide a gravity vector via pitch and roll angular velocities, and wide-angle or panoramic imaging devices. As the number of powerful devices on a single robot increases, an important problem arises in how to fuse the information coming from multiple sources to obtain an accurate and efficient motion estimate. The IMU provides real-time readings which can be employed in orientation estimation, while in principle an Omnidirectional camera provides enough information to estimate the full rigid motion (up to translational scale). However, in addition to being computationally overwhelming, such an estimation is traditionally based on the sensitive search for feature correspondences between image frames. In this paper we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:IWHjjKOFINEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Real time trinocular stereo for tele-immersion",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/958284/",
            "Abstract": "Tele-immersion is a technology that augments your space with real-time 3D projections of remote spaces thus facilitating the interaction of people from different places in virtually the same environment. Tele-immersion combines 3D scene recovery from computer vision, and rendering and interaction from computer graphics. We describe the real-time 3D scene acquisition using a new algorithm for trinocular stereo. We extend this method in time by combining motion and stereo in order to increase speed and robustness.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:kNdYIx-mwKoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Automatic alignment of a camera with a line scan lidar system",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5980513/",
            "Abstract": "We propose a new method for extrinsic calibration of a line-scan LIDAR with a perspective projection camera. Our method is a closed-form, minimal solution to the problem. The solution is a symbolic template found via variable elimination and the multi-polynomial Macaulay resultant. It does not require initialization, and can be used in an automatic calibration setting when paired with RANSAC and least-squares refinement. We show the efficacy of our approach through a set of simulations and a real calibration.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:M05iB0D1s5AC",
            "Publisher": "IEEE"
        },
        {
            "Title": "EventGAN: Leveraging Large Scale Image Datasets for Event Cameras",
            "Publication year": 2019,
            "Publication url": "https://ui.adsabs.harvard.edu/abs/2019arXiv191201584Z/abstract",
            "Abstract": "Event cameras provide a number of benefits over traditional cameras, such as the ability to track incredibly fast motions, high dynamic range, and low power consumption. However, their application into computer vision problems, many of which are primarily dominated by deep learning solutions, has been limited by the lack of labeled training data for events. In this work, we propose a method which leverages the existing labeled data for images by simulating events from a pair of temporal image frames, using a convolutional neural network. We train this network on pairs of images and events, using an adversarial discriminator loss and a pair of cycle consistency losses. The cycle consistency losses utilize a pair of pre-trained self-supervised networks which perform optical flow estimation and image reconstruction from events, and constrain our network to generate events which result in accurate outputs from both \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:ohlv4i5L2AoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Dynamic scene understanding: The role of orientation features in space and time in scene classification",
            "Publication year": 2012,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6247815/",
            "Abstract": "Natural scene classification is a fundamental challenge in computer vision. By far, the majority of studies have limited their scope to scenes from single image stills and thereby ignore potentially informative temporal cues. The current paper is concerned with determining the degree of performance gain in considering short videos for recognizing natural scenes. Towards this end, the impact of multiscale orientation measurements on scene classification is systematically investigated, as related to: (i) spatial appearance, (ii) temporal dynamics and (iii) joint spatial appearance and dynamics. These measurements in visual space, x-y, and spacetime, x-y-t, are recovered by a bank of spatiotemporal oriented energy filters. In addition, a new data set is introduced that contains 420 image sequences spanning fourteen scene categories, with temporal scene information due to objects and surfaces decoupled from camera \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:yB1At4FlUx8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fully automatic registration of 3D point clouds",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1640899/",
            "Abstract": "We propose a novel technique for the registration of 3D point clouds which makes very few assumptions: we avoid any manual rough alignment or the use of landmarks, displacement can be arbitrarily large, and the two point sets can have very little overlap. Crude alignment is achieved by estimation of the 3D-rotation from two Extended Gaussian Images even when the data sets inducing them have partial overlap. The technique is based on the correlation of the two EGIs in the Fourier domain and makes use of the spherical and rotational harmonic transforms. For pairs with low overlap which fail a critical verification step, the rotational alignment can be obtained by the alignment of constellation images generated from the EGIs. Rotationally aligned sets are matched by correlation using the Fourier transform of volumetric functions. A fine alignment is acquired in the final step by running Iterative Closest Points with \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:UeHWp8X0CEIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "An Adversarial Objective for Scalable Exploration",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2003.06082",
            "Abstract": "Model-based curiosity combines active learning approaches to optimal sampling with the information gain based incentives for exploration presented in the curiosity literature. Existing model-based curiosity methods look to approximate prediction uncertainty with approaches which struggle to scale to many prediction-planning pipelines used in robotics tasks. We address these scalability issues with an adversarial curiosity method minimizing a score given by a discriminator network. This discriminator is optimized jointly with a prediction model and enables our active learning approach to sample sequences of observations and actions which result in predictions considered the least realistic by the discriminator. We demonstrate progressively increasing advantages as compute is restricted of our adversarial curiosity approach over leading model-based exploration strategies in simulated environments. We further demonstrate the ability of our adversarial curiosity method to scale to a robotic manipulation prediction-planning pipeline where we improve sample efficiency and prediction performance for a domain transfer problem.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:WQTnNU6cpycC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fast, robust, continuous monocular egomotion computation",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7487206/",
            "Abstract": "We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:-1WLWRmjvKAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Learning to Map for Active Semantic Goal Navigation",
            "Publication year": 2021,
            "Publication url": "https://arxiv.org/abs/2106.15648",
            "Abstract": "We consider the problem of object goal navigation in unseen environments. In our view, solving this problem requires learning of contextual semantic priors, a challenging endeavour given the spatial and semantic variability of indoor environments. Current methods learn to implicitly encode these priors through goal-oriented navigation policy functions operating on spatial representations that are limited to the agent's observable areas. In this work, we propose a novel framework that actively learns to generate semantic maps outside the field of view of the agent and leverages the uncertainty over the semantic classes in the unobserved areas to decide on long term goals. We demonstrate that through this spatial prediction strategy, we are able to learn semantic priors in scenes that can be leveraged in unknown environments. Additionally, we show how different objectives can be defined by balancing exploration with exploitation during searching for semantic targets. Our method is validated in the visually realistic environments offered by the Matterport3D dataset and show state of the art results on the object goal navigation task.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:Y3Sh7dCAXz0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "6-dof object pose from semantic keypoints",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7989233/",
            "Abstract": "This paper presents a novel approach to estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training image data. Furthermore, the approach can be applied to instance- and class-based pose recovery. Empirically, we show that the proposed approach can accurately recover the 6-DoF object pose for both instance- and class-based scenarios with a cluttered background. For class-based object pose estimation, state-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:1fM7sLBYaPUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Equivariant multi-view networks",
            "Publication year": 2019,
            "Publication url": "http://openaccess.thecvf.com/content_ICCV_2019/html/Esteves_Equivariant_Multi-View_Networks_ICCV_2019_paper.html",
            "Abstract": "Several popular approaches to 3D vision tasks process multiple views of the input independently with deep neural networks pre-trained on natural images, where view permutation invariance is achieved through a single round of pooling over all views. We argue that this operation discards important information and leads to subpar global descriptors. In this paper, we propose a group convolutional approach to multiple view aggregation where convolutions are performed over a discrete subgroup of the rotation group, enabling, thus, joint reasoning over all views in an equivariant (instead of invariant) fashion, up to the very last layer. We further develop this idea to operate on smaller discrete homogeneous spaces of the rotation group, where a polar view representation is used to maintain equivariance with only a fraction of the number of input views. We set the new state of the art in several large scale 3D shape retrieval tasks, and show additional applications to panoramic scene classification.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:RiW20FJDrgsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Reconstruction of 3D pose for surfaces of revolution from range data",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7335536/",
            "Abstract": "Axial symmetry is a common property of everyday objects. Bottles, cups, cans and bowls, all usually fall in that category and can be modeled by surfaces of revolution (SOR). In this paper, we address the problem of estimating the parameters of an SOR (axis and generatrix) from range data. Although SOR reconstruction from RGB images is well studied, previous works using depth measurements are limited. We propose a formulation similar to the 3D registration problem and our solution is based on an alternating procedure that recovers the complete surface geometry, i.e. The axis and the profile curve of the SOR. We evaluate our method both quantitatively and qualitatively using four different datasets that provide depth images from a large variety of axially symmetric objects.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:7Pm5v_kJJ6IC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Shape-based object classification and recognition through continuum manipulation",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8202193/",
            "Abstract": "We introduce a novel approach to shape-based object classification and recognition through the use of a continuum manipulator. Noticing the fact that when a continuum manipulator wraps around an object in a whole-arm grasping, its own shape is indicative of the shape of the object, our approach enables learning and recognition of object classes based on the shapes of continuum wraps. It offers the following advantages: (1) recognition of objects that are not easily detected by vision, such as transparent objects, and (2) highly efficient recognition of such objects of varied sizes due to high-level and rich shape information in each wrap, unlike recognition based on tactile sensing via conventional grasping. Simulation and experiments demonstrate the effectiveness of our approach.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:yhXl426oeGMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Conformal rectification of omnidirectional stereo pairs",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4624335/",
            "Abstract": "A pair of stereo images are said to be rectified if corresponding image points have the same y-coordinate in their respective images. In this paper we consider the rectification of two omnidirectional cameras, specifically two parabolic catadioptric cameras. Such systems consist of a parabolic mirror and an orthographically projecting lens. We show that if the image coordinates are represented as a point z in the complex plane, then the rectification is specified by coth -1z. This rectification is shown to be conformal, in that it is locally distortionless, and furthermore, it is unique up to scale and transformation. We show an experiment in which two real images have been rectified and a stereo matching performed.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:ULOm3_A8WrAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Normalized cross-correlation for spherical images",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-24671-8_43",
            "Abstract": "Recent advances in vision systems have spawned a new generation of image modalities. Most of today\u2019s robot vehicles are equipped with omnidirectional sensors which facilitate navigation as well as immersive visualization. When an omnidirectional camera with a single viewpoint is calibrated, the original image can be warped to a spherical image. In this paper, we study the problem of template matching in spherical images. The natural transformation of a pattern on the sphere is a 3D rotation and template matching is the localization of a target in any orientation. Cross-correlation on the sphere is a function of 3D-rotation and it can be computed in a space-invariant way through a 3D inverse DFT of a linear combination of spherical harmonics. However, if we intend to normalize the cross-correlation, the computation of the local image variance is a space variant operation. In this paper, we present a new \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:k_IJM867U9cC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Multiple Motion Analysis Using 3D Orientation Steerable Filters",
            "Publication year": 2000,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.8375&rep=rep1&type=pdf",
            "Abstract": "In this paper, we address multiple motion analysis from the standpoint of orientation analysis. Using the fact that multiple motions are equivalent to multiple planes in the derivative space or in the spectral domain, we apply a new kind of 3D steerable filter in motion estimation. This new method is based on the decomposition of the sphere with a set of overlapping basis filters in the feature space. It is superior to principal axis analysis based approaches and current 3D steerability approaches in achieving high orientation resolution. Our approach is similar to the 3D Hough transform, but more efficient and robust. It also improves the performance of the expectation-maximization algorithm. Based on the spatial coherence in image sequences we use the \u201cshift-and-subtract\u201d technique to localize occlusion boundaries and to track their movement in occlusion sequences. This technique can also be used to distinguish occlusion from transparency and to decompose transparency scenes into multi-layers.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:JQOojiI6XY0C",
            "Publisher": "Inst. f\u00fcr Informatik und Praktische Mathematik"
        },
        {
            "Title": "3d tele-collaboration over internet2",
            "Publication year": 2002,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.5321",
            "Abstract": "Our long-term vision is to provide a better every-day working environment, with high-fidelity scene reconstruction for life-sized 3D telecollaboration. In particular, we want to provide the user with a true sense of presence with our remote collaborator and their real surroundings, and the ability to share and interact with 3D documents. The challenges related to this vision are enormous and involve many technical tradeoffs, particularly in scene reconstruction.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:0EnyYjriUFMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised learning of image motion by recomposing sequences",
            "Publication year": 2016,
            "Publication url": "https://www.researchgate.net/profile/Kostas-Daniilidis/publication/311411524_Unsupervised_learning_of_image_motion_by_recomposing_sequences/links/5a137aafaca27217b5a2f8b3/Unsupervised-learning-of-image-motion-by-recomposing-sequences.pdf",
            "Abstract": "We propose a new method for learning a representation of image motion in an unsupervised fashion. We do so by learning an image sequence embedding that respects associativity and invertibility properties of composed sequences with known temporal order. This procedure makes minimal assumptions about scene content, and the resulting networks learn to exploit rigid and non-rigid motion cues. We show that a deep neural network trained to respect these constraints implicitly identifies the characteristic motion patterns of many different sequence types. Our network architecture consists of a CNN followed by an LSTM and is structured to learn motion representations over sequences of arbitrary length. We demonstrate that a network trained using our unsupervised procedure on realworld sequences of human actions and vehicle motion can capture semantic regions corresponding to the motion in the scene, and not merely image-level differences, without requiring any motion labels. Furthermore, we present results that suggest our method can be used to extract information useful for independent motion tracking, localization, and nearest neighbor identification. Our results suggest that this representation may be useful for motion-related tasks where explicit labels are often very difficult to obtain.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:KrOX6H5u0oYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Monocular 3D Pose Recovery via Nonconvex Sparsity with Theoretical Analysis",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1812.11295",
            "Abstract": "For recovering 3D object poses from 2D images, a prevalent method is to pre-train an over-complete dictionary  of 3D basis poses. During testing, the detected 2D pose  is matched to dictionary by  where , by estimating the rotation , projection  and sparse combination coefficients . In this paper, we propose non-convex regularization  to learn coefficients , including novel leaky capped -norm regularization (LCNR), \\begin{align*} H(c)=\\alpha \\sum_{i } \\min(|c_i|,\\tau)+ \\beta \\sum_{i } \\max(| c_i|,\\tau), \\end{align*} where  and  is a certain threshold, so the invalid components smaller than  are composed with larger regularization and other valid components with smaller regularization. We propose a multi-stage optimizer with convex relaxation and ADMM. We prove that the estimation error  decays w.r.t. the stages , \\begin{align*} Pr\\left(\\mathcal L(l) < \\rho^{l-1} \\mathcal L(0) + \\delta \\right) \\geq 1- \\epsilon, \\end{align*} where . Experiments on large 3D human datasets like H36M are conducted to support our improvement upon previous approaches. To the best of our knowledge, this is the first theoretical analysis in this line of research, to understand how the recovery error is affected by fundamental factors, e.g. dictionary size, observation noises, optimization times. We characterize the trade-off between speed and accuracy towards real-time inference in applications.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:5XSfyxoPzb8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "An optimization approach to bearing-only visual homing with applications to a 2-D unicycle model",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6907475/",
            "Abstract": "We consider the problem of bearing-based visual homing: Given a mobile robot which can measure bearing directions corresponding to known landmarks, the goal is to guide the robot toward a desired \u201chome\u201d location. We propose a control law based on the gradient field of a Lyapunov function, and give sufficient conditions for global convergence. We show that the well-known Average Landmark Vector method (for which no convergence proof was known) can be obtained as a particular case of our framework. We then derive a sliding mode control law for a unicycle model which follows this gradient field. Both controllers do not depend on range information. Finally, we also show how our framework can be used to characterize the sensitivity of a home location with respect to noise in the specified bearings.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:sCWLdL-sCz8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Approximate orientation steerability based on angular Gaussians",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/902274/",
            "Abstract": "Junctions are significant features in images with intensity variation that exhibits multiple orientations. This makes the detection and characterization of junctions a challenging problem. The characterization of junctions would ideally be given by the response of a filter at every orientation. This can be achieved by the principle of steerability that enables the decomposition of a filter into a linear combination of basis functions. However, current steerability approaches suffer from the consequences of the uncertainty principle: in order to achieve high resolution in orientation they need a large number of basis filters increasing, thus, the computational complexity. Furthermore, these functions have usually a wide support which only accentuates the computational burden. We propose a novel alternative to current steerability approaches. It is based on utilizing a set of polar separable filters with small support to sample \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:roLk4NBRz8UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Graph Neural Networks For Multi-Image Matching",
            "Publication year": 2019,
            "Publication url": "https://openreview.net/forum?id=Hkgpnn4YvH",
            "Abstract": "In geometric computer vision applications, multi-image feature matching gives more accurate and robust solutions compared to simple two-image matching. In this work, we formulate multi-image matching as a graph embedding problem, then use a Graph Neural Network to learn an appropriate embedding function for aligning image features. We use cycle consistency to train our network in an unsupervised fashion, since ground truth correspondence can be difficult or expensive to acquire. Geometric consistency losses are added to aid training, though unlike optimization based methods no geometric information is necessary at inference time. To the best of our knowledge, no other works have used graph neural networks for multi-image feature matching. Our experiments show that our method is competitive with other optimization based approaches.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:HZAC7m4lW-wC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A triangle histogram for object classification by tactile sensing",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7759724/",
            "Abstract": "We present a new descriptor for tactile 3D object classification. It is invariant to object movement and simple to construct, using only the relative geometry of points on the object surface. We demonstrate successful classification of 185 objects in 10 categories, at sparse to dense surface sampling rate in point cloud simulation, with an accuracy of 77.5% at the sparsest and 90.1% at the densest. In a physics-based simulation, we show that contact clouds resembling the object shape can be obtained by a series of gripper closures using a robotic hand equipped with sparse tactile arrays. Despite sparser sampling of the object's surface, classification still performs well, at 74.7%. On a real robot, we show the ability of the descriptor to discriminate among different object instances, using data collected by a tactile hand.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:zBYNw3HCx4kC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Optimizing polynomial solvers for minimal geometry problems",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6126341/",
            "Abstract": "In recent years polynomial solvers based on algebraic geometry techniques, and specifically the action matrix method, have become popular for solving minimal problems in computer vision. In this paper we develop a new method for reducing the computational time and improving numerical stability of algorithms using this method. To achieve this, we propose and prove a set of algebraic conditions which allow us to reduce the size of the elimination template (polynomial coefficient matrix), which leads to faster LU or QR decomposition. Our technique is generic and has potential to improve performance of many solvers that use the action matrix method. We demonstrate the approach on specific examples, including an image stitching algorithm where computation time is halved and single precision arithmetic can be used.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:a0OBvERweLwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Action for better prediction",
            "Publication year": 2020,
            "Publication url": "https://rss2020vlrrm.github.io/papers/10_CameraReadySubmission_ActionForBetterPrediction_CameraReady.pdf",
            "Abstract": "Good prediction is necessary for autonomous robotics to make informed decisions in dynamic environments. Improvements can be made to the performance of a given datadriven prediction model by using better sampling strategies when collecting training data. Active learning approaches to optimal sampling have been combined with the mathematically general approaches to incentivizing exploration presented in the curiosity literature via model-based formulations of curiosity. We present an adversarial curiosity method which minimizes a score given by a discriminator network. This score gives a measure of prediction certainty enabling our approach to sample sequences of observations and actions which result in outcomes considered the least realistic by the discriminator. We demonstrate the ability of our active sampling method to achieve higher prediction performance and higher sample efficiency in a domain transfer problem for robotic manipulation tasks.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:viTTOddtVMkC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Eliminating outliers in motion occlusion analysis",
            "Publication year": 2000,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-59802-9_47",
            "Abstract": "Occlusion boundaries are considered either as outliers or as noise in most optical flow algorithms. In order to treat the boundary problem, many probabilistic algorithms like maximum likelihood [6] or expectation-maximization (EM) [17,3] decrease the weights of pixels in boundary regions gradually during estimation iterations. However, these approaches still include the outliers in the estimation. If the number of pixels in boundary regions is comparable to the number of pixels with single motion, we will not be able to estimate the motion parameters robustly since probabilistic methods are purely based on statistics. In this paper, we mark the outliers directly using a method based on eigenvalue analysis [9]. Then we eliminate these outliers in the multiple motion estimation. Comparisons show that this method can improve the precision of estimation results. We also use the \u201cwarp-and-subtract\u201d technique to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:xtRiw3GOFMkC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Template gradient matching in spherical images",
            "Publication year": 2004,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/5298/0000/Template-gradient-matching-in-spherical-images/10.1117/12.527043.short",
            "Abstract": "Most of today's robot vehicles are equipped with omnidirectional  sensors which provide surround awareness and easier navigation.  Due to the persistence of the appearance in omnidirectional images, many global navigation or formation control tasks, instead of using landmarks or fiducials, they need only reference images of target positions or objects.  In this paper, we study the problem of template matching in spherical images. The natural transformation of a pattern on the sphere is a 3D rotation and template matching is the localization of a target in any orientation given by a reference image. Unfortunately, the support of the template is space variant on the Euler angle parameterization. Here we propose a new method  which matches the gradients of the image and the template, with space-invariant operation.   Using properties of the angular momentum, we have proved in fact that the gradient correlation can \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:ZHo1McVdvXMC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "Properties of the Catadioptric Fundamental Matrix",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-47967-8_10",
            "Abstract": "The geometry of two uncalibrated views obtained with a parabolic catadioptric device is the subject of this paper. We introduce the notion of circle space, a natural representation of line images, and the set of incidence preserving transformations on this circle space which happens to equal the Lorentz group. In this space, there is a bilinear constraint on transformed image coordinates in two parabolic catadioptric views involving what we call the catadioptric fundamental matrix. We prove that the angle between corresponding epipolar curves is preserved and that the transformed image of the absolute conic is in the kernel of that matrix, thus enabling a Euclidean reconstruction from two views. We establish the necessary and sufficient conditions for a matrix to be a catadioptric fundamental matrix.",
            "Abstract entirety": 1,
            "Author pub id": "dGs2BcIAAAAJ:uLbwQdceFCQC",
            "Publisher": "Berlin: Springer-Verlag, 1973-"
        },
        {
            "Title": "A novel stereoscopic cue for figure-ground segregation of semi-transparent objects",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6130373/",
            "Abstract": "The visual perception of semi-transparent objects, such as drinking glasses, is an open challenging problem. Unlike opaque objects, semi-transparent objects violate many of the standard vision assumptions, among them that figure-ground segmentation contains salient boundaries. More specifically, reliable motion and stereo cues for segmenting semi-transparent objects are not present because of the infeasibility of establishing correspondence. This paper describes a new discovery that semi-transparent objects are salient on the plane-parallax image generated by the inverse perspective mapping. A novel cue is introduced that reveals objects extruding from a planar support surface. Points on the support plane are consistent with a planar homography transformation, whereas extruding points from textured surfaces violate this mapping. Furthermore, extruding semi-transparent objects violate the mapping due to \u2026",
            "Abstract entirety": 0,
            "Author pub id": "dGs2BcIAAAAJ:tOudhMTPpwUC",
            "Publisher": "IEEE"
        }
    ]
}]