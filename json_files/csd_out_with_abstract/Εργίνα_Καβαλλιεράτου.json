[{
    "name": "\u0395\u03c1\u03b3\u03af\u03bd\u03b1 \u039a\u03b1\u03b2\u03b1\u03bb\u03bb\u03b9\u03b5\u03c1\u03ac\u03c4\u03bf\u03c5",
    "romanize name": "Ergina Kavallieratou",
    "School-Department": "\u039c\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ce\u03bd \u03a0\u03bb\u03b7\u03c1\u03bf\u03c6\u03bf\u03c1\u03b9\u03b1\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 \u0395\u03c0\u03b9\u03ba\u03bf\u03b9\u03bd\u03c9\u03bd\u03b9\u03b1\u03ba\u03ce\u03bd \u03a3\u03c5\u03c3\u03c4\u03b7\u03bc\u03ac\u03c4\u03c9\u03bd",
    "University": "aegean",
    "Rank": "\u0391\u03bd\u03b1\u03c0\u03bb\u03b7\u03c1\u03c9\u03c4\u03ae\u03c2 \u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 9365,
    "Scholar name": "Ergina Kavallieratou",
    "Scholar id": "uJVGw7kAAAAJ",
    "Affiliation": "Associate Professor of Image Processing and Computer Vision, Department of Information",
    "Citedby": 1775,
    "Interests": [
        "Image processing",
        "Document Image Processing",
        "OCR",
        "Pattern Recognition",
        "Artificial Intelligence"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=uJVGw7kAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Novel approaches towards slope and slant correction for tri-script handwritten word images",
            "Publication year": 2019,
            "Publication url": "https://www.tandfonline.com/doi/abs/10.1080/13682199.2019.1574368",
            "Abstract": "Slope and slant correction of offline handwritten word images are two of the major pre-processing steps in document image processing, because these reduce the variations in writing, thereby make further processing of the same much easier. This paper presents novel slope and slant correction methods that are applied in three different script handwritten words namely Devanagari, Bangla and Roman. The language dependency and the computational complexity of state-of-the-art approaches towards the word level slope and slant correction are addressed here. A new technique for approximate core region detection is introduced here for skew detection and then linear regression is recursively applied to de-skew the word image. Whereas, in case of slant correction, a novel cost function over the vertical projection of de-skewed image is designed and optimized to fix the uniform slant angle of text words. A new \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:2l5NCbZemmgC",
            "Publisher": "Taylor & Francis"
        },
        {
            "Title": "Skeleton Hinge Distribution for Writer Identification",
            "Publication year": 2016,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/S0218213016500159",
            "Abstract": "In this paper, a feature that is based on statistical directional features is presented. Specifically, an improvement of the statistical feature: edge hinge distribution, is attempted. Furthermore, different matching techniques are applied. For the evaluation, the Firemaker DB was used, which consists of samples from 250 writers, including 4 pages per writer. The suggested feature, the skeleton hinge distribution, achieved accuracy of 90.8% using nearest neighbor with Manhattan distance for matching.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:ce2CqMG-AY4C",
            "Publisher": "World Scientific Publishing Company"
        },
        {
            "Title": "Retrieving Events in Life Logging.",
            "Publication year": 2018,
            "Publication url": "http://ceur-ws.org/Vol-2125/paper_86.pdf",
            "Abstract": "This paper describes our contribution for the Lifelog Moment Retrieval (LMRT) challenge of ImageCLEF Lifelog2018. Lifelogging has a tremendous potential in many applications. However, the wide range of possible moment events along with the lack of fully annotated databases make this task very challenging. This work proposes an interactive and weakly supervised learning approach that can dramatically reduce the time to retrieve any kind of events in huge databases. Impressive results have been obtained in the referred challenge, reaching the first rank.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:5bg8sr1QxYwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A document image retrieval system",
            "Publication year": 2010,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197610000771",
            "Abstract": "In this paper, a system is presented that locates words in document image archives. This technique performs the word matching directly in the document images bypassing character recognition and using word images as queries. First, it makes use of document image processing techniques, in order to extract powerful features for the description of the word images. The features used for the comparison are capable of capturing the general shape of the query, and escape details due to noise or different fonts. In order to demonstrate the effectiveness of our system, we used a collection of noisy documents and we compared our results with those of a commercial optical character recognition (OCR) package.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "An Evaluation Technique for Binarization Algorithms.",
            "Publication year": 2008,
            "Publication url": "http://www.jucs.org/jucs_14_18/an_evaluation_technique_for/jucs_14_18_3011_3030_stathis.pdf",
            "Abstract": "Document binarization is an active research area for many years. The choice of the most appropriate binarization algorithm for each case proved to be a very difficult procedure itself. In this paper, we propose a new technique for the validation of document binarization algorithms. Our method is simple in its implementation and can be performed on any binarization algorithm since it doesn\u2019t require anything more than the binarization stage. As a demonstration of the proposed technique, we use the case of degraded historical documents. Then we apply the proposed technique to 30 binarization algorithms. Experimental results and conclusions are presented.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Firefly algorithm-based Kapur\u2019s thresholding and Hough transform to extract leukocyte section from hematological images",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-981-15-0306-1_10",
            "Abstract": "Computerized disease examination techniques are widely adopted in the literature to evaluate a considerable number of medical images ranging from the RGB scale to gray scale. This work proposes a novel image extraction method by combining Kapur\u2019s thresholding and Hough transform (HT) to extract the leukocyte segment from the RGB-scaled blood smear image (BSI). Automated mining of the leukocyte region is always preferred in medical clinics for fast disease examination and treatment for the planning process. This study aims to implement a hybrid procedure to extort the leukocyte segment. Kapur\u2019s is considered to enhance the RGB-scaled test image, and the HT is used to detect and extract the circle section from the image. In this work, the hematological images of leukocyte images for segmentation and classification (LISC) database are adopted for the examination. The extracted leukocyte picture is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:QUX0mv85b1cC",
            "Publisher": "Springer, Singapore"
        },
        {
            "Title": "Skew angle estimation for printed and handwritten documents using the Wigner\u2013Ville distribution",
            "Publication year": 2002,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0262885602000914",
            "Abstract": "A skew estimation algorithm for printed and handwritten documents, based on the document's horizontal projection profile and its Wigner\u2013Ville distribution, is presented. The proposed algorithm is able to correct skew angles that range between \u221289 and +89\u00b0 detecting the right oriented position of the page by the alternations of the horizontal projection profile. It is able of processing successfully handwritten documents, even if they consist of non-parallel text lines. It deals with the presence of graphics, while a few text lines suffice for the application of the algorithm. Furthermore, the latter permits the use of only a part of the page for the skew estimation minimizing the computational complexity. The proposed algorithm was evaluated on a wide variety of pages (i.e. printed, handwritten, multi-column, application forms etc.) achieving a success rate of 100% within a confidence range of \u00b10.3\u00b0.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:u-x6o8ySG0sC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A review of image steganalysis techniques for digital forensics",
            "Publication year": 2018,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S2214212617300777",
            "Abstract": "Steganalysis and steganography are the two different sides of the same coin. Steganography tries to hide messages in plain sight while steganalysis tries to detect their existence or even more to retrieve the embedded data. Both steganography and steganalysis received a great deal of attention, especially from law enforcement. While cryptography in many countries is being outlawed or limited, cyber criminals or even terrorists are extensively using steganography to avoid being arrested with encrypted incriminating material in their possession. Therefore, understanding the ways that messages can be embedded in a digital medium \u2013in most cases in digital images-, and knowledge of state of the art methods to detect hidden information, is essential in exposing criminal activity. Digital image steganography is growing in use and application. Many powerful and robust methods of steganography and steganalysis \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:oi2SiIJ9l4AC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Machine-printed from handwritten text discrimination",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1363929/",
            "Abstract": "This paper deals with the discrimination between machine-printed and handwritten text, a prerequisite for many OCR applications. An easy-to-follow approach is proposed based on an integrated system able to localize text areas and split them in text-lines. A set of simple structural characteristics that capture the differences between machine-printed and handwritten text-lines is presented and preliminary experiments on document images taken from databases of different languages and characteristics show a remarkable performance.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:roLk4NBRz8UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Improving handwritten character segmentation by incorporating Bayesian knowledge with support vector machines",
            "Publication year": 2002,
            "Publication url": "https://www.academia.edu/download/35948623/icassp_2002.pdf",
            "Abstract": "Learning Bayesian Belief Networks (BBN) from corpora and incorporating the extracted inferring knowledge with a Support Vector Machines (SVM) classifier has been applied to character segmentation for unconstrained handwritten text. By taking advantage of the plethora in unlabeled data found in image databases in addition to some available labeled examples, we overcome the expensive task of annotating the whole set of training data and the performance of the character segmentation learner is increased. Apart from this approach, which has not previously used for this task, we have experimented with two well-known machine learning methods (Learning Vector Quantization and a simplified version of the Transformation-Based Learning theory). We argue that a classifier generated from BBN and SVM is well suited for learning to identify the correct segment boundaries. Empirical results will support this claim. Performance has been methodically evaluated using both English and Modern Greek corpora in order to determine the unbiased behaviour of the trained models. Limited training data are proved to endow with satisfactory results. We have been able to achieve precision exceeding 86%.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:MXK_kJrjxJIC",
            "Publisher": "IEEE; 1999"
        },
        {
            "Title": "Handwritten word recognition based on structural characteristics and lexical support",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1227727/",
            "Abstract": "In this paper a handwritten recognition algorithm based on structural characteristics, histograms and profiles, is presented. The well-known horizontal and vertical histograms are used, in combination with the newly introduced radial histogram, out-in radial and in-out radial profiles for representing 32 /spl times/ 32 matrices of characters, as 280-dimension vectors. The recognition process has been supported by a lexical component based on dynamic acyclic FSAs (Finite-State-Automata).",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:_FxGoFyzp5QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A binarization algorithm specialized on document images and photos",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1575589/",
            "Abstract": "In this paper, a new method for document images or photos binarization is presented. The method is simple, fast and robust and appropriate for normal as well as for special cases of documents like photos, historical documents etc. The proposed method is applied to problematic cases of documents and it is compared to other traditional methods.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:d1gkVwhDpl0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Text and non-text recognition using modified HOG descriptor",
            "Publication year": 2017,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8280697/",
            "Abstract": "In order to convert a document image in its editable version, an OCR engine must identify and separate the nontext regions from text regions of a given document image. In the present work, a technique is developed to classify various text and non-text regions present in a document image. For that purpose, a modified version of Histogram of Oriented Gradient (HOG) is used as a feature descriptor. Multi-Layer Perceptron (MLP) is chosen from a pool of classifier by comparing the recognition accuracy of it with two other well-known classifiers viz., Random Forest (RF), Nave Bayes (NB). The designed technique is evaluated on a dataset, containing 862 images of manually extracted regions from two standard databases namely, RDCL2015 dataset and Media Team Document dataset. The proposed system has achieved 96.44% recognition accuracy and outperformed some of the state-of-the-art feature descriptors \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:-nhnvRiOwuoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Overview of the ImageCLEFsecurity 2019: File Forgery Detection Tasks.",
            "Publication year": 2019,
            "Publication url": "http://www.dei.unipd.it/~ferro/CLEF-WN-Drafts/CLEF2019/paper_271.pdf",
            "Abstract": "The File Forgery Detection tasks is in its first edition, in 2019. This year, it is composed by three subtasks: a) Forged file discovery, b) Stego image discovery and c) Secret message discovery. The data set contained 6,400 images and pdf files, divided into 3 sets. There were 61 participants and the majority of them participated in all the subtasks. This highlights the major concern the scientific community shows for security issues and the importance of each subtask. Submissions varied from a) 8, b) 31 and c) 14 submissions for each subtask, respectively. Although the datasets were small, most of the participants used deep learning techniques, especially in subtasks 2 & 3. The results obtained in subtask 3-which was the most difficult one-showed that there is room for improvement, as more advanced techniques are needed to achieve better results. Deep learning techniques adopted by many researchers is a preamble in that direction, and proved that they may provide a promising steganalysis tool to a digital forensics examiner.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:eO3_k5sD8BwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "How conditional independence assumption affects handwritten character segmentation",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/953792/",
            "Abstract": "This paper deals with the use of Bayesian Belief Networks in order to improve the accuracy and training time of character segmentation for unconstrained handwritten text. Comparative experimental results have been evaluated against Naive Bayes classification, which is based on the assumption of the independence of the parameters and two additional previous commonly used methods. Results have depicted that obtaining the inferential dependencies of the training data, could lead to the reduction of the required training time and size by a factor of 55%. Moreover, the achieved accuracy in detecting segment boundaries exceeds 86% whereas limited training data are proved to endow with very satisfactory results.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:0EnyYjriUFMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Developing document image retrieval system",
            "Publication year": 2008,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.488.7249&rep=rep1&type=pdf",
            "Abstract": "A system was developed able to retrieve specific documents from a document collection. In this system the query is given in text by the user and then transformed into image. Appropriate features were in order to capture the general shape of the query, and ignore details due to noise or different fonts. In order to demonstrate the effectiveness of our system, we used a collection of noisy documents and we compared our results with those of a commercial OCR package.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:4JMBOYKVnBMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Binarization: a Tool for Text Localization",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6981093/",
            "Abstract": "This paper presents a novel procedure for localizing text on scene photos. It takes advantage of the fact that text should present some contrast in comparison with the background, in order to be distinguished by the human eye. A procedure of binarization is applied in order to create appropriate images for the text detection. The connected components of the image are extracted and some heuristic rules are applied in order to identify areas containing text. Finally, the overlaps are handled and the false detections are rejected. The method is evaluated using images of natural scene taken from the robust reading competition of ICDAR2011. The results are promising and some useful conclusions are drawn.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:NDuN12AVoxsC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Document image processing",
            "Publication year": 2018,
            "Publication url": "https://www.mdpi.com/2313-433X/4/7/84/htm",
            "Abstract": "The Special Issue \u201cDocument Image Processing\u201d in the Journal of Imaging aims at presenting approaches which contribute to access the content of document images. These approaches are related to low level tasks such as image preprocessing, skew/slant corrections, binarization and document segmentation, as well as high level tasks such as OCR, handwriting recognition, word spotting or script identification. This special issue brings together 12 papers that discuss such approaches. The first three articles deal with historical document preprocessing. The work by Hanif et al.[1] aims at removing bleed-through using a non-linear model, and at reconstructing the background by an inpainting approach based on non-local patch similarity. The paper by Almeida et al.[2] proposes a new binarization approach that includes a decision-based process for finding the best threshold for each RGB channel. In the paper by Kavallieratou et al.[3], a segmentation-free approach based on the Wigner-Ville distribution is used to detect the slant of a document and correct it. Once a document image is preprocessed, a next step described in the paper by Ghosh et al.[4] consists in separating text components from non-text ones, using a classifier based on LBP features. Following steps may consist in recognizing text components or searching from word queries. In the paper by Nashwan et al.[5] a holistic-based approach for the recognition of printed Arabic words is proposed, coupled with an efficient dictionary reduction. In the work by Nagendar et al.[6] it is shown that using a query specific fast Dynamic Time Warping distance, improves the Direct Query Classifier \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:YsrPvlHIBpEC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Imageclef 2019: Multimedia retrieval in lifelogging, medical, nature, and security applications",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-15719-7_40",
            "Abstract": "This paper presents an overview of the foreseen ImageCLEF 2019 lab that will be organized as part of the Conference and Labs of the Evaluation Forum - CLEF Labs 2019. ImageCLEF is an ongoing evaluation initiative (started in 2003) that promotes the evaluation of technologies for annotation, indexing and retrieval of visual data with the aim of providing information access to large collections of images in various usage scenarios and domains. In 2019, the 17th edition of ImageCLEF will run four main tasks: (i) a Lifelog task (videos, images and other sources) about daily activities understanding, retrieval and summarization, (ii) a Medical task that groups three previous tasks (caption analysis, tuberculosis prediction, and medical visual question answering) with newer data, (iii) a new Coral task about segmenting and labeling collections of coral images for 3D modeling, and (iv) a new Security task \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:SGW5VrABaM0C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "A slant removal technique for document page",
            "Publication year": 2014,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9021/90210T/A-slant-removal-technique-for-document-page/10.1117/12.2037924.short",
            "Abstract": "The slant removal is a necessary preprocessing task in many document image processing systems. In this paper, we describe a technique for removing the slant from the entire page, avoiding the segmentation procedure. The presented technique could be combined with the most existed slant removal algorithms. Experimental results are presented on two databases.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:maZDTaKrznsC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "A tool for tuning binarization techniques",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6065265/",
            "Abstract": "In this paper a user friendly tool appropriate to get user feedback for the application of binarization algorithms is presented. The human feedback is very useful in order to apply next the algorithm to similar images. The tool supports Image Selection and Display, Selection of Binarization Algorithm and Parameter Configuration, Feedback gathering and Creation of log file for further processing.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:KlAtU1dfN6UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A comprehensive survey of fingerprint presentation attack detection",
            "Publication year": 2021,
            "Publication url": "https://jsssjournal.com/article/view/4368",
            "Abstract": "Nowadays, the number of people that utilize either digital applications or machines is increasing exponentially. Therefore, trustworthy verification schemes are required to ensure security and to authenticate the identity of an individual. Since traditional passwords have become more vulnerable to attack, the need to adopt new verification schemes is now compulsory. Biometric traits have gained significant interest in this area in recent years due to their uniqueness, ease of use and development, user convenience and security. Biometric traits cannot be borrowed, stolen or forgotten like traditional passwords or RFID cards. Fingerprints represent one of the most utilized biometric factors. In contrast to popular opinion, fingerprint recognition is not an inviolable technique. Given that biometric authentication systems are now widely employed, fingerprint presentation attack detection has become crucial. In this review, we investigate fingerprint presentation attack detection by highlighting the recent advances in this field and addressing all the disadvantages of the utilization of fingerprints as a biometric authentication factor. Both hardware-and software-based state-of-the-art methods are thoroughly presented and analyzed for identifying real fingerprints from artificial ones to help researchers to design securer biometric systems.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:IsPWOBWtZBwC",
            "Publisher": "OAE Publishing Inc."
        },
        {
            "Title": "Word segmentation using Wigner-Ville distribution",
            "Publication year": 2015,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7333852/",
            "Abstract": "In this paper, a novel technique for Word-segmentation is presented, based on Wigner-Ville distribution. The technique does not require training, while it is adapted to the writing style of the document image. Evaluation is performed on a standard dataset and the results are promising.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:sszUF3NjhM4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "An image processing self-training system for ruling line removal algorithms",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6622767/",
            "Abstract": "Ruling line removal is an important pre-processing step in document image processing. Several algorithms have been proposed for this task. However, it is important to be able to take full advantage of the existing algorithms by adapting them to the specific properties of a document image collection. In this paper, a system is presented, appropriate for fine-tuning the parameters of ruling line removal algorithms or appropriately adapt them to a specific document image collection, in order to improve the results. The application of our method to an existed line removal algorithms is presented.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:isC4tDSrTZIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Ruling line removal in handwritten page images",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5595806/",
            "Abstract": "In this paper we present a procedure for removing ruling lines from a handwritten document image that does not break existing characters. We take advantage of common ruling line properties such as uniform width, predictable spacing, position vs. text, etc. The proposed process has no effect on document images without ruling lines, hence no a priori discrimination is required. The system is evaluated on synthetic page images in five different languages.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:3fE2CSJIrl8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Slant estimation algorithm for OCR systems",
            "Publication year": 2001,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0031320300001539",
            "Abstract": "A slant removal algorithm is presented based on the use of the vertical projection profile of word images and the Wigner\u2013Ville distribution. The slant correction does not affect the connectivity of the word and the resulting words are natural. The evaluation of our algorithm was equally made by subjective and objective means. The algorithm has been tested in English and Modern Greek samples of more than 500 writers, taken from the databases IAM-DB and GRUHD. The extracted results are natural, and almost always improved with respect to the original image, even in the case of variant-slanted writing. The performance of an existed character recognition system showed an increase of up to 9% for the same data, while the training time cost was significantly reduced. Due to its simplicity, this algorithm can be easily incorporated into any optical character recognition system.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:9yKSN-GCB0IC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Automatic text extraction from arabic newspapers",
            "Publication year": 2018,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-319-93000-8_57",
            "Abstract": "A system for extracting the textual information from document images with complex layouts is presented. It is based on both layout analysis and text localization techniques. Layout analysis is first applied to segment the page in text and non-text blocks and then text localization is used to detect text that may be embedded inside images, charts, diagrams, tables etc. Detailed experiments on scanned Arabic newspapers showed that combining layout analysis and text localization methods can lead to improved page segmentation and text extraction results.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:mUJArPsKIAAC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "A Dilated Convolutional Neural Network as Feature Selector for Spatial Image Steganalysis\u2013A Hybrid Classification Scheme",
            "Publication year": 2020,
            "Publication url": "https://link.springer.com/article/10.1134/S1054661820030098",
            "Abstract": "Nowadays, while steganography is the main mean of illegal secret communication, the need of detecting steganographic content and especially stego images is becoming more compulsory. Since multimedia content can be easily spread over the internet and more complicated steganography algorithms in different domains i.e. spatial, transform are utilized, the task of identifying stego images becomes very difficult. Early steganalysis methods deploy statistical attacks on stego images while more recent ones use deep learning techniques. The latter ones mainly utilize convolutional neural networks and show promising results. In this paper we propose a novel method to identify stego images derived from two different steganographic algorithms S-UNIWARD (Spatial-UNIversal WAvelet Relative Distortion) and WOW (Wavelet Obtained Weights) for various embedding rates. The proposed method initially \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:HJSXoJQnj-YC",
            "Publisher": "Pleiades Publishing"
        },
        {
            "Title": "An evaluation survey of binarization algorithms on historical documents",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4761546/",
            "Abstract": "Document binarization is an active research area for many years. There are many difficulties associated with satisfactory binarization of document images and especially in cases of degraded historical documents. In this paper, we try to answer the question ldquohow well an existing binarization algorithm can binarize a degraded document image?rdquo We propose a new technique for the validation of document binarization algorithms. Our method is simple in its implementation and can be performed on any binarization algorithm since it doesnpsilat require anything more than the binarization stage. Then we apply the proposed technique to 30 existing binarization algorithms. Experimental results and conclusions are presented.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:LkGwnXOMwfcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Cleaning and enhancing historical document images",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11558484_86",
            "Abstract": "In this paper we present a recursive algorithm for the cleaning and the enhancing of historical documents. Most of the algorithms, used to clean and enhance documents or transform them to binary images, implement combinations of complicated image processing techniques which increase the computational cost and complexity. Our algorithm simplifies the procedure by taking into account special characteristics of the document images. Moreover, the fact that the algorithm consists of iterated steps, makes it more flexible concerning the needs of the user. At the experimental results, comparison with other methods is provided.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Unified layout analysis and text localization framework",
            "Publication year": 2017,
            "Publication url": "https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-26/issue-1/013009/Unified-layout-analysis-and-text-localization-framework/10.1117/1.JEI.26.1.013009.short",
            "Abstract": "A technique appropriate for extracting textual information from documents with complex layouts, such as newspapers and journals, is presented. It is a combination of a foreground analysis and a text localization method. The first one is used to segment the page in text and nontext blocks, whereas the second one is used to detect text that may be embedded inside images, charts, diagrams, tables, etc. Detailed experiments on two public databases showed that mixing layout analysis and text localization techniques can lead to improved page segmentation and text extraction results.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:pAkWuXOU-OoC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "Music performer verification based on learning ensembles",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-24674-9_14",
            "Abstract": "In this paper the problem of music performer verification is introduced. Given a certain performance of a musical piece and a set of candidate pianists the task is to examine whether or not a particular pianist is the actual performer. A database of 22 pianists playing pieces by F. Chopin in a computer-controlled piano is used in the presented experiments. An appropriate set of features that captures the idiosyncrasies of music performers is proposed. Well-known machine learning techniques for constructing learning ensembles are applied and remarkable results are described in verifying the actual pianist, a very difficult task even for human experts.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:kNdYIx-mwKoC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "The impact of ruling lines on writer identification",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5693603/",
            "Abstract": "Paper often includes pre-printed ruling lines to help people write more neatly. This particular example of real- world noise can have a serious impact on applications such as handwriting recognition and writer identification, however. In this work, we investigate the effects of ruling lines on writer ID. We study a method for detecting and removing ruling lines and test its utility for Arabic writer identification through a series of experiments. Our preliminary results show that under realistic assumptions where ruling lines are expected to have different properties across the collection, e.g., thickness, spacing, etc., removing them significantly improves identification performance. We conclude with a discussion of work-in-progress to examine follow up questions raised by our initial investigations.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:hqOjcs7Dif8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "An effective stochastic estimation of handwritten character segmentation bounds",
            "Publication year": 2003,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.6744&rep=rep1&type=pdf",
            "Abstract": "Networks in order to improve the accuracy and training time of character segmentation for unconstrained handwritten text. Comparative experimental results have been evaluated against Naive Bayes classification, which is based on the assumption of the independence of the parameters and two other previous commonly used methods. Results have depicted that obtaining the inferential dependencies of the training data, could lead to the reduction of the required training time and size by a factor of 55%. Moreover, the achieved accuracy in detecting segment boundaries exceeds 86% whereas limited training data are proved to endow with very satisfactory results.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:qUcmZB5y_30C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Handwritten character recognition based on structural characteristics",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1047814/",
            "Abstract": "A handwritten character recognition algorithm based on structural characteristics, histograms and profiles, is presented. The well-known horizontal and vertical histograms are used, in combination with the newly introduced radial histogram, out-in radial and in-out radial profiles for representing 32/spl times/32 matrices of characters, as 280 dimension vectors. The K-means algorithm is used for the classification of these vectors. Detailed experiments performed in NIST and GRUHD databases gave promising accuracy results that vary from 72.8% to 98.8% depending on the difficulty of the database and the character category.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:eQOLeE2rZwMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Text/non-text separation from handwritten document images using LBP based features: An empirical study",
            "Publication year": 2018,
            "Publication url": "https://www.mdpi.com/281328",
            "Abstract": "Isolating non-text components from the text components present in handwritten document images is an important but less explored research area. Addressing this issue, in this paper, we have presented an empirical study on the applicability of various Local Binary Pattern (LBP) based texture features for this problem. This paper also proposes a minor modification in one of the variants of the LBP operator to achieve better performance in the text/non-text classification problem. The feature descriptors are then evaluated on a database, made up of images from 104 handwritten laboratory copies and class notes of various engineering and science branches, using five well-known classifiers. Classification results reflect the effectiveness of LBP-based feature descriptors in text/non-text separation. View Full-Text",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:w1MjKQ0l0TYC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Document image processing",
            "Publication year": 2018,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=vRZxDwAAQBAJ&oi=fnd&pg=PA208&dq=info:VxO0KA5EPv4J:scholar.google.com&ots=Y-Iqeu-FUS&sig=frRCdCnC8QUFt-Wme5MTrD9RpOQ",
            "Abstract": "This book is a printed edition of the Special Issue\" Document Image Processing\" that was published in J. Imaging",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:BrOSOlqYqPUC",
            "Publisher": "MDPI"
        },
        {
            "Title": "Complex layout analysis based on contour classification and morphological operations",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0952197617301860",
            "Abstract": "In this paper, a hybrid technique for complex layout analysis is presented. Morphological operations are applied to both the foreground and the background, in order to connect neighboring regions and detect separator lines and columns respectively. Contour tracing is used for the extraction of shape and size information and classification of the connected components. Evaluated on the RDCL-2015 dataset, the method achieved state-of-art results in less than three seconds per page.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:kWvqk_afx_IC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Icdar 2019 time-quality binarization competition",
            "Publication year": 2019,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8978097/",
            "Abstract": "The ICDAR 2019 Time-Quality Binarization Competition assessed the performance of seventeen new together with thirty previously published binarization algorithms. The quality of the resulting two-tone image and the execution time were assessed. Comparisons were on both in \"real-world\" and synthetic scanned images, and in documents photographed with four models of widely used portable phones. Most of the submitted algorithms employed machine learning techniques and performed best on the most complex images. Traditional algorithms provided very good results at a fraction of the time.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:DrR-2ekChdkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Interactive learning-based retrieval technique for visual lifelogging",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-28577-7_19",
            "Abstract": "Currently, there is a plethora of video wearable devices that can easily collect data from daily user life. This fact has promoted the development of lifelogging applications for security, healthcare, and leisure. However, the retrieval of not-pre-defined events is still a challenge due to the impossibility of having a potentially unlimited number of fully annotated databases covering all possible events. This work proposes an interactive and weakly supervised learning approach that is able of retrieving any kinds of events using general and weakly annotated databases. The proposed system has been evaluated with the database provided by the Lifelog Moment Retrieval (LMRT) challenge of ImageCLEF (Lifelog2018), where it reached the first position in the final ranking.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:rTD5ala9j4wC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Retrieval of historical documents by word spotting",
            "Publication year": 2009,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7247/724706/Retrieval-of-historical-documents-by-word-spotting/10.1117/12.805602.short",
            "Abstract": "The implementation of word spotting is not an easy procedure and it gets even worse in the case of historical documents since it requires character recognition and indexing of the document images. A general technique for word spotting is presented, independent of OCR, using automatic representation of the text queries of the user by word images and comparing them with the word images extracted from the document images. The proposed system does not require training. The only required preprocessing task is the alphabet determination. Global shape features are used to describe the words. They are very general in order to capture the form of the word and appropriately normalized in order to face the usual problems of variance in resolution, width of words and fonts. A novel technique that makes use of the interpolation method is presented. In our experiments, we analyze the system dependence on its \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:ULOm3_A8WrAC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "Handwritten character segmentation using transformation-based learning",
            "Publication year": 2000,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/906155/",
            "Abstract": "Presents a character segmentation algorithm for unconstrained cursive handwritten text. The transformation-based learning method and a simplified variation of it are used in order to extract automatically rules that detect the segment boundaries. Comparative experimental results are given for a collection of multiwriter handwritten words. The achieved accuracy in detecting segment boundaries exceeds 82%. Moreover limited training data can provide very satisfactory results.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:YsMSGLbcyi4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Diagnostic Assessment of Telephone Transmission Impact on ASR Performance and Human-to-Human Speech Quality.",
            "Publication year": 2002,
            "Publication url": "http://www.lrec-conf.org/proceedings/lrec2002/pdf/41.pdf",
            "Abstract": "This paper addresses the transmission channel impact on human-to-human speech communication quality as well as on ASR performance. Transmission channels include standard wireline or mobile telephone networks and IP-based networks, which can be operated via different types of user interfaces. In order to gain control over the transmission channel, a simulation model is developed. It implements all types of stationary impairments which can be found in the mentioned networks. Human-to-human speech communication quality in these situations is estimated using a network planning model. Experiments are carried out for assessing ASR performance over the same channel, with three different types of recognizers: two prototypical recognizers used in a telephone-based information server, and a standardized set-up developed under the AURORA framework for distributed ASR. It turns out that some interesting differences exist in behavior between the ASR system performance and speech quality in human-to-human communication. The differences should be taken into account by both developers of ASR systems and transmission network planners.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:_kc_bZDykSQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Text Line Detection and Segmentation: Uneven Skew Angles and Hill-and-Dale Writing",
            "Publication year": 2011,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1774088.1774102",
            "Abstract": "A line detection and segmentation technique is presented. The proposed technique is an improved version of an older technique. The experiments have been performed on the dataset of the ICDAR 2007 handwriting segmentation contest in order to be able to compare, objectively, the performance of the two techniques. The improvement between the older and newer version is more than 24% while the average extra CPU time cost is less than 200 ms per page.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:R3hNpaxXUhUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An information extraction system from patient historical documents",
            "Publication year": 2012,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2245276.2245428",
            "Abstract": "Nowadays, document image retrieval systems are increasingly applicable by various businesses, governmental and academic organizations. ELEPAP (Hellenic Protection and Rehabilitation Centre for Disabled Children) is an organization which needs more efficient ways of managing its huge volume of archived documents. This paper deals with the preprocessing procedures of well-known OCR systems in order to extract specific features from ELEPAP's patients' cards. It is shown that our proposed methodology can provide good IT solutions for ELEPAP in order to extract information from its old archives.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:M3ejUd6NZC8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Improving the quality of degraded document images",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1612976/",
            "Abstract": "It is common for libraries to provide public access to historical and ancient document image collections. It is common for such document images to require specialized processing in order to remove background noise and become more legible. In this paper, we propose a hybrid binarization approach for improving the quality of old documents using a combination of global and local thresholding. First, a global thresholding technique specifically designed for old document images is applied to the entire image. Then, the image areas that still contain background noise are detected and the same technique is re-applied to each area separately. Hence, we achieve better adaptability of the algorithm in cases where various kinds of noise coexist in different areas of the same image while avoiding the computational and time cost of applying a local thresholding in the entire image. Evaluation results based on a collection of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:2osOgNQ5qMEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "El Greco: a 3d-printed humanoid that anybody can afford",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3200947.3201062",
            "Abstract": "In this paper, the humanoid robot named El Greco is presented (design, hardware and software). The humanoids from DARPA Robotics Challenge inspired us in many aspects, while at the same time the goal was to maintain a low cost and build a humanoid accordingly, so that we can create a model that is affordable and easy to build in order to be used by the youth. The entire assembly of the robot consists of parts such as limbs, head, body, legs and the base which are 3D printed. All the details over the design specifications and the problems incurred during the development of the humanoid are described under each module of the humanoid. Further details and results are given over the open source software of El Greco.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:kzcSZmkxUKAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Writer identification using a statistical and model based approach",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6981083/",
            "Abstract": "The state-of-the-art writer identification systems use a variety of different features and techniques in order to identify the writer of the handwritten text. In this paper several statistical and model based features are presented. Specifically, an improvement of a statistical feature, the edge hinge distribution, is attempted. Furthermore, the combination of this feature with a model-based feature is explored, that is based on a codebook of graphemes. For the evaluation, the Fire maker DB was used, which consists of 250 writers, including 4 pages per writer. The best result for the statistical suggested approach, the skeleton hinge distribution, achieved accuracy of 90.8%, while the combination of this method with the codebook of graphemes reached 96%.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:1DsIQWDZLl8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Language Identification by Using SIFT Features",
            "Publication year": 2015,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.735.8595&rep=rep1&type=pdf",
            "Abstract": "Two novel techniques for language identification of both, machine printed and handwritten document images, are presented. Language identification is the procedure where the language of a given document image is recognized and the appropriate language label is returned. In the proposed approaches, the main body size of the characters for each document image is determined, and accordingly, a sliding window is used, in order to extract the SIFT local features. Once a large number of features have been extracted from the training set, a visual vocabulary is created, by clustering the feature space. Data clustering is performed using K-means or Gaussian Mixture Models and the Expectation-Maximization algorithm. For each document image, a Bag of Visual Words or Fisher Vector representation is constructed, using the visual vocabulary and the extracted features of the document image. Finally, a multi class Support Vector Machine classification scheme is used, to score the system. Experiments are performed on well-known databases and comparative results with another established technique, are also given.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:O0nohqN1r9EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A classification-free word-spotting system",
            "Publication year": 2013,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8658/86580F/A-classification-free-word-spotting-system/10.1117/12.2002975.short",
            "Abstract": "In this paper, a classification-free Word-Spotting system, appropriate for the retrieval of printed historical document images is proposed. The system skips many of the procedures of a common approach. It does not include segmentation, feature extraction or classification. Instead it treats the queries as compact shapes and uses image processing techniques in order to localize a query in the document images. Our system was tested on a historical document collection with many problems and a Google book, printed in 1675. Moreover, some comparative results are given for a traditional word spotting system.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:r0BpntZqJG4C",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "The GRUHD database of Greek unconstrained handwriting",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/953852/",
            "Abstract": "We present the GRUHD database of Greek characters, text, digits and other symbols in unconstrained handwriting mode. The database consists of 1,760 forms that contain 667,583 handwritten symbols and 102,692 words in total, written by 1,000 writers, 500 men and equal number of women. Special attention was paid to gathering data from writers of different age and educational level. The GRUHD database is accompanied by the GRUHD software that facilitates its installation and use and enables the user to extract and process the data from the forms selectively, depending on the application. The various types of possible installations make it appropriate for the training and validation of character recognition, character segmentation and text-dependent writer identification systems.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:ufrVoPGSRksC",
            "Publisher": "IEEE"
        },
        {
            "Title": "StegoPass\u2013Utilization of Steganography to Produce a Novel Unbreakable Biometric Based Password Authentication Scheme",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-87872-6_15",
            "Abstract": "In the digital era we live, trustworthy verification schemes are required to ensure security and to authenticate the identity of an individual. Traditional passwords were proved to be highly vulnerable to attacks and the need of adopting new verification schemes is compulsory. Biometric factors have gained a lot of interest during the last years due to their uniqueness, ease of use, user convenience, and ease of deployment. However, recent research showed that even this unique authentication factors are not inviolable techniques. Thus, it is necessary to employ new verification schemes that cannot be replicated or stolen. In this paper we propose the utilization of steganography as a tool to provide unbreakable passwords. More specifically, we obtain a biometric feature of a user and embed it as a hidden message in an image. This image is then utilized as a password, the so-called StegoPass. Reversely \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:rbm3iO8VlycC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "An objective way to evaluate and compare binarization algorithms",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1363686.1363785",
            "Abstract": "The choice of the best binarization algorithm is very critical for any document image processing system, since it is one of the first tasks and any mistake it performs will be carried through the whole system. Here, a new technique for the validation of document binarization algorithms is proposed. Our method is simple in its implementation and it can be applied to any binarization algorithm since it doesn't require anything more than the binarization stage. It is based on the use of synthetic images from pdf document. Then the binarization algorithm is applied and the result is compared with the original pdf.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:5nxA0vEk-isC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Training NAO using kinect",
            "Publication year": 2016,
            "Publication url": "https://rcs.cic.ipn.mx/2016_123/RCS_123_2016.pdf#page=27",
            "Abstract": "This paper describes how the motions of the humanoid NAO robot can be controlled using the Microsoft sensor, Kinect. An application is implemented by which the robot can be controlled using real time tracking. An option to capture and save some motions is also included in order to test if it is possible to train the robot to execute automated motions. The basic question to answer by this work is whether NAO is able to help the user by lifting up an object. To answer that, a series of experiments were performed to validate if the robot could mimic both the captured and the real time motions successfully.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:DkZNVXde3BIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature",
            "Publication year": 2019,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-28577-7_28",
            "Abstract": "This paper presents an overview of the ImageCLEF 2019 lab, organized as part of the Conference and Labs of the Evaluation Forum - CLEF Labs 2019. ImageCLEF is an ongoing evaluation initiative (started in 2003) that promotes the evaluation of technologies for annotation, indexing and retrieval of visual data with the aim of providing information access to large collections of images in various usage scenarios and domains. In 2019, the 17th edition of ImageCLEF runs four main tasks: (i) a medical task that groups three previous tasks (caption analysis, tuberculosis prediction, and medical visual question answering) with new data, (ii) a lifelog task (videos, images and other sources) about daily activities understanding, retrieval and summarization, (iii) a new security task addressing the problems of automatically identifying forged content and retrieve hidden information, and (iv) a new coral task about \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:pS0ncopqnHgC",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "Slant removal technique for historical document images",
            "Publication year": 2018,
            "Publication url": "https://www.mdpi.com/303970",
            "Abstract": "Slanted text has been demonstrated to be a salient feature of handwriting. Its estimation is a necessary preprocessing task in many document image processing systems in order to improve the required training. This paper describes and evaluates a new technique for removing the slant from historical document pages that avoids the segmentation procedure into text lines and words. The proposed technique first relies on slant angle detection from an accurate selection of fragments. Then, a slant removal technique is applied. However, the presented slant removal technique may be combined with any other slant detection algorithm. Experimental results are provided for four document image databases: two historical document databases, the TrigraphSlant database (the only database dedicated to slant removal), and a printed database in order to check the precision of the proposed technique. View Full-Text",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:kF1pexMAQbMC",
            "Publisher": "Multidisciplinary Digital Publishing Institute"
        },
        {
            "Title": "Firefly Algorithm-Based Kapur\u2019s Thresholding and Hough Transform",
            "Publication year": 2019,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=o1S9DwAAQBAJ&oi=fnd&pg=PA221&dq=info:mEvRPvy5qFkJ:scholar.google.com&ots=lLHtlFD6vo&sig=vehVsH0zu0lZ8XPrtxCSnCosGKY",
            "Abstract": "In recent years, a significant number of traditional and soft computing-assisted techniques are extensively considered to preprocess and post-process and the digital pictures are existing in various domains, such as image enhancement [1], biometrics [2], document processing [3\u20137], and clinical level disease evaluation [7\u201311]. Assessment of a chosen image using a selected traditional technique may require more computations, and sometimes the traditional procedure may not be suitable to build an automated image evaluation tool (AIET). AIET is always necessary to implement an automated evaluation of the digital images, since it needs very minimal operator involvement. Most of the AIET techniques are employed with the recent soft computing methods, which will work independently with lesser operator participation.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:mWEH9CqjF64C",
            "Publisher": "Springer Nature"
        },
        {
            "Title": "GCDB: a character database system",
            "Publication year": 2009,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1577802.1577821",
            "Abstract": "In this article the GCDB (Greek Characters DataBase) System is presented. GCDB is a complete database system for storing images of Greek unconstrained handwritten characters. The three elements that compose this database are: a specialized input form, a database that contains the images of the filled forms and the software that allows the inputting of the data from the form into the database and their retrieval. The main purpose of this database system is to make it possible to achieve the automatic storage and organization of images of Greek symbols and letters in view of their use by OCR (Optical Character Recognition) systems or other applications. The GCDB system is designed within the concept of future expansion, providing an up to date database of Greek handwritten characters to cover the growing demands for offline character recognition of the Greek language.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Discrimination of machine-printed from handwritten text using simple structural characteristics",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1334152/",
            "Abstract": "In this paper, we present a trainable approach to discriminate between machine-printed and handwritten text. An integrated system able to localize text areas and split them in text-lines is used. A set of simple and easy-to-compute structural characteristics that capture the differences between machine-printed and handwritten text-lines is introduced. Experiments on document images taken from IAM-DB and GRUHD databases show a remarkable performance of the proposed approach that requires minimal training data.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:WF5omc3nYNoC",
            "Publisher": "IEEE"
        },
        {
            "Title": "ICFHR 2010 contest: Quantitative evaluation of binarization algorithms",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5693651/",
            "Abstract": "This paper describes the ICFHR 2010 Contest for quantitative evaluation of binarization algorithms. These algorithm are applied to synthetic images of modern pdf documents with noise from historical documents. Today, many scientists work on the binarization task and many algorithms have been proposed. However, the selection of the most appropriate one is not a simple procedure. The evaluation of these algorithms proved to be another difficult task since there is no objective way to compare the results. Here, 4 groups with 6 systems are participating in the competition. The experimental setting is described in detail. Moreover, a short description of the participating groups, their systems, and the results achieved are finally presented.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:UebtZRa9Y70C",
            "Publisher": "IEEE"
        },
        {
            "Title": "An integrated system for handwritten document image processing",
            "Publication year": 2003,
            "Publication url": "https://www.worldscientific.com/doi/abs/10.1142/S0218001403002538",
            "Abstract": "In this paper we attempt to face common problems of handwritten documents such as nonparallel text lines in a page, hill and dale writing, slanted and connected characters. Towards this end an integrated system for document image preprocessing is presented. This system consists of the following modules: skew angle estimation and correction, line and word segmentation, slope and slant correction. The skew angle correction, slope correction and slant removing algorithms are based on a novel method that is a combination of the projection profile technique and the Wigner\u2013Ville distribution. Furthermore, the skew angle correction algorithm can cope with pages whose text line skew angles vary, and handle them by areas. Our system can be used as a preprocessing stage to any handwriting character recognition or segmentation system as well as to any writer identification system. It was tested in a wide variety of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:UeHWp8X0CEIC",
            "Publisher": "World Scientific Publishing Company"
        },
        {
            "Title": "Incorporating conditional independence assumption with support vector machines to enhance handwritten character segmentation performance",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1048181/",
            "Abstract": "Learning Bayesian belief networks (BBN) from corpora and incorporating the extracted inferring knowledge with a support vector machines (SVM) classifier has been applied to character segmentation for unconstrained handwritten text. By taking advantage of the plethora of unlabeled data found in image databases in addition to available labeled examples, we overcome the expensive task of annotating the whole set of training data and the performance of the character segmentation learner is increased. In addition to this approach, which has not yet been used for this task, we have experimented with two well-known machine learning methods (learning vector quantization and a simplified version of the transformation-based learning theory). We argue that a classifier generated from BBN and SVM is well suited for learning to identify the correct segment boundaries. Empirical results support this claim \u2026",
            "Abstract entirety": 0,
            "Author pub id": "uJVGw7kAAAAJ:ZeXyd9-uunAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "AI in Greece: The Case of Research on Linked Geospa al Data",
            "Publication year": 2018,
            "Publication url": "https://ojs.aaai.org/index.php/aimagazine/article/view/2801",
            "Abstract": "Artificial Intelligence has been an active research field in Greece for over forty years, and there are more than thirty AI groups throughout the country covering almost all subareas of AI. One milestone for AI research in Greece was in 1988, when the Hellenic Artificial Intelligence Society (EETN) was founded as a non-profit, scientific organization on devoted to organizing and promo ng AI research in Greece and abroad. This article explores current lines of AI research in Greece and gives some history of Greek AI research since 1968.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:raTqNPD5sRQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Android based electronic travel aid system for blind people",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-662-44654-6_58",
            "Abstract": "Blindness is the condition of lacking visual perception due to physiological or neurological factors. Blind people do not have the full perception of the surrounding environment, though navigating, in an unknown environment or/and with obstacles on route, can be a very difficult task.  In this paper, an information mobile system is presented, that acts as an electronic travel aid, and can guide a blind person through a route, inform him about imminent obstacles in his path and help him orientate himself. The current prototype consists of a mobile phone, and the developed application.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:PyEswDtIyv0C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "ICDAR 2021 Competition on Time-Quality Document Image Binarization",
            "Publication year": 2021,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-030-86337-1_47",
            "Abstract": "The ICDAR 2021 Time-Quality Binarization Competition assessed the performance of 12 new and 49 other previously published binarization algorithms for scanned document images. Four test sets of \u201creal-world\u201d documents with different features were used. For each test set, the top twenty algorithms in the quality of the resulting two-tone images had their average processing time presented, yielding an account of their time complexity.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:PkcyUWeTMh0C",
            "Publisher": "Springer, Cham"
        },
        {
            "Title": "An unconstrained handwriting recognition system",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s100320200079.pdf",
            "Abstract": " In this paper, an integrated offline recognition system for unconstrained handwriting is presented. The proposed system consists of seven main modules: skew angle estimation and correction, printed-handwritten text discrimination, line segmentation, slant removing, word segmentation, and character segmentation and recognition, stemming from the implementation of already existing algorithms as well as novel algorithms. This system has been tested on the NIST, IAM-DB, and GRUHD databases and has achieved accuracy that varies from 65.6% to 100% depending on the database and the experiment.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:u5HHmVD_uO8C",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "Comparison of classification algorithms for file type detection a digital forensics perspective",
            "Publication year": 2017,
            "Publication url": "https://www.polibits.cidetec.ipn.mx/ojs/index.php/polibits/article/download/3749/3067",
            "Abstract": "Computer Science and it focuses on the acquisition, preservation and analysis of digital evidence, in a way that that these evidences are suitable for presentation in a court of law. Forensic investigators follow a standard set of procedures. One major and difficult problem is the correct identification of file types. Criminals often hide evidence in a digital device, by changing the file type. It is very common, a child predator to try to hide image files with immoral content in order to fool police authorities. In this paper we examine a methodology for file type identification, which uses computational intelligence techniques for feature selection and classification. This methodology was applied to the three most common image file types (jpg, png and gif). In order to ascertain the method\u2019s accuracy, different machine learning classifiers were utilized. A three stage process involving feature extraction (Byte Frequency Distribution), feature selection (genetic algorithm) and classification (decision tree, support vector machine, neural network, logistic regression and k-nearest neighbor) was examined. Experiments were conducted having files altered in a digital forensics perspective and the results are presented. The examined methodology showed-in most casesvery high and exceptional accuracy in file type identification.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:HhcuHIWmDEUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A new shape transformation approach to handwritten character recognition",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1044808/",
            "Abstract": "A new simple algorithm, based on dynamic programming is presented, for handwritten character recognition, with improved accuracy. The proposed shape transform (ST) approach is based on the calculation of the cost of transforming the image of a given character into that of another, thus taking into account local geometrical similarities and differences. A large experiment is conducted on the NIST database, and the effectiveness of the proposed method is compared to the Karhunen Loeve transform method, with which a similar experiment was contacted reporting the best results in the literature. The experiments performed show that this new approach leads to improved recognition. It is more demanding in computer time, which is becoming ever more plentiful, but it lends itself to very efficient parallel hardware implementation.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:8k81kl-MbHgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Control a robot via internet using a block programming platform for educational purposes",
            "Publication year": 2018,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3200947.3201063",
            "Abstract": "This paper deals with a system consisting of a robot named El Greco that can be programmed via internet and livestreaming from any place, anywhere, to move on a play room as well as perform other functions such as to produce voice in many languages, recognize voice commands, recognize faces, perceive its environment, perform combined movements and provide information by searching the Internet. All these capabilities of the robot can be programmed using a friendly block programming platform that has been developed to be used by students for educational purposes. The system was used by a number of students and the results of two questionnaires before and after the use are reported.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:WAzi4Gm8nLoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Detecting main body size in document images",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6628796/",
            "Abstract": "In this paper, two techniques are presented, appropriate to detect the text main body size in a document image. One measures it directly, while the other estimates the baselines first. Both are segmentation free. Experimental results are presented over a collection of handwritten text, as well as for a small collection of 10 printed document images, in order to give more objective results.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:k_IJM867U9cC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A slant removal algorithm",
            "Publication year": 2000,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.6546&rep=rep1&type=pdf",
            "Abstract": "A robust optical character recognition (OCR) system has to be able to cope with slanted words. Such words may dramatically a! ect the performance of the segmentation algorithms. Even in cases where segmentation is not a prerequisite, the training procedure of the recognition module is more di $ cult and complicated while attempting to cover slanted characters or words. Watanabe [1] conducted comparative experiments showing that slant normalization minimizes the error of recognition. The majority of recent OCR systems contains a preprocessing stage dealing with slanted correction. This stage is usually located before the segmentation module, if it exists, or just before the recognition stage, if it does not.The most commonly used method for slant estimation is the calculation of the average angle of near-vertical strokes [2} 4]. This approach requires the detection of the edges of the characters and its accuracy depends on the included characters in the word. Shridar [5] presents two more methods for slant estimation and correction. In the",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:Y0pCki6q_DkC",
            "Publisher": "Pergamon"
        },
        {
            "Title": "Image retrieval systems based on compact shape descriptor and relevance feedback information",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1047320311000368",
            "Abstract": "One of the most important and most used low-level image feature is the shape employed in a variety of systems such as document image retrieval through word spotting. In this paper an MPEG-like descriptor is proposed that contains conventional contour and region shape features with a wide applicability from any arbitrary shape to document retrieval through word spotting. Its size and storage requirements are kept to minimum without limiting its discriminating ability. In addition to that, a relevance feedback technique based on Support Vector Machines is provided that employs the proposed descriptor with the purpose to measure how well it performs with it. In order to evaluate the proposed descriptor it is compared against different descriptors at the MPEG-7 CE1 Set B database.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Handwritten text localization in skewed documents",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/959242/",
            "Abstract": "A system for handwritten text localization in document images is proposed. Our system performs skew angle correction using the Wigner-Ville distribution and localizes the handwritten areas of the document based on several measures concerning regularity in shape (self-correlation, horizontal and vertical symmetry) and in dimensions (aspect ratio, distribution of heights and widths). The proposed technique was tested on a variety of documents and successfully handled more than 88% of the set while the misclassified areas in the rest of the documents did not exceed six in any document.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:Se3iqnhoufwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Ruling line detection and removal",
            "Publication year": 2011,
            "Publication url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7874/78740V/Ruling-line-detection-and-removal/10.1117/12.873311.short",
            "Abstract": "In this paper we present a procedure for removing ruling lines from a handwritten document image that does not require any preprocessing or postprocessing tasks and it does not break existing characters. We take advantage of common ruling line properties such as uniform width, predictable spacing, position vs. text, etc. The deletion procedure of the detected ruling line is based on the fact that the coordinates of three collinear points have a determinant equal to zero. The system is evaluated on synthetic page images in five different languages and is compared to a previous methodology.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:-f6ydRqryjwC",
            "Publisher": "International Society for Optics and Photonics"
        },
        {
            "Title": "Adaptive binarization of historical document images",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1699632/",
            "Abstract": "In this paper, we present a binarization technique specifically designed for historical document images. Existing methods for this problem focus on either finding a good global threshold or adapting the threshold for each area to remove smear, strains, uneven illumination etc. We propose a hybrid approach that first applies a global thresholding method and, then, identifies the image areas that are more likely to still contain noise. Each of these areas is re-processed separately to achieve better quality of binarization. We evaluate the proposed approach for different kinds of degradation problems. The results show that our method can handle hard cases while documents already in good condition are not affected drastically",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:W7OEmFMy1HYC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A hybrid binarization technique for document images",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-642-22913-8_8",
            "Abstract": "In this chapter, a binarization technique specifically designed for historical document images is presented. Existing binarization techniques focus either on finding an appropriate global threshold or adapting a local threshold for each area in order to remove smear, strains, uneven illumination etc. Here, a hybrid approach is presented that first applies a global thresholding technique and, then, identifies the image areas that are more likely to still contain noise. Each of these areas is re-processed separately to achieve better quality of binarization. Evaluation results are presented that compare our technique with existing ones and indicate that the proposed approach is effective, combining the advantages of global and local thresholding. Finally, future directions of our research are mentioned.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:hFOr9nPyWt4C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "GRUHD: A Greek database of Unconstrained Handwriting.",
            "Publication year": 2000,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.8735&rep=rep1&type=pdf",
            "Abstract": "In this paper we present the GRUHD database of Greek characters, text, digits, and other symbols in unconstrained handwriting mode. The database consists of 1,760 forms that contain 667,583 handwritten symbols and 102,692 words in total, written by 1,000 writers, 500 men and equal number of women. Special attention was paid in gathering data from writers of different age and educational level. The GRUHD database is accompanied by the GRUHD software that facilitates its installation and use and enables the user to extract and process the data from the forms selectively, depending on the application. The various types of possible installations make it appropriate for the training and validation of character recognition, character segmentation and text-dependent writer identification systems.",
            "Abstract entirety": 1,
            "Author pub id": "uJVGw7kAAAAJ:YOwf2qJgpHMC",
            "Publisher": "Unknown"
        }
    ]
}]