[{
    "name": "Petros Drineas",
    "romanize name": "Petros Drineas",
    "School-Department": "Computer Science",
    "University": "Rensselaer Polytechnic Institute",
    "Rank": "\u0391\u03bd\u03b1\u03c0\u03bb\u03b7\u03c1\u03c9\u03c4\u03ae\u03c2 \u039a\u03b1\u03b8\u03b7\u03b3\u03b7\u03c4\u03ae\u03c2",
    "Apella_id": 6835,
    "Scholar name": "Petros Drineas",
    "Scholar id": "Yw2PquQAAAAJ",
    "Affiliation": "Professor, Computer Science Department, Purdue University",
    "Citedby": 11850,
    "Interests": [
        "Randomized Algorithms",
        "Numerical Linear Algebra"
    ],
    "Scholar url": "https://scholar.google.com/citations?user=Yw2PquQAAAAJ&hl=en",
    "Publications": [
        {
            "Title": "Handbook of big data",
            "Publication year": 2016,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=Kx2VCwAAQBAJ&oi=fnd&pg=PP1&dq=info:2u26_HSmssoJ:scholar.google.com&ots=FLtakEWjK5&sig=Z8EjkuWujTlUGirac4nEhkBjcIk",
            "Abstract": "Handbook of Big Data provides a state-of-the-art overview of the analysis of large-scale datasets. Featuring contributions from well-known experts in statistics and computer science, this handbook presents a carefully curated collection of techniques from both industry and academia. Thus, the text instills a working understanding of key statistical",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:_Re3VWB3Y0AC",
            "Publisher": "CRC Press"
        },
        {
            "Title": "A randomized least squares solver for terabyte-sized dense overdetermined systems",
            "Publication year": 2019,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1877750316301508",
            "Abstract": "We present a fast randomized least-squares solver for distributed-memory platforms. Our solver is based on the Blendenpik algorithm, but employs multiple random projection schemes to construct a sketch of the input matrix. These random projection sketching schemes, and in particular the use of the randomized Discrete Cosine Transform, enable our algorithm to scale the distributed memory vanilla implementation of Blendenpik to terabyte-sized matrices and provide up to \u00d77.5 speedup over a state-of-the-art scalable least-squares solver based on the classic QR algorithm. Experimental evaluations on terabyte scale matrices demonstrate excellent speedups on up to 16,384 cores on a Blue Gene/Q supercomputer.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:t6usbXjVLHcC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "A randomized algorithm for a tensor-based generalization of the singular value decomposition",
            "Publication year": 2007,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0024379506003867",
            "Abstract": "An algorithm is presented and analyzed that, when given as input a d-mode tensor A, computes an approximation A\u223c. The approximation A\u223c is computed by performing the following for each of the d modes: first, form (implicitly) a matrix by \u201cunfolding\u201d the tensor along that mode; then, choose columns from the matrices thus generated; and finally, project the tensor along that mode onto the span of those columns. An important issue affecting the quality of the approximation is the choice of the columns from the matrices formed by \u201cunfolding\u201d the tensor along each of its modes. In order to address this issue, two algorithms of independent interest are presented that, given an input matrix A and a target rank k, select columns that span a space close to the best rank k subspace of the matrix. For example, in one of the algorithms, a number c (that depends on k, an error parameter \u03f5, and a failure probability \u03b4) of columns \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:8k81kl-MbHgC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Concurrent fault detection in random combinational logic",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1194770/",
            "Abstract": "We discuss a non-intrusive methodology for concurrent fault detection in random combinational logic. The proposed method is similar to duplication, wherein a replica of the circuit acts as a predictor that immediately detects potential faults by comparison to the original circuit. However, instead of duplicating the circuit, the proposed method selects a small number of prediction logic functions which only partially replicate it. Selection is guided by the objective of minimizing the incurred hardware overhead at the cost of introducing fault detection latency. To achieve this, the proposed method replicates only a reduced width output function for every input combination, yet without compromising the ability to detect all faults. In contrast to concurrent error detection schemes which presume the ability to re-synthesize the circuit, the proposed method does not interfere with the implementation of the original design. As \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:_kc_bZDykSQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Polynomial time algorithm for column-row based relative-error low-rank matrix approximation",
            "Publication year": 2006,
            "Publication url": "http://archive.dimacs.rutgers.edu/TechnicalReports/abstracts/2006/2006-04.html",
            "Abstract": "Given an  matrix  and an integer  less than the rank of , the best--with respect to the Frobenius norm--rank  approximation to  is , which is obtained by truncating the Singular Value Decomposition (SVD) of . While  is routinely used in data analysis, it is difficult to interpret and understand it in terms of the {\\em original data}, namely the rows and columns of  which come from the application domain.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:MXK_kJrjxJIC",
            "Publisher": "Technical report 2006-04, DIMACS"
        },
        {
            "Title": "Approximating a gram matrix for improved kernel-based learning",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/11503415_22",
            "Abstract": "A problem for many kernel-based methods is that the amount of computation required to find the solution scales as O(n 3), where n is the number of training examples. We develop and analyze an algorithm to compute an easily-interpretable low-rank approximation to an n \u00d7 n Gram matrix G such that computations of interest may be performed more rapidly. The approximation is of the form , where C is a matrix consisting of a small number c of columns of G and W  k  is the best rank-k approximation to W, the matrix formed by the intersection between those c columns of G and the corresponding c rows of G. An important aspect of the algorithm is the probability distribution used to randomly sample the columns; we will use a judiciously-chosen and data-dependent nonuniform probability distribution. Let || \u00b7||2 and || \u00b7|| F  denote the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:YOwf2qJgpHMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Fast approximation of matrix coherence and statistical leverage",
            "Publication year": 2012,
            "Publication url": "https://www.jmlr.org/papers/volume13/drineas12a/drineas12a.pdf",
            "Abstract": "The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr\u00f6m-based low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n\u00d7 d matrix A, with n\u226b d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O (nd logn) time, as opposed to the O (nd2) time required by the na\u0131ve algorithm that involves computing an orthogonal basis for the range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n\u2248 d, and the extension to streaming environments.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:hC7cP41nSMkC",
            "Publisher": "JMLR. org"
        },
        {
            "Title": "Deterministic feature selection for regularized least squares classification",
            "Publication year": 2014,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-662-44851-9_34",
            "Abstract": "We introduce a deterministic sampling based feature selection technique for regularized least squares classification. The method is unsupervised and gives worst-case guarantees of the generalization power of the classification function after feature selection with respect to the classification function obtained using all features. We perform experiments on synthetic and real-world datasets, namely a subset of TechTC-300 datasets, to support our theory. Experimental results indicate that the proposed method performs better than the existing feature selection methods.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:OU6Ihb5iCvQC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "RandNLA: Randomization in Numerical Linear Algebra",
            "Publication year": 2015,
            "Publication url": "https://scholar.google.com/scholar?cluster=15367969379649123989&hl=en&oi=scholarr",
            "Abstract": "The goal of RandNLA is to design novel algorithms for numerical linear algebra problems by using randomization, eg, random sampling and random projections. It is a topic that has received a great deal of interdisciplinary interest in recent years, with contributions coming from numerical linear algebra, theoretical computer science, scientific computing, statistics, optimization, data analysis, and machine learning, as well as application areas such as genetics, physics, astronomy, and internet modeling.The summer school is designed to bring graduate students up to date on the state of the art in the theory, numerical aspects, and data analysis applications of RandNLA. Since RandNLA is quite interdisciplinary, students will be selected from a wide range of backgrounds. Thus the courses are designed to provide students with an overview of RandNLA and also an understanding of the complementary strengths and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:VOx2b1Wkg3QC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Faster Randomized Infeasible Interior Point Methods for Tall/Wide Linear Programs.",
            "Publication year": 2020,
            "Publication url": "https://web.ics.purdue.edu/~chowdhu5/chowdhury_et_al_neurips2020.pdf",
            "Abstract": "Linear programming (LP) is used in many machine learning applications, such as l1-regularized SVMs, basis pursuit, nonnegative matrix factorization, etc. Interior Point Methods (IPMs) are one of the most popular methods to solve LPs both in theory and in practice. Their underlying complexity is dominated by the cost of solving a system of linear equations at each iteration. In this paper, we consider infeasible IPMs for the special case where the number of variables is much larger than the number of constraints (ie, wide), or vice-versa (ie, tall) by taking the dual. Using tools from Randomized Linear Algebra, we present a preconditioning technique that, when combined with the Conjugate Gradient iterative solver, provably guarantees that infeasible IPM algorithms (suitably modified to account for the error incurred by the approximate solver), converge to a feasible, approximately optimal solution, without increasing their iteration complexity. Our empirical evaluations verify our theoretical results on both real and synthetic data.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:ML0RJ9NH7IQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An improved approximation algorithm for the column subset selection problem",
            "Publication year": 2009,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/1.9781611973068.105",
            "Abstract": "We consider the problem of selecting the \u201cbest\u201d subset of exactly k columns from an m \u00d7 n matrix A. In particular, we present and analyze a novel two-stage algorithm that runs in O(min{mn2, m2n}) time and returns as output an m \u00d7 k matrix C consisting of exactly k columns of A. In the first stage (the randomized stage), the algorithm randomly selects O(k log k) columns according to a judiciously-chosen probability distribution that depends on information in the top-k right singular subspace of A. In the second stage (the deterministic stage), the algorithm applies a deterministic column-selection procedure to select and return exactly k columns from the set of columns selected in the first stage. Let C be the m \u00d7 k matrix containing those k columns, let PC denote the projection matrix onto the span of those columns, and let Ak denote the \u201cbest\u201d rank-k approximation to the matrix A as computed with the singular value \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:ufrVoPGSRksC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Connectivity in time-graphs",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S1574119210001288",
            "Abstract": "Dynamic networks are characterized by topologies that vary with time and are represented by time-graphs. The notion of connectivity in time-graphs is fundamentally different from that in static graphs. End-to-end connectivity is achieved opportunistically by the store\u2013carry-forward paradigm if the network is so sparse that source\u2013destination pairs are usually not connected by complete paths. In static graphs, it is well known that the network connectivity is tied to the spectral gap of the underlying adjacency matrix of the topology: if the gap is large, the network is well connected. In this paper, a similar metric is investigated for time-graphs. To this end, a time-graph is represented by a 3-mode reachability tensor which indicates whether a node is reachable from another node in t steps. To evaluate connectivity, we consider the expected hitting time of a random walk, and the time it takes for epidemic routing to infect all \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:r0BpntZqJG4C",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Atomic-level characterization of the ensemble of the A\u03b2 (1\u201342) monomer in water using unbiased molecular dynamics simulations and spectral algorithms",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S002228361001123X",
            "Abstract": "A\u03b2(1\u201342) is the highly pathologic isoform of amyloid-\u03b2, the peptide constituent of fibrils and neurotoxic oligomers involved in Alzheimer's disease. Recent studies on the structural features of A\u03b2 in water have suggested that the system can be described as an ensemble of distinct conformational species in fast exchange. Here, we use replica exchange molecular dynamics (REMD) simulations to characterize the conformations accessible to A\u03b242 in explicit water solvent, under the ff99SB force field. Monitoring the correlation between J-coupling(3JHNH\u03b1) and residual dipolar coupling (RDC) data calculated from the REMD trajectories to their experimental values, as determined by NMR, indicates that the simulations converge towards sampling an ensemble that is representative of the experimental data after 60 ns/replica of simulation time. We further validate the converged MD-derived ensemble through direct \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:0EnyYjriUFMC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Column selection via adaptive sampling",
            "Publication year": 2015,
            "Publication url": "https://proceedings.neurips.cc/paper/2015/hash/d395771085aab05244a4fb8fd91bf4ee-Abstract.html",
            "Abstract": "Selecting a good column (or row) subset of massive data matrices has found many applications in data analysis and machine learning. We propose a new adaptive sampling algorithm that can be used to improve any relative-error column selection algorithm. Our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known relative-error column subset selection algorithms. Our experimental results on synthetic and real-world data show that our algorithm outperforms non-adaptive sampling as well as prior adaptive sampling approaches.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:sSrBHYA8nusC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Future directions in tensor-based computation and modeling",
            "Publication year": 2009,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.4420&rep=rep1&type=pdf",
            "Abstract": "High-dimensional modeling is becoming ubiquitous across the sciences and engineering because of advances in sensor technology and storage technology. Computationally-oriented researchers no longer have to avoid what were once intractably large, tensor-structured data sets. The current NSF promotion of \u201ccomputational thinking\u201d is timely: we need a focused international effort to oversee the transition from matrix-based to tensor-based computational thinking. The successful problem-solving tools provided by the numerical linear algebra community need to be broadened and generalized. However, tensor-based research is not just matrix-based research with additional subscripts. Tensors are data objects in their own right and there is much to learn about their geometry and their connections to statistics and operator theory. This requires full participation of researchers from engineering, the natural sciences, and the information sciences, together with statisticians, mathematicians, numerical analysts, and software/language designers. Representatives from these disciplines participated in the Workshop. We believe that the NSF can help ensure the vitality of \u201cbig N\u201d engineering and science by systematically supporting research in tensor-based computation and modeling.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:J_g5lzvAfSwC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Structural Properties Underlying High-Quality Randomized Numerical Linear Algebra Algorithms.",
            "Publication year": 2016,
            "Publication url": "https://scholar.google.com/scholar?cluster=11516533063351063521&hl=en&oi=scholarr",
            "Abstract": "In recent years, the amount of data that has been generated and recorded has grown enormously, and data are now seen to be at the heart of modern economic activity, innovation, and growth. See, for example, the report by the McKinsey Global Institute [51], which identifies ways in which Big Data have transformed the modern world, as well as the report by the National Research Council [19], which discusses reasons for and technical challenges in massive data analysis. In many cases, these so-called Big Data are modeled as matrices, basically since an m\u00d7 n matrix A provides a natural mathematical structure with which to encode information about m objects, each of which is described by n features. As a result, while linear algebra algorithms have been of interest for decades in areas such as numerical linear algebra (NLA) and scientific computing, in recent years, there has been renewed interest in \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:5Ul4iDaHHb8C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Genetics of the peloponnesean populations and the theory of extinction of the medieval peloponnesean Greeks",
            "Publication year": 2017,
            "Publication url": "https://www.amad.org/jspui/handle/123456789/66685",
            "Abstract": "Beschreibung: Peloponnese has been one of the cradles of the Classical European civilization and an important contributor to the ancient European history. It has also been the subject of a controversy about the ancestry of its population. In a theory hotly debated by scholars for over 170 years, the German historian Jacob Philipp Fallmerayer proposed that the medieval Peloponneseans were totally extinguished by Slavic and Avar invaders and replaced by Slavic settlers during the 6th century CE. Here we use 2.5 million single-nucleotide polymorphisms to investigate the genetic structure of Peloponnesean populations in a sample of 241 individuals originating from all districts of the peninsula and to examine predictions of the theory of replacement of the medieval Peloponneseans by Slavs. We find considerable heterogeneity of Peloponnesean populations exemplified by genetically distinct subpopulations and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:hMsQuOkrut0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Competitive recommendation systems",
            "Publication year": 2002,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/509907.509922",
            "Abstract": "A recommendation system tracks past purchases of a group of users to make product recommendations to individual members of the group. In this paper we present a notion of competitive recommendation systems, building on recent theoretical work on this subject. We reduce the problem of achieving competitiveness to a problem in matrix reconstruction. We then present a matrix reconstruction scheme that is competitive: it requires a small overhead in the number of users and products to be sampled, delivering in the process a net utility that closely approximates the best possible with full knowledge of all user-product preferences.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:IjCSPb-OGe4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Structural convergence results for approximation of dominant subspaces from block Krylov spaces",
            "Publication year": 2018,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/16M1091745",
            "Abstract": "This paper is concerned with approximating the dominant left singular vector space of a real matrix  of arbitrary dimension, from block Krylov spaces generated by the matrix  and the block vector . Two classes of results are presented. First are bounds on the distance, in the two- and Frobenius norms, between the Krylov space and the target space. The distance is expressed in terms of principal angles. Second are bounds for the low-rank approximation computed from the Krylov space compared to the best low-rank approximation, in the two- and Frobenius norms. For starting guesses  of full column-rank, the bounds depend on the tangent of the principal angles between  and the dominant right singular vector space of . The results presented here form the structural foundation for the analysis of randomized Krylov space methods. The innovative feature is a combination of traditional Lanczos \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:Y5dfb0dijaUC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Meta-analysis of tourette syndrome and attention deficit hyperactivity disorder provides support for a shared genetic basis",
            "Publication year": 2016,
            "Publication url": "https://www.frontiersin.org/articles/10.3389/fnins.2016.00340/full",
            "Abstract": "Gilles de la Tourette Sydrome (TS) is a childhood onset neurodevelopmental disorder, characterized phenotypically by the presence of multiple motor and vocal tics. It is often accompanied by multiple psychiatric comorbidities, with Attention Deficit/Hyperactivity Disorder (ADHD) among the most common. The extensive co-occurrence of the two disorders suggests a shared genetic background. A major step towards the elucidation of the genetic architecture of TS was undertaken by the first TS Genome-wide Association Study (GWAS) reporting 552 SNPs that were moderately associated with TS (p<1E-3). Similarly, initial ADHD GWAS attempts and meta-analysis were not able to produce genome-wide significant findings, but have provided insight to the genetic basis of the disorder. Here, we examine the common genetic background of the two neuropsychiatric phenotypes, by meta-analyzing the 552 top hits in the TS GWAS with the results of ADHD first GWASs. We identify 19 significant SNPs, with the top four implicated genes being TBC1D7, GUCY1A3, RAP1GDS1 and CHST11. TBCD17 harbors the top scoring SNP, rs1866863 (p:3.23E-07), located in a regulatory region downstream of the gene, and the third best-scoring SNP, rs2458304 (p:2.54E-06), located within an intron of the gene. Both variants were in linkage disequilibrium with eQTL rs499818, indicating a role in the expression levels of the gene. TBC1D7 is the third subunit of the TSC1/TSC2 complex, an inhibitor of the mTOR signaling pathway, with a central role in cell growth and autophagy. The top genes implicated by our study indicate a complex and intricate interplay between \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:wbdj-CoPYUoC",
            "Publisher": "Frontiers"
        },
        {
            "Title": "Constructing Compact Signatures for Individual Fingerprinting of Brain Connectomes",
            "Publication year": 2021,
            "Publication url": "https://www.frontiersin.org/articles/10.3389/fnins.2021.549322/full",
            "Abstract": "Recent neuroimaging studies have shown that functional connectomes are unique to individuals, i.e., two distinct fMRIs taken over different sessions of the same subject are more similar in terms of their connectomes than those from two different subjects. In this study, we present new results that identify specific parts of resting-state and task-specific connectomes that are responsible for the unique signatures. We show that a very small part of the connectome can be used to derive features for discriminating between individuals. A network of these features is shown to achieve excellent training and test accuracy in matching imaging datasets. We show that these features are statistically significant, robust to perturbations, invariant across populations, and are localized to a small number of structural regions of the brain. Furthermore, we show that for task-specific connectomes, the regions identified by our method are consistent with their known functional characterization. We present a new matrix sampling technique to derive computationally efficient and accurate methods for identifying the discriminating sub-connectome and support all of our claims using state-of-the-art statistical tests and computational techniques.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:vbGhcppDl1QC",
            "Publisher": "Frontiers"
        },
        {
            "Title": "Fast Monte Carlo algorithms for matrices III: Computing a compressed approximate matrix decomposition",
            "Publication year": 2006,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/S0097539704442702",
            "Abstract": "In many applications, the data consist of (or may be naturally formulated as) an  matrix A which may be stored on disk but which is too large to be read into random access memory (RAM) or to practically perform superlinear polynomial time computations on it. Two algorithms are presented which, when given an  matrix A, compute approximations to A which are the product of three smaller matrices, C, U, and R, each of which may be computed rapidly. Let  be the computed approximate decomposition; both algorithms have provable bounds for the error matrix . In the first algorithm, c columns of A and r rows of A are randomly chosen. If the  matrix C consists of those c columns of A (after appropriate rescaling) and the  matrix R consists of those r rows of A (also after appropriate rescaling), then the  matrix U may be calculated from C and R. For any matrix X, let  and  denote its Frobenius norm and its spectral norm \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:2osOgNQ5qMEC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Unsupervised feature selection for principal components analysis",
            "Publication year": 2008,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1401890.1401903",
            "Abstract": "Principal Components Analysis (PCA) is the predominant linear dimensionality reduction technique, and has been widely applied on datasets in all scientific domains. We consider, both theoretically and empirically, the topic of unsupervised feature selection for PCA, by leveraging algorithms for the so-called Column Subset Selection Problem (CSSP). In words, the CSSP seeks the\" best\" subset of exactly k columns from an m x n data matrix A, and has been extensively studied in the Numerical Linear Algebra community. We present a novel two-stage algorithm for the CSSP. From a theoretical perspective, for small to moderate values of k, this algorithm significantly improves upon the best previously-existing results [24, 12] for the CSSP. From an empirical perspective, we evaluate this algorithm as an unsupervised feature selection strategy in three application domains of modern statistical data analysis: finance \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:LkGwnXOMwfcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On concurrent error detection with bounded latency in FSMs",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1268910/",
            "Abstract": "We discuss the problem of concurrent error detection (CED) with bounded latency in finite state machines (FSMs). The objective of this approach is to reduce the overhead of CED, albeit at the cost of introducing a small latency in the detection of errors. In order to ensure no loss of error detection capabilities as compared to CED without latency, an upper bound is imposed on the introduced latency. We examine the necessary conditions for performing CED with bounded latency, based on which we extend a parity-based method to permit bounded latency. We formulate the problem of minimizing the number of required parity bits as an integer program and we propose an algorithm based on linear program relaxation and randomized rounding to solve it. Experimental results indicate that allowing a small bounded latency reduces the hardware cost of the CED circuitry.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:ZeXyd9-uunAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Inferring geographic coordinates of origin for Europeans using small panels of ancestry informative markers",
            "Publication year": 2010,
            "Publication url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0011892",
            "Abstract": "Recent large-scale studies of European populations have demonstrated the existence of population genetic structure within Europe and the potential to accurately infer individual ancestry when information from hundreds of thousands of genetic markers is used. In fact, when genomewide genetic variation of European populations is projected down to a two-dimensional Principal Components Analysis plot, a surprising correlation with actual geographic coordinates of self-reported ancestry has been reported. This substructure can hamper the search of susceptibility genes for common complex disorders leading to spurious correlations. The identification of genetic markers that can correct for population stratification becomes therefore of paramount importance. Analyzing 1,200 individuals from 11 populations genotyped for more than 500,000 SNPs (Population Reference Sample), we present a systematic exploration of the extent to which geographic coordinates of origin within Europe can be predicted, with small panels of SNPs. Markers are selected to correlate with the top principal components of the dataset, as we have previously demonstrated. Performing thorough cross-validation experiments we show that it is indeed possible to predict individual ancestry within Europe down to a few hundred kilometers from actual individual origin, using information from carefully selected panels of 500 or 1,000 SNPs. Furthermore, we show that these panels can be used to correctly assign the HapMap Phase 3 European populations to their geographic origin. The SNPs that we propose can prove extremely useful in a variety of different settings, such as \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:aqlVkmm33-oC",
            "Publisher": "Public Library of Science"
        },
        {
            "Title": "Genetics of the peloponnesean populations and the theory of extinction of the medieval peloponnesean Greeks",
            "Publication year": 2017,
            "Publication url": "https://www.nature.com/articles/ejhg201718?fbclid=IwAR0_m_h8RldhRgGkGgj0O25Ug6Xr8G9me-wcAhdsF7iwIi5-CI3BMHci1YY",
            "Abstract": "Peloponnese has been one of the cradles of the Classical European civilization and an important contributor to the ancient European history. It has also been the subject of a controversy about the ancestry of its population. In a theory hotly debated by scholars for over 170 years, the German historian Jacob Philipp Fallmerayer proposed that the medieval Peloponneseans were totally extinguished by Slavic and Avar invaders and replaced by Slavic settlers during the 6th century CE. Here we use 2.5 million single-nucleotide polymorphisms to investigate the genetic structure of Peloponnesean populations in a sample of 241 individuals originating from all districts of the peninsula and to examine predictions of the theory of replacement of the medieval Peloponneseans by Slavs. We find considerable heterogeneity of Peloponnesean populations exemplified by genetically distinct subpopulations and by gene flow \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:AXPGKjj_ei8C",
            "Publisher": "Nature Publishing Group"
        },
        {
            "Title": "Non-RF to RF test correlation using learning machines: A case study",
            "Publication year": 2007,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4209884/",
            "Abstract": "The authors present a case study that employs production test data from an RF device to assess the effectiveness of four different methods in predicting the pass/fail labels of fabricated devices based on a subset of performances and, thereby, in decreasing test cost. The device employed is a zero-IF down-converter for cell-phone applications and the four methods range from a sample maximum-cover algorithm to an advanced ontogenic neural network. The results indicate that a subset of non-RF performances suffice to predict correctly the pass/fail label for the vast majority of the devices and that the addition of a few select RF performances holds great potential for reducing misprediction to industrially acceptable levels. Based on these results, the authors then discuss enhancements and experiments that will further corroborate the utility of these methods within the cost realities of analog/RF production testing.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:roLk4NBRz8UC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Structural conditions for projection-cost preservation via randomized matrix multiplication",
            "Publication year": 2019,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0024379519301144",
            "Abstract": "Projection-cost preservation is a low-rank approximation guarantee which ensures that the cost of any rank-k projection can be preserved using a smaller sketch of the original data matrix. We present a general structural result outlining four sufficient conditions to achieve projection-cost preservation. These conditions can be satisfied using tools from the Randomized Linear Algebra literature.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:eMMeJKvmdy0C",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Fast approximation of matrix coherence and statistical leverage.",
            "Publication year": 2012,
            "Publication url": "https://pdfs.semanticscholar.org/aef0/161883cfd49bbe26826cf2e40f8195ce59cf.pdf",
            "Abstract": "Fast Approximation of Matrix Coherence and Statistical Leverage Page 1 Fast Approximation \nof Matrix Coherence and Statistical Leverage Michael W. Mahoney Stanford University ( For \nmore info, see: http:// cs.stanford.edu/people/mmahoney/ or Google on \u201cMichael Mahoney\u201d) \nPage 2 Statistical leverage Def: Let A be nxd matrix, with n >> d, ie, a tall matrix. \u2022 The statistical \nleverage scores are the diagonal elements of the projection matrix onto the left singular vectors \nof A. \u2022 The coherence of the rows of A is the largest score. Basic idea: Statistical leverage \nmeasures: \u2022 correlation b/w singular vectors of a matrix and the standard basis \u2022 how much \ninfluence/leverage a row has on the the best LS fit \u2022 where in the high-dimensional space the \n(singular value) information of A is being sent, independent of what that information is \u2022 the \nextent to which a data point is an outlier Page 3 Who cares? Statistical Data Analysis and \u2022 \u201c, \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:bFI3QPDXJZMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Bridging the gap between numerical linear algebra, theoretical computer science, and data applications",
            "Publication year": 2006,
            "Publication url": "https://www.stat.uchicago.edu/~lekheng/work/siamnews.pdf",
            "Abstract": "The Workshop on Algorithms for Modern Massive Data Sets (MMDS 2006), sponsored by the National Science Foundation, Yahoo! Research, and Ask. com, was held at Stanford University, June 21\u201324. The objectives were to explore novel techniques for modeling and analyzing massive, high-dimensional, and nonlinearly structured data sets, and to bring together computer scientists, computational and applied mathematicians, statisticians, and practitioners to promote cross-fertilization of ideas. The program, with 45 talks and 24 poster presentations, drew 232 participants\u2014far exceeding the anticipated 75!MMDS 2006 grew out of discussions among the four organizers (who are also the authors of this article) about the complementary perspectives brought by the numerical linear algebra (NLA) and the theoretical computer science (TCS) communities to linear algebra and matrix computations. These discussions were motivated by data applications, and, in particular, by technological developments over the last two decades (in both scientific and Internet domains) that permit the automatic generation of very large data sets. Such data are often modeled as matrices: An m\u00d7 n real-valued matrix A provides a natural structure for encoding information about m objects, each of which is described by n features. In genetics, for example, microarray expression data can be represented in such a framework, with Aij representing the expression level of the ith gene in the jth experimental condition. Similarly, term\u2013document matrices can be constructed in many Internet applications, with Aij indicating the frequency of the jth term in the ith document. Such \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:7PzlFSSx8tAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Deterministic feature selection for linear SVM with provable guarantees",
            "Publication year": 2014,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.749.9006&rep=rep1&type=pdf",
            "Abstract": "We introduce single-set spectral sparsification as a provably accurate deterministic sampling based feature-selection technique for linear SVM which can be used in both unsupervised and supervised settings. We develop a new supervised technique of feature selection from the support vectors based on the sampling method and prove theoretically that the margin in the feature space is preserved to within \u01eb-relative error by selecting features proportional to the number of support vectors. We prove that, in the case where the sampling method is used in an unsupervised manner, we preserve both the margin and radius of minimum enclosing ball in the feature space to within \u01eb-relative error, thus ensuring comparable generalization as in the original space. By using the sampling method in an unsupervised manner for linear SVM, we solve an open problem posed in [1]. We present extensive experiments on medium and large-scale real-world datasets to support our theory and to demonstrate that our method is competitive and often better than prior state-of-the-art, which did not come with provable guarantees.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:olpn-zPbct0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Unsupervised Feature Selection for the -means Clustering Problem",
            "Publication year": 2009,
            "Publication url": "https://www.cs.purdue.edu/homes/pdrineas/documents/publications/Drineas_NIPS_09.pdf",
            "Abstract": "We present a novel feature selection algorithm for the k-means clustering problem. Our algorithm is randomized and, assuming an accuracy parameter \u03f5\u2208(0, 1), selects and appropriately rescales in an unsupervised manner \u0398 (k log (k/\u03f5)/\u03f52) features from a dataset of arbitrary dimensions. We prove that, if we run any \u03b3-approximate k-means algorithm (\u03b3\u2265 1) on the features selected using our method, we can find a (1+(1+ \u03f5) \u03b3)-approximate partition with high probability.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:ULOm3_A8WrAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "SPaRe: selective partial replication for concurrent fault-detection in FSMs",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1246543/",
            "Abstract": "We discuss SPaRe: selective partial replication, a methodology for concurrent fault detection in finite state machine (FSMs). The proposed method is similar to duplication, wherein a replica of the circuit acts as a predictor that immediately detects errors by comparison to the original FSM. However, instead of duplicating the FSM, SPaRe selects a few prediction functions which only partially replicate it. Selection is guided by the objective of minimizing the incurred hardware overhead without compromising the ability to detect all faults, yet possibly introducing fault-detection latency. SPaRe is nonintrusive and does not interfere with the encoding and implementation of the original FSM. Experimental results indicate that SPaRe achieves significant hardware overhead reduction over both duplication and test vector logic replication (TVLR), a previously reported concurrent fault-detection method. Moreover, as compared \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:4DMP91E08xMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Near-optimal column-based matrix reconstruction",
            "Publication year": 2014,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/12086755X",
            "Abstract": "We consider low-rank reconstruction of a matrix using a subset of its columns and present asymptotically optimal algorithms for both spectral norm and Frobenius norm reconstruction. The main tools we introduce to obtain our results are (i) the use of fast approximate SVD-like decompositions for column-based matrix reconstruction, and (ii) two deterministic algorithms for selecting rows from matrices with orthonormal columns, building upon the sparse representation theorem for decompositions of the identity that appeared in [J. D. Batson, D. A. Spielman, and N. Srivastava, Twice-Ramanujan sparsifiers, in Proceedings of the 41st Annual ACM Symposium on Theory of Computing (STOC), 2009, pp. 255--262].",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:4TOpqqG69KYC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Fast universalization of investment strategies",
            "Publication year": 2004,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/S0097539702405619",
            "Abstract": "A universalization of a parameterized investment strategy is an online algorithm whose average daily performance approaches that of the strategy operating with the optimal parameters determined offline in hindsight. We present a general framework for universalizing investment strategies and discuss conditions under which investment strategies are universalizable. We present examples of common investment strategies that fit into our framework. The examples include both trading strategies that decide positions in individual stocks, and portfolio strategies that allocate wealth among multiple stocks. This work extends in a natural way Cover's universal portfolio work. We also discuss the runtime efficiency of universalization algorithms. While a straightforward implementation of our algorithms runs in time exponential in the number of parameters, we show that the efficient universal portfolio computation technique of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:RHpTSmoSYBkC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Sketching-based Algorithms for Ridge Regression and Applications",
            "Publication year": 2020,
            "Publication url": "http://users.ba.cnr.it/iac/irmanm21/HHXXI/book_of_abstracts_HHXXI0.pdf#page=41",
            "Abstract": "In statistics and machine learning, ridge regression (also known as Tikhonov regularization or weight decay) is a variant of regularized least squares problems where the choice of the penalty function is the squared l2-norm. Formally, let A\u2208 Rn\u00d7 d be the design matrix and let b\u2208 Rn be the response vector. Then, the linear algebraic formulation of the ridge regression problem is as follows: x\u2217= arg min x\u2208 Rd",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:uc_IGeMz5qoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "TS-EUROTRAIN: a European-wide investigation and training network on the etiology and pathophysiology of Gilles de la Tourette syndrome",
            "Publication year": 2016,
            "Publication url": "https://www.frontiersin.org/articles/10.3389/fnins.2016.00384/full",
            "Abstract": "Gilles de la Tourette Syndrome (GTS) is characterised by the presence of multiple motor and phonic tics with a fluctuating course of intensity, frequency and severity. Up to 90% of patients with GTS present with comorbid conditions, most commonly attention-deficit/hyperactivity disorder (ADHD) and obsessive-compulsive disorder (OCD), thus providing an excellent model for the exploration of shared aetiology across disorders. TS-EUROTRAIN (FP7-PEOPLE-2012-ITN, Grant Agr.No.316978) is a Marie Curie Initial Training Network (http://ts-eurotrain.eu) that aims to elucidate the complex aetiology of the onset and clinical course of GTS, investigate the neurobiological underpinnings of GTS and related disorders, translate research findings into clinical applications and establish a pan-European infrastructure for the study of GTS. This includes the challenges of (i) assembling a large genetic database for the evaluation of the genetic architecture with high statistical power; (ii) exploring the role of gene-environment interactions including the effects of epigenetic phenomena; (iii) employing endophenotype-based approaches to understand the shared aetiology between GTS, OCD and ADHD; (iv) establishing a developmental animal model for GTS; (v) gaining new insights into the neurobiological mechanisms of GTS via cross-sectional and longitudinal neuroimaging studies; and (vi) partaking in outreach activities including the dissemination of scientific knowledge about GTS to the public. Fifteen partners from academia and industry and twelve PhD candidates pursue the project. Our ultimate aims are to elucidate the complex aetiology and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:5ugPr518TE4C",
            "Publisher": "Frontiers"
        },
        {
            "Title": "Compaction-based concurrent error detection for digital circuits",
            "Publication year": 2005,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0026269205002107",
            "Abstract": "We present a non-intrusive concurrent error detection (CED) method for combinational and sequential digital circuits. We analyze the optimal solution model and point out the limitations that prevent logic synthesis from yielding a minimal-cost monolithic CED implementation. We then propose a compaction-based alternative approach for restricted error models. The proposed method alleviates these limitations by decomposing the CED functionality into: compaction of the circuit outputs, prediction of the compacted responses, and comparison. We model the fault-free and erroneous responses as connected vertices in a graph and perform graph coloring in order to derive the compacted responses. The proposed method is first discussed within the context of combinational circuits, with zero detection latency, and subsequently extended to Finite State Machines (FSMs), with a constant detection latency of one clock \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:NMxIlDl6LWMC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Tracing sub-structure in the European American population with PCA-informative markers",
            "Publication year": 2008,
            "Publication url": "https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000114",
            "Abstract": "Genetic structure in the European American population reflects waves of migration and recent gene flow among different populations. This complex structure can introduce bias in genetic association studies. Using Principal Components Analysis (PCA), we analyze the structure of two independent European American datasets (1,521 individuals\u2013307,315 autosomal SNPs). Individual variation lies across a continuum with some individuals showing high degrees of admixture with non-European populations, as demonstrated through joint analysis with HapMap data. The CEPH Europeans only represent a small fraction of the variation encountered in the larger European American datasets we studied. We interpret the first eigenvector of this data as correlated with ancestry, and we apply an algorithm that we have previously described to select PCA-informative markers (PCAIMs) that can reproduce this structure. Importantly, we develop a novel method that can remove redundancy from the selected SNP panels and show that we can effectively remove correlated markers, thus increasing genotyping savings. Only 150\u2013200 PCAIMs suffice to accurately predict fine structure in European American datasets, as identified by PCA. Simulating association studies, we couple our method with a PCA-based stratification correction tool and demonstrate that a small number of PCAIMs can efficiently remove false correlations with almost no loss in power. The structure informative SNPs that we propose are an important resource for genetic association studies of European Americans. Furthermore, our redundancy removal algorithm can be applied on sets of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:Se3iqnhoufwC",
            "Publisher": "Public Library of Science"
        },
        {
            "Title": "Reconstructing genotypes from GWAS Summary Statistics",
            "Publication year": 2021,
            "Publication url": "https://www.biorxiv.org/content/10.1101/2021.04.02.438281v1.abstract",
            "Abstract": "The emergence of genomewide association studies (GWAS) has led to the creation of large repositories of human genetic variation, creating enormous opportunities for genetic research and worldwide collaboration. Methods that are based on GWAS summary statistics seek to leverage such records, overcoming barriers that often exist in individual-level data access while also offering significant computational savings. Here, we propose a novel framework that can reconstruct allelic and genotypic counts/frequencies for each SNP from case-control GWAS summary statistics. Our framework is simple and efficient without the need of any complicated underlying assumptions. Illustrating the great potential of this framework we also propose three summary-statistics-based applications implemented in a new software package (ReACt): GWAS meta-analysis (with and without sample overlap), case-case GWAS, and, for the first time, group polygenic risk score (PRS) estimation. We evaluate our methods against the current state-of-the-art on both synthetic data and real genotype data and show high performance in power and error control. Our novel group PRS method based on summary statistics could not be achieved prior to our proposed framework. We demonstrate here the potential applications and advantages of this approach. Our work further highlights the great potential of summary-statistics-based methodologies towards elucidating the genetic background of complex disease and opens up new avenues for research.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:EYYDruWGBe4C",
            "Publisher": "Cold Spring Harbor Laboratory"
        },
        {
            "Title": "On boosting the accuracy of non-RF to RF correlation-based specification test compaction",
            "Publication year": 2009,
            "Publication url": "https://link.springer.com/article/10.1007/s10836-009-5113-7",
            "Abstract": "Several existing methodologies have leveraged the correlation between the non-RF and the RF performances of a circuit in order to predict the latter from the former and, thus, reduce test cost. While this form of specification test compaction eliminates the need for expensive RF measurements, it also comes at the cost of reduced test accuracy, since the retained non-RF measurements and pertinent correlation models do not always suffice for adequately predicting the omitted RF measurements. To alleviate this problem, we explore several methodologies that estimate the confidence in the obtained test outcome. Subsequently, devices for which this confidence is insufficient are retested through the complete specification test suite. As we demonstrate on production test data from a zero-IF down-converter fabricated at IBM, the proposed methodologies overcome the inability of standard specification test \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:R3hNpaxXUhUC",
            "Publisher": "Springer US"
        },
        {
            "Title": "A scalable randomized least squares solver for dense overdetermined systems",
            "Publication year": 2015,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2832080.2832083",
            "Abstract": "We present a fast randomized least-squares solver for distributed-memory platforms. Our solver is based on the Blendenpik algorithm, but employs a batchwise randomized unitary transformation scheme. The batchwise transformation enables our algorithm to scale the distributed memory vanilla implementation of Blendenpik by up to\u00d7 3 and provides up to\u00d7 7.5 speedup over a state-of-the-art scalable least-squares solver based on the classic QR based algorithm. Experimental evaluations on terabyte scale matrices demonstrate excellent speedups on up to 16384 cores on a Blue Gene/Q supercomputer.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:geHnlv5EZngC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sampling algorithms for l2 regression and applications",
            "Publication year": 2006,
            "Publication url": "https://dl.acm.org/doi/abs/10.5555/1109557.1109682",
            "Abstract": "We present and analyze a sampling algorithm for the basic linear-algebraic problem of l 2 regression. The l 2 regression (or least-squares fit) problem takes as input a matrix A\u2208 R n\u00d7 d (where we assume n> d) and a target vector b\u2208 R n, and it returns as output Z= min x\u2208 R d| b-Ax| 2. Also of interest is x opt= A+b, where A+ is the Moore-Penrose generalized inverse, which is the minimum-length vector achieving the minimum. Our algorithm randomly samples r rows from the matrix A and vector b to construct an induced l 2 regression problem with many fewer rows, but with the same number of columns. A crucial feature of the algorithm is the nonuniform sampling probabilities. These probabilities depend in a sophisticated manner on the lengths, ie, the Euclidean norms, of the rows of the left singular vectors of A and the manner in which b lies in the complement of the column space of A. Under appropriate \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:eQOLeE2rZwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Efficient Genomewide Selection of PCA\u2010Correlated tSNPs for Genotype Imputation",
            "Publication year": 2011,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.2011.00673.x",
            "Abstract": "The linkage disequilibrium structure of the human genome allows identification of small sets of single nucleotide polymorphisms (SNPs) (tSNPs) that efficiently represent dense sets of markers. This structure can be translated into linear algebraic terms as evidenced by the well documented principal components analysis (PCA)\u2010based methods. Here we apply, for the first time, PCA\u2010based methodology for efficient genomewide tSNP selection; and explore the linear algebraic structure of the human genome. Our algorithm divides the genome into contiguous nonoverlapping windows of high linear structure. Coupling this novel window definition with a PCA\u2010based tSNP selection method, we analyze 2.5 million SNPs from the HapMap phase 2 dataset. We show that 10\u201325% of these SNPs suffice to predict the remaining genotypes with over 95% accuracy. A comparison with other popular methods in the ENCODE \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:isC4tDSrTZIC",
            "Publisher": "Blackwell Publishing Ltd"
        },
        {
            "Title": "Introduction to HiCOMB Workshop",
            "Publication year": 2017,
            "Publication url": "https://store.computer.org/csdl/proceedings-article/ipdpsw/2017/07965052/12OmNyRPgVB",
            "Abstract": "Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:uJ-U7cs_P_0C",
            "Publisher": "IEEE Computer Society"
        },
        {
            "Title": "Non-intrusive design of concurrently self-testable FSMs",
            "Publication year": 2002,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1181681/",
            "Abstract": "We propose a methodology for the non-intrusive design of concurrently self-testable FSMs. The proposed method is similar to duplication, wherein a replica of the original FSM acts as a predictor that immediately detects potential faults by comparison to the original FSM. However, instead of duplicating the complete FSM, the proposed method replicates only a minimal portion adequate to detect all possible faults, yet at the cost of introducing potential fault detection latency. Furthermore, in contrast to concurrent error detection approaches, which presume the ability to re-synthesize the FSM and exploit parity-based state encoding, the proposed method is non-intrusive and does not interfere with the encoding and implementation of the original FSM. Experimental results on FSMs of various sizes and densities indicate that the proposed method detects 100% of the faults with very low average fault detection latency \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:mB3voiENLucC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Feature selection for ridge regression with provable guarantees",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7439920/",
            "Abstract": "We introduce single-set spectral sparsification as a deterministic sampling\u2013based feature selection technique for regularized least-squares classification, which is the classification analog to ridge regression. The method is unsupervised and gives worst-case guarantees of the generalization power of the classification function after feature selection with respect to the classification function obtained using all features. We also introduce leverage-score sampling as an unsupervised randomized feature selection method for ridge regression. We provide risk bounds for both single-set spectral sparsification and leverage-score sampling on ridge regression in the fixed design setting and show that the risk in the sampled space is comparable to the risk in the full-feature space. We perform experiments on synthetic and real-world data sets; a subset of TechTC-300 data sets, to support our theory. Experimental results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:tS2w5q8j5-wC",
            "Publisher": "MIT Press"
        },
        {
            "Title": "Randomized Iterative Algorithms for Fisher Discriminant Analysis",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1809.03045",
            "Abstract": "Fisher discriminant analysis (FDA) is a widely used method for classification and dimensionality reduction. When the number of predictor variables greatly exceeds the number of observations, one of the alternatives for conventional FDA is regularized Fisher discriminant analysis (RFDA). In this paper, we present a simple, iterative, sketching-based algorithm for RFDA that comes with provable accuracy guarantees when compared to the conventional approach. Our analysis builds upon two simple structural results that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized linear algebra. We analyze the behavior of RFDA when the ridge leverage and the standard leverage scores are used to select predictor variables and we prove that accurate approximations can be achieved by a sample whose size depends on the effective degrees of freedom of the RFDA problem. Our results yield significant improvements over existing approaches and our empirical evaluations support our theoretical analyses.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:JQOojiI6XY0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A note on element-wise matrix sparsification via a matrix-valued Bernstein inequality",
            "Publication year": 2011,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0020019011000196",
            "Abstract": "Given a matrix A\u2208 R n\u00d7 n, we present a simple, element-wise sparsification algorithm that zeroes out all sufficiently small elements of A and then retains some of the remaining elements with probabilities proportional to the square of their magnitudes. We analyze the approximation accuracy of the proposed algorithm using a recent, elegant non-commutative Bernstein inequality, and compare our bounds with all existing (to the best of our knowledge) element-wise matrix sparsification algorithms.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:M3NEmzRMIkIC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "Intra-and interpopulation genotype reconstruction from tagging SNPs",
            "Publication year": 2007,
            "Publication url": "https://genome.cshlp.org/content/17/1/96.short",
            "Abstract": "The optimal method to be used for tSNP selection, the applicability of a reference LD map to unassayed populations, and the scalability of these methods to genome-wide analysis, all remain subjects of debate. We propose novel, scalable matrix algorithms that address these issues and we evaluate them on genotypic data from 38 populations and four genomic regions (248 SNPs typed for \u223c2000 individuals). We also evaluate these algorithms on a second data set consisting of genotypes available from the HapMap database (1336 SNPs for four populations) over the same genomic regions. Furthermore, we test these methods in the setting of a real association study using a publicly available family data set. The algorithms we use for tSNP selection and unassayed SNP reconstruction do not require haplotype inference and they are, in principle, scalable even to genome-wide analysis. Moreover, they are greedy \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:UebtZRa9Y70C",
            "Publisher": "Cold Spring Harbor Lab"
        },
        {
            "Title": "Variant Ranker: a web-tool to rank genomic data according to functional significance",
            "Publication year": 2017,
            "Publication url": "https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1752-3",
            "Abstract": "The increasing volume and complexity of high-throughput genomic data make analysis and prioritization of variants difficult for researchers with limited bioinformatics skills. Variant Ranker allows researchers to rank identified variants and determine the most confident variants for experimental validation. We describe Variant Ranker, a user-friendly simple web-based tool for ranking, filtering and annotation of coding and non-coding variants. Variant Ranker facilitates the identification of causal variants based on novelty, effect and annotation information. The algorithm implements and aggregates multiple prediction algorithm scores, conservation scores, allelic frequencies, clinical information and additional open-source annotations using accessible databases via ANNOVAR. The available information for a variant is transformed into user-specified weights, which are in turn encoded into the ranking algorithm. Through its different modules, users can (i) rank a list of variants (ii) perform genotype filtering for case-control samples (iii) filter large amounts of high-throughput data based on user custom filter requirements and apply different models of inheritance (iv) perform downstream functional enrichment analysis through network visualization. Using networks, users can identify clusters of genes that belong to multiple ontology categories (like pathways, gene ontology, disease categories) and therefore expedite scientific discoveries. We demonstrate the utility of Variant Ranker to identify causal genes using real and synthetic datasets. Our results indicate that Variant Ranker exhibits excellent performance by correctly identifying and ranking the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:PELIpwtuRlgC",
            "Publisher": "BioMed Central"
        },
        {
            "Title": "Improving analog and RF device yield through performance calibration",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5639022/",
            "Abstract": "As the semiconductor industry continues scaling devices toward smaller process nodes, maintaining acceptable yields despite process variations has become increasingly challenging. Analog and RF circuits are particularly sensitive to process variations. This article discusses the challenges of cost-effective postfabrication performance calibration in such analog and RF devices and introduces a single-test, single-tuning-step method to constrain cost and complexity while reaping the benefits of a tunable design.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:j3f4tGmQtD8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Randomized algorithms for matrices and massive data sets",
            "Publication year": 2006,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.7388&rep=rep1&type=pdf",
            "Abstract": "The tutorial will cover randomized sampling algorithms that extract structure from very large data sets modeled as matrices or tensors. Both provable algorithmic results and recent work on applying these methods to large biological and internet data sets will be discussed.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:dfsIfKJdRG4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Exploring genomic structure differences and similarities between the Greek and European HapMap populations: implications for association studies",
            "Publication year": 2012,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.2012.00730.x",
            "Abstract": "Studies of the genomic structure of the Greek population and Southeastern Europe are limited, despite the central position of the area as a gateway for human migrations into Europe. HapMap has provided a unique tool for the analysis of human genetic variation. Europe is represented by the CEU (Northwestern Europe) and the TSI populations (Tuscan Italians from Southern Europe), which serve as reference for the design of genetic association studies. Furthermore, genetic association findings are often transferred to unstudied populations. Although initial studies support the fact that the CEU can, in general, be used as reference for the selection of tagging SNPs in European populations, this has not been extensively studied across Europe. We set out to explore the genomic structure of the Greek population (56 individuals) and compare it to the HapMap TSI and CEU populations. We studied 1112 SNPs (27 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:fPk4N6BV_jEC",
            "Publisher": "Blackwell Publishing Ltd"
        },
        {
            "Title": "Genetics and Population Analysis",
            "Publication year": 2019,
            "Publication url": "https://scholar.google.com/scholar?cluster=18293269424116814018&hl=en&oi=scholarr",
            "Abstract": "Population genetics, the systematic study of patterns of genetic variation, has been undergoing an unprecedented, revolutionary phase in the recent years. The advances of modern technology have enabled the rapid and accurate mass-scale output of modern and ancient genetic data. In this article, we delve into the state-of-the-art methods utilized for computational genetic analysis, the knowledge base required for crafting analytical protocols to avoid biased data, along with important considerations for the proper assessment and interpretation of the data, the methods and their output. We also present illustrative examples of population genetics in the exploration of the human past, as well as its applications in disease mapping and association studies.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:PR6Y55bgFSsC",
            "Publisher": "Academic Press"
        },
        {
            "Title": "Matrix sparsification via the Khintchine inequality",
            "Publication year": 2009,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.4755&rep=rep1&type=pdf",
            "Abstract": "2 Rensselaer Polytechnic Institute Abstract. Given a matrix A\u2208 Rn\u00d7 n, we present a simple, element-wise sparsification algorithm that zeroes out all sufficiently small elements of A, keeps all sufficiently large elements of A, and retains some of the remaining elements with probabilities proportional to the square of their magnitudes. We analyze the approximation accuracy of the proposed algorithm using a powerful inequality bounding the norms of sums of random matrices, the so-called Khintchine inequality. As a result, we obtain improved bounds for the matrix sparsification problem.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:YFjsv_pBGBYC",
            "Publisher": "manuscript"
        },
        {
            "Title": "Confidence estimation in non-RF to RF correlation-based specification test compaction",
            "Publication year": 2008,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4556025/",
            "Abstract": "Several existing methodologies have leveraged the correlation between the non-RF and the RF performances of a circuit in order to predict the latter from the former and, thus, reduce test cost. While this form of specification test compaction eliminates the need for expensive RF measurements, it also comes at the cost of reduced test accuracy, since the retained non-RF measurements and pertinent correlation models do not always suffice for adequately predicting the omitted RF measurements. To alleviate this problem, we develop a methodology that estimates the confidence in the obtained test outcome. Subsequently, devices for which this confidence is insufficient are retested through the complete specification test suite. As we demonstrate on production test data from a zero-IF down-converter fabricated at IBM, the proposed method outperforms previous defect filtering and guard banding methods and enables \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:-f6ydRqryjwC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Randomized sketching for large-scale sparse ridge regression problems",
            "Publication year": 2016,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/7836598/",
            "Abstract": "We present a fast randomized ridge regression solver for sparse overdetermined matrices in distributed-memory platforms. Our solver is based on the Blendenpik algorithm, but employs sparse random projection schemes to construct a sketch of the input matrix. These sparse random projection sketching schemes, and in particular the use of the Randomized Sparsity-Preserving Transform, enable our algorithm to scale the distributed memory vanilla implementation of Blendenpik and provide up to \u00d7 13 speedup over a state-of-the-art parallel Cholesky-like sparse-direct solver.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:HE397vMXCloC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sampling Algorithms and Coresets for  Regression",
            "Publication year": 2009,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/070696507",
            "Abstract": "The  regression problem takes as input a matrix , a vector , and a number , and it returns as output a number  and a vector $x_{\\text{{\\sc opt}}}\\in\\mathbb{R}^d$ such that ${\\cal Z}=\\min_{x\\in\\mathbb{R}^d}\\|Ax-b\\|_p=\\|Ax_{\\text{{\\sc opt}}}-b\\|_p$. In this paper, we construct coresets and obtain an efficient two-stage sampling-based approximation algorithm for the very overconstrained () version of this classical problem, for all . The first stage of our algorithm nonuniformly samples  rows of A and the corresponding elements of b, and then it solves the  regression problem on the sample; we prove this is an 8-approximation. The second stage of our algorithm uses the output of the first stage to resample  constraints, and then it solves the  regression problem on the new sample; we prove this is a -approximation. Our algorithm unifies, improves upon, and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:kNdYIx-mwKoC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Concurrent error detection for combinational and sequential logic via output compaction",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1283716/",
            "Abstract": "We discuss the problem of non-intrusive concurrent error detection (CED) for random logic. We analyze the optimal solution model and we point out the limitations that prevent logic synthesis from yielding a minimal cost implementation. We explain how duplication-based CED exploits decomposition to alleviate these limitations for the unrestricted error model. We then examine a compaction-based CED method, which employs a similar decomposition principle to alleviate synthesis limitations for restricted error models. We demonstrate the cost reduction achieved by the decomposed method through experimental results and we discuss the points where optimality is lost, possible remedies, and extension to finite state machines (FSMs).",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:e5wmG9Sq2KIC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Feature selection for linear svm with provable guarantees",
            "Publication year": 2014,
            "Publication url": "https://arxiv.org/abs/1406.0167",
            "Abstract": "We give two provably accurate feature-selection techniques for the linear SVM. The algorithms run in deterministic and randomized time respectively. Our algorithms can be used in an unsupervised or supervised setting. The supervised approach is based on sampling features from support vectors. We prove that the margin in the feature space is preserved to within -relative error of the margin in the full feature space in the worst-case. In the unsupervised setting, we also provide worst-case guarantees of the radius of the minimum enclosing ball, thereby ensuring comparable generalization as in the full feature space and resolving an open problem posed in Dasgupta et al. We present extensive experiments on real-world datasets to support our theory and to demonstrate that our method is competitive and often better than prior state-of-the-art, for which there are no known provable guarantees.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:epqYDVWIO7EC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Non-intrusive concurrent error detection in FSMs through State/Output compaction and monitoring via parity trees",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1253783/",
            "Abstract": "We discuss a non-intrusive methodology for concurrent error detection in FSMs. The proposed method is based on compaction and monitoring of the state/output bits of an FSM via parity trees. While errors may affect more than one state/output bit, not all combinations of state/output bits constitute potential erroneous cases for a given fault model. Therefore, it is possible to compact them without loss of error information. Thus, concurrent error detection is performed through hardware that predicts the values of the compacted state/output bits and compares them to the actual values of the FSM. In order to minimize the incurred hardware overhead, a randomized algorithm is proposed for selecting the minimum number of required parity functions.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:bEWYMUwI8FkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Randomized Dimensionality Reduction for-Means Clustering",
            "Publication year": 2014,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6967844/",
            "Abstract": "We study the topic of dimensionality reduction for k-means clustering. Dimensionality reduction encompasses the union of two approaches: 1) feature selection and 2) feature extraction. A feature selection-based algorithm for k-means clustering selects a small subset of the input features and then applies k-means clustering on the selected features. A feature extraction-based algorithm for k-means clustering constructs a small set of new artificial features and then applies k-means clustering on the constructed features. Despite the significance of k-means clustering as well as the wealth of heuristic methods addressing it, provably accurate feature selection methods for k-means clustering are not known. On the other hand, two provably accurate feature extraction methods for k-means clustering are known in the literature; one is based on random projections and the other is based on the singular value decomposition \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:xtRiw3GOFMkC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Variant Ranker",
            "Publication year": 2017,
            "Publication url": "http://ikee.lib.auth.gr/record/298327",
            "Abstract": "Background: The increasing volume and complexity of high-throughput genomic data make analysis and prioritization of variants difficult for researchers with limited bioinformatics skills. Variant Ranker allows researchers to rank identified variants and determine the most confident variants for experimental validation. Results: We describe Variant Ranker, a user-friendly simple web-based tool for ranking, filtering and annotation of coding and non-coding variants. Variant Ranker facilitates the identification of causal variants based on novelty, effect and annotation information. The algorithm implements and aggregates multiple prediction algorithm scores, conservation scores, allelic frequencies, clinical information and additional open-source annotations using accessible databases via ANNOVAR. The available information for a variant is transformed into user-specified weights, which are in turn encoded into the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:Z5m8FVwuT1cC",
            "Publisher": "Aristotle University of Thessaloniki"
        },
        {
            "Title": "Near optimal linear algebra in the online and sliding window models",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/9317959/",
            "Abstract": "We initiate the study of numerical linear algebra in the sliding window model, where only the most recent    updates in a stream form the underlying data set. Although many existing algorithms in the sliding window model use or borrow elements from the smooth histogram framework (Braverman and Ostrovsky, FOCS 2007), we show that many interesting linear-algebraic problems, including spectral and vector induced matrix norms, generalized regression, and low-rank approximation, are not amenable to this approach in the row-arrival model. To overcome this challenge, we first introduce a unified row-sampling based framework that gives randomized algorithms for spectral approximation, low-rank approximation/projection-cost preservation, and   -subspace embeddings in the sliding window model, which often use nearly optimal space and achieve nearly input sparsity runtime. Our algorithms are based on \u201creverse \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:N5tVd3kTz84C",
            "Publisher": "IEEE"
        },
        {
            "Title": "CluStrat: a structure informed clustering strategy for population stratification",
            "Publication year": 2020,
            "Publication url": "https://www.biorxiv.org/content/10.1101/2020.01.15.908228v1.abstract",
            "Abstract": "Genome-wide association studies (GWAS) have been extensively used to estimate the signed effects of trait-associated alleles. Recent independent studies failed to replicate the strong evidence of selection for height across Europe implying the shortcomings of standard population stratification correction approaches. Here, we present CluStrat, a stratification correction algorithm for complex population structure that leverages the linkage disequilibrium (LD)-induced distances between individuals. CluStrat performs agglomerative hierarchical clustering using the Mahalanobis distance and then applies sketching-based randomized ridge regression on the genotype data to obtain the association statistics. With the growing size of data, computing and storing the genome wide covariance matrix is a non-trivial task. We get around this overhead by computing the GRM directly using a connection between statistical leverage scores and the Mahalanobis distance. We test CluStrat on a large simulation study of discrete and admixed, arbitrarily-structured sub-populations identifying two to three-fold more true causal variants when compared to Principal Component (PC) based stratification correction methods while trading off for a slightly higher spurious associations. Applying CluStrat on WTCCC2 Parkinson\u2019s disease (PD) data, we identified loci mapped to a host of genes associated with PD such as BACH2, MAP2, NR4A2, SLC11A1, UNC5C to name a few.CluStrat source code and user manual is available at: https://github.com/aritra90/CluStrat",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:t7zJ5fGR-2UC",
            "Publisher": "Cold Spring Harbor Laboratory"
        },
        {
            "Title": "Subspace sampling and relative-error matrix approximation: Column-based methods",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/11830924_30",
            "Abstract": "Given an m \u00d7n matrix A and an integer k less than the rank of A, the \u201cbest\u201d rank k approximation to A that minimizes the error with respect to the Frobenius norm is A  k , which is obtained by projecting A on the top k left singular vectors of A. While A  k  is routinely used in data analysis, it is difficult to interpret and understand it in terms of the original data, namely the columns and rows of A. For example, these columns and rows often come from some application domain, whereas the singular vectors are linear combinations of (up to all) the columns or rows of A. We address the problem of obtaining low-rank approximations that are directly interpretable in terms of the original columns or rows of A. Our main results are two polynomial time randomized algorithms that take as input a matrix A and return as output a matrix C, consisting of a \u201csmall\u201d (i.e \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:3fE2CSJIrl8C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Sampling sub-problems of heterogeneous Max-Cut problems and approximation algorithms",
            "Publication year": 2005,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-31856-9_5",
            "Abstract": "Recent work in the analysis of randomized approximation algorithms for NP-hard optimization problems has involved approximating the solution to a problem by the solution of a related sub-problem of constant size, where the sub-problem is constructed by sampling elements of the original problem uniformly at random. In light of interest in problems with a heterogeneous structure, for which uniform sampling might be expected to yield sub-optimal results, we investigate the use of nonuniform sampling probabilities. We develop and analyze an algorithm which uses a novel sampling method to obtain improved bounds for approximating the Max-Cut of a graph. In particular, we show that by judicious choice of sampling probabilities one can obtain error bounds that are superior to the ones obtained by uniform sampling, both for weighted and unweighted versions of Max-Cut. Of at least as much interest as the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:4JMBOYKVnBMC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "System and method of feature selection for text classification using subspace sampling",
            "Publication year": 2011,
            "Publication url": "https://patents.google.com/patent/US8046317B2/en",
            "Abstract": "An improved system and method is provided for feature selection for text classification using subspace sampling. A text classifier generator may be provided for selecting a small set of features using subspace sampling from the corpus of training data to train a text classifier for using the small set of features for classification of texts. To select the small set of features, a subspace of features from the corpus of training data may be randomly sampled according to a probability distribution over the set of features where a probability may be assigned to each of the features that is proportional to the square of the Euclidean norms of the rows of left singular vectors of a matrix of the features representing the corpus of training texts. The small set of features may classify texts using only the relevant features among a very large number of training features.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:rO6llkc54NcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Feature selection for ridge regression with provable guarantees",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1506.05173",
            "Abstract": "We introduce single-set spectral sparsification as a deterministic sampling based feature selection technique for regularized least squares classification, which is the classification analogue to ridge regression. The method is unsupervised and gives worst-case guarantees of the generalization power of the classification function after feature selection with respect to the classification function obtained using all features. We also introduce leverage-score sampling as an unsupervised randomized feature selection method for ridge regression. We provide risk bounds for both single-set spectral sparsification and leverage-score sampling on ridge regression in the fixed design setting and show that the risk in the sampled space is comparable to the risk in the full-feature space. We perform experiments on synthetic and real-world datasets, namely a subset of TechTC-300 datasets, to support our theory. Experimental results indicate that the proposed methods perform better than the existing feature selection methods.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:vDijr-p_gm4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "A randomized rounding algorithm for sparse PCA",
            "Publication year": 2017,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/3046948",
            "Abstract": "We present and analyze a simple, two-step algorithm to approximate the optimal solution of the sparse PCA problem. In the proposed approach, we first solve an \u21131-penalized version of the NP-hard sparse PCA optimization problem and then we use a randomized rounding strategy to sparsify the resulting dense solution. Our main theoretical result guarantees an additive error approximation and provides a tradeoff between sparsity and accuracy. Extensive experimental evaluation indicates that the proposed approach is competitive in practice, even compared to state-of-the-art toolboxes such as Spasm.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:eJXPG6dFmWUC",
            "Publisher": "ACM"
        },
        {
            "Title": "Constructing Compact Brain Connectomes for Individual Fingerprinting",
            "Publication year": 2018,
            "Publication url": "https://arxiv.org/abs/1805.08649",
            "Abstract": "Recent neuroimaging studies have shown that functional connectomes are unique to individuals, i.e., two distinct fMRIs taken over different sessions of the same subject are more similar in terms of their connectomes than those from two different subjects. In this study, we present significant new results that identify, for the first time, specific parts of resting-state and task-specific connectomes that code the unique signatures. We show that a very small part of the connectome codes the signatures. A network of these features is shown to achieve excellent training and test accuracy in matching imaging datasets. We show that these features are statistically significant, robust to perturbations, invariant across populations, and are localized to a small number of structural regions of the brain. Furthermore, we show that for task-specific connectomes, the regions identified by our method are consistent with their known functional characterization. We present a new matrix sampling technique to derive computationally efficient and accurate methods for identifying the discriminating sub-connectome and support all of our claims using state-of-the-art statistical tests and computational techniques.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:ZuybSZzF8UAC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fast universalization of investment strategies with provably good relative returns",
            "Publication year": 2002,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-45465-9_76",
            "Abstract": "A universalization of a parameterized investment strategy is an online algorithm whose average daily performance approaches that of the strategy operating with the optimal parameters determined offline in hindsight. We present a general framework for universalizing investment strategies and discuss conditions under which investment strategies are universalizable. We present examples of common investment strategies that fit into our framework. The examples include both trading strategies that decide positions in individual stocks, and portfolio strategies that allocate wealth among multiple stocks. This work extends Cover\u2019s universal portfolio work. We also discuss the runtime efficiency of universalization algorithms. While a straightforward implementation of our algorithms runs in time exponential in the number of parameters, we show that the efficient universal portfolio computation technique of Kalai \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:TFP_iSt0sucC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Spectral counting of triangles via element-wise sparsification and triangle-based link recommendation",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s13278-010-0001-9.pdf",
            "Abstract": "Triangle counting is an important problem in graph mining. The clustering coefficient and the transitivity ratio, two commonly used measures effectively quantify the triangle density in order to quantify the fact that friends of friends tend to be friends themselves. Furthermore, several successful graph-mining applications rely on the number of triangles in the graph. In this paper, we study the problem of counting triangles in large, power-law networks. Our algorithm, SparsifyingEigenTriangle, relies on the spectral properties of power-law networks and the Achlioptas\u2013McSherry sparsification process. SparsifyingEigenTriangle is easy to parallelize, fast, and accurate. We verify the validity of our approach with several experiments in real-world graphs, where we achieve at the same time high accuracy and considerable speedup versus a straight-forward exact counting competitor. Finally, our contributions include a \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:L8Ckcad2t8MC",
            "Publisher": "Springer Vienna"
        },
        {
            "Title": "Post-production performance calibration in analog/RF devices",
            "Publication year": 2010,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5699225/",
            "Abstract": "In semiconductor device fabrication, continual demand for high performance, high yield devices has caused designers to look to post-production tunable circuits as the next logical step in analog/RF design and test development. These approaches have not yet achieved the maturity necessary for industrial adoption, primarily due to complexity and cost. In this work, we develop a general model which systematically outlines several key observations constraining the complexity of performance calibration in analog/RF devices. Moreover, we develop a detailed cost model permitting direct comparison of performance calibration methods to industry standard specification testing. Our analysis is demonstrated on a tunable RF LNA device simulated in 0.18\u03bcm RFCMOS.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:JV2RwH3_ST0C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Targeted re-sequencing approach of candidate genes implicates rare potentially functional variants in Tourette syndrome etiology",
            "Publication year": 2016,
            "Publication url": "https://www.frontiersin.org/articles/10.3389/fnins.2016.00428/full",
            "Abstract": "Although the genetic basis of Tourette Syndrome (TS) remains unclear, several candidate genes have been implicated. Using a set of 382 TS individuals of European ancestry we investigated four candidate genes for TS (HDC, SLITRK1, BTBD9 and SLC6A4) in an effort to identify possibly causal variants using a targeted re-sequencing approach by next generation sequencing technology. Identification of possible disease causing variants under different modes of inheritance was performed using the algorithms implemented in VAAST. We prioritized variants using Variant ranker and validated five rare variants via Sanger sequencing in HDC and SLITRK1, all of which are predicted to be deleterious. Intriguingly, one of the identified variants is in linkage disequilibrium with a variant that is included among the top hits of a genome-wide association study tolerance to citalopram treatment, an antidepressant drug with off-label use also in obsessive compulsive disorder. Our findings provide additional evidence for the implication of these two genes in TS susceptibility and the possible role of these proteins in the pathobiology of TS should be revisited.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:XiVPGOgt02cC",
            "Publisher": "Frontiers"
        },
        {
            "Title": "Entropy-Driven Parity-Tree Selection for Low-Overhead Concurrent Error Detection in Finite State Machines",
            "Publication year": 2006,
            "Publication url": "https://scholar.google.com/scholar?cluster=11463567297057694341&hl=en&oi=scholarr",
            "Abstract": "This paper presents discuss the problem of parity-tree selection for performing concurrent error detection (CED) with low overhead in finite state machines (FSMs). We first develop a nonintrusive CED method based on compaction of the state/output bits of an FSM via parity trees and comparison to the correct responses, which are generated through additional on-chip parity prediction hardware. Similar to off-line test-response-compaction practices, this method minimizes the number of parity trees required for performing lossless compaction. However, while a few parity trees are typically sufficient, the area and the power consumption of the corresponding parity predictor is not always in proportion with the number of implemented functions. Therefore, parity-tree-selection methods that minimize the overhead of the parity predictor, rather than the number of parity trees, are required. Towards this end, we then extend \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:ZfRJV9d4-WMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Column Subset Selection for Unsupervised Feature Selection",
            "Publication year": 2007,
            "Publication url": "https://www.researchgate.net/profile/Christos-Boutsidis/publication/228581147_Column_Subset_Selection_for_Unsupervised_Feature_Selection/links/02e7e52aea2e7e1e61000000/Column-Subset-Selection-for-Unsupervised-Feature-Selection.pdf",
            "Abstract": "We consider, both theoretically and empirically, the problem of selecting the \u201cbest\u201d subset of exactly k columns from an m\u00d7 n data matrix A. From a theoretical perspective, we present and analyze a novel two-stage algorithm. In the first phase, the algorithm randomly selects O (k log k) columns according to a judiciously-chosen probability distribution that depends on information in the top-k right singular subspace of A. In the second stage the algorithm applies a deterministic column-selection procedure to select and return exactly k columns from the set of columns selected in the first phase. Let C be the m\u00d7 k matrix containing those k columns, let PC denote the projection matrix onto the span of those columns, and let Ak denote the \u201cbest\u201d rank-k approximation to the matrix A as computed with the singular value decomposition. Then, we prove that",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:zA6iFVUQeVQC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Random projections for support vector machines",
            "Publication year": 2013,
            "Publication url": "http://proceedings.mlr.press/v31/paul13a.html",
            "Abstract": "Let X be a data matrix of rank \u03c1, representing n points in d-dimensional space. The linear support vector machine constructs a hyperplane separator that maximizes the 1-norm soft margin. We develop a new oblivious dimension reduction technique which is precomputed and can be applied to any input matrix X. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \u03b5-relative error, ensuring comparable generalization as in the original space. We present extensive experiments with real and synthetic data to support our theory.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:RGFaLdJalmkC",
            "Publisher": "PMLR"
        },
        {
            "Title": "Roving Concurrent Error Detection for Logic Circuits",
            "Publication year": 2004,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.449.9175&rep=rep1&type=pdf",
            "Abstract": "The continuous increase in the complexity of current designs generates logic circuits that are less immune to soft errors. As the soft error rate increases, Concurrent Error Detection (CED) techniques are becoming ever more essential. A plethora of research efforts have been expended in developing CED techniques that provide high levels of reliability [1, 2, 3]. Yet, the high overhead associated with these CED methods led to little attention in the industry. In a recent trend, low-cost CED methods [4, 5, 6] have been proposed that trade fault coverage for area overhead. Low-cost CED in logic circuits is more appealing to the industry as it increases the reliability of a circuit at an acceptable area overheard. In this paper, we present a new method for the synthesis of low-cost CED circuitry for logic circuits based on the notion of test roving. As illustrated in figure 1. a, multiple substructures that appear twice in a logic circuit are extracted and added to the CED circuitry in figure 1. b. In addition, we duplicate all the gates that are not part of a substructure in the roving CED circuitry to provide complete test of all potential failure points. As the circuit operates, every substructure in the CED circuitry roves-using input and output multiplexors-between the two instances that it matches. An output comparator is utilized to detects any faults in the substructure being tested.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:P5F9QuxV20EC",
            "Publisher": "North Atlantic Test Workshop (NATW)"
        },
        {
            "Title": "Rich coresets for constrained linear regression",
            "Publication year": 2012,
            "Publication url": "https://www.researchgate.net/profile/Christos-Boutsidis/publication/221664568_Near-optimal_Coresets_For_Least-Squares_Regression/links/02e7e52aea2e97b211000000/Near-optimal-Coresets-For-Least-Squares-Regression.pdf",
            "Abstract": "A rich coreset is a subset of the data which contains nearly all the essential information. We give deterministic, low order polynomial-time algorithms to construct rich coresets for simple and multiple response linear regression, together with lower bounds indicating that there is not much room for improvement upon our results.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:BqipwSGYUEgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Parity-based concurrent error detection with bounded latency infinite state machines",
            "Publication year": 2009,
            "Publication url": "https://scholar.google.com/scholar?cluster=1279572185344547618&hl=en&oi=scholarr",
            "Abstract": "We extend a previously-developed non-intrusive concurrent error detection (CED) method for\u00ae nite state machines (FSMs) to perform CED with bounded latency. The proposed method is based on compaction of the state/output bits of the FSM via parity trees and comparison to the error-free compacted responses, which are predicted through additional hardware. The corresponding optimization problem is formulated as an integer linear program and an algorithm to approximate its optimal solution through the use of linear program relaxation and randomized rounding is outlined. In order to reduce the incurred area overhead, we approximate the entropy of parity trees required for performing lossless compaction, and we select low-entropy ones. We then extend this method to support CED with bounded latency, wherein the overhead is further reduced at the cost of introducing a small latency in the detection of \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:g5m5HwL7SMYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Randomized linear algebra approaches to estimate the von neumann entropy of density matrices",
            "Publication year": 2020,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8985428/",
            "Abstract": "The  von Neumann entropy , named after John von Neumann, is an extension of the classical concept of entropy to the field of quantum mechanics. From a numerical perspective, von Neumann entropy can be computed simply by computing all eigenvalues of a density matrix, an operation that could be prohibitively expensive for large-scale density matrices. We present and analyze three randomized algorithms to approximate von Neumann entropy of real density matrices: our algorithms leverage recent developments in the Randomized Numerical Linear Algebra (RandNLA) literature, such as randomized trace estimators, provable bounds for the power method, and the use of random projections to approximate the eigenvalues of a matrix. All three algorithms come with provable accuracy guarantees and our experimental evaluations support our theoretical findings showing considerable speedup with small loss \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:WqliGbK-hY8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Genetic history of the population of Crete",
            "Publication year": 2019,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/ahg.12328",
            "Abstract": "The medieval history of several populations often suffers from scarcity of contemporary records resulting in contradictory and sometimes biased interpretations by historians. This is the situation with the population of the island of Crete, which remained relatively undisturbed until the Middle Ages when multiple wars, invasions, and occupations by foreigners took place. Historians have considered the effects of the occupation of Crete by the Arabs (in the 9th and 10th centuries C.E.) and the Venetians (in the 13th to the 17th centuries C.E.) to the local population. To obtain insights on such effects from a genetic perspective, we studied representative samples from 17 Cretan districts using the Illumina 1 million or 2.5 million arrays and compared the Cretans to the populations of origin of the medieval conquerors and settlers. Highlights of our findings include (1) small genetic contributions from the Arab occupation to the \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:j8SEvjWlNXcC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Structural Properties Underlying High-Quality Randomized Numerical Linear Algebra Algorithms",
            "Publication year": 2016,
            "Publication url": "https://www.taylorfrancis.com/chapters/structural-properties-underlying-high-quality-randomized-numerical-linear-algebra-algorithms-michael-mahoney-petros-drineas/e/10.1201/b19567-16",
            "Abstract": "Handbook of Big Data provides a state-of-the-art overview of the analysis of large-scale datasets. Featuring contributions from well-known experts in statistics and computer science, this handbook presents a carefully curated collection of techniques from both industry and academia. Thus, the text instills a working understanding of key statistical",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:ILKRHgRFtOwC",
            "Publisher": "Chapman and Hall/CRC"
        },
        {
            "Title": "Randomized algorithms for matrix operations",
            "Publication year": 2003,
            "Publication url": "https://search.proquest.com/openview/bd32d3ee337e20c498f0dc37900fd3f3/1?pq-origsite=gscholar&cbl=18750&diss=y",
            "Abstract": "In this thesis, we present fast, Monte-Carlo algorithms for performing useful computations on large matrices (eg matrix multiplication, Singular Value Decomposition (SVD), etc.). Our algorithms are quite different from traditional numerical analysis approaches and generally fit the following model: we assume that the input matrices are prohibitively large to store in RAM and only external memory storage is possible. We are allowed to read the matrices a few times and keep a \u201csketch\u201d of the matrices in RAM. We only allow constant processing time after reading each element of the matrices; computing the \u201csketch\u201d should be a very fast process. Indeed, one of the contributions of our work is to demonstrate that a \u201csketch\u201d consisting only of a random sample of rows and/or columns of the matrices is adequate for efficient approximations. A crucial issue is how to form the random sample; an obvious choice is \u201cblind \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:k_IJM867U9cC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Coreset Construction via Randomized Matrix Multiplication.",
            "Publication year": 2017,
            "Publication url": "https://openreview.net/forum?id=8mIGnlsLSuQ",
            "Abstract": "Projection-cost preservation is a low-rank approximation guarantee which ensures that the cost of any rank- projection can be preserved using a smaller sketch of the original data matrix. We present a general structural result outlining four sufficient conditions to achieve projection-cost preservation. These conditions can be satisfied using tools from the Randomized Linear Algebra literature.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:XD-gHx7UXLsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Random projections for the nonnegative least-squares problem",
            "Publication year": 2009,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0024379509001633",
            "Abstract": "Constrained least-squares regression problems, such as the Nonnegative Least Squares (NNLS) problem, where the variables are restricted to take only nonnegative values, often arise in applications. Motivated by the recent development of the fast Johnson\u2013Lindestrauss transform, we present a fast random projection type approximation algorithm for the NNLS problem. Our algorithm employs a randomized Hadamard transform to construct a much smaller NNLS problem and solves this smaller problem using a standard NNLS solver. We prove that our approach finds a nonnegative solution vector that, with high probability, is close to the optimum nonnegative solution in a relative error approximation sense. We experimentally evaluate our approach on a large collection of term-document data and verify that it does offer considerable speedups without a significant loss in accuracy. Our analysis is based on a novel \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:Zph67rFs4hoC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "Maritime route of colonization of Europe",
            "Publication year": 2014,
            "Publication url": "https://www.pnas.org/content/111/25/9211?fbclid=IwAR2U731Fny2x2XlVeJohASWZMX6-dmUDJCQ0e-UHOJU82SVhBnj2MqORYDk",
            "Abstract": "The Neolithic populations, which colonized Europe approximately 9,000 y ago, presumably migrated from Near East to Anatolia and from there to Central Europe through Thrace and the Balkans. An alternative route would have been island hopping across the Southern European coast. To test this hypothesis, we analyzed genome-wide DNA polymorphisms on populations bordering the Mediterranean coast and from Anatolia and mainland Europe. We observe a striking structure correlating genes with geography around the Mediterranean Sea with characteristic east to west clines of gene flow. Using population network analysis, we also find that the gene flow from Anatolia to Europe was through Dodecanese, Crete, and the Southern European coast, compatible with the hypothesis that a maritime coastal route was mainly used for the migration of Neolithic farmers to Europe.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:uWQEDVKXjbEC",
            "Publisher": "National Academy of Sciences"
        },
        {
            "Title": "Sampling subproblems of heterogeneous Max\u2010Cut problems and approximation algorithms",
            "Publication year": 2008,
            "Publication url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.20196",
            "Abstract": "Recent work in the analysis of randomized approximation algorithms for NP\u2010hard optimization problems has involved approximating the solution to a problem by the solution of a related subproblem of constant size, where the subproblem is constructed by sampling elements of the original problem uniformly at random. In light of interest in problems with a heterogeneous structure, for which uniform sampling might be expected to yield suboptimal results, we investigate the use of nonuniform sampling probabilities. We develop and analyze an algorithm which uses a novel sampling method to obtain improved bounds for approximating the Max\u2010Cut of a graph. In particular, we show that by judicious choice of sampling probabilities one can obtain error bounds that are superior to the ones obtained by uniform sampling, both for unweighted and weighted versions of Max\u2010Cut. Of at least as much interest as the results \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:qUcmZB5y_30C",
            "Publisher": "Wiley Subscription Services, Inc., A Wiley Company"
        },
        {
            "Title": "PPM-accuracy error estimates for low-cost analog test: A case study",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6132735/",
            "Abstract": "Over the past decade, interest has increased in leveraging low-cost \"alternate\" test methods as a drop-in replacement for testing analog and RF devices. The current practice for testing such devices, known as specification test, can be very expensive by comparison. By substituting less-costly alternate tests for specification test, substantial test cost reduction can be achieved. Despite this promised cost reduction, the testing community has been reluctant to adopt such alternate test methods in industrial settings, as the error incurred by alternate test can be quite difficult to capture. Estimating the expected test error metrics early in the fabrication process is, therefore, extremely desirable. In this work, we present a case study in which we investigate the reliability of kernel density estimation as a means of providing such early estimates of alternate test error. We also introduce the novel application of Laplacian score \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:lSLTfruPkqcC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Approximating sparse pca from incomplete data",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1503.03903",
            "Abstract": "We study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements. We show that for a wide class of optimization problems, if the sketch is close (in the spectral norm) to the original data matrix, then one can recover a near optimal solution to the optimization problem by using the sketch. In particular, we use this approach to obtain sparse principal components and show that for \\math{m} data points in \\math{n} dimensions, \\math{O(\\epsilon^{-2}\\tilde k\\max\\{m,n\\})} elements gives an \\math{\\epsilon}-additive approximation to the sparse PCA problem (\\math{\\tilde k} is the stable rank of the data matrix). We demonstrate our algorithms extensively on image, text, biological and financial data. The results show that not only are we able to recover the sparse PCAs from the incomplete data, but by using our sparse sketch, the running time drops by a factor of five or more.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:K3LRdlH-MEoC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Distance matrix reconstruction from incomplete distance information for sensor network localization",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/4068311/",
            "Abstract": "This paper focuses on the principled study of distance reconstruction for distance-based node localization. We address an important issue in node localization by showing that a highly incomplete set of inter-node distance measurements obtained in ad-hoc node deployments carries sufficient information for the accurate reconstruction of the missing distances, even in the presence of noise and sensor node failures. We provide an efficient and provably accurate algorithm for this reconstruction, and we show that the resulting error is bounded, decreasing at a rate that is inversely proportional to radicn, the square root of the number of nodes in the region of deployment. Although this result is applicable to many localization schemes, in this paper we illustrate its use in conjunction with the popular multidimensional scaling algorithm. Our analysis reveals valuable insights and key factors to consider during the sensor \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:5nxA0vEk-isC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Low-rank matrix approximations do not need a singular value gap",
            "Publication year": 2019,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/18M1163658",
            "Abstract": "Low-rank approximations to a real matrix  can be computed from , where  is a matrix with orthonormal columns, and the accuracy of the approximation can be estimated from some norm of . We show that  computing   in the two-norm, Frobenius norms, and more generally  any Schatten -norm is a well-posed mathematical problem; and, in contrast to dominant subspace computations, it  does not require a singular value gap. We also show that this problem is well-conditioned (insensitive) to additive perturbations in  and , and to dimension-changing or multiplicative perturbations in ---regardless of the accuracy of the approximation. For the special case when  does indeed have a singular values gap, connections are established between low-rank approximations and subspace angles.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:5awf1xo2G04C",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Fast Monte-Carlo algorithms for approximate matrix multiplication",
            "Publication year": 2001,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/959921/",
            "Abstract": "Given an m x n matrix A and an n x p matrix B, we present 2 simple and intuitive algorithms to compute an approximation P to the pr oductA\u00b7B, with provable bounds for the norm of the err or matrix \"P- A\u00b7B. Both algorithms run in O(mp+mn+np) time. In both algorithms, we randomly pick s = O(1) columns of A to form an m x s matrix S and the corresponding rows of B to form an s x p matrix R. After scaling the columns of S and the rows of R, we multiply them together to obtain our approximation P. The choice of the probability distribution we use for picking the columns of A and the scaling are the crucial features which enable us to give fairly elementary proofs of the error bounds. Our first algorithm can be implemented without storing the matrices A and B in Random Access Memory, provided we can make two passes through the matrices (stored in external memory). The second algorithm has a smaller bound on the 2 \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:SP6oXDckpogC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Participants list",
            "Publication year": 2018,
            "Publication url": "https://jifsan.umd.edu/files/wordpress/wp-content/uploads/2019/04/Talc-Symposium-Participants-Final.pdf",
            "Abstract": "Participants List Page 1 Asbestos In Talc Symposium The Hotel at the University of Maryland \nSpeakers and Moderators November 28, 2018 Participants List Robeena Aziz \nOND/CDER/FDA 240-402-1014 robeena.aziz@fda.hhs.gov Paul C. Brown Food & Drug \nAdministration 10903 New Hampshire Ave, Silver Spring, MD 20993 301-796-0856 paul.brown@fda.hhs.gov \nEd Cahill EMSL Analytical, Inc. 200 Route 130 North, Cinnaminson, NJ 08077 856-303-2565 \necahill@emsl.com John H. Callahan Food & Drug Administration 5001 Campus Drive, \nCollege Park, MD 20740 240-402-2039 (Office) 240-701-7429 (Mobile) john.callahan@fda.hhs.gov \nSteve Compton MVA Scientific Consultants 3300 Breckinridge Blvd, Suite 400, Duluth, GA \n30096 770-662-8509 scompton@mvainc.com Kevon Congo Ventext David Cooper Food and \nDrug Administration david.cooper@fda.hhs.gov William Correll Food and Drug 240-402-...\u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:4OULZ7Gr8RgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Recovering PCA and sparse PCA via hybrid- (l1, l2)sparse sampling of data elements",
            "Publication year": 2017,
            "Publication url": "https://www.jmlr.org/papers/volume18/16-258/16-258.pdf",
            "Abstract": "This paper addresses how well we can recover a data matrix when only given a few of its elements. We present a randomized algorithm that element-wise sparsifies the data, retaining only a few of its entries. Our new algorithm independently samples the data using probabilities that depend on both squares (l2 sampling) and absolute values (l1 sampling) of the entries. We prove that this hybrid algorithm (i) achieves a near-PCA reconstruction of the data, and (ii) recovers sparse principal components of the data, from a sketch formed by a sublinear sample size. Hybrid-(l1, l2) inherits the l2-ability to sample the important elements, as well as the regularization properties of l1 sampling, and maintains strictly better quality than either l1 or l2 on their own. Extensive experimental results on synthetic, image, text, biological, and financial data show that not only are we able to recover PCA and sparse PCA from incomplete data, but we can speed up such computations significantly using our sparse sketch.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:dTyEYWd-f8wC",
            "Publisher": "JMLR. org"
        },
        {
            "Title": "Tensor sparsification via a bound on the spectral norm of random tensors",
            "Publication year": 2010,
            "Publication url": "https://arxiv.org/abs/1005.4732",
            "Abstract": "Given an order- tensor $\\tensor A \\in \\R^{n \\times n \\times...\\times n}$, we present a simple, element-wise sparsification algorithm that zeroes out all sufficiently small elements of $\\tensor A$, keeps all sufficiently large elements of $\\tensor A$, and retains some of the remaining elements with probabilities proportional to the square of their magnitudes. We analyze the approximation accuracy of the proposed algorithm using a powerful inequality that we derive. This inequality bounds the spectral norm of a random tensor and is of independent interest. As a result, we obtain novel bounds for the tensor sparsification problem.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:8AbLer7MMksC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Sampling Sub-problems of Heterogeneous Max-cut Problems and Approximation",
            "Publication year": 2005,
            "Publication url": "https://scholar.google.com/scholar?cluster=6682909812842529199&hl=en&oi=scholarr",
            "Abstract": "Recent work in the analysis of randomized approximation al-gorithms for NP-hard optimization problems has involved approximat-ing the solution to a problem by the solution of a related sub-problem of constant size, where the sub-problem is constructed by sampling elements of the original problem uniformly at random. In light of interest in prob-lems with a heterogeneous structure, for which uniform sampling might be expected to yield sub-optimal results, we investigate the use of nonuniform sampling probabilities. We develop and analyze an algorithm which uses a novel sampling method to obtain improved bounds for approximating the Max-Cut of a graph. In particular, we show that by judicious choice of sampling probabilities one can obtain error bounds that are superior to the ones obtained by uniform sampling, both for weighted and unweighted versions of Max-Cut. Of at least as much interest as the results we derive are the techniques we use. The first technique is a method to compute a compressed approximate decomposition of a matrix as the product of three smaller matrices, each of which has several appealing properties. The second technique is a method to approximate the feasi-bility or infeasibility of a large linear program by checking the feasibility or infeasibility of a nonuniformly randomly chosen sub-program of the original linear program. We expect that these and related techniques will prove fruitful for the future development of randomized approximation algorithms for problems whose input instances contain heterogeneities.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:4MWp96NkSFoC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "A European population in minoan Bronze age crete",
            "Publication year": 2013,
            "Publication url": "https://www.nature.com/articles/ncomms2871?WTMay",
            "Abstract": "The first advanced Bronze Age civilization of Europe was established by the Minoans about 5,000 years before present. Since Sir Arthur Evans exposed the Minoan civic centre of Knossos, archaeologists have speculated on the origin of the founders of the civilization. Evans proposed a North African origin; Cycladic, Balkan, Anatolian and Middle Eastern origins have also been proposed. Here we address the question of the origin of the Minoans by analysing mitochondrial DNA from Minoan osseous remains from a cave ossuary in the Lassithi plateau of Crete dated 4,400\u20133,700 years before present. Shared haplotypes, principal component and pairwise distance analyses refute the Evans North African hypothesis. Minoans show the strongest relationships with Neolithic and modern European populations and with the modern inhabitants of the Lassithi plateau. Our data are compatible with the hypothesis of an \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:f2IySw72cVMC",
            "Publisher": "Nature Publishing Group"
        },
        {
            "Title": "Tracing cattle breeds with principal components analysis ancestry informative SNPs",
            "Publication year": 2011,
            "Publication url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0018007",
            "Abstract": "The recent release of the Bovine HapMap dataset represents the most detailed survey of bovine genetic diversity to date, providing an important resource for the design and development of livestock production. We studied this dataset, comprising more than 30,000 Single Nucleotide Polymorphisms (SNPs) for 19 breeds (13 taurine, three zebu, and three hybrid breeds), seeking to identify small panels of genetic markers that can be used to trace the breed of unknown cattle samples. Taking advantage of the power of Principal Components Analysis and algorithms that we have recently described for the selection of Ancestry Informative Markers from genomewide datasets, we present a decision-tree which can be used to accurately infer the origin of individual cattle. In doing so, we present a thorough examination of population genetic structure in modern bovine breeds. Performing extensive cross-validation experiments, we demonstrate that 250-500 carefully selected SNPs suffice in order to achieve close to 100% prediction accuracy of individual ancestry, when this particular set of 19 breeds is considered. Our methods, coupled with the dense genotypic data that is becoming increasingly available, have the potential to become a valuable tool and have considerable impact in worldwide livestock production. They can be used to inform the design of studies of the genetic basis of economically important traits in cattle, as well as breeding programs and efforts to conserve biodiversity. Furthermore, the SNPs that we have identified can provide a reliable solution for the traceability of breed-specific branded products.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:hFOr9nPyWt4C",
            "Publisher": "Public Library of Science"
        },
        {
            "Title": "Relative-error CUR matrix decompositions",
            "Publication year": 2008,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/07070471X",
            "Abstract": "Many data analysis applications deal with large matrices and involve approximating the matrix using a small number of \u201ccomponents.\u201d Typically, these components are linear combinations of the rows and columns of the matrix, and are thus difficult to interpret in terms of the original features of the input data. In this paper, we propose and study matrix approximations that are explicitly expressed in terms of a small number of columns and/or rows of the data matrix, and thereby more amenable to interpretation in terms of the original data. Our main algorithmic results are two randomized algorithms which take as input an  matrix A and a rank parameter k. In our first algorithm, C is chosen, and we let , where  is the Moore\u2013Penrose generalized inverse of C. In our second algorithm C, U, R are chosen, and we let . (C and R are matrices that consist of actual columns and rows, respectively, of A, and U is a generalized \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:Y0pCki6q_DkC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "On the Nystr\u00f6m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning.",
            "Publication year": 2005,
            "Publication url": "https://www.jmlr.org/papers/volume6/drineas05a/drineas05a.pdf",
            "Abstract": "A problem for many kernel-based methods is that the amount of computation required to find the solution scales as O (n3), where n is the number of training examples. We develop and analyze an algorithm to compute an easily-interpretable low-rank approximation to an n\u00d7 n Gram matrix G such that computations of interest may be performed more rapidly. The approximation is of the form Gk= CW+ k CT, where C is a matrix consisting of a small number c of columns of G and Wk is the best rank-k approximation to W, the matrix formed by the intersection between those c columns of G and the corresponding c rows of G. An important aspect of the algorithm is the probability distribution used to randomly sample the columns; we will use a judiciously-chosen and data-dependent nonuniform probability distribution. Let\u00b7 2 and\u00b7 F denote the spectral norm and the Frobenius norm, respectively, of a matrix, and let Gk be the best rank-k approximation to G. We prove that by choosing O (k/\u03b54) columns",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:d1gkVwhDpl0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix",
            "Publication year": 2006,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/S0097539704442696",
            "Abstract": "In many applications, the data consist of (or may be naturally formulated as) an  matrix A. It is often of interest to find a low-rank approximation to A, i.e., an approximation D to the matrix A of rank not greater than a specified rank k, where k is much smaller than m and n. Methods such as the singular value decomposition (SVD) may be used to find an approximation to A which is the best in a well-defined sense. These methods require memory and time which are superlinear in m and n; for many applications in which the data sets are very large this is prohibitive. Two simple and intuitive algorithms are presented which, when given an  matrix A, compute a description of a low-rank approximation  to A, and which are qualitatively faster than the SVD. Both algorithms have provable bounds for the error matrix . For any matrix X, let  and  denote its Frobenius norm and its spectral norm, respectively. In the first algorithm, c columns of A are randomly \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:9yKSN-GCB0IC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Familial early-onset dementia with complex neuropathologic phenotype and genomic background",
            "Publication year": 2016,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0197458016002104",
            "Abstract": "Despite significant progress in our understanding of hereditary neurodegenerative diseases, the list of genes associated with early-onset dementia is not yet complete. In the present study, we describe a familial neurodegenerative disorder characterized clinically as the behavioral and/or dysexecutive variant of Alzheimer\u2019s disease with neuroradiologic features of Alzheimer\u2019s disease, however, lacking amyloid-\u03b2 deposits in the brain. Instead, we observed a complex, 4 repeat predominant, tauopathy, together with a TAR DNA-binding protein of 43 kDa proteinopathy. Whole-exome sequencing on 2 affected siblings and 1 unaffected aunt uncovered a large number of candidate genes, including LRRK2 and SYNE2. In addition, DDI1, KRBA1, and TOR1A genes possessed novel stop-gain mutations only in the patients. Pathway, gene ontology, and network interaction analysis indicated the involvement of pathways \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:mvPsJ3kp5DgC",
            "Publisher": "Elsevier"
        },
        {
            "Title": "RandNLA, Pythons, and the CUR for Your Data Problems",
            "Publication year": 2016,
            "Publication url": "https://www.stat.berkeley.edu/~mmahoney/pubs/siam-news-2105g2s3.pdf",
            "Abstract": "The theme of this year\u2019s school, Randomized Numerical Linear Algebra (RandNLA), is an interdisciplinary research area that exploits randomness as an algorithmic resource for the development of improved matrix algorithms for ubiquitous problems in largescale data analysis. It utilizes ideas from theoretical computer science, numerical linear algebra, highperformance computing, and machine learning and statistics to develop, analyze, implement, and apply novel matrix algorithms. These algorithms can then facilitate the manipulation and analysis of socalled big data in numerous areas. Many popular machine learning and data analysis computations can be formulated as problems in linear algebra, but the questions of interest in machine learning and data analysis applications are very different from those historically considered in numerical linear algebra.For instance, NPhard problems have made their way into numerical linear algebra, a significant paradigm shift for numerical analysts who have traditionally formulated their problems to be solvable in polynomial time. As an example, most formulations of the Column Subset Selection problem are intractable, as are most formulations of the so called Nonnegative Matrix Factorization (NMF) problem. Alternatively, matrix factorizations\u2014if",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:BrmTIyaxlBUC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Approximation algorithms for sparse principal component analysis",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2006.12748",
            "Abstract": "We present three provably accurate, polynomial time, approximation algorithms for the Sparse Principal Component Analysis (SPCA) problem, without imposing any restrictive assumptions on the input covariance matrix. The first algorithm is based on randomized matrix multiplication; the second algorithm is based on a novel deterministic thresholding scheme; and the third algorithm is based on a semidefinite programming relaxation of SPCA. All algorithms come with provable guarantees and run in low-degree polynomial time. Our empirical evaluations confirm our theoretical findings.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:URolC5Kub84C",
            "Publisher": "Unknown"
        },
        {
            "Title": "RF specification test compaction using learning machines",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5169847/",
            "Abstract": "We present a machine learning approach to the problem of RF specification test compaction. The proposed compaction flow relies on a multi-objective genetic algorithm, which searches in the power-set of specification tests to select appropriate subsets, and a classifier, which makes pass/fail decisions based solely on these subsets. The method is demonstrated on production test data from an RF device fabricated by IBM. The results indicate that machine learning can identify intricate correlations between specification tests, which allows us to infer the outcome of all tests from a subset of tests. Thereby, the number of tests that need to be explicitly carried out and the corresponding cost are reduced significantly without adversely impacting test accuracy.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:HDshCWvjkbEC",
            "Publisher": "IEEE"
        },
        {
            "Title": "RandNLA: randomized numerical linear algebra",
            "Publication year": 2016,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2842602",
            "Abstract": "Randomization offers new benefits for large-scale linear algebra computations.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:V3AGJWp-ZtQC",
            "Publisher": "ACM"
        },
        {
            "Title": "Pass efficient algorithms for approximating large matrices.",
            "Publication year": 2003,
            "Publication url": "https://www.cs.purdue.edu/homes/pdrineas/documents/publications/Drineas_SODA_03.pdf",
            "Abstract": "In many applications, an m\u00d7 n matrix A is stored on disk and is too large to be read into RAM. Our main result is a succinct, easily computed, approximation A to A, which is also an m\u00d7 n matrix; A has the following properties (s is a positive integer under our choice, usually constant):(i) A= CUR, where C is an m\u00d7 s matrix consisting of s (randomly picked) columns of A; R is an s\u00d7 n matrix consisting of s (randomly picked) rows of A and U is an s\u00d7 s matrix computed from C, R.(ii) C, U, R can be constructed after making two passes through the whole matrix A from disk,(iii) using RAM space and additional time (in addition to the two full passes) O (m+ n) and (iv) satisfies max x:| x|= 1",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:YsMSGLbcyi4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "The fast cauchy transform: with applications to basis construction, regression, and subspace approximation in l1",
            "Publication year": 2012,
            "Publication url": "https://scholar.google.com/scholar?cluster=11175192129089467911&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:3s1wT3WcHBgC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On the compaction of independent test sequences for sequential circuits",
            "Publication year": 2003,
            "Publication url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.421.1553&rep=rep1&type=pdf",
            "Abstract": "Deterministic test generation methods typically target a primary fault and generate a test sequence for detecting it. Since the generated test sequence may also detect ancillary faults, fault simulation is subsequently employed and both the primary and the ancillary faults are eliminated from the fault list. The same fault dropping mechanism is also employed in simulation-based test generation methods, wherein random, pseudo-random, or algorithmically constructed test sequences are fault-simulated on the circuit. In either case, the primary objective is the derivation of a set of test sequences that detects all faults and fault dropping is an essential element in order to reduce test generation time. As a result, test generation methods typically produce a suboptimal set of test sequences, ie a set wherein some test sequences (or portions thereof) may be redundant. Elimination or pruning of redundant test sequences is the objective of test compaction, which may be performed either during test generation (dynamic compaction), or after test generation (static compaction). Efficient test compaction methods are very important in order to reduce test storage, test application time, and by extension, test cost. In this paper, we study a specific instance of the problem, namely the compaction of independent test sequences for sequential circuits. Such test sequences do not rely on any assumptions regarding the initial state of the circuit and are, thus, independent of it. It is also assumed that each test sequence is fault simulated only once, yet without fault dropping so that all detectable faults are obtained. Based on this information, it is possible that some test \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:pyW8ca7W8N0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "TeraPCA: a fast and scalable software package to study genetic variation in tera-scale genotypes",
            "Publication year": 2019,
            "Publication url": "https://academic.oup.com/bioinformatics/article-abstract/35/19/3679/5430929",
            "Abstract": "Principal Component Analysis is a key tool in the study of population structure in human genetics. As modern datasets become increasingly larger in size, traditional approaches based on loading the entire dataset in the system memory (Random Access Memory) become impractical and out-of-core implementations are the only viable alternative.We present TeraPCA, a C++ implementation of the Randomized Subspace Iteration method to perform Principal Component Analysis of large-scale datasets. TeraPCA can be applied both in-core and out-of-core and is able to successfully operate even on commodity hardware with a system memory of just a few gigabytes. Moreover, TeraPCA has minimal dependencies on external libraries and only requires a working installation of the BLAS and LAPACK libraries. When applied to a dataset containing a million \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:35r97b3x0nAC",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "Random projections for linear support vector machines",
            "Publication year": 2014,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/2641760",
            "Abstract": "Let X be a data matrix of rank \u03c1, whose rows represent n points in d-dimensional space. The linear support vector machine constructs a hyperplane separator that maximizes the 1-norm soft margin. We develop a new oblivious dimension reduction technique that is precomputed and can be applied to any input matrix X. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \u03b5-relative error, ensuring comparable generalization as in the original space in the case of classification. For regression, we show that the margin is preserved to \u03b5-relative error with high probability. We present extensive experiments with real and synthetic data to support our theory.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:p__nRnzSRKYC",
            "Publisher": "ACM"
        },
        {
            "Title": "Dissecting population substructure in india via correlation optimization of genetics and geodemographics",
            "Publication year": 2017,
            "Publication url": "https://www.biorxiv.org/content/10.1101/164640v1.full-text",
            "Abstract": "India represents an intricate tapestry of population sub-structure shaped by geography, language, culture and social stratification operating in concert [1\u20133]. To date, no study has attempted to model and evaluate how these evolutionary forces have interacted to shape the patterns of genetic diversity within India. Geography has been shown to closely correlate with genetic structure in other parts of the world [4, 5]. However, the strict endogamy imposed by the Indian caste system, and the large number of spoken languages add further levels of complexity. We merged all publicly available data from the Indian subcontinent into a dataset of 835 individuals across 48,373 SNPs from 84 well-defined groups [2, 6\u20139]. Bringing together geography, sociolinguistics and genetics, we developed COGG (Correlation Optimization of Genetics and Geodemographics) in order to build a model that optimally explains the observed population genetic sub-structure. We find that shared language rather than geography or social structure has been the most powerful force in creating paths of gene flow within India. Further investigating the origins of Indian substructure, we create population genetic networks across Eurasia. We observe two major corridors towards mainland India; one through the Northwestern and another through the Northeastern frontier with the Uygur population acting as a bridge across the two routes. Importantly, network, ADMIXTURE analysis and f 3 statistics support a far northern path connecting Europe to Siberia and gene flow from Siberia and Mongolia towards Central Asia and India.The genetic structure of human populations reflects gene \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:JoZmwDi-zQgC",
            "Publisher": "Cold Spring Harbor Laboratory"
        },
        {
            "Title": "Lectures on randomized numerical linear algebra",
            "Publication year": 2018,
            "Publication url": "https://books.google.com/books?hl=en&lr=&id=7HJ6DwAAQBAJ&oi=fnd&pg=PA1&dq=info:Ux8VV_zDVn8J:scholar.google.com&ots=L_cwjPWDkG&sig=-08ZHW0DETqjj6m6pyqW1_PKYAA",
            "Abstract": "Matrices are ubiquitous in computer science, statistics, and applied mathematics. An m\u00d7 n matrix can encode information about m objects (each described by n features), or the behavior of a discretized differential operator on a finite element mesh; an n\u00d7 n positive-definite matrix can encode the correlations between all pairs of n objects, or the edge-connectivity between all pairs of n nodes in a social network; and so on. Motivated largely by technological developments that generate extremely large scientific and Internet data sets, recent years have witnessed exciting developments in the theory and practice of matrix algorithms. Particularly remarkable is the use of randomization\u2014typically assumed to be a property of the input data due to, eg, noise in the data generation mechanisms\u2014as an algorithmic or computational resource for the development of improved algorithms for fundamental matrix problems such as matrix multiplication, leastsquares (LS) approximation, low-rank matrix approximation, etc. Randomized Numerical Linear Algebra (RandNLA) is an interdisciplinary research area that exploits randomization as a computational resource to develop improved algorithms for large-scale linear algebra problems. From a foundational perspective, RandNLA has its roots in theoretical computer science (TCS), with deep connections to mathematics (convex analysis, probability theory, metric embedding theory) and applied mathematics (scientific computing, signal processing, numerical linear algebra). From an applied perspective, RandNLA is a vital new tool for machine learning, statistics, and data analysis. Well-engineered implementations \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:eq2jaN3J8jMC",
            "Publisher": "AMS/IAS/SIAM"
        },
        {
            "Title": "Effective resistances, statistical leverage, and applications to linear equation solving",
            "Publication year": 2010,
            "Publication url": "https://arxiv.org/abs/1005.3097",
            "Abstract": "Recent work in theoretical computer science and scientific computing has focused on nearly-linear-time algorithms for solving systems of linear equations. While introducing several novel theoretical perspectives, this work has yet to lead to practical algorithms. In an effort to bridge this gap, we describe in this paper two related results. Our first and main result is a simple algorithm to approximate the solution to a set of linear equations defined by a Laplacian (for a graph  with  nodes and  edges) constraint matrix. The algorithm is a non-recursive algorithm; even though it runs in $O(n^2 \\cdot \\polylog(n))$ time rather than  time (given an oracle for the so-called statistical leverage scores), it is extremely simple; and it can be used to compute an approximate solution with a direct solver. In light of this result, our second result is a straightforward connection between the concept of graph resistance (which has proven useful in recent algorithms for linear equation solvers) and the concept of statistical leverage (which has proven useful in numerically-implementable randomized algorithms for large matrix problems and which has a natural data-analytic interpretation).",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:iH-uZ7U-co4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Iterative Randomized Algorithms for Low Rank Approximation of Tera-scale Matrices with Small Spectral Gaps",
            "Publication year": 2018,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/8638171/",
            "Abstract": "Randomized approaches for low rank matrix approximations have become popular in recent years and often offer significant advantages over classical algorithms because of their scalability and numerical robustness on distributed memory platforms. We present a distributed implementation of randomized block iterative methods to compute low rank matrix approximations for dense tera-scale matrices. We are particularly interested in the behavior of randomized block iterative methods on matrices with small spectral gaps. Our distributed implementation is based on four iterative algorithms: block subspace iteration, the block Lanczos method, the block Lanczos method with explicit restarts, and the thick-restarted block Lanczos method. We analyze the scalability and numerical stability of the four block iterative methods and demonstrate the performance of these methods for various choices of the spectral gap \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:_Ybze24A_UAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Clustering large graphs via the singular value decomposition",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/article/10.1023/B:MACH.0000033113.59016.96",
            "Abstract": "We consider the problem of partitioning a set of m points in the n-dimensional Euclidean space into k clusters (usually m and n are variable, while k is fixed), so as to minimize the sum of squared distances between each point and its cluster center. This formulation is usually the objective of the k-means clustering algorithm (Kanungo et al. (2000)). We prove that this problem in NP-hard even for k = 2, and we consider a continuous relaxation of this discrete problem: find the k-dimensional subspace V that minimizes the sum of squared distances to V of the m points. This relaxation can be solved by computing the Singular Value Decomposition (SVD) of the m \u00d7 n matrix A that represents the m points; this solution can be used to get a 2-approximation algorithm for the original problem. We then argue that in fact the relaxation provides a generalized clustering which is useful in its own right.Finally, we \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:u5HHmVD_uO8C",
            "Publisher": "Kluwer Academic Publishers-Plenum Publishers"
        },
        {
            "Title": "On selecting exactly k columns from a matrix",
            "Publication year": 2008,
            "Publication url": "https://www.researchgate.net/profile/Christos-Boutsidis/publication/228451455_On_selecting_exactly_k_columns_from_a_matrix/links/02e7e52aea2e80833a000000/On-selecting-exactly-k-columns-from-a-matrix.pdf",
            "Abstract": "We consider the problem of selecting the \u201cbest\u201d subset of exactly k columns from an m\u00d7 n matrix A. In particular, we present and analyze a novel two-stage algorithm that runs in O (min {mn2, m2n}) time and returns as output an m\u00d7 k matrix C consisting of exactly k columns of A. In the first stage (the randomized stage), the algorithm randomly selects O (k log k) columns according to a judiciously-chosen probability distribution that depends on information in the top-k right singular subspace of A. In the second stage (the deterministic stage), the algorithm applies a deterministic column-selection procedure to select and return exactly k columns from the set of columns selected in the first stage. Let C be the m\u00d7 k matrix containing those k columns, let PC denote the projection matrix onto the span of those columns, and let Ak denote the \u201cbest\u201d rank-k approximation to the matrix A as computed with the singular value decomposition. Then, we prove that",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:vV6vV6tmYwMC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Random Projections for -means Clustering",
            "Publication year": 2010,
            "Publication url": "https://arxiv.org/abs/1011.4632",
            "Abstract": "This paper discusses the topic of dimensionality reduction for -means clustering. We prove that any set of  points in  dimensions (rows in a matrix $A \\in \\RR^{n \\times d}$) can be projected into $t = \\Omega(k / \\eps^2)$ dimensions, for any $\\eps \\in (0,1/3)$, in $O(n d \\lceil \\eps^{-2} k/ \\log(d) \\rceil )$ time, such that with constant probability the optimal -partition of the point set is preserved within a factor of $2+\\eps$. The projection is done by post-multiplying  with a  random matrix  having entries  or  with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:QIV2ME_5wuYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "An experimental evaluation of a Monte-Carlo algorithm for singular value decomposition",
            "Publication year": 2001,
            "Publication url": "https://link.springer.com/chapter/10.1007/3-540-38076-0_19",
            "Abstract": "We demonstrate that an algorithm proposed by Drineas et. al. in [7] to approximate the singular vectors/values ofa matrix A, is not only oft heoretical interest but also a fast, viable alternative to traditional algorithms. The algorithm samples a small number ofro ws (or columns) oft he matrix, scales them appropriately to form a small matrix S and computes the singular value decomposition (SVD) of S, which is a good approximation to the SVD ofthe original matrix. We experimentally evaluate the accuracy and speed oft his randomized algorithm using image matrices and three different sampling schemes. Our results show that our approximations oft he singular vectors of A span almost the same space as the corresponding exact singular vectors of A.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:KlAtU1dfN6UC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Recovering PCA from Hybrid- Sparse Sampling of Data Elements",
            "Publication year": 2015,
            "Publication url": "https://arxiv.org/abs/1503.00547",
            "Abstract": "This paper addresses how well we can recover a data matrix when only given a few of its elements. We present a randomized algorithm that element-wise sparsifies the data, retaining only a few its elements. Our new algorithm independently samples the data using sampling probabilities that depend on both the squares ( sampling) and absolute values ( sampling) of the entries. We prove that the hybrid algorithm recovers a near-PCA reconstruction of the data from a sublinear sample-size: hybrid-() inherits the -ability to sample the important elements as well as the regularization properties of  sampling, and gives strictly better performance than either  or  on their own. We also give a one-pass version of our algorithm and show experiments to corroborate the theory.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:WbkHhVStYXYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Entropy-driven parity-tree selection for low-overhead concurrent error detection in finite state machines",
            "Publication year": 2006,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1637743/",
            "Abstract": "This paper presents discuss the problem of parity-tree selection for performing concurrent error detection (CED) with low overhead in finite state machines (FSMs). We first develop a nonintrusive CED method based on compaction of the state/output bits of an FSM via parity trees and comparison to the correct responses, which are generated through additional on-chip parity prediction hardware. Similar to off-line test-response-compaction practices, this method minimizes the number of parity trees required for performing lossless compaction. However, while a few parity trees are typically sufficient, the area and the power consumption of the corresponding parity predictor is not always in proportion with the number of implemented functions. Therefore, parity-tree-selection methods that minimize the overhead of the parity predictor, rather than the number of parity trees, are required. Towards this end, we then extend \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:mVmsd5A6BfQC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Sparse features for PCA-like linear regression",
            "Publication year": 2011,
            "Publication url": "https://www.researchgate.net/profile/Christos-Boutsidis/publication/228446763_Sparse_features_for_PCA-like_linear_regression/links/02e7e52aea2e8264ce000000/Sparse-features-for-PCA-like-linear-regression.pdf",
            "Abstract": "Principal Components Analysis (PCA) is often used as a feature extraction procedure. Given a matrix X\u2208 Rn\u00d7 d, whose rows represent n data points with respect to d features, the top k right singular vectors of X (the so-called eigenfeatures), are arbitrary linear combinations of all available features. The eigenfeatures are very useful in data analysis, including the regularization of linear regression. Enforcing sparsity on the eigenfeatures, ie, forcing them to be linear combinations of only a small number of actual features (as opposed to all available features), can promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such sparse eigenfeatures while provably achieving in-sample performance comparable to regularized linear regression. Our algorithms are relatively simple and practically efficient, and we demonstrate their performance on several data sets.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:O3NaXMp0MMsC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Near-optimal coresets for least-squares regression",
            "Publication year": 2013,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6555821/",
            "Abstract": "We study the (constrained) least-squares regression as well as multiple response least-squares regression and ask the question of whether a subset of the data, a coreset, suffices to compute a good approximate solution to the regression. We give deterministic, low-order polynomial-time algorithms to construct such coresets with approximation guarantees, together with lower bounds indicating that there is not much room for improvement upon our results.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:_xSYboBqXhAC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication",
            "Publication year": 2006,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/S0097539704442684",
            "Abstract": "Motivated by applications in which the data may be formulated as a matrix, we consider algorithms for several common linear algebra problems. These algorithms make more efficient use of computational resources, such as the computation time, random access memory (RAM), and the number of passes over the data, than do previously known algorithms for these problems. In this paper, we devise two algorithms for the matrix multiplication problem. Suppose A and B (which are  and , respectively) are the two input matrices. In our main algorithm, we perform c independent trials, where in each trial we randomly sample an element of  with an appropriate probability distribution  on . We form an  matrix C consisting of the sampled columns of A, each scaled appropriately, and we form a  matrix R using the corresponding rows of B, again scaled appropriately. The choice of  and the column and row \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:qjMakFHDy7sC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Speeding up Linear Programming using Randomized Linear Algebra",
            "Publication year": 2020,
            "Publication url": "https://arxiv.org/abs/2003.08072",
            "Abstract": "Linear programming (LP) is an extremely useful tool and has been successfully applied to solve various problems in a wide range of areas, including operations research, engineering, economics, or even more abstract mathematical areas such as combinatorics. It is also used in many machine learning applications, such as -regularized SVMs, basis pursuit, nonnegative matrix factorization, etc. Interior Point Methods (IPMs) are one of the most popular methods to solve LPs both in theory and in practice. Their underlying complexity is dominated by the cost of solving a system of linear equations at each iteration. In this paper, we consider \\emph{infeasible} IPMs for the special case where the number of variables is much larger than the number of constraints. Using tools from Randomized Linear Algebra, we present a preconditioning technique that, when combined with the Conjugate Gradient iterative solver, provably guarantees that infeasible IPM algorithms (suitably modified to account for the error incurred by the approximate solver), converge to a feasible, approximately optimal solution, without increasing their iteration complexity. Our empirical evaluations verify our theoretical results on both real-world and synthetic data.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:0KyAp5RtaNEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tensor-CUR decompositions for tensor-based data",
            "Publication year": 2008,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/060665336",
            "Abstract": "Motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more indices, we develop a tensor-based extension of the matrix CUR decomposition. The tensor-CUR decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different from the others. In this case, the tensor-CUR decomposition approximately expresses the original data tensor in terms of a basis consisting of underlying subtensors that are actual data elements and thus that have a natural interpretation in terms of the processes generating the data. Assume the data may be modeled as a -tensor, i.e., an  tensor  in which the first two modes are similar and the third is qualitatively different. We refer to each of the p different  matrices as \u201cslabs\u201d and each of the  different p-vectors as \u201cfibers.\u201d In this case, the tensor-CUR algorithm computes an approximation to the data tensor  that is of the form \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:_FxGoFyzp5QC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Mining Large Graphs.",
            "Publication year": 2016,
            "Publication url": "https://scholar.google.com/scholar?cluster=10071230279015797403&hl=en&oi=scholarr",
            "Abstract": "Graphs provide a general representation or data model for many types of data, where pairwise relationships are known or thought to be particularly important.\u2217 Thus, it should not be surprising that interest in graph mining has grown with the recent interest in big data. Much of the big data generated and analyzed involves pair-wise relationships among a set of entities. For example, in e-commerce applications such as with Amazon\u2019s product database, customers are related to products through their purchasing activities; on the web, web pages are related through hypertext linking relationships; on social networks such as Facebook, individuals are related through their friendships; and so on. Similarly, in scientific applications, research articles are related through citations; proteins are related through metabolic pathways, co-expression, and regulatory network effects within a cell; materials are related through models \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:tKAzc9rXhukC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Independent test sequence compaction through integer programming",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1240924/",
            "Abstract": "We discuss the compaction of independent test sequences for sequential circuits. Our first contribution is the formulation of this problem as an integer program, which we then solve through a well-known method employing linear programming relaxation and randomized rounding. The key contribution of this approach is that it yields the first polynomial time approximation algorithm for this problem. More specifically, it provides a provably good approximation guarantee while running in time polynomial with respect to the number of vectors in the original test sequences and the number of faults. Another virtue of our approach is that it provides a lower bound for the compacted set of test sequences and, therefore, a quality measure for the test compaction algorithm. Experimental results on benchmark circuits demonstrate that the proposed solution efficiently identifies nearly optimal sets of compacted test sequences.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:hqOjcs7Dif8C",
            "Publisher": "IEEE"
        },
        {
            "Title": "The fast cauchy transform and faster robust linear regression",
            "Publication year": 2016,
            "Publication url": "https://epubs.siam.org/doi/abs/10.1137/140963698",
            "Abstract": "We provide fast algorithms for overconstrained  regression and related problems: for an  input matrix  and vector , in  time we reduce the problem  to the same problem with input matrix  of dimension  and corresponding  of dimension . Here,   and  are a coreset for the problem, consisting of sampled and rescaled rows of  and ; and  is independent of  and polynomial in . Our results improve on the best previous algorithms when  for all  except ; in particular, they improve the  running time of Sohler and Woodruff [Proceedings of the 43rd Annual ACM Symposium on Theory of Computing, 2011, pp. 755--764] for , which uses asymptotically fast matrix multiplication, and the  time of Dasgupta et al. [SIAM J. Comput., 38 (2009), pp. 2060--2078] for general , which uses ellipsoidal rounding. We also provide a suite of improved results for finding well-conditioned bases via ellipsoidal rounding \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:ns9cj8rnVeAC",
            "Publisher": "Society for Industrial and Applied Mathematics"
        },
        {
            "Title": "Who benefits?",
            "Publication year": 2019,
            "Publication url": "https://dl.acm.org/doi/fullHtml/10.1145/3332807",
            "Abstract": "Considering the case of smart cities.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:J-pR_7NvFogC",
            "Publisher": "ACM"
        },
        {
            "Title": "Energy minimization via graph cuts: Settling what is possible",
            "Publication year": 2005,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1467543/",
            "Abstract": "The recent explosion of interest in graph cut methods in computer vision naturally spawns the question: what energy functions can be minimized via graph cuts? This question was first attacked by two papers of Kolmogorov and Zabih, in which they dealt with functions with pair-wise and triplewise pixel interactions. In this work, we extend their results in two directions. First, we examine the case of k-wise pixel interactions; the results are derived from a purely algebraic approach. Second, we discuss the applicability of provably approximate algorithms. Both of these developments should help researchers best understand what can and cannot be achieved when designing graph cut based algorithms.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:zYLM7Y9cAGgC",
            "Publisher": "IEEE"
        },
        {
            "Title": "PATHWAY ANALYSIS ON WHOLE GENOME DATA FOR GILLES DE LA TOURETTE SYNDROME IMPLICATES TCF3",
            "Publication year": 2017,
            "Publication url": "https://scholar.google.com/scholar?cluster=4890434012113369859&hl=en&oi=scholarr",
            "Abstract": "Unknown",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:SdhP9T11ey4C",
            "Publisher": "ELSEVIER SCIENCE BV"
        },
        {
            "Title": "On compaction-based concurrent error detection",
            "Publication year": 2003,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1214383/",
            "Abstract": "We examine a low-cost, zero-latency, non-intrusive CED method for restricted error models. The method is based on compaction of the circuit outputs, prediction of the compacted responses, and comparison. This method also achieves significant hardware cost reduction by utilizing the information available through the restricted error model. We assume that the error model is not defined through permanent or transient faults in the hardware, but rather in terms of the erroneous behavior that such faults induce. Thus, any fault model can be described by providing for every input combination the error-free response and all erroneous responses resulting from faults in the model.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:35N4QoGY0k4C",
            "Publisher": "IEEE"
        },
        {
            "Title": "Faster least squares approximation",
            "Publication year": 2011,
            "Publication url": "https://link.springer.com/content/pdf/10.1007/s00211-010-0331-6.pdf",
            "Abstract": "Least squares approximation is a technique to find an approximate solution to a system of linear equations that has no exact solution. In a typical setting, one lets n be the number of constraints and d be the number of variables, with . Then, existing exact methods find a solution vector in O(nd 2) time. We present two randomized algorithms that provide accurate relative-error approximations to the optimal value and the solution vector of a least squares approximation problem more rapidly than existing exact algorithms. Both of our algorithms preprocess the data with the Randomized Hadamard transform. One then uniformly randomly samples constraints and solves the smaller problem on those constraints, and the other performs a sparse random projection and solves the smaller problem on those projected coordinates. In both cases, solving the smaller problem provides relative-error \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:WF5omc3nYNoC",
            "Publisher": "Springer-Verlag"
        },
        {
            "Title": "PCA-correlated SNPs for structure identification in worldwide human populations",
            "Publication year": 2007,
            "Publication url": "https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0030160",
            "Abstract": "Existing methods to ascertain small sets of markers for the identification of human population structure require prior knowledge of individual ancestry. Based on Principal Components Analysis (PCA), and recent results in theoretical computer science, we present a novel algorithm that, applied on genomewide data, selects small subsets of SNPs (PCA-correlated SNPs) to reproduce the structure found by PCA on the complete dataset, without use of ancestry information. Evaluating our method on a previously described dataset (10,805 SNPs, 11 populations), we demonstrate that a very small set of PCA-correlated SNPs can be effectively employed to assign individuals to particular continents or populations, using a simple clustering algorithm. We validate our methods on the HapMap populations and achieve perfect intercontinental differentiation with 14 PCA-correlated SNPs. The Chinese and Japanese populations can be easily differentiated using less than 100 PCA-correlated SNPs ascertained after evaluating 1.7 million SNPs from HapMap. We show that, in general, structure informative SNPs are not portable across geographic regions. However, we manage to identify a general set of 50 PCA-correlated SNPs that effectively assigns individuals to one of nine different populations. Compared to analysis with the measure of informativeness, our methods, although unsupervised, achieved similar results. We proceed to demonstrate that our algorithm can be effectively used for the analysis of admixed populations without having to trace the origin of individuals. Analyzing a Puerto Rican dataset (192 individuals, 7,257 SNPs), we show that PCA \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:UeHWp8X0CEIC",
            "Publisher": "Public Library of Science"
        },
        {
            "Title": "Random walks in time-graphs",
            "Publication year": 2010,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1755743.1755761",
            "Abstract": "Dynamic networks are characterized by topologies that vary with time and are represented by time-graphs. The notion of connectivity in time-graphs is fundamentally different than that in static graphs. End-to-end connectivity is achieved opportunistically by store-forward-carry paradigm if the network is so sparse that source-destination pairs are usually not connected by complete paths. In static graphs, it is well known that the network connectivity is tied to the spectral gap of the underlying adjacency matrix of the topology: if the gap is large, the network is well connected and a random walk on this graph has a small hitting time. In this paper, we investigate a similar metric for time-graphs, which indicates how quickly opportunistic methods deliver packets to destinations, speed of convergence in estimating an entity and quickness in the online optimization of protocol parameters, etc. To this end, a time-graph is \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:IWHjjKOFINEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Feature selection for linear SVM with provable guarantees",
            "Publication year": 2015,
            "Publication url": "http://proceedings.mlr.press/v38/paul15.html",
            "Abstract": "We give two provably accurate feature-selection techniques for the linear SVM. The algorithms run in deterministic and randomized time respectively. Our algorithms can be used in an unsupervised or supervised setting. The supervised approach is based on sampling features from support vectors. We prove that the margin in the feature space is preserved to within \u03b5-relative error of the margin in the full feature space in the worst-case. In the unsupervised setting, we also provide worst-case guarantees of the radius of the minimum enclosing ball, thereby ensuring comparable generalization as in the full feature space and resolving an open problem posed in Dasgupta et al. We present extensive experiments on real-world datasets to support our theory and to demonstrate that our methods are competitive and often better than prior state-of-the-art, for which there are no known provable guarantees.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:UxriW0iASnsC",
            "Publisher": "PMLR"
        },
        {
            "Title": "An iterative, sketching-based framework for ridge regression",
            "Publication year": 2018,
            "Publication url": "http://proceedings.mlr.press/v80/chowdhury18a.html",
            "Abstract": "Ridge regression is a variant of regularized least squares regression that is particularly suitable in settings where the number of predictor variables greatly exceeds the number of observations. We present a simple, iterative, sketching-based algorithm for ridge regression that guarantees high-quality approximations to the optimal solution vector. Our analysis builds upon two simple structural results that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized linear algebra. An important contribution of our work is the analysis of the behavior of subsampled ridge regression problems when the ridge leverage scores are used: we prove that accurate approximations can be achieved by a sample whose size depends on the degrees of freedom of the ridge-regression problem rather than the dimensions of the design matrix. Our experimental evaluations verify our theoretical results on both real and synthetic data.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:dQ2og3OwTAUC",
            "Publisher": "PMLR"
        },
        {
            "Title": "Spectral counting of triangles in power-law networks via element-wise sparsification",
            "Publication year": 2009,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/5231934/",
            "Abstract": "Triangle counting is an important problem in graph mining. The clustering coefficient and the transitivity ratio,two commonly used measures effectively quantify the triangle density in order to quantify the fact that friends of friends tend to be friends themselves. Furthermore, several successful graph mining applications rely on the number of triangles. In this paper, we study the problem of counting triangles in large, power-law networks. Our algorithm, SparsifyingEigenTriangle, relies on the spectral properties of power-law networks and the Achlioptas-McSherry sparsification process. SparsifyingEigenTriangle is easy to parallelize, fast and accurate. We verify the validity of our approach with several experiments in real-world graphs, where we achieve at the same time high accuracy and important speedup versus a straight-forward exact counting competitor.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:dhFuZR0502QC",
            "Publisher": "IEEE"
        },
        {
            "Title": "A note on randomized element-wise matrix sparsification",
            "Publication year": 2014,
            "Publication url": "https://arxiv.org/abs/1404.0320",
            "Abstract": "Given a matrix A \\in R^{m x n}, we present a randomized algorithm that sparsifies A by retaining some of its elements by sampling them according to a distribution that depends on both the square and the absolute value of the entries. We combine the ideas of [4, 1] and provide an elementary proof of the approximation accuracy of our algorithm following [4] without the truncation step.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:KxtntwgDAa4C",
            "Publisher": "Unknown"
        },
        {
            "Title": "RandNLA: Randomization in Numerical Linear Algebra",
            "Publication year": 2018,
            "Publication url": "https://ipsen.math.ncsu.edu/ps/RandNLA_Tutorial_ALA2015.pdf",
            "Abstract": "Remark 1: We already know from the Central Limit Theorem that a sum of n independent, identically distributed random variables with bounded, common, mean and variance converges to a normal distribution as n approaches infinity. The inequalities that we will discuss deal with smaller, finite values of n.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:LPZeul_q3PIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "On proving the efficiency of alternative RF tests",
            "Publication year": 2011,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/6105415/",
            "Abstract": "The deployment of alternative, low-cost RF test methods in industry has been, to date, rather limited. This is due to the potentially impaired ability to identify device pass/fail labels when departing from traditional specification test. By relying on alternative tests, pass/fail labels must be derived indirectly through new test limits defined for the alternative tests, which may incur error in the form of test escapes or yield loss. Clearly, estimating these test metrics as early as possible in the test development process is key to the success of an alternative test approach. In this work, we employ a test metrics estimation technique based on non-parametric kernel density estimation to obtain such early estimates, and, for the first time, demonstrate a real-world case study of test metric estimation efficiency at parts-per-million levels. To achieve this, we employ a set of more than 1 million RF devices fabricated by Texas Instruments \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:hMod-77fHWUC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Subspace sampling and relative-error matrix approximation: Column-row-based methods",
            "Publication year": 2006,
            "Publication url": "https://link.springer.com/chapter/10.1007/11841036_29",
            "Abstract": "Much recent work in the theoretical computer science, linear algebra, and machine learning has considered matrix decompositions of the following form: given an m \u00d7n matrix A, decompose it as a product of three matrices, C, U, and R, where C consists of a small number of columns of A, R consists of a small number of rows of A, and U is a small carefully constructed matrix that guarantees that the product CUR is \u201cclose\u201d to A. Applications of such decompositions include the computation of matrix \u201csketches\u201d, speeding up kernel-based statistical learning, preserving sparsity in low-rank matrix representation, and improved interpretability of data analysis methods. Our main result is a randomized, polynomial algorithm which, given as input an m \u00d7n matrix A, returns as output matrices C, U, R such that  with probability at least 1\u2013\u03b4. Here, A \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:M3ejUd6NZC8C",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Studying e-mail graphs for intelligence monitoring and analysis in the absence of semantic information",
            "Publication year": 2004,
            "Publication url": "https://link.springer.com/chapter/10.1007/978-3-540-25952-7_22",
            "Abstract": "This work describes a methodology that can be used to identify structure and communication patterns within an organization based on e-mail data. The first step of the method is the construction of an e-mail graph; we then experimentally show that the adjacency matrix of the graph is well approximated by a low-rank matrix. The low-rank property indicates that Principal Component Analysis techniques may be used to remove the noise and extract the structural information (e.g. user communities, communication patterns, etc.). Furthermore, it is shown that the e-mail graph degree distribution (both with respect to indegrees and outdegrees) follows power laws; we also demonstrate that there exists a giant component connecting 70% of the nodes.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:Wp0gIr-vW9MC",
            "Publisher": "Springer, Berlin, Heidelberg"
        },
        {
            "Title": "Identifying influential entries in a matrix",
            "Publication year": 2013,
            "Publication url": "https://arxiv.org/abs/1310.3556",
            "Abstract": "For any matrix A in R^(m x n) of rank \\rho, we present a probability distribution over the entries of A (the element-wise leverage scores of equation (2)) that reveals the most influential entries in the matrix. From a theoretical perspective, we prove that sampling at most s = O ((m + n) \\rho^2 ln (m + n)) entries of the matrix (see eqn. (3) for the precise value of s) with respect to these scores and solving the nuclear norm minimization problem on the sampled entries, reconstructs A exactly. To the best of our knowledge, these are the strongest theoretical guarantees on matrix completion without any incoherence assumptions on the matrix A. From an experimental perspective, we show that entries corresponding to high element-wise leverage scores reveal structural properties of the data matrix that are of interest to domain scientists.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:EUQCXRtRnyEC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A randomized singular value decomposition algorithm for image processing applications",
            "Publication year": 2001,
            "Publication url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.450.4027&rep=rep1&type=pdf",
            "Abstract": "The main contribution of this paper is to demonstrate that a new randomized SVD algorithm, proposed by Drineas et. al. in [4], is not only of theoretical interest but also a viable and fast alternative to traditional SVD algorithms in applications (eg image processing). This algorithm samples a constant number of rows (or columns) of the matrix, scales them appropriately to form a small matrix, say S, and then computes the SVD of S (which is a good approximation to the SVD of the original matrix). We experimentally evaluate the accuracy and speed of this algorithm for image matrices, using various probability distributions to perform the sampling.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:TQgYirikUcIC",
            "Publisher": "Unknown"
        },
        {
            "Title": "Ancestry informative markers for fine-scale individual assignment to worldwide populations",
            "Publication year": 2010,
            "Publication url": "https://jmg.bmj.com/content/47/12/835.short",
            "Abstract": "The analysis of large-scale genetic data from thousands of individuals has revealed the fact that subtle population genetic structure can be detected at levels that were previously unimaginable. Using the Human Genome Diversity Panel as reference (51 populations - 650,000 SNPs), this works describes a systematic evaluation of the resolution that can be achieved for the inference of genetic ancestry, even when small panels of genetic markers are used.A comprehensive investigation of human population structure around the world is undertaken by leveraging the power of Principal Components Analysis (PCA). The problem is dissected into hierarchical steps and a decision tree for the prediction of individual ancestry is proposed. A complete leave-one-out validation experiment demonstrates that, using all available SNPs, assignment of individuals to their self-reported \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:qxL8FJ1GzNcC",
            "Publisher": "BMJ Publishing Group Ltd"
        },
        {
            "Title": "A randomized rounding algorithm for sparse PCA",
            "Publication year": 2015,
            "Publication url": "https://scholar.google.com/scholar?cluster=1618348721467756027&hl=en&oi=scholarr",
            "Abstract": "We present and analyze a simple, two-step algorithm to approximate the optimal solution of the sparse PCA problem. Our approach first solves a convex (i1) relaxation of the NP-hard sparse PCA optimization problem and then uses a randomized rounding strategy to sparsify the resulting dense solution. Our main theoretical result guarantees an additive error approximation and provides a tradeoff between sparsity and accuracy. Our experimental evaluation indicates that our approach is competitive in practice, even compared to state-of-the-art toolboxes such as Spasm.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:4fKUyHm3Qg0C",
            "Publisher": "Unknown"
        },
        {
            "Title": "Tensor sparsification via a bound on the spectral norm of random tensors",
            "Publication year": 2015,
            "Publication url": "https://academic.oup.com/imaiai/article-abstract/4/3/195/691349",
            "Abstract": "Given an order- tensor , we present a simple, element-wise sparsification algorithm that zeroes out all sufficiently small elements of , keeps all sufficiently large elements of  and retains some of the remaining elements with probabilities proportional to the square of their magnitudes. We analyze the approximation accuracy of the proposed algorithm using a powerful inequality that we derive. This inequality bounds the spectral norm of a random tensor and is of independent interest. As a result, we obtain novel bounds for the tensor sparsification problem.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:_Qo2XoVZTnwC",
            "Publisher": "Oxford University Press"
        },
        {
            "Title": "Feature selection methods for text classification",
            "Publication year": 2007,
            "Publication url": "https://dl.acm.org/doi/abs/10.1145/1281192.1281220",
            "Abstract": "We consider feature selection for text classification both theoretically and empirically. Our main result is an unsupervised feature selection strategy for which we give worst-case theoretical guarantees on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the features. To the best of our knowledge, this is the first feature selection method with such guarantees. In addition, the analysis leads to insights as to when and why this feature selection strategy will perform well in practice. We then use the TechTC-100, 20-Newsgroups, and Reuters-RCV2 data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against two commonly-used strategies. Our empirical evaluation shows that the strategy with provable performance guarantees performs well in comparison with other commonly-used \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:W7OEmFMy1HYC",
            "Publisher": "Unknown"
        },
        {
            "Title": "A randomized algorithm for approximating the log determinant of a symmetric positive definite matrix",
            "Publication year": 2017,
            "Publication url": "https://www.sciencedirect.com/science/article/pii/S0024379517304147",
            "Abstract": "We introduce a novel algorithm for approximating the logarithm of the determinant of a symmetric positive definite (SPD) matrix. The algorithm is randomized and approximates the traces of a small number of matrix powers of a specially constructed matrix, using the method of Avron and Toledo [1]. From a theoretical perspective, we present additive and relative error bounds for our algorithm. Our additive error bound works for any SPD matrix, whereas our relative error bound works for SPD matrices whose eigenvalues lie in the interval (\u03b8 1, 1), with 0< \u03b8 1< 1; the latter setting was proposed in [16]. From an empirical perspective, we demonstrate that a C++ implementation of our algorithm can approximate the logarithm of the determinant of large matrices very accurately in a matter of seconds.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:738O_yMBCRsC",
            "Publisher": "North-Holland"
        },
        {
            "Title": "CUR matrix decompositions for improved data analysis",
            "Publication year": 2009,
            "Publication url": "https://www.pnas.org/content/106/3/697.short",
            "Abstract": "Principal components analysis and, more generally, the Singular Value Decomposition are fundamental data analysis tools that express a data matrix in terms of a sequence of orthogonal or uncorrelated vectors of decreasing importance. Unfortunately, being linear combinations of up to all the data points, these vectors are notoriously difficult to interpret in terms of the data and processes generating the data. In this article, we develop CUR matrix decompositions for improved data analysis. CUR decompositions are low-rank matrix decompositions that are explicitly expressed in terms of a small number of actual columns and/or actual rows of the data matrix. Because they are constructed from actual data elements, CUR decompositions are interpretable by practitioners of the field from which the data are drawn (to the extent that the original data are). We present an algorithm that preferentially chooses columns and \u2026",
            "Abstract entirety": 0,
            "Author pub id": "Yw2PquQAAAAJ:Tyk-4Ss8FVUC",
            "Publisher": "National Academy of Sciences"
        },
        {
            "Title": "Cost-driven selection of parity trees",
            "Publication year": 2004,
            "Publication url": "https://ieeexplore.ieee.org/abstract/document/1299259/",
            "Abstract": "We discuss the problem of parity tree selection for lossless compaction of the output responses of a circuit. Earlier methods assume off-chip storage of the correct compacted responses and therefore minimize the number of necessary parity trees. In contrast, our method targets on-chip generation of the correct compacted responses and therefore minimizes the actual implementation cost of the corresponding parity prediction functions. We present a systematic search approach that exploits the correlation between the hardware cost of a function and its entropy, in order to select parity trees that minimize the incurred cost, while achieving lossless compaction. Experimental results demonstrate that our method achieves significant hardware reduction over methods that minimize the number of parity trees.",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:9ZlFYXVOiuMC",
            "Publisher": "IEEE"
        },
        {
            "Title": "Technical Report, YALEU/DCS/TR-1269, February 2004.",
            "Publication year": 2001,
            "Publication url": "https://www.stat.berkeley.edu/~mmahoney/pubs/matrix1.pdf",
            "Abstract": "Motivated by applications in which the data may be formulated as a matrix, we consider algorithms for several common Linear Algebra problems. These algorithms make more efficient use of computational resources, such as the computation time, Random Access Memory (RAM), and the number of passes over the data, than do previously known algorithms for these problems; in addition, they achieve their greater efficiency at the cost of some error. In this paper, we devise two algorithms for the Matrix Multiplication Problem. Suppose A and B (which are m\u00a2 n and n\u00a2 p respectively) are the two input matrices. In our main algorithm, we perform \u0441= O (1) independent trials, where in each trial we randomly sample an element of 1, 2,... n with an appropriate probability distribution \u00c8 on 1, 2,... n. We form a m\u00a2 \u0441 matrix C consisting of the sampled columns of A, each scaled appropriately, and we form a \u0441\u00a2 n matrix R using the same rows of B, again scaled appropriately. The choice of \u00c8 and the column and row scaling are crucial features of the algorithm. When these are chosen judiciously, we show that CR is a good approximation to AB; more precisely, we show that, with high probability,",
            "Abstract entirety": 1,
            "Author pub id": "Yw2PquQAAAAJ:a0OBvERweLwC",
            "Publisher": "Unknown"
        }
    ]
}]