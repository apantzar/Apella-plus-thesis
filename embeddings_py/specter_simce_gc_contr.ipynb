{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a63094-5728-45d2-9d78-3183f2485213",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install ipywidgets --user\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, InputExample\n",
    "from sentence_transformers import util\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f80fd7-d6aa-43fd-a9b2-a680e636226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_samples(sentences_path):\n",
    "    train_samples = []\n",
    "    df = pd.read_csv(sentences_path, sep=\",\")\n",
    "    for i in tqdm(range(len(df))):\n",
    "      train_samples.append(InputExample(texts=[df.iloc[i,0], df.iloc[i,1]]))\n",
    "    return train_samples\n",
    "\n",
    "def fine_tune_bertlike_model(sentences_path,\n",
    "                             model_name: str,\n",
    "                             train_batch_size: int,\n",
    "                             max_seq_length: int,\n",
    "                             learning_rate: str,\n",
    "                             num_epochs: int,\n",
    "                             train_samples: list):\n",
    "    \n",
    "    print(f\"FINE TUNE BERT LIKE MODEL max_seq_length:{max_seq_length}, train_batch_size:{train_batch_size}, learning_rate:{learning_rate}\")\n",
    "    print(f\"Cuda is available:{torch.cuda.is_available()}\")\n",
    "    print(f\"GPU total available memory {round(torch.cuda.get_device_properties(0).total_memory / 1000000000, 2)} GB\")\n",
    "    \n",
    "    #### Just some code to print debug information to stdout\n",
    "    logging.basicConfig(format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S', level=logging.INFO, handlers=[LoggingHandler()])\n",
    "\n",
    "    # Save path to store our model\n",
    "    model_output_path = f\"./models/train_simcse-{max_seq_length}_{train_batch_size}_{learning_rate}\"\n",
    "    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "\n",
    "    # Apply mean pooling to get one fixed sized sentence vector\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    # We train our model using the MultipleNegativesRankingLoss\n",
    "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size, drop_last=True)\n",
    "    logging.info(\"Train sentences: {}\".format(len(train_samples)))\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data for warm-up\n",
    "    logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "              epochs=num_epochs,\n",
    "              warmup_steps=warmup_steps,\n",
    "              optimizer_params={'lr': 3e-5},\n",
    "              checkpoint_path=model_output_path,\n",
    "              show_progress_bar=True,\n",
    "              use_amp=False)  # Set to True, if your GPU supports FP16 cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1439c3-78fb-4824-97bb-062eeb33dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function - Used to call bertlike fine tuner\n",
    "output_name = \"./data/dataset_titles_abstract_large.csv\"\n",
    "\n",
    "train_samples = get_train_samples(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916ece5-d89f-4427-b849-991531d1d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "model_name = 'allenai/specter'\n",
    "train_batch_size = 40\n",
    "max_seq_length = 300\n",
    "learning_rate = \"3e-5\"\n",
    "num_epochs = 1\n",
    "\n",
    "fine_tune_bertlike_model(sentences_path=output_name,\n",
    "                         model_name=model_name,\n",
    "                         train_batch_size=train_batch_size,\n",
    "                         max_seq_length=max_seq_length,\n",
    "                         learning_rate=learning_rate,\n",
    "                         num_epochs=num_epochs,\n",
    "                         train_samples=train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614976d-e6c0-4067-a6cd-7cf23a185252",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r /home/jupyter/models/trained_models.zip /home/jupyter/models/train_simcse-300_40_3e-5"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
