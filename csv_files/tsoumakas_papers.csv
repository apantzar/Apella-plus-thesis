Title,Publication Year,Publication url,Abstract,Abstract entirety
Multi-label classification: An overview,2007,https://www.igi-global.com/article/multi-label-classification/1786,"Multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization, and semantic scene classification. This article introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multilabel classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.",1
Mining multi-label data,2009,https://link.springer.com/chapter/10.1007/978-0-387-09823-4_34,"A large body of research in supervised learning deals with the analysis of single-label data, where training examples are associated with a single label λ from a set of disjoint labels L. However, training examples in several application domains are often associated with a set of labels Y ⊆ L. Such data are called multi-label.Textual data, such as documents and web pages, are frequently annotated with more than a single label. For example, a news article concerning the reactions of the Christian church to the release of the “Da Vinci Code” film can be labeled as both religion and movies. The categorization of textual data is perhaps the dominant multi-label application.",1
Random k-Labelsets: An Ensemble Method for Multilabel Classification,2007,https://link.springer.com/chapter/10.1007/978-3-540-74958-5_38,"This paper proposes an ensemble method for multilabel classification. The RAndom k-labELsets (RAKEL) algorithm constructs each member of the ensemble by considering a small random subset of labels and learning a single-label classifier for the prediction of each element in the powerset of this subset. In this way, the proposed algorithm aims to take into account label correlations using single-label classifiers that are applied on subtasks with manageable number of labels and adequate number of examples per label. Experimental results on common multilabel domains involving protein, document and scene classification show that better performance can be achieved compared to popular multilabel classification approaches.",1
Multi-label classification of music into emotions.,2008,https://books.google.com/books?hl=en&lr=&id=OHp3sRnZD-oC&oi=fnd&pg=PA325&dq=info:7dzC1REq6pwJ:scholar.google.com&ots=oGMPpLfCa8&sig=y5KCxX5h9OCwlNNBhk_LQFYdjdo,"In this paper, the automated detection of emotion in music is modeled as a multilabel classification task, where a piece of music may belong to more than one class. Four algorithms are evaluated and compared in this task. Furthermore, the predictive power of several audio features is evaluated using a new multilabel feature selection method. Experiments are conducted on a set of 593 songs with 6 clusters of music emotions based on the Tellegen-Watson-Clark model. Results provide interesting insights into the quality of the discussed algorithms and features.",1
Random k-labelsets for multi-label classification,2010,https://ieeexplore.ieee.org/abstract/document/5567103/,"A simple yet effective multilabel learning method, called label powerset (LP), considers each distinct combination of labels that exist in the training set as a different class value of a single-label classification task. The computational efficiency and predictive performance of LP is challenged by application domains with large number of labels and training examples. In these cases, the number of classes may become very large and at the same time many classes are associated with very few training examples. To deal with these problems, this paper proposes breaking the initial set of labels into a number of small random subsets, called labelsets and employing LP to train a corresponding classifier. The labelsets can be either disjoint or overlapping depending on which of two strategies is used to construct them. The proposed method is called RAkEL (RAndom k labELsets), where k is a parameter that specifies the size …",0
Mulan: A java library for multi-label learning,2011,https://www.jmlr.org/papers/volume12/tsoumakas11a/tsoumakas11a.pdf,"MULAN is a Java library for learning from multi-label data. It offers a variety of classification, ranking, thresholding and dimensionality reduction algorithms, as well as algorithms for learning from hierarchically structured labels. In addition, it contains an evaluation framework that calculates a rich variety of performance measures.",1
Effective and efficient multilabel classification in domains with large number of labels,2008,http://www.ecmlpkdd2008.org/files/pdf/workshops/mmd/4.pdf,"This paper contributes a novel algorithm for effective and computationally efficient multilabel classification in domains with large label sets L. The HOMER algorithm constructs a Hierarchy Of Multilabel classifiERs, each one dealing with a much smaller set of labels compared to L and a more balanced example distribution. This leads to improved predictive performance along with linear training and logarithmic testing complexities with respect to| L|. Label distribution from parent to children nodes is achieved via a new balanced clustering algorithm, called balanced k means.",1
Multilabel text classification for automated tag suggestion,2008,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.183.2636&rep=rep1&type=pdf,"The increased popularity of tagging during the last few years can be mainly attributed to its embracing by most of the recently thriving user-centric content publishing and management Web 2.0 applications. However, tagging systems have some limitations that have led researchers to develop methods that assist users in the tagging process, by automatically suggesting an appropriate set of tags. We have tried to model the automated tag suggestion problem as a multilabel text classification task in order to participate in the ECML/PKDD 2008 Discovery Challenge.",1
An empirical study of lazy multilabel classification algorithms,2008,https://link.springer.com/chapter/10.1007/978-3-540-87881-0_40,"Multilabel classification is a rapidly developing field of machine learning. Despite its short life, various methods for solving the task of multilabel classification have been proposed. In this paper we focus on a subset of these methods that adopt a lazy learning approach and are based on the traditional k-nearest neighbor (kNN) algorithm. Two are our main contributions. Firstly, we implement BRkNN, an adaptation of the kNN algorithm for multilabel classification that is conceptually equivalent to using the popular Binary Relevance problem transformation method in conjunction with the kNN algorithm, but much faster. We also identify two useful extensions of BRkNN that improve its overall predictive performance. Secondly, we compare this method against two other lazy multilabel classification methods, in order to determine the overall best performer. Experiments on different real-world multilabel datasets …",0
Protein classification with multiple algorithms,2005,https://link.springer.com/chapter/10.1007/11573036_42,"Nowadays, the number of protein sequences being stored in central protein databases from labs all over the world is constantly increasing. From these proteins only a fraction has been experimentally analyzed in order to detect their structure and hence their function in the corresponding organism. The reason is that experimental determination of structure is labor-intensive and quite time-consuming. Therefore there is the need for automated tools that can classify new proteins to structural families. This paper presents a comparative evaluation of several algorithms that learn such classification models from data concerning patterns of proteins with known structure. In addition, several approaches that combine multiple learning algorithms to increase the accuracy of predictions are evaluated. The results of the experiments provide insights that can help biologists and computer scientists design high …",0
On the stratification of multi-label data,2011,https://link.springer.com/chapter/10.1007/978-3-642-23808-6_10,"Stratified sampling is a sampling method that takes into account the existence of disjoint groups within a population and produces samples where the proportion of these groups is maintained. In single-label classification tasks, groups are differentiated based on the value of the target variable. In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratified sampling could/should be performed. This paper investigates stratification in the multi-label data context. It considers two stratification methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria. The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.",1
Tracking recurring contexts using ensemble classifiers: an application to email filtering,2010,https://link.springer.com/article/10.1007/s10115-009-0206-2,"Concept drift constitutes a challenging problem for the machine learning and data mining community that frequently appears in real world stream classification problems. It is usually defined as the unforeseeable concept change of the target variable in a prediction task. In this paper, we focus on the problem of recurring contexts, a special sub-type of concept drift, that has not yet met the proper attention from the research community. In the case of recurring contexts, concepts may re-appear in future and thus older classification models might be beneficial for future classifications. We propose a general framework for classifying data streams by exploiting stream clustering in order to dynamically build and update an ensemble of incremental classifiers. To achieve this, a transformation function that maps batches of examples into a new conceptual representation model is proposed. The clustering algorithm is …",0
Multi-target regression via input space expansion: treating targets as inputs,2016,https://link.springer.com/article/10.1007/s10994-016-5546-z,"In many practical applications of supervised learning the task involves the prediction of multiple target variables from a common set of input variables. When the prediction targets are binary the task is called multi-label classification, while when the targets are continuous the task is called multi-target regression. In both tasks, target variables often exhibit statistical dependencies and exploiting them in order to improve predictive accuracy is a core challenge. A family of multi-label classification methods address this challenge by building a separate model for each target on an expanded input space where other targets are treated as additional input variables. Despite the success of these methods in the multi-label classification domain, their applicability and effectiveness in multi-target regression has not been studied until now. In this paper, we introduce two new methods for multi-target regression, called …",0
On the utility of incremental feature selection for the classification of textual data streams,2005,https://link.springer.com/chapter/10.1007/11573036_32,"In this paper we argue that incrementally updating the features that a text classification algorithm considers is very important for real-world textual data streams, because in most applications the distribution of data and the description of the classification concept changes over time. We propose the coupling of an incremental feature ranking method and an incremental learning algorithm that can consider different subsets of the feature vector during prediction (what we call a feature based classifier), in order to deal with the above problem. Experimental results with a longitudinal database of real spam and legitimate emails shows that our approach can adapt to the changing nature of streaming data and works much better than classical incremental learning algorithms.",1
Correlation-based pruning of stacked binary relevance models for multi-label learning,2009,https://www.academia.edu/download/30764166/learning-from-multi-label-data.pdf#page=102,"Binary relevance (BR) learns a single binary model for each different label of multi-label data. It has linear complexity with respect to the number of labels, but does not take into account label correlations and may fail to accurately predict label combinations and rank labels according to relevance with a new instance. Stacking the models of BR in order to learn a model that associates their output to the true value of each label is a way to alleviate this problem. In this paper we propose the pruning of the models participating in the stacking process, by explicitly measuring the degree of label correlation using the phi coefficient. Exploratory analysis of phi shows that the correlations detected are meaningful and useful. Empirical evaluation of the pruning approach shows that it leads to substantial reduction of the computational cost of stacking and occasional improvements in predictive performance.",1
A comprehensive study over VLAD and product quantization in large-scale image retrieval,2014,https://ieeexplore.ieee.org/abstract/document/6847226/,"This paper deals with content-based large-scale image retrieval using the state-of-the-art framework of VLAD and Product Quantization proposed by Jegou as a starting point. Demonstrating an excellent accuracy-efficiency trade-off, this framework has attracted increased attention from the community and numerous extensions have been proposed. In this work, we make an in-depth analysis of the framework that aims at increasing our understanding of its different processing steps and boosting its overall performance. Our analysis involves the evaluation of numerous extensions (both existing and novel) as well as the study of the effects of several unexplored parameters. We specifically focus on: a) employing more efficient and discriminative local features; b) improving the quality of the aggregated representation; and c) optimizing the indexing scheme. Our thorough experimental evaluation provides new insights …",0
An ensemble pruning primer,2009,https://link.springer.com/chapter/10.1007/978-3-642-03999-7_1,Ensemble pruning deals with the reduction of an ensemble of predictive models in order to improve its efficiency and predictive performance. The last 12 years a large number of ensemble pruning methods have been proposed. This work proposes a taxonomy for their organization and reviews important representative methods of each category. It abstracts their key components and discusses their main advantages and disadvantages. We hope that this work will serve as a good starting point and reference for researchers working on the development of new ensemble pruning methods.,1
Selective fusion of heterogeneous classifiers,2005,https://content.iospress.com/articles/intelligent-data-analysis/ida00225,"There are two main paradigms in combining different classification algorithms: Classifier Selection and Classifier Fusion. The first one selects a single model for classifying a new instance, while the latter combines the decisions of all models. The work presented in this paper stands in between these two paradigms aiming to tackle the disadvantages and benefit from the advantages of both. In particular, this paper proposes the use of statistical procedures for the selection of the best subgroup among different classification algorithms and the subsequent fusion of the decision of the models in this subgroup with simple methods like Weighted Voting. Extensive experimental results show that the proposed approach, Selective Fusion, improves over simple selection and fusion methods, leading to performance comparable with the state-of-the-art heterogeneous classifier combination method of Stacking, without the …",0
Clustering based multi-label classification for image annotation and retrieval,2009,https://ieeexplore.ieee.org/abstract/document/5346902/,"This paper presents a novel multi-label classification framework for domains with large numbers of labels. Automatic image annotation is such a domain, as the available semantic concepts are typically hundreds. The proposed framework comprises an initial clustering phase that breaks the original training set into several disjoint clusters of data. It then trains a multi-label classifier from the data of each cluster. Given a new test instance, the framework first finds the nearest cluster and then applies the corresponding model. Empirical results using two clustering algorithms, four multi-label classification algorithms and three image annotation data sets suggest that the proposed approach can improve the performance and reduce the training time of standard multi-label classification algorithms, particularly in the case of large number of labels.",1
The 9th annual MLSP competition: New methods for acoustic classification of multiple simultaneous bird species in a noisy environment,2013,https://ieeexplore.ieee.org/abstract/document/6661934/,"Birds have been widely used as biological indicators for ecological research. They respond quickly to environmental changes and can be used to infer about other organisms (e.g., insects they feed on). Traditional methods for collecting data about birds involves costly human effort. A promising alternative is acoustic monitoring. There are many advantages to recording audio of birds compared to human surveys, including increased temporal and spatial resolution and extent, applicability in remote sites, reduced observer bias, and potentially lower cost. However, it is an open problem for signal processing and machine learning to reliably identify bird sounds in real-world audio data collected in an acoustic monitoring scenario. Some of the major challenges include multiple simultaneously vocalizing birds, other sources of non-bird sound (e.g., buzzing insects), and background noise like wind, rain, and motor vehicles.",1
An ensemble uncertainty aware measure for directed hill climbing ensemble pruning,2010,https://link.springer.com/article/10.1007/s10994-010-5172-0,"This paper proposes a new measure for ensemble pruning via directed hill climbing, dubbed Uncertainty Weighted Accuracy (UWA), which takes into account the uncertainty of the decision of the current ensemble. Empirical results on 30 data sets show that using the proposed measure to prune a heterogeneous ensemble leads to significantly better accuracy results compared to state-of-the-art measures and other baseline methods, while keeping only a small fraction of the original models. Besides the evaluation measure, the paper also studies two other parameters of directed hill climbing ensemble pruning methods, the search direction and the evaluation dataset, with interesting conclusions on appropriate values.",1
Dealing with concept drift and class imbalance in multi-label stream classification,2011,https://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/viewPaper/2928,"Data streams containing objects that are (or can be) associated with more than one label at the same time are ubiquitous. In spite of its important applications, classification of streaming multi-label data is largely unexplored. Existing approaches try to tackle the problem by transferring traditional single-label stream classification practices to the multi-label domain. Nevertheless, they fail to consider some of the unique properties of the problem such as within and between class imbalance and multiple concept drift. To deal with these challenges, this paper proposes a novel multi-label stream classification approach that employs two windows for each label, one for positive and one for negative examples. Instance-sharing is exploited for space efficiency, while a time-efficient instantiation based on the k-Nearest Neighbor algorithm is also proposed. Finally, a batch-incremental thresholding technique is proposed to further deal with the class imbalance problem. Results of an empirical comparison against two other methods on three real world datasets are in favor of the proposed approach.",1
Pruning an ensemble of classifiers via reinforcement learning,2009,https://www.sciencedirect.com/science/article/pii/S0925231208003184,"This paper studies the problem of pruning an ensemble of classifiers from a reinforcement learning perspective. It contributes a new pruning approach that uses the Q-learning algorithm in order to approximate an optimal policy of choosing whether to include or exclude each classifier from the ensemble. Extensive experimental comparisons of the proposed approach against state-of-the-art pruning and combination methods show very promising results. Additionally, we present an extension that allows the improvement of the solutions returned by the proposed approach over time, which is very useful in certain performance-critical domains.",1
Focused Ensemble Selection: A Diversity-Based Method for Greedy Ensemble Selection.,2008,https://books.google.com/books?hl=en&lr=&id=3Wsn3SwEBScC&oi=fnd&pg=PA117&dq=info:matxk_HzgacJ:scholar.google.com&ots=a2qukM7CqR&sig=QsPBAWuQmTWtMoyXiKw4eHuz6-Y,"Ensemble selection deals with the reduction of an ensemble of predictive models in order to improve its efficiency and predictive performance. A number of ensemble selection methods that are based on greedy search of the space of all possible ensemble subsets have recently been proposed. This paper contributes a novel method, based on a new diversity measure that takes into account the strength of the decision of the current ensemble. Experimental comparison of the proposed method, dubbed Focused Ensemble Selection (FES), against state-of-the-art greedy ensemble selection methods shows that it leads to small ensembles with high predictive performance.",1
An adaptive personalized news dissemination system,2009,https://link.springer.com/content/pdf/10.1007/s10844-008-0053-8.pdf,"With the explosive growth of the Word Wide Web, information overload became a crucial concern. In a data-rich information-poor environment like the Web, the discrimination of useful or desirable information out of tons of mostly worthless data became a tedious task. The role of Machine Learning in tackling this problem is thoroughly discussed in the literature, but few systems are available for public use. In this work, we bridge theory to practice, by implementing a web-based news reader enhanced with a specifically designed machine learning framework for dynamic content personalization. This way, we get the chance to examine applicability and implementation issues and discuss the effectiveness of machine learning methods for the classification of real-world text streams. The main features of our system named PersoNews are: (a) the aggregation of many different news sources that offer an RSS …",0
Multi-label classification of music by emotion,2011,https://link.springer.com/article/10.1186/1687-4722-2011-426793,"This work studies the task of automatic emotion detection in music. Music may evoke more than one different emotion at the same time. Single-label classification and regression cannot model this multiplicity. Therefore, this work focuses on multi-label classification approaches, where a piece of music may simultaneously belong to more than one class. Seven algorithms are experimentally compared for this task. Furthermore, the predictive power of several audio features is evaluated using a new multi-label feature selection method. Experiments are conducted on a set of 593 songs with six clusters of emotions based on the Tellegen-Watson-Clark model of affect. Results show that multi-label modeling is successful and provide interesting insights into the predictive quality of the algorithms and features.",1
A taxonomy and short review of ensemble selection,2008,https://www.academia.edu/download/5748662/partalas08-suema.pdf,"Ensemble selection deals with the reduction of an ensemble of predictive models in order to improve its efficiency and predictive performance. The last 10 years a large number of very diverse ensemble selection methods have been proposed. In this paper we make a first approach to categorize them into a taxonomy. We also present a short review of some of these methods. We particularly focus on a category of methods that are based on greedy search of the space of all possible ensemble subsets. Such methods use different directions for searching this space and different measures for evaluating the available actions at each state. Some use the training set for subset evaluation, while others a separate validation set. This paper abstracts the key points of these methods and offers a general framework of the greedy ensemble selection algorithm, discussing its important parameters and the different options for instantiating these parameters.",1
Effective voting of heterogeneous classifiers,2004,https://link.springer.com/chapter/10.1007/978-3-540-30115-8_43,"This paper deals with the combination of classification models that have been derived from running different (heterogeneous) learning algorithms on the same data set. We focus on the Classifier Evaluation and Selection (ES) method, that evaluates each of the models (typically using 10-fold cross-validation) and selects the best one. We examine the performance of this method in comparison with the Oracle selecting the best classifier for the test set and show that 10-fold cross-validation has problems in detecting the best classifier. We then extend ES by applying a statistical test to the 10-fold accuracies of the models and combining through voting the most significant ones. Experimental results show that the proposed method, Effective Voting, performs comparably with the state-of-the-art method of Stacking with Multi-Response Model Trees without the additional computational cost of meta-training.",1
Software Defect Prediction Using Regression via Classification.,2006,https://www.academia.edu/download/39820881/Software_Defect_Prediction_Using_Regress20151109-10467-18qewqk.pdf,"In this paper we apply a machine learning approach to the problem of estimating the number of defects called Regression via Classification (RvC). RvC initially automatically discretizes the number of defects into a number of fault classes, then learns a model that predicts the fault class of a software system. Finally, RvC transforms the class output of the model back into a numeric prediction. This approach includes uncertainty in the models because apart from a certain number of faults, it also outputs an associated interval of values, within which this estimate lies, with a certain confidence. To evaluate this approach we perform a comparative experimental study of the effectiveness of several machine learning algorithms in a software dataset. The data was collected by Pekka Forselious and involves applications maintained by a bank of Finland.",1
An empirical study of multi-label learning methods for video annotation,2009,https://ieeexplore.ieee.org/abstract/document/5137810/,This paper presents an experimental comparison of different approaches to learning from multi-labeled video data. We compare state-of-the-art multi-label learning methods on the Media mill Challenge dataset. We employ MPEG-7 and SIFT-based global image descriptors independently and in conjunction using variations of the stacking approach for their fusion. We evaluate the results comparing the different classifiers using both MPEG-7 and SIFT-based descriptors and their fusion. A variety of multi-label evaluation measures is used to explore advantages and disadvantages of the examined classifiers. Results give rise to interesting conclusions.,1
An ensemble of classifiers for coping with recurring contexts in data streams,2008,https://ebooks.iospress.nl/volumearticle/4488,"This paper proposes a general framework for classifying data streams by exploiting incremental clustering in order to dynamically build and update an ensemble of incremental classifiers. To achieve this, a transformation function that maps batches of examples into a new conceptual feature space is proposed. The clustering algorithm is then applied in order to group different concepts and identify recurring contexts. The ensemble is produced by maintaining an classifier for every concept discovered in the stream",1
A systematic review of multi-label feature selection and a new method based on label construction,2016,https://www.sciencedirect.com/science/article/pii/S0925231215016197,"Each example in a multi-label dataset is associated with multiple labels, which are often correlated. Learning from this data can be improved when dimensionality reduction tasks, such as feature selection, are applied. The standard approach for multi-label feature selection transforms the multi-label dataset into single-label datasets before using traditional feature selection algorithms. However, this approach often ignores label dependence. In this work, we propose an alternative method, LCFS, that constructs new labels based on relations between the original labels. By doing so, the label set from the data is augmented with second-order information before applying the standard approach. To assess LCFS, an experimental evaluation using Information Gain as a measure to estimate the importance of features was carried out on 10 benchmark multi-label datasets. This evaluation compared four LCFS settings with …",0
Multi-label classification methods for multi-target regression,2012,https://www.academia.edu/download/34394145/1211.6581v4.pdf,"Real world prediction problems often involve the simultaneous prediction of multiple target variables using the same set of predictive variables. When the target variables are binary, the prediction task is called multi-label classification while when the target variables are realvalued the task is called multi-target regression. Although multi-target regression attracted the attention of the research community prior to multi-label classification, the recent advances in this field motivate a study of whether newer state-of-the-art algorithms developed for multilabel classification are applicable and equally successful in the domain of multi-target regression. In this paper we introduce two new multitarget regression algorithms: multi-target stacking (MTS) and ensemble of regressor chains (ERC), inspired by two popular multi-label classification approaches that are based on a single-target decomposition of the multi-target problem and the idea of treating the other prediction targets as additional input variables that augment the input space. Furthermore, we detect an important shortcoming on both methods related to the methodology used to create the additional input variables and develop modified versions of the algorithms (MTSC and ERCC) to tackle it. All methods are empirically evaluated on 12 real-world multi-target regression datasets, 8 of which are first introduced in this paper and are made publicly available for future benchmarks. The experimental results show that ERCC performs significantly better than both a strong baseline that learns a single model for each target using bagging of regression trees and the state-of-the-art multi-objective random …",0
Multi-target regression via random linear target combinations,2014,https://link.springer.com/chapter/10.1007/978-3-662-44845-8_15,"Multi-target regression is concerned with the simultaneous prediction of multiple continuous target variables based on the same set of input variables. It arises in several interesting industrial and environmental application domains, such as ecological modelling and energy forecasting. This paper presents an ensemble method for multi-target regression that constructs new target variables via random linear combinations of existing targets. We discuss the connection of our approach with multi-label classification algorithms, in particular RAkEL, which originally inspired this work, and a family of recent multi-label classification algorithms that involve output coding. Experimental results on 12 multi-target datasets show that it performs significantly better than a strong baseline that learns a single model for each target using gradient boosting and compares favourably to multi-objective random forest approach …",0
Regression via Classification applied on software defect estimation,2008,https://www.sciencedirect.com/science/article/pii/S0957417407000875,"In this paper we apply Regression via Classification (RvC) to the problem of estimating the number of software defects. This approach apart from a certain number of faults, it also outputs an associated interval of values, within which this estimate lies with a certain confidence. RvC also allows the production of comprehensible models of software defects exploiting symbolic learning algorithms. To evaluate this approach we perform an extensive comparative experimental study of the effectiveness of several machine learning algorithms in two software data sets. RvC manages to get better regression error than the standard regression approaches on both datasets.",1
Learning from multi-label data,2009,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.207.2982,"This volume contains research papers accepted for presentation at the 1st International Workshop on Learning from Multi-Label Data (MLD’09), which will be held in Bled, Slovenia, at September 7, 2009 in conjunction with ECML/PKDD 2009. MLD’09 is devoted to multi-label learning, which is an emerging and promising research topic of machine learning. In multi-label learning, each example is associated with multiple labels simultaneously, which therefore encompasses traditional supervised learning (single-label) as its special case. Multi-label learning is related to various machine learning paradigms, such as classification, ranking, semi-supervised learning, active learning, multi-instance learning, dimensionality reduction, etc. Initial attempts on multi-label learning date back to 1999 with works on multi-label text categorization. In recent years, the task of learning from multi-label data has been addressed by a number of methods adapted from various popular learning techniques, such as neural networks, decision trees, k-nearest neighbors, kernel methods, ensemble methods, etc. More impressively, multi-label learning has manifested its effectiveness in a diversity of real-world applications, such as image/video annotation, bioinformatics,",1
Greedy regression ensemble selection: Theory and an application to water quality prediction,2008,https://www.sciencedirect.com/science/article/pii/S0020025508001576,"This paper studies the greedy ensemble selection family of algorithms for ensembles of regression models. These algorithms search for the globally best subset of regressors by making local greedy decisions for changing the current subset. We abstract the key points of the greedy ensemble selection algorithms and present a general framework, which is applied to an application domain with important social and commercial value: water quality prediction.",1
A review of multi-label classification methods,2006,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.474.6415&rep=rep1&type=pdf,"Nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. This paper introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multi-label classification methods. It also contributes the presentation of an undocumented method and the definition of a concept for the quantification of the multi-label nature of a data set.",1
Ensemble pruning using reinforcement learning,2006,https://link.springer.com/chapter/10.1007/11752912_31,"Multiple Classifier systems have been developed in order to improve classification accuracy using methodologies for effective classifier combination. Classical approaches use heuristics, statistical tests, or a meta-learning level in order to find out the optimal combination function. We study this problem from a Reinforcement Learning perspective. In our modeling, an agent tries to learn the best policy for selecting classifiers by exploring a state space and considering a future cumulative reward from the environment. We evaluate our approach by comparing with state-of-the-art combination methods and obtain very promising results.",1
Clustering classifiers for knowledge discovery from physically distributed databases,2004,https://www.sciencedirect.com/science/article/pii/S0169023X03001538,"Most distributed classification approaches view data distribution as a technical issue and combine local models aiming at a single global model. This however, is unsuitable for inherently distributed databases, which are often described by more than one classification models that might differ conceptually. In this paper we present an approach for clustering distributed classifiers in order to discover groups of similar classifiers and thus similar databases with respect to a specific classification task. We also show that clustering distributed classifiers as a pre-processing step for classifier combination enhances the achieved predictive performance of the ensemble.",1
An empirical study on sea water quality prediction,2008,https://www.sciencedirect.com/science/article/pii/S0950705108000506,"This paper studies the problem of predicting future values for a number of water quality variables, based on measurements from under-water sensors. It performs both exploratory and automatic analysis of the collected data with a variety of linear and nonlinear modeling methods. The paper investigates issues, such as the ability to predict future values for a varying number of days ahead and the effect of including values from a varying number of past days. Experimental results provide interesting insights on the predictability of the target variables and the performance of the different learning algorithms.",1
Distributed data mining,2009,https://www.igi-global.com/chapter/distributed-data-mining/7907,"The continuous developments in information and communication technology have recently led to the appearance of distributed computing environments, which comprise several, and different sources of large volumes of data and several computing units. The most prominent example of a distributed environment is the Internet, where increasingly more databases and data streams appear that deal with several areas, such as meteorology, oceanography, economy and others. In addition the Internet constitutes the communication medium for geographicallydistributed information systems, as for example the earth observing system of NASA (eos. gsfc. nasa. gov). Other examples of distributed environments that have been developed in the last few years are sensor networks for process monitoring and grids where a large number of computing and storage units are interconnected over a highspeed network.",1
A review of keyphrase extraction,2020,https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1339,"Keyphrase extraction is a textual information processing task concerned with the automatic extraction of representative and characteristic phrases from a document that express all the key aspects of its content. Keyphrases constitute a succinct conceptual summary of a document, which is very useful in digital information management systems for semantic indexing, faceted search, document clustering and classification. This article introduces keyphrase extraction, provides a well‐structured review of the existing work, offers interesting insights on the different evaluation approaches, highlights open issues and presents a comparative experimental study of popular unsupervised techniques on five datasets.This article is categorized under: Ensemble Methods > Web Mining Ensemble Methods > Text Mining ",1
A survey of machine learning techniques for food sales prediction,2019,https://link.springer.com/article/10.1007/s10462-018-9637-z,"Food sales prediction is concerned with estimating future sales of companies in the food industry, such as supermarkets, groceries, restaurants, bakeries and patisseries. Accurate short-term sales prediction allows companies to minimize stocked and expired products inside stores and at the same time avoid missing sales. This paper reviews existing machine learning approaches for food sales prediction. It discusses important design decisions of a data analyst working on food sales prediction, such as the temporal granularity of sales data, the input variables to use for predicting sales and the representation of the sales output variable. In addition, it reviews machine learning algorithms that have been applied to food sales prediction and appropriate measures for evaluating their accuracy. Finally, it discusses the main challenges and opportunities for applied machine learning in the domain of food sales …",0
PersoNews: a personalized news reader enhanced by machine learning and semantic filtering,2006,https://link.springer.com/chapter/10.1007/11914853_62,"In this paper, we present a web-based, machine-learning enhanced news reader (PersoNews). The main advantages of PersoNews are the aggregation of many different news sources, machine learning filtering offering personalization not only per user but also for every feed a user is subscribed to, and finally the ability for every user to watch a more abstracted topic of interest by employing a simple form of semantic filtering through a taxonomy of topics.",1
Distributed data mining of large classifier ensembles,2002,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3906,"Nowadays, classifier ensembles are often used for distributed data mining in order to discover knowledge from inherently distributed information sources and scale up learning to very large databases. One of the most successful methods used for combining multiple classifiers is Stacking. However, this method suffers from very high computational cost in the case of large number of distributed nodes. This paper presents a new classifier combination strategy that scales up efficiently and achieves both high predictive accuracy and tractability of problems with high complexity. It induces a global model by learning from the averages of the local classifiers' output. This way, fast and effective combination of large number of classifiers is achieved.",1
Introduction to the special issue on learning from multi-label data,2012,https://link.springer.com/content/pdf/10.1007/s10994-012-5292-9.pdf,"In traditional supervised classification, objects belong to only one class out of two or more disjoint classes. However in many real-world applications, objects may belong to more than one class at the same time. For example an article on the Greek debt crisis could among others belong to the following classes of a financial newspaper’s taxonomy: Greece, Eurozone, Economy and Markets. Such data are called multi-label. Early research on learning from multi-label data focused on automated document categorization (McCallum 1999; Schapire and Singer 2000), motivated by the need for low-cost annotation of large document collections maintained by news agencies, academic publishers and intellectual property organizations. As our ability to collect and store large amounts of digital content increased in recent years due to technological advances, so did the need for automated annotation of such content, leading …",0
Obtaining bipartitions from score vectors for multi-label classification,2010,https://ieeexplore.ieee.org/abstract/document/5670068/,"Multi-label classification is a popular learning task. However, some of the algorithms that learn from multi-label data, can only output a score for each label, so they cannot be readily used in applications that require bipartitions. In addition, several of the recent state-of-the-art multi-label classification algorithms, actually output a score vector primarily and employ one (sometimes simple) thresholding method in order to be able to output bipartitions. Furthermore, some approaches can naturally output both a score vector and a bipartition, but whether a better bipartition can be obtained through thresholding has not been investigated. This paper contributes a theoretical and empirical comparative study of existing thresholding methods, highlighting their importance for obtaining bipartitions of high quality.",1
Effective stacking of distributed classifiers,2002,https://books.google.com/books?hl=en&lr=&id=5ZuuF0ogxU4C&oi=fnd&pg=PA340&dq=info:MxbZXDZ6s8sJ:scholar.google.com&ots=e1jo5oP7DS&sig=JPBhu-IcEEEXEQFzeh5XJ4zOSSM,"One of the most promising lines of research towards discovering global predictive models from physically distributed data sets is local learning and model integration. Local learning avoids moving raw data around the distributed nodes and minimizes com-munication, coordination and synchronization cost. However, the in-tegration of local models is not a straightforward process. Majority Voting is a simple solution that works well in some domains, but it does not always offer the best predictive performance. Stacking on the other hand, offers flexibility in modelling, but brings along the problem of how to train on sufficient and at the same time in-dependent data without the cost of moving raw data around the distributed nodes. In addition, the scalability of Stacking with respect to the number of distributed nodes is another important issue that has not yet been substantially investigated. This paper presents a framework for constructing a global predictive model from local classifiers that does not require moving raw data around, achieves high predictive accuracy and scales up efficiently with respect to large numbers of distributed data sets. of distributed nodes is another important issue that has not yet been substantially investigated. This paper presents a framework for combining distributed classi-fiers without moving raw data around that achieves high predictive accuracy and scales up efficiently with respect to large numbers of distributed data sets. The main contribution of this work is: i) a new methodology for training-based approaches to distributed data min-ing that avoids the problem of gathering parts of the raw data for training purposes and ii …",0
Local word vectors guiding keyphrase extraction,2018,https://www.sciencedirect.com/science/article/pii/S0306457317308427,"Automated keyphrase extraction is a fundamental textual information processing task concerned with the selection of representative phrases from a document that summarize its content. This work presents a novel unsupervised method for keyphrase extraction, whose main innovation is the use of local word embeddings (in particular GloVe vectors), i.e., embeddings trained from the single document under consideration. We argue that such local representation of words and keyphrases are able to accurately capture their semantics in the context of the document they are part of, and therefore can help in improving keyphrase extraction quality. Empirical results offer evidence that indeed local representations lead to better keyphrase extraction results compared to both embeddings trained on very large third corpora or larger corpora consisting of several documents of the same scientific field and to other state-of-the …",0
On the combination of textual and semantic descriptions for automated semantic web service classification,2009,https://link.springer.com/chapter/10.1007/978-1-4419-0221-4_13,"Semantic Web services have emerged as the solution to the need for automating several aspects related to service-oriented architectures, such as service discovery and composition, and they are realized by combining Semantic Web technologies and Web service standards. In the present paper, we tackle the problem of automated classification of Web services according to their application domain taking into account both the textual description and the semantic annotations of OWL-S advertisements. We present results that we obtained by applying machine learning algorithms on textual and semantic descriptions separately and we propose methods for increasing the overall classification accuracy through an extended feature vector and an ensemble of classifiers.",1
Evaluating feature selection methods for multi-label text classification,2013,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.7376&rep=rep1&type=pdf,"Multi-label text classification deals with problems in which each document is associated with a subset of categories. These documents often consist of a large number of words, which can hinder the performance of learning algorithms. Feature selection is a popular task to find representative words and remove unimportant ones, which could speed up learning and even improve learning performance. This work evaluates eight feature selection algorithms in text benchmark datasets. The best algorithms are subsequently compared with random feature selection and classifiers built using all features. Results agree with literature by finding that well-known approaches, such as maximum chi-squared scoring across all labels, are good choices to reduce text dimensionality while reaching competitive multi-label classification performance.",1
E-mail mining: Emerging techniques for e-mail management,2007,https://www.igi-global.com/chapter/web-data-management-practices/31103,"Email has met tremendous popularity over the past few years. People are sending and receiving many messages per day, communicating with partners and friends, or exchanging files and information. Unfortunately, the phenomenon of email overload has grown over the past years becoming a personal headache for users and a financial issue for companies. In this chapter, we will discuss how disciplines like Machine Learning and Data Mining can contribute to the solution of the problem by constructing intelligent techniques which automate email managing tasks and what advantages they hold over other conventional solutions. We will also discuss the particularity of email data and what special treatment it requires. Some interesting email mining applications like mail categorization, summarization, automatic answering and spam filtering will be also presented.",1
An empirical study on the combination of SURF features with VLAD vectors for image search,2012,https://ieeexplore.ieee.org/abstract/document/6226771/,"The study of efficient image representations has attracted significant interest due to the computational needs of large-scale applications. In this paper we study the performance of the recently proposed VLAD method for aggregating local image descriptors when combined with SURF features, in the domain of image search. The experiments show that when SURF features are used as local image descriptors, VLAD attains better performance compared to using SIFT features. We also study how the average number of local image descriptors extracted per image affects the performance and show that by controlling this number we are able to adjust the trade off between feature extraction time and search accuracy. Finally, we examine the retrieval performance of the proposed scheme with varying levels of distractor images.",1
Dense distributions from sparse samples: improved gibbs sampling parameter estimators for LDA,2017,https://www.jmlr.org/papers/volume18/16-526/16-526.pdf,"We introduce a novel approach for estimating Latent Dirichlet Allocation (LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single additional collapsed Gibbs sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. Our estimators can straightforwardly be applied to the output of any existing implementation of CGS, including modern accelerated variants. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multi-label classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so.",1
A study on greedy algorithms for ensemble pruning,2012,http://lpis.csd.auth.gr/publications/partalas-tr-2012.pdf,"Ensemble selection deals with the reduction of an ensemble of predictive models in order to improve its efficiency and predictive performance. A number of ensemble selection methods that are based on greedy search of the space of all possible ensemble subsets have recently been proposed. They use different directions for searching this space and different measures for evaluating the available actions at each state. Some use the training set for subset evaluation, while others a separate validation set. This paper abstracts the key points of these methods and offers a general framework of the greedy ensemble selection algorithm, discussing its important parameters and the different options for instantiating these parameters.",1
A triple-random ensemble classification method for mining multi-label data,2010,https://ieeexplore.ieee.org/abstract/document/5693281/,"This paper presents a triple-random ensemble learning method for handling multi-label classification problems. The proposed method integrates and develops the concepts of random subspace, bagging and random k-label sets ensemble learning methods to form an approach to classify multi-label data. It applies the random subspace method to feature space, label space as well as instance space. The devised subsets selection procedure is executed iteratively. Each multi-label classifier is trained using the randomly selected subsets. At the end of the iteration, optimal parameters are selected and the ensemble MLC classifiers are constructed. The proposed method is implemented and its performance compared against that of popular multi-label classification methods. The experimental results reveal that the proposed method outperforms the examined counterparts in most occasions when tested on six small to …",0
Learning Rules for Adaptive Planning.,2003,https://www.aaai.org/Papers/ICAPS/2003/ICAPS03-009.pdf,"This paper presents a novel idea, which combines Planning, Machine Learning and Knowledge-Based techniques. It is concerned with the development of an adaptive planning system that can fine-tune its planning parameters based on the values of specific measurable characteristics of the given planning problem. Adaptation is guided by a rule-based system, whose knowledge has been acquired through machine learning techniques. Specifically, the algorithm of classification based on association rules was applied to a large dataset produced by results from experiments on a large number of problems used in the three AIPS Planning competitions. The paper presents experimental results with the adaptive planner, which demonstrate the boost in performance of the planning system.",1
Dynamic ensemble pruning based on multi-label classification,2015,https://www.sciencedirect.com/science/article/pii/S0925231214012284,"Dynamic (also known as instance-based) ensemble pruning selects a (potentially) different subset of models from an ensemble during prediction based on the given unknown instance with the goal of maximizing prediction accuracy. This paper models dynamic ensemble pruning as a multi-label classification task, by considering the members of the ensemble as labels. Multi-label training examples are constructed by evaluating whether ensemble members are accurate or not on the original training set via cross-validation. We show that classification accuracy is maximized when learning algorithms that optimize example-based precision are used in the multi-label classification task. Results comparing the proposed framework against state-of-the-art dynamic ensemble pruning approaches in a variety of datasets using a heterogeneous ensemble of 200 classifiers show that it leads to significantly improved accuracy.",1
Transferring task models in reinforcement learning agents,2013,https://www.sciencedirect.com/science/article/pii/S0925231212007771,"The main objective of transfer learning is to reuse knowledge acquired in a previous learned task, in order to enhance the learning procedure in a new and more complex task. Transfer learning comprises a suitable solution for speeding up the learning procedure in Reinforcement Learning tasks. This work proposes a novel method for transferring models to Reinforcement Learning agents. The models of the transition and reward functions of a source task, will be transferred to a relevant but different target task. The learning algorithm of the target task's agent takes a hybrid approach, implementing both model-free and model-based learning, in order to fully exploit the presence of a source task model. Moreover, a novel method is proposed for transferring models of potential-based reward shaping functions. The empirical evaluation, of the proposed approaches, demonstrated significant results and performance …",0
PASER: a curricula synthesis system based on automated problem solving,2007,https://www.inderscienceonline.com/doi/abs/10.1504/IJTCS.2007.014217,"This paper presents PASER, a system for automatically synthesising curricula using AI Planning and Machine Learning techniques based on an ontology of educational resources metadata. Given the initial state of the problem (learner's profile, preferences, needs and abilities), the available actions (study an educational resource, take an exam, join an e-learning course, etc.) and the goals (obtain a certificate, learn a subject, acquire a skill, etc.), the planning module of PASER constructs a complete educational curriculum that achieves the goals. The Machine Learning module of PASER matches textually described learning requests, objectives and prerequisites to concepts of the ontology.",1
Dealing with class imbalance in classifier chains via random undersampling,2020,https://www.sciencedirect.com/science/article/pii/S0950705119305830,"Class imbalance is an intrinsic characteristic of multi-label data. Most of the labels in multi-label data sets are associated with a small number of training examples, much smaller compared to the size of the data set. Class imbalance poses a key challenge that plagues most multi-label learning methods. Ensemble of Classifier Chains (ECC), one of the most prominent multi-label learning methods, is no exception to this rule, as each of the binary models it builds is trained from all positive and negative examples of a label. To make ECC resilient to class imbalance, we first couple it with random undersampling. We then present two extensions of this basic approach, where we build a varying number of binary models per label and construct chains of different sizes, in order to improve the exploitation of majority examples with approximately the same computational budget. Experimental results on 16 multi-label datasets …",0
TRES: identification of discriminatory and informative SNPs from population genomic data,2015,https://academic.oup.com/jhered/article-abstract/106/5/672/2960031,"The advent of high-throughput genomic technologies is enabling analyses on thousands or even millions of single-nucleotide polymorphisms (SNPs). At the same time, the selection of a minimum number of SNPs with the maximum information content is becoming increasingly problematic. Available locus ranking programs have been accused of providing upwardly biased results (concerning the predicted accuracy of the chosen set of markers for population assignment), cannot handle high-dimensional datasets, and some of them are computationally intensive. The toolbox for ranking and evaluation of SNPs (TRES) is a collection of algorithms built in a user-friendly and computationally efficient software that can manipulate and analyze datasets even in the order of millions of genotypes in a matter of seconds. It offers a variety of established methods for evaluating and ranking SNPs on user defined groups of …",0
Ensemble Approaches for Large-Scale Multi-Label Classification and Question Answering in Biomedicine.,2014,http://ceur-ws.org/Vol-1180/CLEF2014wn-QA-PapanikolaouEt2014.pdf,"This paper documents the systems that we developed for our participation in the BioASQ 2014 large-scale bio-medical semantic indexing and question answering challenge. For the large-scale semantic indexing task, we employed a novel multi-label ensemble method consisting of support vector machines, labeled Latent Dirichlet Allocation models and meta-models predicting the number of relevant labels. This method proved successful in our experiments as well as during the competition. For the question answering task we combined different techniques for scoring of candidate answers based on recent literature.",1
Instance-based ensemble pruning via multi-label classification,2010,https://ieeexplore.ieee.org/abstract/document/5670063/,"Ensemble pruning is concerned with the reduction of the size of an ensemble prior to its combination. Its purpose is to reduce the space and time complexity of the ensemble and/or to increase the ensemble's accuracy. This paper focuses on instance-based approaches to ensemble pruning, where a different subset of the ensemble may be used for each different unclassified instance. We propose modeling this task as a multi-label learning problem, in order to take advantage of the recent advances in this area for the construction of effective ensemble pruning approaches. Results comparing the proposed framework against a variety of other instance-based ensemble pruning approaches in a variety of datasets using a heterogeneous ensemble of 200 classifiers, show that it leads to improved accuracy.",1
WISE 2014 challenge: Multi-label classification of print media articles to topics,2014,https://link.springer.com/chapter/10.1007/978-3-319-11746-1_40,"The WISE 2014 challenge was concerned with the task of multi-label classification of articles coming from Greek print media. Raw data comes from the scanning of print media, article segmentation, and optical character segmentation, and therefore is quite noisy. Each article is examined by a human annotator and categorized to one or more of the topics being monitored. Topics range from specific persons, products, and companies that can be easily categorized based on keywords, to more general semantic concepts, such as environment or economy. Building multi-label classifiers for the automated annotation of articles into topics can support the work of human annotators by suggesting a list of all topics by order of relevance, or even automate the annotation process for media and/or categories that are easier to predict. This saves valuable time and allows a media monitoring company to expand the …",0
Predicting drug-target interactions with multi-label classification and label partitioning,2019,https://ieeexplore.ieee.org/abstract/document/8890853/,"Identifying drug-target interactions is crucial for drug discovery. Despite modern technologies used in drug screening, experimental identification of drug-target interactions is an extremely demanding task. Predicting drug-target interactions in silico can thereby facilitate drug discovery as well as drug repositioning. Various machine learning models have been developed over the years to predict such interactions. Multi-output learning models in particular have drawn the attention of the scientific community due to their high predictive performance and computational efficiency. These models are based on the assumption that all the labels are correlated with each other. However, this assumption is too optimistic. Here, we address drug-target interaction prediction as a multi-label classification task that is combined with label partitioning. We show that building multi-output learning models over groups (clusters) of labels …",0
Label construction for multi-label feature selection,2014,https://ieeexplore.ieee.org/abstract/document/6984838/,"Multi-label learning handles datasets where each instance is associated with multiple labels, which are often correlated. As other machine learning tasks, multi-label learning also suffers from the curse of dimensionality, which can be mitigated by dimensionality reduction tasks, such as feature selection. The standard approach for multi-label feature selection transforms the multi-label dataset into single-label datasets before using traditional feature selection algorithms. However, this approach often ignores label dependence. This work proposes an alternative method, LCFS, which constructs new labels based on relations between the original labels to augment the label set of the original dataset. Afterwards, the augmented dataset is submitted to the standard multi-label feature selection approach. Experiments using Information Gain as a measure to evaluate features were carried out in 10 multi-label benchmark …",0
Synthetic oversampling of multi-label data based on local label distribution,2019,https://arxiv.org/abs/1905.00609,"Class-imbalance is an inherent characteristic of multi-label data which affects the prediction accuracy of most multi-label learning methods. One efficient strategy to deal with this problem is to employ resampling techniques before training the classifier. Existing multilabel sampling methods alleviate the (global) imbalance of multi-label datasets. However, performance degradation is mainly due to rare subconcepts and overlapping of classes that could be analysed by looking at the local characteristics of the minority examples, rather than the imbalance of the whole dataset. We propose a new method for synthetic oversampling of multi-label data that focuses on local label distribution to generate more diverse and better labeled instances. Experimental results on 13 multi-label datasets demonstrate the effectiveness of the proposed approach in a variety of evaluation measures, particularly in the case of an ensemble of classifiers trained on repeated samples of the original data.",1
Discovering and exploiting deterministic label relationships in multi-label learning,2015,https://dl.acm.org/doi/abs/10.1145/2783258.2783302,"This work presents a probabilistic method for enforcing adherence of the marginal probabilities of a multi-label model to automatically discovered deterministic relationships among labels. In particular we focus on discovering two kinds of relationships among the labels. The first one concerns pairwise positive entailment: pairs of labels, where the presence of one implies the presence of the other in all instances of a dataset. The second concerns exclusion: sets of labels that do not coexist in the same instances of the dataset. These relationships are represented as a deterministic Bayesian network. Marginal probabilities are entered as soft evidence in the network and through probabilistic inference become consistent with the discovered knowledge. Our approach offers robust improvements in mean average precision compared to the standard binary relevance approach across all 12 datasets involved in our …",0
A divide-and-conquer approach to the summarization of long documents,2020,https://ieeexplore.ieee.org/abstract/document/9257174/,"We present a novel divide-and-conquer method for the neural summarization of long documents. Our method exploits the discourse structure of the document and uses sentence similarity to split the problem into an ensemble of smaller summarization problems. In particular, we break a long document and its summary into multiple source-target pairs, which are used for training a model that learns to summarize each part of the document separately. These partial summaries are then combined in order to produce a final complete summary. With this approach we can decompose the problem of long document summarization into smaller and simpler problems, reducing computational complexity and creating more training examples, which at the same time contain less noise in the target summaries compared to the standard approach. We demonstrate that this approach paired with different summarization models …",0
Large-scale online semantic indexing of biomedical articles via an ensemble of multi-label classification models,2017,https://link.springer.com/article/10.1186/s13326-017-0150-0,"In this paper we present the approach that we employed to deal with large scale multi-label semantic indexing of biomedical papers. This work was mainly implemented within the context of the BioASQ challenge (2013–2017), a challenge concerned with biomedical semantic indexing and question answering. Our main contribution is a MUlti-Label Ensemble method (MULE) that incorporates a McNemar statistical significance test in order to validate the combination of the constituent machine learning algorithms. Some secondary contributions include a study on the temporal aspects of the BioASQ corpus (observations apply also to the BioASQ’s super-set, the PubMed articles collection) and the proper parametrization of the algorithms used to deal with this challenging classification task. The ensemble method that we developed is compared to other approaches in experimental scenarios with subsets of the …",0
Large-scale semantic indexing of biomedical publications at bioasq,2013,https://www.researchgate.net/profile/Grigorios-Tsoumakas/publication/289393652_Large-scale_semantic_indexing_of_biomedical_publications_at_BioASQ/links/56b9a29808ae3b658a8931fc/Large-scale-semantic-indexing-of-biomedical-publications-at-BioASQ.pdf,"Automated annotation of scientific publications in real-world digital libraries requires dealing with challenges such as large number of concepts and training examples, multi-label training examples and hierarchical structure of concepts. BioASQ is a European project that contributes a large-scale biomedical publications corpus for working on these challenges. This paper documents the participation of our team to the large-scale biomedical semantic indexing task of BioASQ.",1
Lazy adaptive multicriteria planning,2004,http://lpis.csd.auth.gr/publications/tsoumakas-ecai04.pdf,"This paper describes a learning system for the automatic configuration of domain independent planning systems, based on measurable features of planning problems. The purpose of the Lazy Adaptive Multicriteria Planning (LAMP) system is to configure a planner in an optimal way, concerning two quality metrics (ie execution speed and plan quality), for a given problem according to user-specified preferences. The training data are produced by running the planner under consideration on a set of problems using all possible parameter configurations and recording the planning time and the plan length. When a new problem arises, LAMP extracts the values for a number of domain-expert specified problem features and uses them to identify the а nearest problems solved in the past. The system then performs a multicriteria combination of the performances of the retrieved problems according to user-specified weights that specify the relative importance of the quality metrics and selects the configuration with the best score. Experimental results show that LAMP improves the performance of the default configuration of two already well-performing planning systems in a variety of planning problems.",1
Data mining and knowledge discovery handbook. Mining multi-label data,2009,https://scholar.google.com/scholar?cluster=187301861665405320&hl=en&oi=scholarr,unknown,1
On the combination of two decompositive multi-label classification methods,2009,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.192.3699&rep=rep1&type=pdf#page=118,"In this paper, we compare and combine two approaches for multi-label classification that both decompose the initial problem into sets of smaller problems. The Calibrated Label Ranking approach is based on interpreting the multi-label problem as a preference learning problem and decomposes it into a quadratic number of binary classifiers. The HOMER approach reduces the original problem into a hierarchy of considerably simpler multi-label problems. Experimental results indicate that the use of HOMER is beneficial for the pairwise preference-based approach in terms of computational cost and quality of prediction.",1
ISLE: an intelligent system for land evaluation,1999,https://www.researchgate.net/profile/Grigorios-Tsoumakas/publication/2593759_ISLE_An_Intelligent_System_for_Land_Evaluation/links/0f317531a4d5563d5c000000/ISLE-An-Intelligent-System-for-Land-Evaluation.pdf,"Land evaluation is a process that requires the information of a GIS (Geographical Information System) and the expertise of a specialised soil scientist to analyse and interpret this information. Visualising the whole process is a subject of great importance, especially when vast land areas are regarded. This paper presents ISLE, an intelligent system for land evaluation that automates the process of evaluation and graphically illustrates the results on digital maps. Its main features are the support of GIS capabilities on the digital map of an area, and the support of expert analysis of regions of this area, through one sophisticated user interface. ISLE’s knowledge base, models the evaluation of land in accordance with the FAO-SYS model for Land Evaluation. The system has as input a digital map of an area and its geographical database, displays this map, evaluates the land units selected by the user and finally visualises the results colouring properly the analysed land units.",1
Word embeddings and external resources for answer processing in biomedical factoid question answering,2019,https://www.sciencedirect.com/science/article/pii/S153204641930036X,"Biomedical question answering (QA) is a challenging task that has not been yet successfully solved, according to results on international benchmarks, such as BioASQ. Recent progress on deep neural networks has led to promising results in domain independent QA, but the lack of large datasets with biomedical question-answer pairs hinders their successful application to the domain of biomedicine.We propose a novel machine-learning based answer processing approach that exploits neural networks in an unsupervised way through word embeddings. Our approach first combines biomedical and general purpose tools to identify the candidate answers from a set of passages. Candidates are then represented using a combination of features based on both biomedical external resources and input textual sources, including features based on word embeddings. Candidates are then ranked based on the score given …",0
Web Robot detection: A semantic approach,2018,https://ieeexplore.ieee.org/abstract/document/8576148/,"Web robots constitute nowadays more than half of the total web traffic. Malicious robots threaten the security, privacy and performance of the web, while non-malicious ones are involved in analytics skewing. The latter constitutes an important problem for large websites with unique content, as it can lead to false impressions about the popularity and impact of a piece of information. To deal with this problem, we present a novel web robot detection approach for content-rich websites, based on the assumption that human web users are interested in specific topics, while web robots crawl the web randomly. Our approach extends the typical representation of user sessions with a novel set of features that capture the semantics of the content of the requested resources. Empirical results on real-world data from the web portal of an academic publisher, show that the proposed semantic features lead to improved web robot …",0
Large-scale semantic indexing and question answering in biomedicine,2016,https://www.aclweb.org/anthology/W16-3107.pdf,"In this paper we present the methods and approaches employed in terms of our participation in the 2016 version of the BioASQ challenge. For the semantic indexing task, we extended our successful ensemble approach of last year with additional models. The official results obtained so-far demonstrate a continuing consistent advantage of our approaches against the National Library of Medicine (NLM) baselines. For the question answering task, we extended our approach on factoid questions, while we also developed approaches for the document, concept and snippet retrieval sub-tasks.",1
An interoperable and scalable Web-based system for classifier sharing and fusion,2007,https://www.sciencedirect.com/science/article/pii/S0957417406001965,"This paper describes CSF/DC, a Web-based system for classifier sharing and fusion. CSF/DC enables the sharing of classification models, by allowing the upload and download of such models expressed in the industry standard PMML language on the system’s online classifier repository. CSF/DC also leverages the individual knowledge shared by such (potentially heterogeneous) classification models and offers quality decision support to any user with an Internet connection through a guided procedure. However, some organizations or individuals might want to share the predictive capabilities of their classification models without compromising their internal structure. This is accommodated by CSF/DC through the use of Web services. Specifically, CSF/DC allows the participation of classifier Web services in the decision fusion process, by offering the necessary online mechanisms for the registration and …",0
Ethos: an online hate speech detection dataset,2020,https://arxiv.org/abs/2006.08328,"Online hate speech is a newborn problem in our modern society which is growing at a steady rate exploiting weaknesses of the corresponding regimes that characterise several social media platforms. Therefore, this phenomenon is mainly cultivated through such comments, either during users' interaction or on posted multimedia context. Nowadays, giant companies own platforms where many millions of users log in daily. Thus, protection of their users from exposure to similar phenomena for keeping up with the corresponding law, as well as for retaining a high quality of offered services, seems mandatory. Having a robust and reliable mechanism for identifying and preventing the uploading of related material would have a huge effect on our society regarding several aspects of our daily life. On the other hand, its absence would deteriorate heavily the total user experience, while its erroneous operation might raise several ethical issues. In this work, we present a protocol for creating a more suitable dataset, regarding its both informativeness and representativeness aspects, favouring the safer capture of hate speech occurrence, without at the same time restricting its applicability to other classification problems. Moreover, we produce and publish a textual dataset with two variants: binary and multi-label, called `ETHOS', based on YouTube and Reddit comments validated through figure-eight crowdsourcing platform. Our assumption about the production of more compatible datasets is further investigated by applying various classification models and recording their behaviour over several appropriate metrics.",1
Multi-label active learning: key issues and a novel query strategy,2019,https://link.springer.com/article/10.1007/s12530-017-9202-z,"Active learning is an iterative supervised learning task where learning algorithms can actively query an oracle, i.e. a human annotator that understands the nature of the problem, to obtain the ground truth. The motivation behind this approach is to allow the learner to interactively choose the data it will learn from, which can lead to significantly less annotation cost, faster training and improved performance. Active learning is appropriate for machine learning applications where labeled data is costly to obtain but unlabeled data is abundant. Most importantly, it permits a learning model to evolve and adapt to new data unlike conventional supervised learning. Although active learning has been widely considered for single-label learning, applications to multi-label learning have been more limited. In this work, we present the general framework to apply active learning to multi-label data, discussing the key issues …",0
Making classifier chains resilient to class imbalance,2018,http://proceedings.mlr.press/v95/liu18c.html,"Class imbalance is an intrinsic characteristic of multi-label data. Most of the labels in multi-label data sets are associated with a small number of training examples, much smaller compared to the size of the data set. Class imbalance poses a key challenge that plagues most multi-label learning methods. Ensemble of Classifier Chains (ECC), one of the most prominent multi-label learning methods, is no exception to this rule, as each of the binary models it builds is trained from all positive and negative examples of a label. To make ECC resilient to class imbalance, we first couple it with random undersampling. We then present two extensions of this basic approach, where we build a varying number of binary models per label and construct chains of different sizes, in order to improve the exploitation of majority examples with approximately the same computational budget. Experimental results on 16 multi-label datasets demonstrate the effectiveness of the proposed approaches in a variety of evaluation metrics.",1
Ensemble selection for water quality prediction,2007,http://ftp.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-284/page429.pdf,"This paper studies the greedy ensemble selection algorithm for ensembles of regression models. We explore two interesting parameters of this algorithm: a) the direction of search (forward, backward), and b) the performance evaluation dataset (training set, validation set) on a large ensemble (200 models) of neural networks and support vector machines. Experimental comparison of the different parameters are performed on an application domain with important social and commercial value: water quality monitoring. In specific we experiment on real data collected from an underwater sensor system.",1
Using the k-nearest problems for adaptive multicriteria planning,2004,https://link.springer.com/chapter/10.1007/978-3-540-24674-9_15,"This paper concerns the design and development of an adaptive planner that is able to adjust its parameters to the characteristics of a given problem and to the priorities set by the user concerning plan length and planning time. This is accomplished through the implementation of the k nearest neighbor machine learning algorithm on top of a highly adjustable planner, called HAP. Learning data are produced by running HAP offline on several problems from multiple domains using all value combinations of its parameters. When the adaptive planner HAP is faced with a new problem, it locates the k nearest problems, using a set of measurable problem characteristics, retrieves the performance data for all parameter configurations on these problems and performs a multicriteria combination, with user-specified weights for plan length and planning time. Based on this combination, the configuration with the best …",0
Lionets: Local interpretation of neural networks through penultimate layer decoding,2019,https://arxiv.org/abs/1906.06566,"Technological breakthroughs on smart homes, self-driving cars, health care and robotic assistants, in addition to reinforced law regulations, have critically influenced academic research on explainable machine learning. A sufficient number of researchers have implemented ways to explain indifferently any black box model for classification tasks. A drawback of building agnostic explanators is that the neighbourhood generation process is universal and consequently does not guarantee true adjacency between the generated neighbours and the instance. This paper explores a methodology on providing explanations for a neural network's decisions, in a local scope, through a process that actively takes into consideration the neural network's architecture on creating an instance's neighbourhood, that assures the adjacency among the generated neighbours and the instance.",1
Learning-to-rank and relevance feedback for literature appraisal in empirical medicine,2018,https://link.springer.com/chapter/10.1007/978-3-319-98932-7_5,"The constantly expanding medical libraries contain immense amounts of information, including evidence from healthcare research. Gathering and interpreting this evidence can be both challenging and time-consuming for researchers conducting systematic reviews. Technologically assisted review (TAR) aims to assist this process by finding as much relevant information as possible with the least effort. Toward this, we present an incremental learning method that ranks documents, previously retrieved, by automating the process of title and abstract screening. Our approach combines a learning-to-rank model trained across multiple reviews with a model focused on the given review, incrementally trained based on relevance feedback. The classifiers use as features several similarity metrics between the documents and the research topic, such as Levenshtein distance, cosine similarity and BM25, and vectors …",0
Hierarchical partitioning of the output space in multi-label data,2018,https://www.sciencedirect.com/science/article/pii/S0169023X17304512,"Hierarchy Of Multi-label classifiERs (HOMER) is a multi-label learning algorithm that breaks the initial learning task to several, easier sub-tasks by first constructing a hierarchy of labels from a given label set and secondly employing a given base multi-label classifier (MLC) to the resulting sub-problems. The primary goal is to effectively address class imbalance and scalability issues that often arise in real-world multi-label classification problems. In this work, we present the general setup for a HOMER model and a simple extension of the algorithm that is suited for MLCs that output rankings. Furthermore, we provide a detailed analysis of the properties of the algorithm, both from an aspect of effectiveness and computational complexity. A secondary contribution involves the presentation of a balanced variant of the k means algorithm, which serves in the first step of the label hierarchy construction. We conduct …",0
A prediction model of passenger demand using AVL and APC data from a bus fleet,2015,https://dl.acm.org/doi/abs/10.1145/2801948.2801984,"In this paper we present the passenger demand prediction model of BusGrid. BusGrid is a novel information system for the improvement of productivity and customer service in public transport bus services. BusGrid receives and processes real time data from the automated vehicle location (AVL) and the automated passenger counting (APC) sensors installed on a bus fleet and assists their operator on the improvement of bus schedules and the design of new bus routes and stops based on the expected demand. For the prediction of passenger demand in any bus stop, the raw sensor data were pre-processed and several different feature sets were extracted and tested as predictors of passenger demand. The pre-processed data were used for the supervised learning of a regression model that predicts people demand for any given bus stop and route. Experimental results show that the proposed approach achieved …",0
Subset labeled LDA: A topic model for extreme multi-label classification,2018,https://link.springer.com/chapter/10.1007/978-3-319-98539-8_12,"Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard unsupervised Latent Dirichlet Allocation (LDA) algorithm, to address multi-label learning tasks. Previous work has shown it to perform en par with other state-of-the-art multi-label methods. Nonetheless, with increasing number of labels LLDA encounters scalability issues. In this work, we introduce Subset LLDA, a topic model that extends the standard LLDA algorithm, that not only can efficiently scale up to problems with hundreds of thousands of labels but also improves over the LLDA state-of-the-art in terms of prediction accuracy. We conduct experiments on eight data sets, with labels ranging from hundreds to hundreds of thousands, comparing our proposed algorithm with the other LLDA algorithms (Prior–LDA, Dep–LDA), as well as the state-of-the-art in extreme multi-label classification. The results show a steady advantage of …",0
Hatebusters: A Web Application for Actively Reporting YouTube Hate Speech.,2018,https://www.academia.edu/download/61503147/8b41200a4997d0189b6fbffb57e17a8b002620191213-100572-1v7qgg9.pdf,"Hatebusters is a web application for actively reporting YouTube hate speech, aiming to establish an online community of volunteer citizens. Hatebusters searches YouTube for videos with potentially hateful comments, scores their comments with a classifier trained on human-annotated data and presents users those comments with the highest probability of being hate speech. It also employs gamification elements, such as achievements and leaderboards, to drive user engagement.",1
Combining inter-review learning-to-rank and intra-review incremental training for title and abstract screening in systematic reviews,2017,http://ikee.lib.auth.gr/record/299543?ln=en,"We describe the approach we employed for Task II of CLEF eHealth 2017, concerning title and abstract screening in diagnostic test accuracy reviews. Our approach combines a learning-to-rank model trained across multiple reviews with a model focused on the given review, incrementally trained based on relevance feedback. Our learning-to-rank model is built using extreme gradient boosting on features computed by considering the similarity of different fields of the documents (title, abstract), with different fields of the topics (title, query). Our incrementally trained model is a support vector machine trained on a TF-IDF representation of title and abstract of the documents. The results of our approach are promising, reaching 0.658 normalized cumulative gain in the top 10 ranked documents in the simple evaluation setting and 0.846 in the cost-effective evaluation setting, the latter assuming feedback can be obtained …",0
Active learning algorithms for multi-label data,2016,https://link.springer.com/chapter/10.1007/978-3-319-44944-9_23,"Active learning is an iterative supervised learning task where learning algorithms can actively query an oracle, i.e. a human annotator that understands the nature of the pro blem, for labels. As the learner is allowed to interactively choose the data from which it learns, it is expected that the learner will perform better with less training. The active learning approach is appropriate to machine learning applications where training labels are costly to obtain but unlabeled data is abundant. Although active learning has been widely considered for single-label learning, this is not the case for multi-label learning, where objects can have more than one class labels and a multi-label learner is trained to assign multiple labels simultaneously to an object. We discuss the key issues that need to be considered in pool-based multi-label active learning and discuss how existing solutions in the literature deal with each of these …",0
AUTH-Atypon at BioASQ 3: Large-Scale Semantic Indexing in Biomedicine.,2015,http://ceur-ws.org/Vol-1391/29-CR.pdf,"In this paper we present the methods and the approaches employed in terms of our participation to the BioASQ Challenge 2015 and more specifically in task 3a, concerning the automatic semantic annotation of scientific abstracts. Based on the successful approaches of the previous years we considered a variety of ensembles, incorporated journalspecific semantic information and developed an approach to handle the concept drift within the BioASQ corpus. The official results demonstrate a consistent advantage of our approaches against the BioASQ and the National Library of Medicine (NLM) baselines. Specifically, the systems proposed by our team ranked among the top tier ones along the competition, obtaining the second place in 10 out of 15 weeks.",1
A Knowledge-Based Web Information System for the Fusion of Distributed Classifers,2004,https://www.igi-global.com/chapter/web-information-systems/31128,"This chapter presents the design and development of WebDisC, a knowledge-based web information system for the fusion of classifiers induced at geographically distributed databases. The main features of our system are:(i) a declarative rule language for classifier selection that allows the combination of syntactically heterogeneous distributed classifiers;(ii) a variety of standard methods for fusing the output of distributed classifiers;(iii) a new approach for clustering classifiers in order to deal with the semantic heterogeneity of distributed classifiers, detect their interesting similarities and differences, and enhance their fusion; and (iv) an architecture based on the Web services paradigm that utilizes the open and scalable standards of XML and SOAP.",1
Lionforests: Local interpretation of random forests through path selection,2019,https://arxiv.org/abs/1911.08780,"Towards a future where machine learning systems will integrate into every aspect of people's lives, researching methods to interpret such systems is necessary, instead of focusing exclusively on enhancing their performance. Enriching the trust between these systems and people will accelerate this integration process. Many medical and retail banking/finance applications use state-of-the-art machine learning techniques to predict certain aspects of new instances. Tree ensembles, like random forests, are widely acceptable solutions on these tasks, while at the same time they are avoided due to their black-box uninterpretable nature, creating an unreasonable paradox. In this paper, we provide a methodology for shedding light on the predictions of the misjudged family of tree ensemble algorithms. Using classic unsupervised learning techniques and an enhanced similarity metric, to wander among transparent trees inside a forest following breadcrumbs, the interpretable essence of tree ensembles arises. An interpretation provided by these systems using our approach, which we call ""LionForests"", can be a simple, comprehensive rule.",1
Multi-label learning approaches for music instrument recognition,2011,https://link.springer.com/chapter/10.1007/978-3-642-21916-0_77,"This paper presents the two winning approaches that we developed for the instrument recognition track of the ISMIS 2011 contest on Music Information. The solution that ranked first was based on the Binary Relevance approach and built a separate model for each instrument on a selected subset of the available training data. Moreover, a new ranking approach was utilized to produce an ordering of the instruments according to their degree of relevance to a given track. The solution that ranked second was based on the idea of constraining the number of pairs that were being predicted. It applied a transformation to the original dataset and utilized a variety of post-processing filters based on domain knowledge and exploratory analysis of the evaluation set. Both solutions were developed using the Mulan open-source software for multi-label learning.",1
European Conference on Machine Learning,2007,https://scholar.google.com/scholar?cluster=3807634396576768464&hl=en&oi=scholarr,unknown,1
HAP RC: an automatically configurable planning system,2005,https://content.iospress.com/articles/ai-communications/aic335,"This paper presents an adaptive planning system, called HAP RC, which automatically fine-tunes its planning parameters according to the morphology of the problem in hand, through a combination of Planning, Machine Learning and Knowledge-Based techniques. The adaptation is guided by a rule-based system that sets planner configuration parameters based on measurable characteristics of the problem instance. The knowledge of the rule system has been acquired through a rule induction algorithm. Specifically, the approach of propositional rule learning was applied to a dataset produced by results from experiments on a large number of problems from various domains, including those used in the three International Planning Competitions. The improvement of the adaptive system over the original planner is assessed through thorough experiments in problems of both known and unknown domains.",1
Improving Distantly-Supervised Relation Extraction Through BERT-Based Label and Instance Embeddings,2021,https://ieeexplore.ieee.org/abstract/document/9405641/,"Distantly-supervised relation extraction (RE) is an effective method to scale RE to large corpora but suffers from noisy labels. Existing approaches try to alleviate noise through multi-instance learning and by providing additional information but manage to recognize mainly the top frequent relations, neglecting those in the long-tail. We propose REDSandT (Relation Extraction with Distant Supervision and Transformers), a novel distantly-supervised transformer-based RE method that manages to capture a wider set of relations through highly informative instance and label embeddings for RE by exploiting BERT’s pre-trained model, and the relationship between labels and entities, respectively. We guide REDSandT to focus solely on relational tokens by fine-tuning BERT on a structured input, including the sub-tree connecting an entity pair and the entities’ types. Using the extracted informative vectors, we shape label …",0
Machine learning techniques for short-term electric load forecasting,2017,http://ikee.lib.auth.gr/record/294603/files/GRI-2017-20362.pdf,"Economic growth in the modern world, depends directly on the availability of electric energy, especially because most societies, industries, and economies depend almost entirely on its use. The availability of a source of continuous, cheap, and reliable energy is of foremost economic importance. Electric load forecasting is an important tool used to ensure that the energy supplied by utilities meets the consumers needs. To this end, a staff of trained personnel is needed to carry out this specialized function. Load forecasting is always defined as basically the science or art of predicting the future load on a given system, for a specified period of time ahead. These predictions may be just for a fraction of an hour ahead for operation purposes, or as much as 20 years into the future for planning purposes. The purpose of this master thesis is to create accurate machine learning models for short-term electric load demand forecasting in short-term horizon (for 1 day ahead) for the Greek Electric Network Grid. TheloaddatathattheThesisusesoriginatesfromIPTOwhichstandsforIndependentPowerTransmission Operator. Meteorological Features taken from Dark Sky API. To compare the Thesis predictions to the systempredictions, nextdaycurrentloadpredictionsfromOoEMwhichstandsforOperatorofElectricity Market in Greece were used.The datasets were downloaded from its origins, preprocessed, cleaned and six machine learning algorithms was used. After four different types of experiments, the combination of SVM, XGBoost and Model Trees models was used in order to get 2.4% prediction error and on the other hand OoEM gives 2.53% prediction error. Hence …",0
Drawing parallels between multi-label classification and multi-target regression,2014,http://users.auth.gr/users/0/9/022090/public_html/publications/slides/spyromitrosECMLPKDD14MTPW.pdf,"Drawing parallels between multi-label classification and multi-target regression Page 1 
International Workshop on Multi-Target Prediction Nancy, France, September 15th, 2014 
Drawing Parallels between Multi-label Classification and Multi-target Regression Grigorios 
Tsoumakas, Eleftherios Spyromitros-Xioufis, and Ioannis Vlahavas Drawing Parallels between 
Multi-label Classification and Multi-target Regression Grigorios Tsoumakas, Eleftherios 
Spyromitros-Xioufis, and Ioannis Vlahavas Machine Learning and Knowledge Discovery (MLKD) 
group Department of Informatics, Aristotle University of Thessaloniki, Greece Page 2 International 
Workshop on Multi-Target Prediction Nancy, France, September 15th, 2014 Drawing Parallels 
between Multi-label Classification and Multi-target Regression Grigorios Tsoumakas, Eleftherios 
Spyromitros-Xioufis, and Ioannis Vlahavas • Two instances of multi-target prediction -& --…",0
A hybrid approach for cold-start recommendations of videolectures,2011,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.369.3516&rep=rep1&type=pdf,This paper presents the solution which ranked 2nd in the “cold-start” recommendations task of the ECML/PKDD 2011 discovery challenge. The task was the recommendation of new videolectures to new users of the Videolectures. net Web site. The proposed solution is a hybrid recommendation approach which combines content-based and collaborative information. Structured and unstructured textual attributes which describe each lecture are synthesized to create a vector representation with tf/idf weights. Collaborative information is incorporated for query expansion with a novel method which identifies neighboring lectures in a co-viewing graph and uses them to supplement missing attributes. The cosine similarity measure is used to find similar lectures and final recommendations are made by also accounting the coexistence duration of lectures. The results of the competition show that the proposed approach is able to give accurate “cold-start” recommendations.,1
"AUTH@ CLSciSumm 20, LaySumm 20, LongSumm 20",2020,https://www.aclweb.org/anthology/2020.sdp-1.28/,"We present the systems we submitted for the shared tasks of the Workshop on Scholarly Document Processing at EMNLP 2020. Our approaches to the tasks are focused on exploiting large Transformer models pre-trained on huge corpora and adapting them to the different shared tasks. For tasks 1A and 1B of CL-SciSumm we are using different variants of the BERT model to tackle the tasks of “cited text span” and “facet” identification. For the summarization tasks 2 of CL-SciSumm, LaySumm and LongSumm we make use of different variants of the PEGASUS model, with and without fine-tuning, adapted to the nuances of each one of those particular tasks.",1
Beyond MeSH: Fine-grained semantic indexing of biomedical literature based on weak supervision,2020,https://www.sciencedirect.com/science/article/pii/S0306457319312415,"In this work, we propose a method for the automated refinement of subject annotations in biomedical literature at the level of concepts. Semantic indexing and search of biomedical articles in MEDLINE/PubMed are based on semantic subject annotations with MeSH descriptors that may correspond to several related but distinct biomedical concepts. Such semantic annotations do not adhere to the level of detail available in the domain knowledge and may not be sufficient to fulfil the information needs of experts in the domain. To this end, we propose a new method that uses weak supervision to train a concept annotator on the literature available for a particular disease. We test this method on the MeSH descriptors for two diseases: Alzheimer’s Disease and Duchenne Muscular Dystrophy. The results indicate that concept-occurrence is a strong heuristic for automated subject annotation refinement and its use as weak …",0
Structured summarization of academic publications,2019,https://arxiv.org/abs/1905.07695,"We propose SUSIE, a novel summarization method that can work with state-of-the-art summarization models in order to produce structured scientific summaries for academic articles. We also created PMC-SA, a new dataset of academic publications, suitable for the task of structured summarization with neural networks. We apply SUSIE combined with three different summarization models on the new PMC-SA dataset and we show that the proposed method improves the performance of all models by as much as 4 ROUGE points.",1
Using multi-target feature evaluation to discover factors that affect business process behavior,2018,https://www.sciencedirect.com/science/article/pii/S0166361517307704,"Certain business environments, like health-care or customer service, host complex and highly variable business processes. In such situations, we expect fluctuating process behavior, which is difficult to attribute to specific causes, at least automatically. This work aims to provide process analysts with an additional tool to discover factors that affect the process flow. To this end, we propose a three-stage methodology to deal with the several challenges of this goal.Adhering to the process mining paradigm that suggests for evidence-based process analysis and improvement, we introduce a horizontal partitioning approach to identify elements of process behavior during the first stage. Then, during the second stage, we discuss how log manipulations can yield characteristics that reflect various perspectives of the process. Finally, we propose a multi-target feature evaluation step to deliver insights about the associations …",0
Integrating multiple immunogenetic data sources for feature extraction and mining somatic hypermutation patterns: the case of “towards analysis” in chronic lymphocytic leukaemia,2016,https://link.springer.com/article/10.1186/s12859-016-1044-3,"Somatic Hypermutation (SHM) refers to the introduction of mutations within rearranged V(D)J genes, a process that increases the diversity of Immunoglobulins (IGs). The analysis of SHM has offered critical insight into the physiology and pathology of B cells, leading to strong prognostication markers for clinical outcome in chronic lymphocytic leukaemia (CLL), the most frequent adult B-cell malignancy. In this paper we present a methodology for integrating multiple immunogenetic and clinocobiological data sources in order to extract features and create high quality datasets for SHM analysis in IG receptors of CLL patients. This dataset is used as the basis for a higher level integration procedure, inspired form social choice theory. This is applied in the Towards Analysis, our attempt to investigate the potential ontogenetic transformation of genes belonging to specific stereotyped CLL subsets towards other genes or …",0
"Mining multi-label data, data mining and knowledge discovery handbook, O. Maimon, L. Rokach",2010,https://scholar.google.com/scholar?cluster=10825670838484082061&hl=en&oi=scholarr,unknown,1
Web services for adaptive planning,2004,http://lpis.csd.auth.gr/publications/tsoumakas-ecai04workshop.pdf,"This paper presents the design and development of an adaptive planning system using the technology of Web services. The Web-based adaptive planning system consists of two modules that can work independently. The first one is called HAP-WS and is the Web service interface to the domain independent planner HAP (Highly Adjustable Planner) that can be customized through the adjustment of several parameters, either manually or automatically. In the manual mode, the user itself adjusts planner parameters giving explicitly the values. In the automatic mode, the second subsystem, called LAMP-WS, computes the values of the planning parameters of HAP. LAMP-WS is the Web service interface to the learning system LAMP (Lazy Adaptive Multicriteria Planning) that can automatically configure a planning system using instance-based learning on past performance data of that system. The two subsystems are implemented as independent Web services, which can be used stand-alone and reside in different servers in potentially different geographical locations.",1
Towards adaptive heuristic planning through machine learning,2002,https://www.academia.edu/download/42207533/hap.pdf,"In domain independent heuristic planning there is a number of planning systems with very good performance on some problems and very poor on others. Few attempts have been made in the past to explain this phenomenon. In this paper we use machine learning techniques to discover knowledge hidden in the dynamics of the planning process that would relate specific characteristics of a planning problem with specific properties of a planning system that lead to good or bad performance. By this, we aim at shedding light to some of the dark areas of heuristic planning and develop an adaptive planner that would be able to optimize its configuration according to the problem at hand.",1
Keywords lie far from the mean of all words in local vector space,2020,https://arxiv.org/abs/2008.09513,"Keyword extraction is an important document process that aims at finding a small set of terms that concisely describe a document's topics. The most popular state-of-the-art unsupervised approaches belong to the family of the graph-based methods that build a graph-of-words and use various centrality measures to score the nodes (candidate keywords). In this work, we follow a different path to detect the keywords from a text document by modeling the main distribution of the document's words using local word vector representations. Then, we rank the candidates based on their position in the text and the distance between the corresponding local vectors and the main distribution's center. We confirm the high performance of our approach compared to strong baselines and state-of-the-art unsupervised keyword extraction methods, through an extended experimental study, investigating the properties of the local representations.",1
WAMBy: An information retrieval approach to web-based question answering,2018,https://dl.acm.org/doi/abs/10.1145/3200947.3201023,"Nowadays, most answers to natural language questions can be found in the first few results of web search engines. We describe the WAMBy question answering system that attempts to extract answers from the top 10 results of Google. We propose a new ranking method for factoid answers that among others takes into account the semantic similarity of the context of each answer candidate with the question and named entities found in the titles and the snippets of search results. The proposed method gave very promising results in a variety of questions. Also we describe the methods used for non-factoid answer extraction and ranking. An important part of the system is a new text similarity measure that extends TF-IDF by utilizing word vectors. The new text similarity measure solves the problem of synonyms and improves the performance of TF-IDF in the paraphrase identification task. The source code of WAMBy is …",0
Unsupervised keyphrase extraction based on outlier detection,2018,https://www.researchgate.net/profile/Eirini-Papagiannopoulou/publication/327010290_Unsupervised_Keyphrase_Extraction_Based_on_Outlier_Detection/links/5b7b31bb4585151fd123ca62/Unsupervised-Keyphrase-Extraction-Based-on-Outlier-Detection.pdf,"We propose a novel unsupervised keyphrase extraction approach based on outlier detection. Our approach starts by training word embeddings on the target document to capture semantic regularities among the words. It then uses the minimum covariance determinant estimator to model the distribution of non-keyphrase word vectors, under the assumption that these vectors come from the same distribution, indicative of their irrelevance to the semantics expresses by the dimensions of the learned vector representation. Candidate keyphrases are based on words that are outliers of this dominant distribution. Empirical results show that our approach outperforms state-of-the-art unsupervised keyphrase extraction methods.",1
An empirical comparison of methods for multi-label data stream classification,2016,https://link.springer.com/chapter/10.1007/978-3-319-47898-2_16,This paper studies the problem of multi-label classification in the context of data streams. We discuss related work in this area and present our implementation of several existing approaches as part of the Mulan software. We present empirical results on a real-world data stream concerning media monitoring and discuss and draw a number of conclusions regarding their performance.,1
Improving Gibbs Sampling Predictions on Unseen Data for Latent Dirichlet Allocation,2015,https://scholar.google.com/scholar?cluster=8791589631129028331&hl=en&oi=scholarr,"Latent Dirichlet Allocation (LDA) is a generative probabilistic model for discovering the underlying structure of discrete data. LDA and its extensions have been used in unsupervised and supervised learning tasks across a variety of data types including textual, image and biological data. Several methods have been presented for inference of LDA parameters, including Variational Bayesian inference (VB), Collapsed Variational Bayesian inference (CVB) and Collapsed Gibbs Sampling (CGS). This work introduces a novel method for generating LDA predictions on unobserved data, given a model trained by CGS, by employing the full distribution of word tokens over topics, in a fashion similar to the one followed by CVB0 (a variant of CVB). We perform extensive empirical comparisons of our prediction method with the predictions generated by CGS and CVB0 on real-world data for both standard unsupervised LDA and Prior LDA, one of the supervised variants of LDA for multi-label data. The results show a consistent advantage of our method over CGS under all experimental conditions, and over CVB0 under the majority of conditions.",1
"Artificial Intelligence Applications and Innovations: Proceedings of the 5th IFIP Conference on Artificial Intelligence Applications and Innovations (AIAI'2009), April 23-25, 2009, Thessaloniki, Greece",2009,https://books.google.com/books?hl=en&lr=&id=BrGxEVpiiRIC&oi=fnd&pg=PA1&dq=info:5CMCKC9k0awJ:scholar.google.com&ots=gU5I264b_G&sig=O-5KGY7sLw1UXO8Mxp9vhWZz_Yc,"The ever expanding abundance of information and computing power enables-searchers and users to tackle highly interesting issues, such as applications prov-ing personalized access and interactivity to multimodal information based on user preferences and semantic concepts or human-machine interface systems utilizing information on the affective state of the user. The general focus of the AIAI conf-ence is to provide insights on how AI can be implemented in real world applications. This volume contains papers selected for presentation at the 5th IFIP Conf-ence on Artificial Intelligence Applications & Innovations (AIAI 2009) being held from 23rd till 25th of April, in Thessaloniki, Greece. The IFIP AIAI 2009 conf-ence is co-organized by the Aristotle University of Thessaloniki, by the University of Macedonia Thessaloniki and by the Democritus University of Thrace. AIAI 2009 is the official conference of the WG12. 5"" Artificial Intelligence Appli-tions"" working group of IFIP TC12 the International Federation for Information Processing Technical Committee on Artificial Intelligence (AI). It is a conference growing and maintaining high standards of quality. The p-pose of the 5th IFIP AIAI Conference is to bring together researchers, engineers and practitioners interested in the technical advances and business/industrial-plications of intelligent systems. AIAI 2009 is not only focused in providing-sights on how AI can be implemented in real world applications, but it also covers innovative methods, tools and ideas of AI on architectural and algorithmic level.",1
Incremental clustering for the classification of concept-drifting data streams,2008,https://www.researchgate.net/profile/Grigorios-Tsoumakas/publication/228980443_Incremental_Clustering_for_the_Classification_of_Concept-Drifting_Data_Streams/links/57357df008ae9ace840962a9/Incremental-Clustering-for-the-Classification-of-Concept-Drifting-Data-Streams.pdf,"Concept drift is a common phenomenon in streaming data environments and constitutes an interesting challenge for researchers in the machine learning and data mining community. This paper proposes a probabilistic representation model for data stream classification and investigates the use of incremental clustering algorithms in order to identify and adapt to concept drift. An experimental study is performed using three real-world datasets from the text domain, a basic implementation of the proposed framework and three baseline methods for dealing with drifting concepts. Results are promising and encourage further investigation.",1
Rule induction for automatic configuration of planning systems,2003,http://lpis.csd.auth.gr/publications/TR-LPIS-142-03.pdf,"This paper presents a methodology for building an adaptive planning system, which automatically fine-tunes its planning parameters according to the morphology of the problem in hand, through a combination of Planning, Machine Learning and Knowledge-Based techniques. The adaptation is guided by a rule-based system that sets planner configuration parameters based on measurable characteristics of the problem instance. The knowledge of the rule system has been acquired through a rule induction algorithm. Specifically, the approach of propositional rule learning was applied to a dataset produced by results from experiments on a large number of problems from various domains, including those used in the three International Planning Competitions. The validity of our methodology is assessed through thorough experimental results that demonstrate the boost in performance of the planning system in problems of both known and unknown domains.",1
Similarity based distributed classification,2002,https://www.academia.edu/download/46076415/Similarity_Based_Distributed_Classificat20160530-23643-inngf5.pdf,"Most distributed knowledge discovery approaches view data distribution as a technical issue and combine local models aiming at a single global model. This however, is unsuitable for inherently distributed databases, which often produce models that differ semantically. In this paper we present an approach for distributed classification that uses the pairwise similarity of local models in order to produce a better model for each of the distributed databases. This is achieved by averaging the decisions of all local models weighted by their similarity with the model induced from the origin of the unlabelled data.",1
Land Evaluation-An Artificial Intelligence Approach,2001,https://www.igi-global.com/chapter/land-evaluation-artificial-intelligence-approach/18533,"A major environmental concern of today’s scientists is the inefficient exploitation of natural resources. The land is the ultimate source of wealth and the foundation on which civilization is constructed. Inappropriate land use, leads to destruction of the land resource, poverty and other social problems, and even to the destruction of civilization. To avoid such phenomena, land evaluation is employed, for rational land use planning and appropriate and sustainable use of natural and human resources (Rossiter, 1994). The management of land use is an interdisciplinary activity that relies on large amounts of information from different sources. Land evaluators need to collect information from soil surveyors, climatologists and census takers on land resource. They also need the expert knowledge of soil scientists, agronomists and economists on land use. In addition, land evaluators must select and apply the most appropriate …",0
Zero-shot classification of biomedical articles with emerging mesh descriptors,2020,https://dl.acm.org/doi/abs/10.1145/3411408.3411414,"Although numerous applications that have been developed during the last years produce vast amounts of data, the inability to obtain their ground truth target values has triggered the appearance of several new machine learning (ML) variants that tackle such phenomena. The main reasons why this happens are the evolutionary nature that characterizes the majority of real-world problems, highly hindering the conventional approaches to be applied because of incompatibility, as well as the noisy sources of data or even the shortage of available training data to produce robust predictive models. The objective of this work is to provide a new ML approach in the field of zero-shot classification, focused on classifying abstracts that come from PubMed, a well-known resource of publications from the biomedical field. The proposed approach differs in that it uses bioBERT embeddings for transforming the textual data into a …",0
Yes/no question answering in bioasq 2019,2019,https://scholar.google.com/scholar?cluster=15068560430388812484&hl=en&oi=scholarr," The field of question answering has gained greater attention with the rise of deep neural networks. More and more approaches adopt paradigms which are based primarily on the powerful language representations models and transfer learning techniques to build efficient learning models which are able to outperform current state of the art systems. Endorsing this current trend, in this paper, we strive to take a step towards the goal of answering yes/no questions in the field of biomedicine. Specifically, the task is to give a short answer (yes or no) for a question written in natural language, finding clues including in a set of snippets that are related with this question. We propose three different deep neural network models, which are free of assumptions about predefined specific feature functions, while the key elements of these are the ELMo embeddings, the similarity matrices and/or sentiment information. The results …",0
Classifying Biomedical Figures by Modality via Multi-Label Learning,2019,https://ieeexplore.ieee.org/abstract/document/8654612/,"The figures found in biomedical literature are a vital part of biomedical research, education, and clinical decision. The multitude of their modalities and the lack of corresponding metadata constitute search and information, retrieval a difficult task. In this paper, we introduce novel multi-label modality classification approaches for biomedical figures without segmenting the compound figures. In particular, we investigate using both simple and compound figures for training a multi-label model to be used for annotating either all figures or only those predicted as compound by a compound figure detection model. Using data from the medical task of ImageCLEF 2016, we train our approaches with visual features and compare them with the approach involving compound figure separation into sub-figures. Furthermore, we study how multimodal learning, from both visual and textual features affects the tasks of classifying …",0
Unsupervised keyphrase extraction from scientific publications,2018,https://arxiv.org/abs/1808.03712,"We propose a novel unsupervised keyphrase extraction approach that filters candidate keywords using outlier detection. It starts by training word embeddings on the target document to capture semantic regularities among the words. It then uses the minimum covariance determinant estimator to model the distribution of non-keyphrase word vectors, under the assumption that these vectors come from the same distribution, indicative of their irrelevance to the semantics expressed by the dimensions of the learned vector representation. Candidate keyphrases only consist of words that are detected as outliers of this dominant distribution. Empirical results show that our approach outperforms state-of-the-art and recent unsupervised keyphrase extraction methods.",1
Transferring models in hybrid reinforcement learning agents,2011,https://link.springer.com/chapter/10.1007/978-3-642-23957-1_19,"The main objective of transfer learning is to reuse knowledge acquired in a previous learned task, in order to enhance the learning procedure in a new and more complex task. Transfer learning comprises a suitable solution for speeding up the learning procedure in Reinforcement Learning tasks. In this work, we propose a novel method for transferring models to a hybrid reinforcement learning agent. The models of the transition and reward functions of a source task, will be transferred to a relevant but different target task. The learning algorithm of the target task’s agent takes a hybrid approach, implementing both model-free and model-based learning, in order to fully exploit the presence of a model. The empirical evaluation, of the proposed approach, demonstrated significant results and performance improvements in the 3D Mountain Car task, by successfully using the models generated from the standard …",0
Fuzzy meta-learning: preliminary results,2001,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.4912&rep=rep1&type=pdf,"Learning from distributed data is becoming in our times a necessity, but it is also a complex and challenging task. Approaches developed so far have not dealt with the uncertainty, imprecision and vagueness involved in distributed learning. Meta-Learning, a successful approach for distributed data mining, is in this paper extended to handle the imprecision and uncertainty of the local models and the vagueness that characterizes the meta-learning process. The proposed approach, Fuzzy Meta-Learning uses a fuzzy inductive algorithm to meta-learn a global model from the degrees of certainty of the output of local classifiers. This way more accurate models of collective knowledge can be acquired from data with application both to inherently distributed databases and parts of a very large database. Preliminary results are promising and encourage further research towards this direction.",1
Multi-label sampling based on local label imbalance,2022,https://www.sciencedirect.com/science/article/pii/S003132032100474X,"Class imbalance is an inherent characteristic of multi-label data that hinders most multi-label learning methods. One efficient and flexible strategy to deal with this problem is to employ sampling techniques before training a multi-label learning model. Although existing multi-label sampling approaches alleviate the global imbalance of multi-label datasets, it is actually the imbalance level within the local neighbourhood of minority class examples that plays a key role in performance degradation. To address this issue, we propose a novel measure to assess the local label imbalance of multi-label datasets, as well as two multi-label sampling approaches, namely Multi-Label Synthetic Oversampling based on Local label imbalance (MLSOL) and Multi-Label Undersampling based on Local label imbalance (MLUL). By considering all informative labels, MLSOL creates more diverse and better labeled synthetic instances …",0
Drug-target interaction prediction via an ensemble of weighted nearest neighbors with interaction recovery,2021,https://link.springer.com/article/10.1007/s10489-021-02495-z,"Predicting drug-target interactions (DTI) via reliable computational methods is an effective and efficient way to mitigate the enormous costs and time of the drug discovery process. Structure-based drug similarities and sequence-based target protein similarities are the commonly used information for DTI prediction. Among numerous computational methods, neighborhood-based chemogenomic approaches that leverage drug and target similarities to perform predictions directly are simple but promising ones. However, existing similarity-based methods need to be re-trained to predict interactions for any new drugs or targets and cannot directly perform predictions for both new drugs, new targets, and new drug-target pairs. Furthermore, a large amount of missing (undetected) interactions in current DTI datasets hinders most DTI prediction methods. To address these issues, we propose a new method denoted as …",0
Towards Human-Centered Summarization: A Case Study on Financial News,2021,https://www.aclweb.org/anthology/2021.hcinlp-1.4/,"Recent Deep Learning (DL) summarization models greatly outperform traditional summarization methodologies, generating high-quality summaries. Despite their success, there are still important open issues, such as the limited engagement and trust of users in the whole process. In order to overcome these issues, we reconsider the task of summarization from a human-centered perspective. We propose to integrate a user interface with an underlying DL model, instead of tackling summarization as an isolated task from the end user. We present a novel system, where the user can actively participate in the whole summarization process. We also enable the user to gather insights into the causative factors that drive the model’s behavior, exploiting the self-attention mechanism. We focus on the financial domain, in order to demonstrate the efficiency of generic DL models for domain-specific applications. Our work takes a first step towards a model-interface co-design approach, where DL models evolve along user needs, paving the way towards human-computer text summarization interfaces.",1
VisioRed: A Visualisation Tool for Interpretable Predictive Maintenance,2021,https://arxiv.org/abs/2103.17003,"The use of machine learning rapidly increases in high-risk scenarios where decisions are required, for example in healthcare or industrial monitoring equipment. In crucial situations, a model that can offer meaningful explanations of its decision-making is essential. In industrial facilities, the equipment's well-timed maintenance is vital to ensure continuous operation to prevent money loss. Using machine learning, predictive and prescriptive maintenance attempt to anticipate and prevent eventual system failures. This paper introduces a visualisation tool incorporating interpretations to display information derived from predictive maintenance models, trained on time-series data.",1
Improving Zero-Shot Entity Retrieval through Effective Dense Representations,2021,https://arxiv.org/abs/2103.04156,"Entity Linking (EL) seeks to align entity mentions in text to entries in a knowledge-base and is usually comprised of two phases: candidate generation and candidate ranking. While most methods focus on the latter, it is the candidate generation phase that sets an upper bound to both time and accuracy performance of the overall EL system. This work's contribution is a significant improvement in candidate generation which thus raises the performance threshold for EL, by generating candidates that include the gold entity in the least candidate set (top-K). We propose a simple approach that efficiently embeds mention-entity pairs in dense space through a BERT-based bi-encoder. Specifically, we extend (Wu et al., 2020) by introducing a new pooling function and incorporating entity type side-information. We achieve a new state-of-the-art 84.28% accuracy on top-50 candidates on the Zeshel dataset, compared to the previous 82.06% on the top-64 of (Wu et al., 2020). We report the results from extensive experimentation using our proposed model on both seen and unseen entity datasets. Our results suggest that our method could be a useful complement to existing EL approaches.",1
Content-aware web robot detection,2020,https://link.springer.com/article/10.1007/s10489-020-01754-9,"Web crawlers account for more than a third of the total web traffic and they are threatening the security, privacy and veracity of web applications and their users. Businesses in finance, ticketing, and publishing, as well as websites with rich and unique content are the ones mostly affected by their actions. To deal with this problem, we present a novel web robot detection approach that takes advantage of the content of a website based on the assumption that human web users are interested in specific topics, while web robots crawl the web randomly. Our approach extends the typical user session representation of log-based features with a novel set of features that capture the semantics of the content of the requested resources. In addition, we contribute a new real-world dataset, which we make publicly available, towards alleviating the scarcity of open data in this field. Empirical results on this dataset validate our …",0
Altruist: argumentative explanations through local interpretations of predictive models,2020,https://arxiv.org/abs/2010.07650,"Interpretable machine learning is an emerging field providing solutions on acquiring insights into machine learning models' rationale. It has been put in the map of machine learning by suggesting ways to tackle key ethical and societal issues. However, existing techniques of interpretable machine learning are far from being comprehensible and explainable to the end user. Another key issue in this field is the lack of evaluation and selection criteria, making it difficult for the end user to choose the most appropriate interpretation technique for its use. In this study, we introduce a meta-explanation methodology that will provide truthful interpretations, in terms of feature importance, to the end user through argumentation. At the same time, this methodology can be used as an evaluation or selection tool for multiple interpretation techniques based on feature importance.",1
Studying the Evolution of Greek Words via Word Embeddings,2020,https://dl.acm.org/doi/abs/10.1145/3411408.3411425,"The meanings of words change over time, reflecting changes in language and society (political, economic or cultural). This study focuses on the more recent form of modern Greek, ie Demotic, also known as Dimotiki, aiming to trace semantic shifts of Greek words between consecutive time periods. We develop a systematic framework that gathers free online Greek digitized literature books and analyzes them using natural language processing tools to learn representations of human language. Then, we conduct an experimental analysis to evaluate the quality of our trained models as well as their ability to detect semantic shifts. We present representative results of actual semantic shifts in Greek words in the period 1980 to 2020.",1
Local Imbalance based Ensemble for Predicting Interactions between Novel Drugs and Targets,2020,https://lirias.kuleuven.be/retrieve/588030,"Computational prediction of drug-target interactions (DTI) reduces the number of candidate drugs to be verified by the tedious and costly experimental approach and expedites the drug discovery process. The most challenging task for computational DTI prediction methods is to predict interactions between new drugs and new targets due to the unavailability of interacting information for both new drugs and new targets. Although there are several methods that could predict interactions in new drug-target pairs, the accuracy of their predicting results is not adequate. To improve the performance of existing approaches, we propose three ensemble DTI prediction strategies that could accompany any DTI prediction method. The proposed ensemble approaches consist of several DTI prediction models learned on training subsets which have been defined by different sampling strategies. Experiments were conducted on four benchmark datasets and the obtained results indicate that the local imbalance-aware sampling strategy is the most effective.",1
Explaining sentiment prediction by generating exemplars in the latent space,2019,http://ikee.lib.auth.gr/record/309576/files/Thesis_Orestis.pdf,"In this thesis, we present an approach to explain the decisions of black box models for text classification. To this end, we build on the Local Interpretable Model-Agnostic Explanations (LIME) algorithm, which is a standard reference in the literature. LIME works by generating a neighborhood of samples around an instance for which we require an explanation. When it comes to text classification, LIME is only able to generate neighbors by randomly removing words or bi-grams from a given text. As a result, the synthetic data generated may not result into semantically meaningful textual sentences. In order to improve the generated neighborhood, we use a variational auto encoder that is able to generate neighbors of the initial instance which are close to it semantically, by randomly tweaking the latent space. Then we use these sentences along with their respective labels taken by the respective black box model in order to train a decision tree classifier from which we can then extract the exemplars as a final explanation. We conduct an experimental evaluation of the approach on two datasets and two black box models. Experimental results demonstrate that our proposed approach is able to produce accurate explanations.",1
Pattern recognition for the analysis of asthma,2019,https://ikee.lib.auth.gr/record/305480/files/GRI-2019-24496.pdf,"Motivation: The large amount of genomic data makes necessary the use of machine learning and data analysis techniques in order to extract useful information and discover hidden patterns. In this project, we apply biclustering for biomarker detection on real asthma datasets. Data: The datasets that we work with are 4. The first consists of clinical information for subjects that can be organized in 5 categories: general, blood, lung function, sputum, biopsy. The second and third contain gene expression and DNA methylation data, respectively. The last one includes micro RNA expression data.Methods: We use a biclustering method, called’FABIA: Factor Analysis for Bicluster Acquisition’. FABIA is a generative multiplicative model which is based on linear dependencies between gene expression and conditions to form biclusters. t-SNE algorithm is also used for the visualization of the data in 2 dimensions. We run FABIA algorithm multiple times and we rank the resulting biclusters. For the evaluation of the biclusters, 4 quality measures are being used. The first, information content, shows the amount of information each bicluster contains about the data. The next ones are the variance and mean squared residue (MSR). The fourth quality measure is the virtual error, which shows the tendency that genes follow under a set of conditions.Results: After applying FABIA multiple times on the datasets and getting the biclusters, we choose to examine only the robust ones. We consider a bicluster as robust if it has average overlap percentage more than 80% over the runs. These biclusters gave us combinations of particular features from the first dataset that may …",0
Multi-label modality classification for figures in biomedical literature,2017,https://ieeexplore.ieee.org/abstract/document/8104161/,"The figures found in biomedical literature are a vital part of biomedical research, education and clinical decision. The multitude of their modalities and the lack of corresponding meta-data, constitute search and information retrieval a difficult task. We present multi-label modality classification approaches for biomedical figures. In particular, we investigate using both simple and compound figures for training a multi-label model to be used for annotating either all figures, or only those predicted as compound by an initial compound figure detection model. Using data from the medical task of ImageCLEF 2016, we train our approaches with visual features and compare them with the standard approach involving compound figure separation into sub-figures. Furthermore, we present a web application for medical figure retrieval, which is based on one of our classification approaches and allows users to search for figures of …",0
Ensemble feature selection using rank aggregation methods for population genomic data,2016,https://dl.acm.org/doi/abs/10.1145/2903220.2903233,"Single Nucleotide Polymorphisms (SNPs) constitute important genetic markers with numerous medical and biological applications of high scientific and economic interest. SNP datasets are typically high dimensional, containing up to million features. Reasons originating from both biology and machine learning, dictate to perform feature selection which is mainly performed after feature evaluation. In this paper we present methods for SNP evaluation and eventually selection, based on combining results obtained from established genetic marker evaluation methods originating from the field of population genetics. To achieve this we have formulated the feature selection task as a ranking aggregation problem, which is a classical problem in social choice and voting theory.",1
Branty: a social media ranking tool for brands,2014,https://link.springer.com/chapter/10.1007/978-3-662-44845-8_28,"In the competitive world of popular brands, strong presence in social media is of major importance for customer engagement and products advertising. Up to now, many such tools and applications enable end-users to observe and monitor their company’s web profile, their statistics, as well as their market outreach and competition status. This work goes beyond the individual brands statistics since it automates a brand ranking process based on opinions emerging in social media users’ posts. Twitter streaming API is exploited to track micro-blogging activity for a number of famous brands with emphasis on users’ opinions and interactions. The social impact is captured from 3 different perspectives (objective counts, opinion reckoning, influence analysis), which estimate a score assigned to each brand via a multi-criteria algorithm. The results are then exposed in a Web application as a list of the most social …",0
Feature Evaluation Metrics for Population Genomic Data,2014,https://link.springer.com/chapter/10.1007/978-3-319-07064-3_36,"Single Nucleotide Polymorphisms (SNPs) are considered nowadays one of the most important class of genetic markers with a wide range of applications with both scientific and economic interests. Although the advance of biotechnology has made feasible the production of genome wide SNP datasets, the cost of the production is still high. The transformation of the initial dataset into a smaller one with the same genetic information is a crucial task and it is performed through feature selection. Biologists evaluate features using methods originating from the field of population genetics. Although several studies have been performed in order to compare the existing biological methods, there is a lack of comparison between methods originating from the biology field with others originating from the machine learning. In this study we present some early results which support that biological methods perform slightly …",0
Learning from Multi-Label Data,2009,https://www.researchgate.net/profile/Alexandre-Beauvois/post/Any-documents-and-results-about-Learning-from-multi-label-data/attachment/59d622736cda7b8083a1c3e2/AS%3A273616501968896%401442246665612/download/learning-from-multi-label-data.pdf,"∎ Yeast [Elisseeff & Weston, NIPS02]□ 2417 examples, 14 labels (1st FunCat level), 4.2 on average∎ Phenotype (yeast)[Clare & King, ECMLPKDD01]□ 1461 examples, 4 FunCat levels∎ 12 yeast datasets [Clare, PhdThesis03; Vens et al., MLJ08]□ Gene expression, homology, phenotype, secondary structure□ FunCat, 6 levels, 492 labels, 8.8 on average□ GO, 14 levels, 3997 labels, 35.0 on average",1
Transferring experience in reinforcement learning through task decomposition.,2009,https://www.academia.edu/download/39820865/Transferring_experience_in_reinforcement20151109-10836-105lda4.pdf,"Transfer learning refers to the process of conveying experience from a simple task to another more complex (and related) task in order to reduce the amount of time that is required to learn the latter task. Typically, in a transfer learning procedure the agent learns a behavior in a source task, and it uses the gained knowledge in order to speed up the learning process in a target task. Reinforcement Learning algorithms are time expensive when they learn from scratch, especially in complex domains, and transfer learning comprises a suitable solution to speed up the training process. In this work we propose a method that decomposes the target task in several instances of the source task and uses them to extract an adviced action for the target task. We evaluate the efficacy of the proposed approach in the robotic soccer Keepaway domain. The results demonstrate that the proposed method helps to reduce the training time of the target task.",1
Multi-label classification bibliography,2008,http://mlkd.csd.auth.gr/multilabel/multilabel_bib.pdf,"Multi-Label Classification Bibliography Page 1 Multi-Label Classification Bibliography 
Grigorios Tsoumakas, Ioannis Katakis and Ioannis Vlahavas Department of Informatics, 
Aristotle University of Thessaloniki 54124 Thessaloniki, Greece {greg,katak,vlahavas}@csd.auth.gr 
August 31, 2008 1 Multi-Label Classification Bibliography [1], [2], [3], [5], [4], [6], [7], [8], [9], [10], 
[11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [28], [27], [29], [30], 
[31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], 
[51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65]. References [1] Zafer 
Barutcuoglu, Robert E. Schapire, and Olga G. Troyanskaya. Hierarchical multi-label prediction 
of gene function. Bioinformatics, 22(7):830– 836, 2006. [2] H. Blockeel, L. Schietgat, J. 
Struyf, S. Dz?eroski, and A. Clare. Decision trees for hierarchical multilabel classification: . …",0
Machine learning for adaptive planning,2005,https://www.igi-global.com/chapter/intelligent-techniques-planning/24460,This chapter is concerned with the enhancement of planning systems using techniques from Machine Learning in order to automatically configure their planning parameters according to the morphology of the problem in hand. It presents two different adaptive systems that set the planning parameters of a highly adjustable planner based on measurable characteristics of the problem instance. The planners have acquired their knowledge from a large data set produced by results from experiments on many problems from various domains. The first planner is a rule-based system that employs propositional rule learning to induce knowledge that suggests effective configuration of planning parameters based on the problem’s characteristics. The second planner employs instance-based learning in order to find problems with similar structure and adopt the planner configuration that has proved in the past to be effective …,0
Instance-Based Zero-Shot learning for semi-Automatic MeSH indexing,2021,https://www.sciencedirect.com/science/article/pii/S0167865521002865,"Zero-shot learning constitutes a variant of the broader category of weakly supervised learning algorithms. Its main asset is the possibility of identifying entities for which no training data are provided in advance. Under this extreme scenario, conventional supervised learning methods cannot operate properly, while consumption of human resources for obtaining even limited instances may be highly restricted, especially when the label space is quite complex because of its cardinality and the underlying semantic dependencies. However, removing the human factor from the learning loop under complicated tasks cannot guarantee robust performance. Thus, semi-automated solutions are widely accepted by both the research and industrial communities, favoring cooperation of human and machine, mainly for alleviating the spent effort of the former, and for acquiring safer predictions. In contrast with the majority of …",0
Bayesian Active Summarization,2021,https://arxiv.org/abs/2110.04480,"Bayesian Active Learning has had significant impact to various NLP problems, but nevertheless it's application to text summarization has been explored very little. We introduce Bayesian Active Summarization (BAS), as a method of combining active learning methods with state-of-the-art summarization models. Our findings suggest that BAS achieves better and more robust performance, compared to random selection, particularly for small and very small data annotation budgets. Using BAS we showcase it is possible to leverage large summarization models to effectively solve real-world problems with very limited annotated data.",1
Short-Term Renewable Energy Forecasting in Greece Using Prophet Decomposition and Tree-Based Ensembles,2021,https://link.springer.com/chapter/10.1007/978-3-030-87101-7_22,"Energy production using renewable sources exhibits inherent uncertainties due to their intermittent nature. Nevertheless, the unified European energy market promotes the increasing penetration of renewable energy sources (RES) by the regional energy system operators. Consequently, RES forecasting can assist in the integration of these volatile energy sources, since it leads to higher reliability and reduced ancillary operational costs for power systems. This paper presents a new dataset for solar and wind energy generation forecast in Greece and introduces a feature engineering pipeline that enriches the dimensional space of the dataset. In addition, we propose a novel method that utilizes the innovative Prophet model, an end-to-end forecasting tool that considers several kinds of nonlinear trends in decomposing the energy time series before a tree-based ensemble provides short-term predictions …",0
What is all this new MeSH about?,2021,https://link.springer.com/article/10.1007/s00799-021-00304-z,"The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary widely used in biomedical knowledge systems, particularly for semantic indexing of scientific literature. As the MeSH hierarchy evolves through annual version updates, some new descriptors are introduced that were not previously available. This paper explores the conceptual provenance of these new descriptors. In particular, we investigate whether such new descriptors have been previously covered by older descriptors and what is their current relation to them. To this end, we propose a framework to categorize new descriptors based on their current relation to older descriptors. Based on the proposed classification scheme, we quantify, analyze, and present the different types of new descriptors introduced in MeSH during the last fifteen years. The results show that only about 25% of new MeSH descriptors correspond to new …",0
A Multi-instance Multi-label Weakly Supervised Approach for Dealing with Emerging MeSH Descriptors,2021,https://link.springer.com/chapter/10.1007/978-3-030-77211-6_47,"The constant evolution of Medical Subject Headings (MeSH) vocabulary and specifically the changes in its descriptors brings forth a number of issues that need automation. The main one being that changed descriptors often lack proper ground truth articles. Therefore, the learning models which demand strong supervision are not directly applicable, settling the predictions on such changes not a straightforward task. The importance of this problem is also enforced by its multi-label nature and the fine-grained character of the examined class-descriptors, factors that demand a lot of human resources. In this work, we alleviate these issues through retrieving insights from a source of information about those descriptors present in MeSH in order to create a weakly-labeled train set. Furthermore, we exploit short-text information per article, implementing an averaging transformation on the corresponding sentence …",0
Harvesting the Public MeSH Note field,2021,https://arxiv.org/abs/2106.00302,"In this document, we report an analysis of the Public MeSH Note field of the new descriptors introduced in the MeSH thesaurus between 2006 and 2020. The aim of this analysis was to extract information about the previous status of these new descriptors as Supplementary Concept Records. The Public MeSH Note field contains information in semi-structured text, meant to be read by humans. Therefore, we adopted a semi-automated approach, based on regular expressions, to extract information from it. In the large majority of cases, we managed to minimize the required manual effort for extracting the previous state of a new descriptor as a Supplementary Concept Record. The source code for this analysis is openly available on GitHub.",1
Keyword Extraction Using Unsupervised Learning on the Document’s Adjacency Matrix,2021,https://www.aclweb.org/anthology/2021.textgraphs-1.9/,"This work revisits the information given by the graph-of-words and its typical utilization through graph-based ranking approaches in the context of keyword extraction. Recent, well-known graph-based approaches typically employ the knowledge from word vector representations during the ranking process via popular centrality measures (eg, PageRank) without giving the primary role to vectors’ distribution. We consider the adjacency matrix that corresponds to the graph-of-words of a target text document as the vector representation of its vocabulary. We propose the distribution-based modeling of this adjacency matrix using unsupervised (learning) algorithms. The efficacy of the distribution-based modeling approaches compared to state-of-the-art graph-based methods is confirmed by an extensive experimental study according to the F1 score. Our code is available on GitHub.",1
Keyphrase Extraction from Scientific Articles via Extractive Summarization,2021,https://www.aclweb.org/anthology/2021.sdp-1.6/,"Automatically extracting keyphrases from scholarly documents leads to a valuable concise representation that humans can understand and machines can process for tasks, such as information retrieval, article clustering and article classification. This paper is concerned with the parts of a scientific article that should be given as input to keyphrase extraction methods. Recent deep learning methods take titles and abstracts as input due to the increased computational complexity in processing long sequences, whereas traditional approaches can also work with full-texts. Titles and abstracts are dense in keyphrases, but often miss important aspects of the articles, while full-texts on the other hand are richer in keyphrases but much noisier. To address this trade-off, we propose the use of extractive summarization models on the full-texts of scholarly documents. Our empirical study on 3 article collections using 3 keyphrase extraction methods shows promising results.",1
Uncertainty-Aware Abstractive Summarization,2021,https://arxiv.org/abs/2105.10155,"We propose a novel approach to summarization based on Bayesian deep learning. We approximate Bayesian summary generation by first extending state-of-the-art summarization models with Monte Carlo dropout and then using them to perform multiple stochastic forward passes. This method allows us to improve summarization performance by simply using the median of multiple stochastic summaries. We show that our variational equivalents of BART and PEGASUS can outperform their deterministic counterparts on multiple benchmark datasets. In addition, we rely on Bayesian inference to measure the uncertainty of the model when generating summaries. Having a reliable uncertainty measure, we can improve the experience of the end user by filtering out generated summaries of high uncertainty. Furthermore, our proposed metric could be used as a criterion for selecting samples for annotation, and can be paired nicely with active learning and human-in-the-loop approaches.",1
Self-citation Analysis using Sentence Embeddings,2021,https://arxiv.org/abs/2105.05527,"The purpose of citation indexes and metrics is intended to be a measure for scientific innovation and quality for researchers, journals, and institutions. However, those metrics are often prone to abuse and manipulation by excessive and unethical self-citations induced by authors, reviewers, editors, or journals. Identifying whether there are or not legitimate reasons for self-citations is normally determined during the review process, where the participating parts may have intrinsic incentives, rendering the legitimacy of self-citations, after publication, questionable. In this paper, we conduct a large-scale analysis of journal self-citations while taking into consideration the similarity between a publication and its references. Specifically, we look into PubMed Central articles published since 1990 and compute similarities of article-reference pairs using sentence embeddings. We examine journal self-citations with an aim to distinguish between justifiable and unethical self-citations.",1
Optimizing Area Under the Curve Measures via Matrix Factorization for Drug-Target Interaction Prediction,2021,https://arxiv.org/abs/2105.01545,"In drug discovery, identifying drug-target interactions (DTIs) via experimental approaches is a tedious and expensive procedure. Computational methods efficiently predict DTIs and recommend a small part of potential interacting pairs for further experimental confirmation, accelerating the drug discovery process. Area under the precision-recall curve (AUPR) that emphasizes the accuracy of top-ranked pairs and area under the receiver operating characteristic curve (AUC) that heavily punishes the existence of low ranked interacting pairs are two widely used evaluation metrics in the DTI prediction task. However, the two metrics are seldom considered as losses within existing DTI prediction methods. This paper proposes two matrix factorization methods that optimize AUPR and AUC, respectively. The two methods utilize graph regularization to ensure the local invariance of training drugs and targets in the latent feature space, and leverage the optimal decay coefficient to infer more reliable latent features of new drugs and targets. Experimental results over four updated benchmark datasets containing more recently verified interactions show the superiority of the proposed methods in terms of the corresponding evaluation metric they optimize.",1
LioNets: A Neural-Specific Local Interpretation Technique Exploiting Penultimate Layer Information,2021,https://arxiv.org/abs/2104.06057,"Artificial Intelligence (AI) has a tremendous impact on the unexpected growth of technology in almost every aspect. AI-powered systems are monitoring and deciding about sensitive economic and societal issues. The future is towards automation, and it must not be prevented. However, this is a conflicting viewpoint for a lot of people, due to the fear of uncontrollable AI systems. This concern could be reasonable if it was originating from considerations associated with social issues, like gender-biased, or obscure decision-making systems. Explainable AI (XAI) is recently treated as a huge step towards reliable systems, enhancing the trust of people to AI. Interpretable machine learning (IML), a subfield of XAI, is also an urgent topic of research. This paper presents a small but significant contribution to the IML community, focusing on a local-based, neural-specific interpretation process applied to textual and time-series data. The proposed methodology introduces new approaches to the presentation of feature importance based interpretations, as well as the production of counterfactual words on textual datasets. Eventually, an improved evaluation metric is introduced for the assessment of interpretation techniques, which supports an extensive set of qualitative and quantitative experiments.",1
Conclusive Local Interpretation Rules for Random Forests,2021,https://arxiv.org/abs/2104.06040,"In critical situations involving discrimination, gender inequality, economic damage, and even the possibility of casualties, machine learning models must be able to provide clear interpretations for their decisions. Otherwise, their obscure decision-making processes can lead to socioethical issues as they interfere with people's lives. In the aforementioned sectors, random forest algorithms strive, thus their ability to explain themselves is an obvious requirement. In this paper, we present LionForests, which relies on a preliminary work of ours. LionForests is a random forest-specific interpretation technique, which provides rules as explanations. It is applicable from binary classification tasks to multi-class classification and regression tasks, and it is supported by a stable theoretical background. Experimentation, including sensitivity analysis and comparison with state-of-the-art techniques, is also performed to demonstrate the efficacy of our contribution. Finally, we highlight a unique property of LionForests, called conclusiveness, that provides interpretation validity and distinguishes it from previous techniques.",1
What is all this new MeSH about? Exploring the semantic provenance of new descriptors in the MeSH thesaurus,2021,https://arxiv.org/abs/2101.08293,"The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary widely used in biomedical knowledge systems, particularly for semantic indexing of scientific literature. As the MeSH hierarchy evolves through annual version updates, some new descriptors are introduced that were not previously available. This paper explores the conceptual provenance of these new descriptors. In particular, we investigate whether such new descriptors have been previously covered by older descriptors and what is their current relation to them. To this end, we propose a framework to categorize new descriptors based on their current relation to older descriptors. Based on the proposed classification scheme, we quantify, analyse and present the different types of new descriptors introduced in MeSH during the last fifteen years. The results show that only about 25% of new MeSH descriptors correspond to new emerging concepts, whereas the rest were previously covered by one or more existing descriptors, either implicitly or explicitly. Most of them were covered by a single existing descriptor and they usually end up as descendants of it in the current hierarchy, gradually leading towards a more fine-grained MeSH vocabulary. These insights about the dynamics of the thesaurus are useful for the retrospective study of scientific articles annotated with MeSH, but could also be used to inform the policy of updating the thesaurus in the future.",1
Extracting Semantic Relationships in Greek Literary Texts,2021,https://www.mdpi.com/2071-1050/13/16/9391,"In the era of Big Data, the digitization of texts and the advancements in Artificial Intelligence (AI) and Natural Language Processing (NLP) are enabling the automatic analysis of literary works, allowing us to delve into the structure of artifacts and to compare, explore, manage and preserve the richness of our written heritage. This paper proposes a deep-learning-based approach to discovering semantic relationships in literary texts (19th century Greek Literature) facilitating the analysis, organization and management of collections through the automation of metadata extraction. Moreover, we provide a new annotated dataset used to train our model. Our proposed model, REDSandT_Lit, recognizes six distinct relationships, extracting the richest set of relations up to now from literary texts. It efficiently captures the semantic characteristics of the investigating time-period by finetuning the state-of-the-art transformer-based Language Model (LM) for Modern Greek in our corpora. Extensive experiments and comparisons with existing models on our dataset reveal that REDSandT_Lit has superior performance (90% accuracy), manages to capture infrequent relations (100%F in long-tail relations) and can also correct mislabelled sentences. Our results suggest that our approach efficiently handles the peculiarities of literary texts, and it is a promising tool for managing and preserving cultural information in various settings.",1
Semantic Indexing of 19th-Century Greek Literature Using 21st-Century Linguistic Resources,2021,https://www.mdpi.com/2071-1050/13/16/8878,"Manual classification of works of literature with genre/form concepts is a time-consuming task requiring domain expertise. Building automated systems based on language understanding can help humans to achieve this work faster and more consistently. Towards this direction, we present a case study on automatic classification of Greek literature books of the 19th century. The main challenges in this problem are the limited number of literature books and resources of that age and the quality of the source text. We propose an automated classification system based on the Bidirectional Encoder Representations from Transformers (BERT) model trained on books from the 20th and 21st century. We also dealt with BERT’s constraint on the maximum sequence length of the input, leveraging the TextRank algorithm to construct representative sentences or phrases from each book. The results show that BERT trained on recent literature books correctly classifies most of the books of the 19th century despite the disparity between the two collections. Additionally, the TextRank algorithm improves the performance of BERT.",1
From Protocol to Screening: A Hybrid Learning Approach for Technology-Assisted Systematic Literature Reviews,2020,https://arxiv.org/abs/2011.09752,"In the medical domain, a Systematic Literature Review (SLR) attempts to collect all empirical evidence, that fit pre-specified eligibility criteria, in order to answer a specific research question. The process of preparing an SLR consists of multiple tasks that are labor-intensive and time-consuming, involving large monetary costs. Technology-assisted review (TAR) methods automate the different processes of creating an SLR and they are particularly focused on reducing the burden of screening for reviewers. We present a novel method for TAR that implements a full pipeline from the research protocol to the screening of the relevant papers. Our pipeline overcomes the need of a Boolean query constructed by specialists and consists of three different components: the primary retrieval engine, the inter-review ranker and the intra-review ranker, combining learning-to-rank techniques with a relevance feedback method. In addition, we contribute an updated version of the Task 2 of the CLEF 2019 eHealth Lab dataset, which we make publicly available. Empirical results on this dataset show that our approach can achieve state-of-the-art results.",1
DRUG-TARGET INTERACTION PREDICTION USING IMBALANCE AWARE MULTI-LABEL METHODS,2019,https://ikee.lib.auth.gr/record/305452/files/GRI-2019-24470.pdf,"Every year, the pharmaceutical industry faces the challenge of increasing the efficiency of developing new, effective and inexpensive drugs. Over the last few decades, machine learning methods took advantage of the large datasets available to big pharma and became a viable alternative for the development of new drugs. The aim of the present thesis was to experiment with different imbalance aware, multi-label methods on drug-target interaction datasets. The methods we implemented include the classic Binary Relevance approach, the Ensemble of Classifier Chains with Random Undersampling (ECCRU) and the Multi-Label Co-Training method. We experimented with two different datasets, one with a considerable size, comparable to what the pharmaceutical companies use in their day-to-day operations and a much smaller dataset that is widely used by other researchers in the area of drug-target interaction prediction (gold standard datasets). We also examined different problem settings and showed that some can be considered quite unrealistic and falsely boost the performance of methods that are used in the area of drug-target interaction prediction. i",1
Machine Learning Techniques for Vehicle Type Classification,2019,http://ikee.lib.auth.gr/record/306361/files/GRI-2019-25112.pdf,"The main topic of this work is the multi-class classification of vehicle category images in the field of automotive classifieds. This is a collaboration project between the Aristotle University of Thessaloniki and the Greek automotive classifieds company, Car. gr. The company provided us with a dataset of vehicle classifieds which is a subsidiary of their large scale data. Each classified in the dataset contains the vehicle’s images and a set of classified features regarding the vehicle characteristics. The goal is to create a machine learning model that will be able to predict the vehicle’s type by taking a vehicle image as input (eg classify a vehicle in an image as a Pickup/Truck or a Van).The first step of our work was content based information retrieval from the existing images to attain the feature vectors. More specifically, for the feature extraction process, we used an Imagenet pre-trained VGG16 neural network and we extracted the feature vectors from the network’s fully connected layer. We then synthesized the final dataset consisting of the feature vectors and the target vehicle category class for every classified. We performed principal component analysis on the features of the resulting dataset, in order to reduce their dimensionality, giving us the ability to visualize the data and accelerate the training process. We created 3 instances of our dataset. One with 2 classes, one with 3 and the last one with all twelve classes that are present in the dataset. We then trained a classifier with the transformed data and we evaluated our results with the stratified 10-fold cross validation method.",1
Subset Labeled LDA,2018,http://ikee.lib.auth.gr/record/304463,"Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard unsupervised Latent Dirichlet Allocation (LDA) algorithm, to address multi-label learning tasks. Previous work has shown it to perform en par with other state-of-the-art multi-label methods. Nonetheless, with increasing number of labels LLDA encounters scalability issues. In this work, we introduce Subset LLDA, a topic model that extends the standard LLDA algorithm, that not only can efficiently scale up to problems with hundreds of thousands of labels but also improves over the LLDA state-of-the-art in terms of prediction accuracy. We conduct experiments on eight data sets, with labels ranging from hundreds to hundreds of thousands, comparing our proposed algorithm with the other LLDA algorithms (Prior–LDA, Dep–LDA), as well as the state-of-the-art in extreme multi-label classification. The results show a steady advantage of our method …",0
Machine Learning Methods for Customer's Payment Acceptance Prediction in an Electricity Distribution Company,2017,https://dl.acm.org/doi/abs/10.1145/3139367.3139406,"In this short paper, we use machine learning methods to maximise the efficiency in the Hellenic Electricity Distribution Network Operator (HEDNO) SA's project management by predicting which upcoming projects are going to be paid by the customer, thus enabling the company to fetch all needed materials in time, and to avoid unwanted delays in the operations of the company.",1
Segmento: An R-based Visualization-rich System for Customer Segmentation and Targeting,2016,https://dl.acm.org/doi/abs/10.1145/2903220.2903245,"Customer segmentation is one of the most efficient and promising tools in a marketer's toolbox. In this paper, we introduce Segmento, an R-based customer segmentation system that uses clustering techniques to discover customer segments and offers tools to design and evaluate marketing campaigns. We present the features and the functionality of the system, as well as some of its unique, state-of-the-art visualizations.",1
Segmento,2016,http://ikee.lib.auth.gr/record/300843,"Customer segmentation is one of the most ecient and promis-ing tools in a marketer's toolbox. In this paper, we introduce Segmento, an R-based customer segmentation system that uses clustering techniques to discover customer segments and oers tools to design and evaluate marketing campaigns. We present the features and the functionality of the system, as well as some of its unique, state-of-the-art visualizations.",1
On Discovering Deterministic Relationships in Multi-Label Learning via Linked Open Data.,2015,https://intelligence.csd.auth.gr/wp-content/uploads/2019/03/papagiannopoulou-knowlod-15.pdf,"In multi-label learning, each instance can be related with one or more binary target variables. Multi-label learning problems are commonly found in many applications, eg in text classification where a news article is possible to be both on politics and finance. The main motivation of multi-label learning algorithms is the exploitation of label dependencies in order to improve prediction accuracy. In this paper, we present ongoing work on a method that uses the linked open data cloud to detect relationships between labels, enriches the set of labels with new concepts which are super classes of two or more labels, trains a model on the enhanced training set and finally, makes predictions on the enhanced test set in order to improve the prediction accuracy of the initial labels.",1
Large-Scale Semantic Indexing Large-Scale Semantic Indexing of Biomedical Publications at BioASQ,2013,http://ikee.lib.auth.gr/record/261421,"Automated annotation of scientic publications in real-world digital libraries requires dealing with challenges such as large number of concepts and training examples, multi-label training examples and hierarchical structure of concepts. BioASQ is a European project that contributes a large-scale biomedical publications corpus for working on these challenges. This paper documents the participation of our team to the large-scale biomedical semantic indexing task of BioASQ.",1
Evaluation Metrics for Feature Selection in Population Genomic Data,2011,http://lpis.csd.auth.gr/publications/Kavakiotis_SETN14.pdf,"Single Nucleotide Polymorphisms (SNPs) are considered nowadays one of the most important class of genetic markers with a wide range of applications with both scientific and economic interests. Although the advance of biotechnology has made feasible the production of genome wide SNP datasets, the cost of the production is still high. The transformation of the initial dataset into a smaller one with the same genetic information is a crucial task and it is performed through feature selection. Biologists evaluate features using methods originating from the field of population genetics. Although several studies have been performed in order to compare the existing biological methods, there is a lack of comparison between methods originating from the biology field with others originating from the machine learning. In this study we present some early results which support that biological methods perform slightly better than machine learning methods.",1
Obtaining Bipartitions from Score Vectors for Multi-Label Classification,2010,https://scholar.google.com/scholar?cluster=10046395636862074568&hl=en&oi=scholarr,"Multi-label classification is a popular learning task. However, some of the algorithms that learn from multi-label data, can only output a score for each label, so they cannot be readily used in applications that require bipartitions. In addition, several of the recent state-of-the-art multi-label classification algorithms, actually output a score vector primarily and employ one (sometimes simple) thresholding method in order to be able to output bipartitions. Furthermore, some approaches can naturally output both a score vector and a bipartition, but whether a better bipartition can be obtained through thresholding has not been investigated. This paper contributes a theoretical and empirical comparative study of existing thresholding methods, highlighting their importance for obtaining bipartitions of high quality.",1
ECAI'08 Workshop on Mining Social Data PREFACE,2010,https://scholar.google.com/scholar?cluster=11016876086889960901&hl=en&oi=scholarr,unknown,1
Triple random ensemble method for multi-label classification,2010,https://dro.deakin.edu.au/eserv/DU:30028664/nasiersing-triplerandom-post-2009.pdf,"This paper presents a triple-random ensemble learning method for multi-label classification problems, especially aimed at application to image to text translation and automatic image annotation. The proposed randomized learning method integrates the concepts of random subspace, bagging and random k-label sets ensemble learning methods to form an approach to classification of multi-label data. It applies the random subspace method to feature space, label space as well as instance space at the same time. The devised subset selection procedure is executed iteratively. Each multi-label classifier is trained using the randomly selected subsets. At the end of the iterations, the ensemble MLC classifiers are constructed. The proposed method is implemented and its performance is evaluated. The experimental results demonstrate that the proposed method outperforms the examined counterparts in most occasions when tested on six multi-label datasets from different domains. It is shown that the developed method possesses a general usability in dealing with various multi-label classification problems. Therefore, the triple random ensemble learning method is recommended for application to image to text translation system, which is based on the positive outcome of predictive performance of TREMLC on scene image dataset.",1
E-Mail. Mining: Emerging. Techniques. for.,2007,https://books.google.com/books?hl=en&lr=&id=1sneHjL-yGQC&oi=fnd&pg=PA219&dq=info:GkcDrECzQx4J:scholar.google.com&ots=tNRw1LZ1PW&sig=ueHG6i_2sl1jFqN0gqHGx8QLc4c,"E-mail has met tremendous popularity over the past few years. People are sending and receiving many messages per day, communicating with partners and friends or exchanging files and information. Unfortunately, the phenomenon of e-mail overload has grown over the past years, becoming a personal headache for users and a financial issue for companies. In this chapter, we will discuss how disciplines like machine learning and data mining can contribute to the solution of the problem by constructing intelligent techniques that automate e-mail managing tasks and what advantages they hold over other conventional solutions. We will also discuss the particularity of e-mail data and what special treatment they require. Some interesting e-mail mining applications like mail categorization, summarization, automatic answering, and spam filtering will also be presented.",1
E-Mail Mining: Emerging Techniques for,2007,https://scholar.google.com/scholar?cluster=6554413638999585279&hl=en&oi=scholarr,"E-mail has met tremendous popularity over the past few years. People are sending and receiving many messages per day, communicating with partners and friends or exchanging files and information. Unfortunately, the phenomenon of e-mail overload has grown over the past years, becoming a personal headache for users and a financial issue for companies. In this chapter, we will discuss how disciplines like machine learning and data mining can contribute to the solution of the problem by constructing intelligent techniques that automate e-mail managing tasks and what advantages they hold over other conventional solutions. We will also discuss the particularity of e-mail data and what special treatment they require. Some interesting e-mail mining applications like mail categorization, summarization, auto-matic answering, and spam filtering will also be presented.",1
for Adaptive Multicriteria Planning,2004,https://scholar.google.com/scholar?cluster=11825988580296414117&hl=en&oi=scholarr,"This paper concerns the design and development of an adap-tive planner that is able to adjust its parameters to the characteristics of a given problem and to the priorities set by the user concerning plan length and planning time. This is accomplished through the implementation of the k nearest neighbor machine learning algorithm on top of a highly adjustable planner, called HAP. Learning data are produced by running HAP offline on several problems from multiple domains using all value combinations of its parameters. When the adaptive planner HAPnn is faced with a new problem, it locates the k nearest problems, using a set of measurable problem characteristics, retrieves the performance data for all parameter configurations on these problems and performs a mul-ticriteria combination, with user-specified weights for plan length and planning time. Based on this combination, the configuration with the best performance is then used in order to solve the new problem. Comparative experiments with the statistically best static configurations of the planner show that HAPnn manages to adapt successfully to unseen problems, leading to an increased planning performance.",1
Classification of Concept Drifting Data Streams–Bibliography,1996,https://www.academia.edu/download/42207527/conceptdrift_bibliography.pdf,"Classification of Concept Drifting Data Streams – Bibliography Page 1 Classification of Concept 
Drifting Data Streams – Bibliography Ioannis Katakis, Grigorios Tsoumakas and Ioannis 
Vlahavas Department of Informatics, Aristotle University of Thessaloniki 54124 Thessaloniki, 
Greece {katak, greg, vlahavas}@csd.auth.gr Last Update: May 5, 2008 [1] Domingos, P. and 
Hulten, G., Mining high-speed data streams. Knowledge discovery and data mining, 2000: p. 
71-80. [2] Fan, W. Systematic data selection to mine concept-drifting data streams. in Tenth 
ACM SIGKDD international conference on Knowledge Discovery and Data Mining. 2004. 
Seattle, WA, USA: ACM Press: p. 128-137. [3] Forman, G. Tackling Concept Drift by Temporal 
Inductive Transfer. in 29th International ACM SIGIR Conference on Research and Development 
in Information Retrieval. 2006. Washington, USA: ACM Press: p. 252-259. [4] Hulten, G., , L., -…",0
Short-Term Renewable Energy Forecasting in Greece Using Prophet Decomposition and Tree-Based Ensembles,unknown,https://books.google.com/books?hl=en&lr=&id=iuFDEAAAQBAJ&oi=fnd&pg=PA226&dq=info:WmsB11LI6h0J:scholar.google.com&ots=EACRFehN1-&sig=ocVw1iKfzS3MoKpu15vMtTD3qQI,"Energy production using renewable sources exhibits inherent uncertainties due to their intermittent nature. Nevertheless, the unified European energy market promotes the increasing penetration of renewable energy sources (RES) by the regional energy system operators. Consequently, RES forecasting can assist in the integration of these volatile energy sources, since it leads to higher reliability and reduced ancillary operational costs for power systems. This paper presents a new dataset for solar and wind energy generation forecast in Greece and introduces a feature engineering pipeline that enriches the dimensional space of the dataset. In addition, we propose a novel method that utilizes the innovative Prophet model, an end-to-end forecasting tool that considers several kinds of nonlinear trends in decomposing the energy time series before a tree-based ensemble provides short-term predictions. The performance of the system is measured through representative evaluation metrics, and by estimating the model’s generalization under an industryprovided scheme of absolute error thresholds. The proposed hybrid model competes with baseline persistence models, tree-based regression ensembles, and the Prophet model, managing to outperform them, presenting both lower error rates and more favorable error distribution.",1
Multi-label Learning Approaches for Music Instrument Recognition,unknown,https://www.academia.edu/download/39820875/Multi-label_Learning_Approaches_for_Musi20151109-27463-n8x2dh.pdf,"This paper presents the two winning approaches that we developed for the instrument recognition track of the ISMIS 2011 contest on Music Information. The solution that ranked first was based on the Binary Relevance approach and built a separate model for each instrument on a selected subset of the available training data. Moreover, a new ranking approach was utilized to produce an ordering of the instruments according to their degree of relevance to a given track. The solution that ranked second was based on the idea of constraining the number of pairs that were being predicted. It applied a transformation to the original dataset and utilized a variety of post-processing filters based on domain knowledge and exploratory analysis of the evaluation set. Both solutions were developed using the Mulan open-source software for multi-label learning.",1
An Empirical Study of Multi-label Learning Methods for Video,unknown,https://www.academia.edu/download/35531201/tsoumakas-cbmi09.pdf,This paper presents an experimental comparison of different approaches to learning from multi-labeled video data. We compare state-of-the-art multi-label learning methods on the Mediamill Challenge dataset. We employ MPEG-7 and SIFT-based global image descriptors independently and in conjunction using variations of the stacking approach for their fusion. We evaluate the results comparing the different classifiers using both MPEG-7 and SIFT-based descriptors and their fusion. A variety of multi-label evaluation measures is used to explore advantages and disadvantages of the examined classifiers. Results give rise to interesting conclusions.,1
PASER: a Curricula Synthesis System Based on Automated Problem Solving,unknown,https://www.academia.edu/download/1911371/4gfv2z80sgwyj7e.pdf,"This paper presents PASER, a system for automatically synthesizing curricula using AI Planning and Machine Learning techniques on an ontology of educational resources metadata. The ontology is a part–of hierarchy of learning themes which correspond to RDCEO competencies. The system uses an automated planner, which given the initial state of the problem (learner’s profile, preferences, needs and abilities), the available actions (study an educational resource, take an exam, join an e-learning course, etc.) and the goals (obtain a certificate, learn a subject, acquire a skill, etc.) constructs a complete educational curriculum that achieves the goals. PASER is accompanied by a Machine Learning module that classifies textually described users’ learning requests to competencies registered within the ontology. Furthermore, the ML module interactively assists content providers in constructing educational resources metadata (LOM records) that comply with the ontology concerning both learning objectives and prerequisites.",1
INTERNATIONAL JOURNAL OF DATA WAREHOUSING AND MINING,unknown,https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1001&context=computer_eng_pub,"The Web is a continuously evolving environment, since its content is updated on a regular basis. As a result, the traditional usage-based approach to generate recommendations that takes as input the navigation paths recorded on the Web page level, is not as effective. Moreover, most of the content available online is either explicitly or implicitly characterized by a set of categories organized in a taxonomy, allowing the page-level navigation patterns to be generalized to a higher, aggregate level. In this direction, the authors present the Frequent Generalized Pattern (FGP) algorithm. FGP takes as input the transaction data and a hierarchy of categories and produces generalized association rules that contain transaction items and/or item categories. The results can be used to generate association rules and subsequently recommendations for the users. The algorithm can be applied to the log files of a typical Web site …",0
AUTH-Atypon at BioASQ 3; Large-Scale Semantic Indexing and Question Answering in Bio-medicine,unknown,https://www.academia.edu/download/55539830/auth-atypon-bioasq.pdf,"In this paper we present the methods and the approaches employed in terms of our participation to the BioASQ Challenge 2015. The challenge comprises of two separate tasks; semantic annotation of scientific abstracts (task 3a) and question answering of biomedical natural language questions (task 3b). Based on the successful approaches of the previous years, with respect to task 3a, we considered a variety of ensembles, incorporated journal-specific semantic information and developed an approach to handle the concept drift within the BioASQ corpus. The official task 3a results demonstrate a consistent advantage of our approaches against the baselines. Specifically, the systems proposed by our team ranked among the top tier ones along the competition, obtaining the second place in 10 out of 15 weeks. Concerning task 3b, we further extended our previous work...",1
MIREX AUDIO TAG CLASSIFICATION,unknown,https://www.music-ir.org/mirex/abstracts/2008/auth.pdf,"This extended abstract details a submission to the Music Information Retrieval Evaluation eXchange in the audio tag classification task, a new task introduced this year. We model the problem as a multilabel classification task and employ suitable learning algorithms from the Mulan toolkit 1.",1
